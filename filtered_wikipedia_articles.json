[
    {
        "title": "Artificial intelligence",
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "content": "\n Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.[1] Such machines may be called AIs.\n High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"[2][3]\n Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics.[a] General intelligence—the ability to complete any task performed by a human on an at least equal level—is among the field's long-term goals.[4] To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics.[b] AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.[5]\n Artificial intelligence was founded as an academic discipline in 1956,[6] and the field went through multiple cycles of optimism throughout its history,[7][8] followed by periods of disappointment and loss of funding, known as AI winters.[9][10] Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques.[11] This growth accelerated further after 2017 with the transformer architecture,[12] and by the early 2020s many billions of dollars were being invested in AI and the field experienced rapid ongoing progress in what has become known as the AI boom. The emergence of advanced generative AI in the midst of the AI boom and its ability to create and modify content exposed several unintended consequences and harms in the present and raised concerns about the risks of AI and its long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\n The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.[a]\n Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.[13] By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.[14]\n Many of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow.[15] Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.[16] Accurate and efficient reasoning is an unsolved problem.\n Knowledge representation and knowledge engineering[17] allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval,[18] scene interpretation,[19] clinical decision support,[20] knowledge discovery (mining \"interesting\" and actionable inferences from large databases),[21] and other areas.[22]\n A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge.[23] Knowledge bases need to represent things such as objects, properties, categories, and relations between objects;[24] situations, events, states, and time;[25] causes and effects;[26] knowledge about knowledge (what we know about what other people know);[27] default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing);[28] and many other aspects and domains of knowledge.\n Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous);[29] and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally).[16] There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.[c]\n An \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen.[d][32] In automated planning, the agent has a specific goal.[33] In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.[34]\n In classical planning, the agent knows exactly what the effect of any action will be.[35] In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.[36]\n In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences.[37] Information value theory can be used to weigh the value of exploratory or experimental actions.[38] The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.\n A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.[39]\n Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.[40]\n Machine learning is the study of programs that can improve their performance on a given task automatically.[41] It has been a part of AI from the beginning.[e]\n There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.[44] Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).[45]\n In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\".[46] Transfer learning is when the knowledge gained from one problem is applied to a new problem.[47] Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.[48]\n Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.[49]\n Natural language processing (NLP)[50] allows programs to read, write and communicate in human languages such as English. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.[51]\n Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation[f] unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem[29]). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.\n Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning),[52] transformers (a deep learning architecture using an attention mechanism),[53] and others.[54] In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text,[55][56] and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.[57]\n Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.[58]\n The field includes speech recognition,[59] image classification,[60] facial recognition, object recognition,[61]object tracking,[62] and robotic perception.[63]\n Affective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood.[65] For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.\n However, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents.[66] Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject.[67]\n A machine with artificial general intelligence should be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.[4]\n AI research uses a wide variety of techniques to accomplish the goals above.[b]\n AI can solve many problems by intelligently searching through many possible solutions.[68] There are two very different kinds of search used in AI: state space search and local search.\n State space search searches through a tree of possible states to try to find a goal state.[69] For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.[70]\n Simple exhaustive searches[71] are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes.[15] \"Heuristics\" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal.[72]\n Adversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position.[73]\n Local search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.[74]\n Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks,[75] through the backpropagation algorithm.\n Another type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation.[76]\n Distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).[77]\n Formal logic is used for reasoning and knowledge representation.[78]\nFormal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\")[79] and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\").[80]\n Deductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises).[81] Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.\n Given a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem.[82] In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.[83]\n Inference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.[84]\n Fuzzy logic assigns a \"degree of truth\" between 0 and 1. It can therefore handle propositions that are vague and partially true.[85]\n Non-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning.[28] Other specialized versions of logic have been developed to describe many complex domains.\n Many problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.[86] Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,[87] and information value theory.[88] These tools include models such as Markov decision processes,[89] dynamic decision networks,[90] game theory and mechanism design.[91]\n Bayesian networks[92] are a tool that can be used for reasoning (using the Bayesian inference algorithm),[g][94] learning (using the expectation–maximization algorithm),[h][96] planning (using decision networks)[97] and perception (using dynamic Bayesian networks).[90]\n Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).[90]\n The simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers[98] are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.[45]\n There are many kinds of classifiers in use.[99] The decision tree is the simplest and most widely used symbolic machine learning algorithm.[100] K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.[101]\nThe naive Bayes classifier is reportedly the \"most widely used learner\"[102] at Google, due in part to its scalability.[103]\nNeural networks are also used as classifiers.[104]\n An artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.[104]\n Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm.[105] Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.[106]\n In feedforward neural networks the signal passes in only one direction.[107] Recurrent neural networks feed the output signal back into the input, which allows short-term memories of previous input events. Long short term memory is the most successful network architecture for recurrent networks.[108] Perceptrons[109] use only a single layer of neurons; deep learning[110] uses multiple layers. Convolutional neural networks strengthen the connection between neurons that are \"close\" to each other—this is especially important in image processing, where a local set of neurons must identify an \"edge\" before the network can identify an object.[111]\n Deep learning[110] uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.[112]\n Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification,[113] and others. The reason that deep learning performs so well in so many applications is not known as of 2023.[114] The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s)[i] but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.[j]\n Generative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pretrained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\", although this can be reduced with RLHF and quality data. They are used in chatbots, which allow people to ask a question or request a task in simple text.[122][123]\n Current models and services include Gemini (formerly Bard), ChatGPT, Grok, Claude, Copilot, and LLaMA.[124] Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.[125]\n In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.[126] Specialized programming languages such as Prolog were used in early AI research,[127] but general-purpose programming languages like Python have become predominant.[128]\n The transistor density in integrated circuits has been observed to roughly double every 18 months—a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster,[129] a trend sometimes called Huang's law,[130] named after Nvidia co-founder and CEO Jensen Huang.\n AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's iPhoto and TikTok). The deployment of AI may be overseen by a Chief automation officer (CAO).\n The application of AI in medicine and medical research has the potential to increase patient care and quality of life.[131] Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.[132][133]\n For medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication.[134] It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research.[134] New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.[135] In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria.[136] In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.[137][138]\n Applications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer prediction,[139] AI-integrated sex toys (e.g., teledildonics),[140] AI-generated sexual education content,[141] and AI agents that simulate sexual and romantic partners (e.g., Replika).[142]  AI is also used for the production of non-consensual deepfake pornography, raising significant ethical and legal concerns.[143]\n AI technologies have also been used to attempt to identify online gender-based violence and online sexual grooming of minors.[144][145]\n Game playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques.[146] Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997.[147] In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.[148] In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world.[149] Other programs handle imperfect-information games, such as the poker-playing program Pluribus.[150] DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games.[151] In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map.[152] In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning.[153] In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.[154]\n In mathematics, special forms of formal step-by-step reasoning are used.[155] In contrast, LLMs such as GPT-4 Turbo, Gemini Ultra, Claude Opus, LLaMa-2 or Mistral Large are working with probabilistic models, which can produce wrong answers in the form of hallucinations. Therefore, they need not only a large database of mathematical problems to learn from but also methods such as supervised fine-tuning or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections.[156] A 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data.[157]\n Alternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as Alpha Tensor, Alpha Geometry and Alpha Proof all from Google DeepMind,[158] Llemma from eleuther[159] or Julius.[160]\n When natural language is used to describe mathematical problems, converters transform such prompts into a formal language such as Lean to define mathematical tasks.\n Some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.[161]\n Finance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \"robot advisers\" have been in use for some years.[162]\n World Pensions experts like Nicolas Firzli insist it may be too early to see the emergence of highly innovative AI-informed financial products and services: \"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\"[163]\n Various countries are deploying AI military applications.[164] The main applications enhance command and control, communications, sensors, integration and interoperability.[165] Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles.[164] AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams.[165]\n AI has been used in military operations in Iraq, Syria, Israel and Ukraine.[164][166][167][168]\n In the early 2020s, generative AI gained widespread prominence. GenAI is AI capable of generating text, images, videos, or other data using generative models,[169][170] often in response to prompts.[171][172]\n In March 2023, 58% of U.S. adults had heard about ChatGPT and 14% had tried it.[173] The increasing realism and ease-of-use of AI-based text-to-image generators such as Midjourney, DALL-E, and Stable Diffusion sparked a trend of viral AI-generated photos. Widespread attention was gained by a fake photo of Pope Francis wearing a white puffer coat, the fictional arrest of Donald Trump, and a hoax of an attack on the Pentagon, as well as the usage in professional creative arts.[174][175]\n Artificial intelligent (AI) agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.[176][177][178]\n There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes.[179] A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.\n AI applications for evacuation and disaster management are growing. AI has been used to investigate if and how people evacuated in large scale and small scale evacuations using historical data from GPS, videos or social media. Further, AI can provide real time information on the real time evacuation conditions.[180][181][182]\n In agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.\n Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights.\" For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.\n During the 2024 Indian elections, US$50 millions was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages.[183]\n AI has potential benefits and potential risks.[184] AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to \"solve intelligence, and then use that to solve everything else\".[185] However, as the use of AI has become widespread, several unintended consequences and risks have been identified.[186] In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.[187]\n Machine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.\n AI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency.\n Sensitive user data collected may include online activity records, geolocation data, video, or audio.[188] For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them.[189] Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.[190]\n AI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy.[191] Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\"[192]\n Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \"fair use\". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\".[193][194] Website owners who do not wish to have their content scraped can indicate it in a \"robots.txt\" file.[195] In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI.[196][197] Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.[198]\n The commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft.[199][200][201] Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.[202][203]\n In January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use.[204] This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.[205]\n Prodigious power consumption by AI is responsible for the growth of fossil fuels use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources – from nuclear energy to geothermal to fusion. The tech firms argue that – in the long view – AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and \"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.[206]\n A 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means.[207] Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.[208]\n In 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for $650 Million (US).[209] Nvidia CEO Jen-Hsun Huang said nuclear power is a good option for the data centers.[210]\n In September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power – enough for 800,000 homes – of energy will be produced. The cost for re-opening and upgrading is estimated at $1.6 billion (US) and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act.[211] The US government and the state of Michigan are investing almost $2 billion (US) to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon spinoff of Constellation.[212]\n After the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages.[213] Taiwan aims to phase out nuclear power by 2025.[213] On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban.[213]\n Although most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near nuclear power plant for a new data center for generative AI.[214] Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI.[214]\n On 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center.[215] \nAccording to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors.[215]\n YouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation.[216] This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government.[217] The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took steps to mitigate the problem [citation needed].\n In 2022, generative AI began to create images, audio, video and text that are indistinguishable from real photographs, recordings, films, or human writing. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda.[218] AI pioneer Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks.[219]\n Machine learning applications will be biased[k] if they learn from biased data.[221] The developers may not be aware that the bias exists.[222] Bias can be introduced by the way training data is selected and by the way a model is deployed.[223][221] If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination.[224] The field of fairness studies how to prevent harms from algorithmic biases.\n On June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people,[225] a problem called \"sample size disparity\".[226] Google \"fixed\" this problem by preventing the system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.[227]\n COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different—the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend.[228] In 2017, several researchers[l] showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.[230]\n A program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\".[231] Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn't work.\"[232]\n Criticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these \"recommendations\" will likely be racist.[233] Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.[m]\n Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.[226]\n There are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws.[220]\n At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.[dubious – discuss][235]\n Many AI systems are so complex that their designers cannot explain how they reach their decisions.[236] Particularly with deep neural networks, in which there are a large amount of non-linear relationships between inputs and outputs. But some popular explainability techniques exist.[237]\n It is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale.[238] Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.[239]\n People who have been harmed by an algorithm's decision have a right to an explanation.[240] Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists.[n] Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.[241]\n DARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try to solve these problems.[242]\n Several approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output.[243] LIME can locally approximate a model's outputs with a simpler, interpretable model.[244] Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned.[245] Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning.[246] For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts.[247]\n Artificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.\n A lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision.[o] Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction.[249] Even when used in conventional warfare, they currently cannot reliably choose targets and could potentially kill an innocent person.[249] In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed.[250] By 2015, over fifty countries were reported to be researching battlefield robots.[251]\n AI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware.[252] All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China.[253][254]\n There many other ways that AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.[255]\n Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.[256]\n In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI.[257] A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed.[258] Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\".[p][260] The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies.[256] In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.[261][262]\n Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\".[263] Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.[264]\n From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.[265]\n It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \"spell the end of the human race\".[266] This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character.[q] These sci-fi scenarios are misleading in several ways.\n First, AI does not require human-like sentience to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip factory manager).[268] Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can't fetch the coffee if you're dead.\"[269] In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\".[270]\n Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.[271]\n The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI.[272] Personalities such as Stephen Hawking, Bill Gates, and Elon Musk,[273] as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI.\n In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google.\"[274] He notably mentioned risks of an AI takeover,[275] and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.[276]\n In 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".[277]\n Some other researchers were more optimistic. AI pioneer Jürgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\"[278] While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\"[279][280] Andrew Ng also argued that \"it's a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests.\"[281] Yann LeCun \"scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\"[282] In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine.[283] However, after 2016, the study of current and future risks and possible solutions became a serious area of research.[284]\n Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.[285]\n Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.[286]\nThe field of machine ethics is also called computational morality,[286]\nand was founded at an AAAI symposium in 2005.[287]\n Other approaches include Wendell Wallach's \"artificial moral agents\"[288] and Stuart J. Russell's three principles for developing provably beneficial machines.[289]\n Active organizations in the AI open-source community include Hugging Face,[290] Google,[291] EleutherAI and Meta.[292] Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight,[293][294] meaning that their architecture and trained parameters (the \"weights\") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case.[295] Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.[296]\n Artificial Intelligence projects can have their ethical permissibility tested while designing, developing, and implementing an AI system. An AI framework such as the Care and Act Framework containing the SUM values—developed by the Alan Turing Institute tests projects in four main areas:[297][298]\n Other developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others;[299] however, these principles do not go without their criticisms, especially regards to the people chosen contributes to these frameworks.[300]\n Promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.[301]\n The UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under a MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.[302]\n The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms.[303] The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally.[304] According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.[305][306] Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.[307] Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.[307] The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.[307] Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI.[308] In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.[309] In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, governments officials and academics.[310] In 2024, the Council of Europe created the first international legally binding treaty on AI, called the \"Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law\". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.[311]\n In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\".[305] A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity.[312] In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".[313][314]\n In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks.[315] 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence.[316][317] In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.[318][319]\n The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning.[320][321] This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \"electronic brain\".[r] They developed several areas of research that would become part of AI,[323] such as McCullouch and Pitts design for \"artificial neurons\" in 1943,[115] and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that \"machine intelligence\" was plausible.[324][321]\n The field of AI research was founded at a workshop at Dartmouth College in 1956.[s][6] The attendees became the leaders of AI research in the 1960s.[t] They and their students produced programs that the press described as \"astonishing\":[u] computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.[v][7] Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.[321]\n Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field.[328] In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\".[329] In 1967 Marvin Minsky agreed, writing that \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\".[330] They had, however, underestimated the difficulty of the problem.[w] In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill[332] and ongoing pressure from the U.S. Congress to fund more productive projects.[333] Minsky's and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether.[334] The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.[9]\n In the early 1980s, AI research was revived by the commercial success of expert systems,[335] a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research.[8] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.[10]\n Up to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition,[336] and began to look into \"sub-symbolic\" approaches.[337] Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive.[x] Judea Pearl, Lofti Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic.[86][342] But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others.[343] In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.[344]\n AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics).[345] By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as the AI effect).[346]\nHowever, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.[4]\n Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.[11]\nFor many specific tasks, other methods were abandoned.[y]\nDeep learning's success was based on both hardware improvements (faster computers,[348] graphics processing units, cloud computing[349]) and access to large amounts of data[350] (including curated datasets,[349] such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI.[z] The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019.[307]\n In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.[284]\n In the late teens and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text.[351] ChatGPT, launched on November 30, 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months.[352] It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness.[353] These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about $50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\".[354] About 800,000 \"AI\"-related U.S. job openings existed in 2022.[355] According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies.[356]\n Philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines.[357] Another major focus has been whether machines can be conscious, and the associated ethical implications.[358] Many other topics in philosophy are relevant to AI, such as epistemology and free will.[359] Rapid advancements have intensified public discussions on the philosophy and ethics of AI.[358]\n Alan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\"[360] He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\".[360] He devised the Turing test, which measures the ability of a machine to simulate human conversation.[324] Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks.\"[361]\n Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure.[1] However, they are critical that the test requires the machine to imitate humans. \"Aeronautical engineering texts\", they wrote, \"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'\"[363] AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".[364]\n McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\".[365] Another AI founder, Marvin Minsky, similarly describes it as \"the ability to solve hard problems\".[366] The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals.[1] These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine—and no other philosophical discussion is required, or may not even be possible.\n Another definition has been adopted by Google,[367] a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.\n Some authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI,[368] with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\".[369]\n No established unifying theory or paradigm has guided AI research for most of its history.[aa] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.\n Symbolic AI (or \"GOFAI\")[371] simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"[372]\n However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult.[373] Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge.[374] Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.[ab][16]\n The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence,[376][377] in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.\n \"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s,[378] but eventually was seen as irrelevant. Modern AI has elements of both.\n Finding a provably correct or optimal solution is intractable for many important problems.[15] Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.\n AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals.[379][380] General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively.\n The philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\"[381] However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.\n David Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness.[382] The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.[383]\n Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.[384]\n Philosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"[ac] Searle challenges this claim with his Chinese room argument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind.[388]\n It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree.[389] But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals.[390][391] Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights.[390] Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.[392]\n In 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities.[393] Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part to society on their own.[394][395]\n Progress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.[391][390]\n A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.[380] If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an \"intelligence explosion\" and Vernor Vinge called a \"singularity\".[396]\n However, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.[397]\n Robot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger.[398]\n Edward Fredkin argues that \"artificial intelligence is the next step in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.[399]\n Thought-capable artificial beings have appeared as storytelling devices since antiquity,[400] and have been a persistent theme in science fiction.[401]\n A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.[402]\n Isaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \"Multivac\" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics;[403] while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.[404]\n Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.[405]\n The two most widely used textbooks in 2023 (see the Open Syllabus):\n The four most widely used AI textbooks in 2008:\n Other textbooks:\n"
    },
    {
        "title": "Ai",
        "url": "https://en.wikipedia.org/wiki/AI_(disambiguation)",
        "content": "\n AI most frequently refers to artificial intelligence, which is intelligence demonstrated by machines.\n Ai, AI or A.I. may also refer to:\n"
    },
    {
        "title": "Artificial intelligence (disambiguation)",
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_(disambiguation)",
        "content": "Artificial intelligence is the intelligence exhibited by machines and software.\n Artificial intelligence may also refer to:\n"
    },
    {
        "title": "Artificial general intelligence",
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "content": "\n Artificial general intelligence (AGI) is a type of artificial intelligence (AI) that matches or surpasses human cognitive capabilities across a wide range of cognitive tasks. This contrasts with narrow AI, which is limited to specific tasks.[1] Artificial superintelligence (ASI), on the other hand, refers to AGI that greatly exceeds human cognitive capabilities. AGI is considered one of the definitions of strong AI.\n Creating AGI is a primary goal of AI research and of companies such as OpenAI[2] and Meta.[3] A 2020 survey identified 72 active AGI research and development projects across 37 countries.[4]\n The timeline for achieving AGI remains a subject of ongoing debate among researchers and experts. As of 2023, some argue that it may be possible in years or decades; others maintain it might take a century or longer; a minority believe it may never be achieved; and another minority claims that it is already here.[5][6] Notable AI researcher Geoffrey Hinton has expressed concerns about the rapid progress towards AGI, suggesting it could be achieved sooner than many expect.[7]\n There is debate on the exact definition of AGI and regarding whether modern large language models (LLMs) such as GPT-4 are early forms of AGI.[8] AGI is a common topic in science fiction and futures studies.[9][10]\n Contention exists over whether AGI represents an existential risk.[11][12][13] Many experts on AI have stated that mitigating the risk of human extinction posed by AGI should be a global priority.[14][15] Others find the development of AGI to be too remote to present such a risk.[16][17]\n AGI is also known as strong AI,[18][19] full AI,[20] human-level AI,[5] human-level intelligent AI, or general intelligent action.[21]\n Some academic sources reserve the term \"strong AI\" for computer programs that experience sentience or consciousness.[a] In contrast, weak AI (or narrow AI) is able to solve one specific problem but lacks general cognitive abilities.[22][19] Some academic sources use \"weak AI\" to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans.[a]\n Related concepts include artificial superintelligence and transformative AI. An artificial superintelligence (ASI) is a hypothetical type of AGI that is much more generally intelligent than humans,[23] while the notion of transformative AI relates to AI having a large impact on society, for example, similar to the agricultural or industrial revolution.[24]\n A framework for classifying AGI in levels was proposed in 2023 by Google DeepMind researchers. They define five levels of AGI: emerging, competent, expert, virtuoso, and superhuman. For example, a competent AGI is defined as an AI that outperforms 50% of skilled adults in a wide range of non-physical tasks, and a superhuman AGI (i.e. an artificial superintelligence) is similarly defined but with a threshold of 100%. They consider large language models like ChatGPT or LLaMA 2 to be instances of emerging AGI.[25]\n Various popular definitions of intelligence have been proposed. One of the leading proposals is the Turing test. However, there are other well-known definitions, and some researchers disagree with the more popular approaches. [b]\n Researchers generally hold that intelligence is required to do all of the following:[27]\n Many interdisciplinary approaches (e.g. cognitive science, computational intelligence, and decision making) consider additional traits such as imagination (the ability to form novel mental images and concepts)[28] and autonomy.[29]\n Computer-based systems that exhibit many of these capabilities exist (e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent). There is debate about whether modern AI systems possess them to an adequate degree.\n Other capabilities are considered desirable in intelligent systems, as they may affect intelligence or aid in its expression. These include:[30]\n This includes the ability to detect and respond to hazard.[31]\n Although the ability to sense (e.g. see, hear, etc.) and the ability to act (e.g. move and manipulate objects, change location to explore, etc.) can be desirable for some intelligent systems,[30] these physical capabilities are not strictly required for an entity to qualify as AGI—particularly under the thesis that large language models (LLMs) may already be or become AGI. Even from a less optimistic perspective on LLMs, there is no firm requirement for an AGI to have a human-like form; being a silicon-based computational system is sufficient, provided it can process input (language) from the external world in place of human senses. This interpretation aligns with the understanding that AGI has never been proscribed a particular physical embodiment and thus does not demand a capacity for locomotion or traditional \"eyes and ears\".[32]\n Several tests meant to confirm human-level AGI have been considered, including:[33][34]\n The idea of the test is that the machine has to try and pretend to be a man, by answering questions put to it, and it will only pass if the pretence is reasonably convincing. A considerable portion of a jury, who should not be expert about machines, must be taken in by the pretence.[37] A problem is informally called \"AI-complete\" or \"AI-hard\" if it is believed that in order to solve it, one would need to implement AGI, because the solution is beyond the capabilities of a purpose-specific algorithm.[47]\n There are many problems that have been conjectured to require general intelligence to solve as well as humans. Examples include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem.[48] Even a specific task like translation requires a machine to read and write in both languages, follow the author's argument (reason), understand the context (knowledge), and faithfully reproduce the author's original intent (social intelligence). All of these problems need to be solved simultaneously in order to reach human-level machine performance.\n However, many of these tasks can now be performed by modern large language models. According to Stanford University's 2024 AI index, AI has reached human-level performance on many benchmarks for reading comprehension and visual reasoning.[49]\n Modern AI research began in the mid-1950s.[50] The first generation of AI researchers were convinced that artificial general intelligence was possible and that it would exist in just a few decades.[51] AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do.\"[52]\n Their predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's character HAL 9000, who embodied what AI researchers believed they could create by the year 2001. AI pioneer Marvin Minsky was a consultant[53] on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time. He said in 1967, \"Within a generation... the problem of creating 'artificial intelligence' will substantially be solved\".[54]\n Several classical AI projects, such as Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project, were directed at AGI.\n However, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. Funding agencies became skeptical of AGI and put researchers under increasing pressure to produce useful \"applied AI\".[c] In the early 1980s, Japan's Fifth Generation Computer Project revived interest in AGI, setting out a ten-year timeline that included AGI goals like \"carry on a casual conversation\".[58] In response to this and the success of expert systems, both industry and government pumped money into the field.[56][59] However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled.[60] For the second time in 20 years, AI researchers who predicted the imminent achievement of AGI had been mistaken. By the 1990s, AI researchers had a reputation for making vain promises. They became reluctant to make predictions at all[d] and avoided mention of \"human level\" artificial intelligence for fear of being labeled \"wild-eyed dreamer[s]\".[62]\n In the 1990s and early 21st century, mainstream AI achieved commercial success and academic respectability by focusing on specific sub-problems where AI can produce verifiable results and commercial applications, such as speech recognition and recommendation algorithms.[63] These \"applied AI\" systems are now used extensively throughout the technology industry, and research in this vein is heavily funded in both academia and industry. As of 2018[update], development in this field was considered an emerging trend, and a mature stage was expected to be reached in more than 10 years.[64]\n \nAt the turn of the century, many mainstream AI researchers[65] hoped that strong AI could be developed by combining programs that solve various sub-problems. Hans Moravec wrote in 1988:  I am confident that this bottom-up route to artificial intelligence will one day meet the traditional top-down route more than half way, ready to provide the real-world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs. Fully intelligent machines will result when the metaphorical golden spike is driven uniting the two efforts.[65] \nHowever, even at the time, this was disputed. For example, Stevan Harnad of Princeton University concluded his 1990 paper on the symbol grounding hypothesis by stating:  The expectation has often been voiced that \"top-down\" (symbolic) approaches to modeling cognition will somehow meet \"bottom-up\" (sensory) approaches somewhere in between. If the grounding considerations in this paper are valid, then this expectation is hopelessly modular and there is really only one viable route from sense to symbols: from the ground up. A free-floating symbolic level like the software level of a computer will never be reached by this route (or vice versa) – nor is it clear why we should even try to reach such a level, since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings (thereby merely reducing ourselves to the functional equivalent of a programmable computer).[66] The term \"artificial general intelligence\" was used as early as 1997, by Mark Gubrud[67] in a discussion of the implications of fully automated military production and operations. A mathematical formalism of AGI was proposed by Marcus Hutter in 2000. Named AIXI, the proposed AGI agent maximises \"the ability to satisfy goals in a wide range of environments\".[68] This type of AGI, characterized by the ability to maximise a mathematical definition of intelligence rather than exhibit human-like behaviour,[69] was also called universal artificial intelligence.[70]\n The term AGI was re-introduced and popularized by Shane Legg and Ben Goertzel around 2002.[71] AGI research activity in 2006 was described by Pei Wang and Ben Goertzel[72] as \"producing publications and preliminary results\". The first summer school in AGI was organized in Xiamen, China in 2009[73] by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010[74] and 2011[75] at Plovdiv University, Bulgaria by Todor Arnaudov. MIT presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers.\n As of 2023[update], a small number of computer scientists are active in AGI research, and many contribute to a series of AGI conferences. However, increasingly more researchers are interested in open-ended learning,[76][77] which is the idea of allowing AI to continuously learn and innovate like humans do.\n As of 2023, the development and potential achievement of AGI remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist.[78] AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\".[79] Writing in The Guardian, roboticist Alan Winfield claimed the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight.[80]\n A further challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions?[81]\n Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI.[82][83] John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted.[84] AI experts' views on the feasibility of AGI wax and wane. Four polls conducted in 2012 and 2013 suggested that the median estimate among experts for when they would be 50% confident AGI would arrive was 2040 to 2050, depending on the poll, with the mean being 2081. Of the experts, 16.5% answered with \"never\" when asked the same question but with a 90% confidence instead.[85][86] Further current AGI progress considerations can be found above Tests for confirming human-level AGI.\n A report by Stuart Armstrong and Kaj Sotala of the Machine Intelligence Research Institute found that \"over [a] 60-year time frame there is a strong bias towards predicting the arrival of human-level AI as between 15 and 25 years from the time the prediction was made\". They analyzed 95 predictions made between 1950 and 2012 on when human-level AI will come about.[87]\n In 2023, Microsoft researchers published a detailed evaluation of GPT-4. They concluded: \"Given the breadth and depth of GPT-4’s capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\"[88] Another study in 2023 reported that GPT-4 outperforms 99% of humans on the Torrance tests of creative thinking.[89][90]\n Blaise Agüera y Arcas and Peter Norvig wrote in 2023 that a significant level of general intelligence has already been achieved with frontier models. They wrote that reluctance to this view comes from four main reasons: a \"healthy skepticism about metrics for AGI\", an \"ideological commitment to alternative AI theories or techniques\", a \"devotion to human (or biological) exceptionalism\", or a \"concern about the economic implications of AGI\".[91]\n 2023 also marked the emergence of large multimodal models (large language models capable of processing or generating multiple modalities such as text, audio, and images).[92]\n In 2024, OpenAI released o1-preview, the first of a series of models that \"spend more time thinking before they respond\". According to Mira Murati, this ability to think before responding represents a new, additional paradigm. It improves model outputs by spending more computing power when generating the answer, whereas the model scaling paradigm improves outputs by increasing the model size, training data and training compute power.[93][94]\n An OpenAI employee, Vahid Kazemi, claimed in 2024 that the company had achieved AGI, stating, \"In my opinion, we have already achieved AGI and it’s even more clear with O1.\" Kazemi clarified that while the AI is not yet \"better than any human at any task\", it is \"better than most humans at most tasks.\" He also addressed criticisms that large language models (LLMs) merely follow predefined patterns, comparing their learning process to the scientific method of observing, hypothesizing, and verifying. These statements have sparked debate, as they rely on a broad and unconventional definition of AGI—traditionally understood as AI that matches human intelligence across all domains. Critics argue that, while OpenAI's models demonstrate remarkable versatility, they may not fully meet this standard. Notably, Kazemi's comments came shortly after OpenAI removed \"AGI\" from the terms of its partnership with Microsoft, prompting speculation about the company’s strategic intentions.[95]\n Progress in artificial intelligence has historically gone through periods of rapid progress separated by periods when progress appeared to stop.[82] Ending each hiatus were fundamental advances in hardware, software or both to create space for further progress.[82][98][99] For example, the computer hardware available in the twentieth century was not sufficient to implement deep learning, which requires large numbers of GPU-enabled CPUs.[100]\n In the introduction to his 2006 book,[101] Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century. As of 2007[update], the consensus in the AGI research community seemed to be that the timeline discussed by Ray Kurzweil in 2005 in The Singularity is Near[102] (i.e. between 2015 and 2045) was plausible.[103] Mainstream AI researchers have given a wide range of opinions on whether progress will be this rapid. A 2012 meta-analysis of 95 such opinions found a bias towards predicting that the onset of AGI would occur within 16–26 years for modern and historical predictions alike. That paper has been criticized for how it categorized opinions as expert or non-expert.[104]\n In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton developed a neural network called AlexNet, which won the ImageNet competition with a top-5 test error rate of 15.3%, significantly better than the second-best entry's rate of 26.3% (the traditional approach used a weighted sum of scores from different pre-defined classifiers).[105] AlexNet was regarded as the initial ground-breaker of the current deep learning wave.[105]\n In 2017, researchers Feng Liu, Yong Shi, and Ying Liu conducted intelligence tests on publicly available and freely accessible weak AI such as Google AI, Apple's Siri, and others. At the maximum, these AIs reached an IQ value of about 47, which corresponds approximately to a six-year-old child in first grade. An adult comes to about 100 on average. Similar tests were carried out in 2014, with the IQ score reaching a maximum value of 27.[106][107]\n In 2020, OpenAI developed GPT-3, a language model capable of performing many diverse tasks without specific training. According to Gary Grossman in a VentureBeat article, while there is consensus that GPT-3 is not an example of AGI, it is considered by some to be too advanced to be classified as a narrow AI system.[108]\n In the same year, Jason Rohrer used his GPT-3 account to develop a chatbot, and provided a chatbot-developing platform called \"Project December\". OpenAI asked for changes to the chatbot to comply with their safety guidelines; Rohrer disconnected Project December from the GPT-3 API.[109]\n In 2022, DeepMind developed Gato, a \"general-purpose\" system capable of performing more than 600 different tasks.[110]\n In 2023, Microsoft Research published a study on an early version of OpenAI's GPT-4, contending that it exhibited more general intelligence than previous AI models and demonstrated human-level performance in tasks spanning multiple domains, such as mathematics, coding, and law. This research sparked a debate on whether GPT-4 could be considered an early, incomplete version of artificial general intelligence, emphasizing the need for further exploration and evaluation of such systems.[111]\n In 2023, the AI researcher Geoffrey Hinton stated that:[112]\n The idea that this stuff could actually get smarter than people – a few people believed that, [...]. But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that. In May 2023, Demis Hassabis similarly said that \"The progress in the last few years has been pretty incredible\", and that he sees no reason why it would slow down, expecting AGI within a decade or even a few years.[113] In March 2024, Nvidia's CEO, Jensen Huang, stated his expectation that within five years, AI would be capable of passing any test at least as well as humans.[114] In June 2024, the AI researcher Leopold Aschenbrenner, a former OpenAI employee, estimated AGI by 2027 to be \"strikingly plausible\".[115]\n While the development of transformer models like in ChatGPT is considered the most promising path to AGI,[116][117] whole brain emulation can serve as an alternative approach. With whole brain simulation, a brain model is built by scanning and mapping a biological brain in detail, and then copying and simulating it on a computer system or another computational device. The simulation model must be sufficiently faithful to the original, so that it behaves in practically the same way as the original brain.[118] Whole brain emulation is a type of brain simulation that is discussed in computational neuroscience and neuroinformatics, and for medical research purposes. It has been discussed in artificial intelligence research[103] as an approach to strong AI. Neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly, and futurist Ray Kurzweil in the book The Singularity Is Near[102] predicts that a map of sufficient quality will become available on a similar timescale to the computing power required to emulate it.\n  For low-level brain simulation, a very powerful cluster of computers or GPUs would be required, given the enormous quantity of synapses within the human brain. Each of the 1011 (one hundred billion) neurons has on average 7,000 synaptic connections (synapses) to other neurons. The brain of a three-year-old child has about 1015 synapses (1 quadrillion). This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 1014 to 5×1014 synapses (100 to 500 trillion).[120] An estimate of the brain's processing power, based on a simple switch model for neuron activity, is around 1014 (100 trillion) synaptic updates per second (SUPS).[121]\n In 1997, Kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 1016 computations per second (cps).[e] (For comparison, if a \"computation\" was equivalent to one \"floating-point operation\" – a measure used to rate current supercomputers – then 1016 \"computations\" would be equivalent to 10 petaFLOPS, achieved in 2011, while 1018 was achieved in 2022.) He used this figure to predict the necessary hardware would be available sometime between 2015 and 2025, if the exponential growth in computer power at the time of writing continued.\n The Human Brain Project, an EU-funded initiative active from 2013 to 2023, has developed a particularly detailed and publicly accessible atlas of the human brain.[124] In 2023, researchers from Duke University performed a high-resolution scan of a mouse brain.\n The artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently understood only in broad outline. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition, the estimates do not account for glial cells, which are known to play a role in cognitive processes.[125]\n A fundamental criticism of the simulated brain approach derives from embodied cognition theory which asserts that human embodiment is an essential aspect of human intelligence and is necessary to ground meaning.[126][127] If this theory is correct, any fully functional brain model will need to encompass more than just the neurons (e.g., a robotic body). Goertzel[103] proposes virtual embodiment (like in metaverses like Second Life) as an option, but it is unknown whether this would be sufficient.\n In 1980, philosopher John Searle coined the term \"strong AI\" as part of his Chinese room argument.[128] He proposed a distinction between two hypotheses about artificial intelligence:[f]\n The first one he called \"strong\" because it makes a stronger statement: it assumes something special has happened to the machine that goes beyond those abilities that we can test. The behaviour of a \"weak AI\" machine would be precisely identical to a \"strong AI\" machine, but the latter would also have subjective conscious experience. This usage is also common in academic AI research and textbooks.[129]\n In contrast to Searle and mainstream AI, some futurists such as Ray Kurzweil use the term \"strong AI\" to mean \"human level artificial general intelligence\".[102] This is not the same as Searle's strong AI, unless it is assumed that consciousness is necessary for human-level AGI. Academic philosophers such as Searle do not believe that is the case, and to most artificial intelligence researchers the question is out-of-scope.[130]\n Mainstream AI is most interested in how a program behaves.[131] According to Russell and Norvig, \"as long as the program works, they don't care if you call it real or a simulation.\"[130] If the program can behave as if it has a mind, then there is no need to know if it actually has mind – indeed, there would be no way to tell. For AI research, Searle's \"weak AI hypothesis\" is equivalent to the statement \"artificial general intelligence is possible\". Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\"[130] Thus, for academic AI research, \"Strong AI\" and \"AGI\" are two different things.\n Consciousness can have various meanings, and some aspects play significant roles in science fiction and the ethics of artificial intelligence:\n These traits have a moral dimension. AI sentience would give rise to concerns of welfare and legal protection, similarly to animals.[136] Other aspects of consciousness related to cognitive capabilities are also relevant to the concept of AI rights.[137] Figuring out how to integrate advanced AI with existing legal and social frameworks is an emergent issue.[138]\n AGI could have a wide variety of applications. If oriented towards such goals, AGI could help mitigate various problems in the world such as hunger, poverty and health problems.[139]\n AGI could improve productivity and efficiency in most jobs. For example, in public health, AGI could accelerate medical research, notably against cancer.[140] It could take care of the elderly,[141] and democratize access to rapid, high-quality medical diagnostics. It could offer fun, cheap and personalized education.[141] The need to work to subsist could become obsolete if the wealth produced is properly redistributed.[141][142] This also raises the question of the place of humans in a radically automated society.\n AGI could also help to make rational decisions, and to anticipate and prevent disasters. It could also help to reap the benefits of potentially catastrophic technologies such as nanotechnology or climate engineering, while avoiding the associated risks.[143] If an AGI's primary goal is to prevent existential catastrophes such as human extinction (which could be difficult if the Vulnerable World Hypothesis turns out to be true),[144] it could take measures to drastically reduce the risks[143] while minimizing the impact of these measures on our quality of life.\n AGI may represent multiple types of existential risk, which are risks that threaten \"the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development\".[145] The risk of human extinction from AGI has been the topic of many debates, but there is also the possibility that the development of AGI would lead to a permanently flawed future. Notably, it could be used to spread and preserve the set of values of whoever develops it. If humanity still has moral blind spots similar to slavery in the past, AGI might irreversibly entrench it, preventing moral progress.[146] Furthermore, AGI could facilitate mass surveillance and indoctrination, which could be used to create a stable repressive worldwide totalitarian regime.[147][148] There is also a risk for the machines themselves. If machines that are sentient or otherwise worthy of moral consideration are mass created in the future, engaging in a civilizational path that indefinitely neglects their welfare and interests could be an existential catastrophe.[149][150] Considering how much AGI could improve humanity's future and help reduce other existential risks, Toby Ord calls these existential risks \"an argument for proceeding with due caution\", not for \"abandoning AI\".[147]\n The thesis that AI poses an existential risk for humans, and that this risk needs more attention, is controversial but has been endorsed in 2023 by many public figures, AI researchers and CEOs of AI companies such as Elon Musk, Bill Gates, Geoffrey Hinton, Yoshua Bengio, Demis Hassabis and Sam Altman.[151][152]\n In 2014, Stephen Hawking criticized widespread indifference:\n So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here—we'll leave the lights on?' Probably not—but this is more or less what is happening with AI.[153] The potential fate of humanity has sometimes been compared to the fate of gorillas threatened by human activities. The comparison states that greater intelligence allowed humanity to dominate gorillas, which are now vulnerable in ways that they could not have anticipated. As a result, the gorilla has become an endangered species, not out of malice, but simply as a collateral damage from human activities.[154]\n The skeptic Yann LeCun considers that AGIs will have no desire to dominate humanity and that we should be careful not to anthropomorphize them and interpret their intents as we would for humans. He said that people won't be \"smart enough to design super-intelligent machines, yet ridiculously stupid to the point of giving it moronic objectives with no safeguards\".[155] On the other side, the concept of instrumental convergence suggests that almost whatever their goals, intelligent agents will have reasons to try to survive and acquire more power as intermediary steps to achieving these goals. And that this does not require having emotions.[156]\n Many scholars who are concerned about existential risk advocate for more research into solving the \"control problem\" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximise the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence?[157][158] Solving the control problem is complicated by the AI arms race (which could lead to a race to the bottom of safety precautions in order to release products before competitors),[159] and the use of AI in weapon systems.[160]\n The thesis that AI can pose existential risk also has detractors. Skeptics usually say that AGI is unlikely in the short-term, or that concerns about AGI distract from other issues related to current AI.[161] Former Google fraud czar Shuman Ghosemajumder considers that for many people outside of the technology industry, existing chatbots and LLMs are already perceived as though they were AGI, leading to further misunderstanding and fear.[162]\n Skeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God.[163] Some researchers believe that the communication campaigns on AI existential risk by certain AI groups (such as OpenAI, Anthropic, DeepMind, and Conjecture) may be an at attempt at regulatory capture and to inflate interest in their products.[164][165]\n In 2023, the CEOs of Google DeepMind, OpenAI and Anthropic, along with other industry leaders and researchers, issued a joint statement asserting that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\"[152]\n Researchers from OpenAI estimated that \"80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while around 19% of workers may see at least 50% of their tasks impacted\".[166][167] They consider office workers to be the most exposed, for example mathematicians, accountants or web designers.[167] AGI could have a better autonomy, ability to make decisions, to interface with other computer tools, but also to control robotized bodies.\n According to Stephen Hawking, the outcome of automation on the quality of life will depend on how the wealth will be redistributed:[142]\n Everyone can enjoy a life of luxurious leisure if the machine-produced wealth is shared, or most people can end up miserably poor if the machine-owners successfully lobby against wealth redistribution. So far, the trend seems to be toward the second option, with technology driving ever-increasing inequality Elon Musk considers that the automation of society will require governments to adopt a universal basic income.[168]\n"
    },
    {
        "title": "Intelligent agent",
        "url": "https://en.wikipedia.org/wiki/Intelligent_agent",
        "content": "In intelligence and artificial intelligence, an intelligent agent (IA) is an agent that perceives its environment, takes actions autonomously in order to achieve goals, and may improve its performance with learning or acquiring knowledge. \n An intelligent agent may be simple or complex: A thermostat or other control system is considered an example of an intelligent agent, as is a human being, as is any system that meets the definition, such as a firm, a state, or a biome.[1]\n Leading AI textbooks define \"artificial intelligence\" as the \"study and design of intelligent agents\", a definition that considers goal-directed behavior to be the essence of intelligence. Goal-directed agents are also described using a term borrowed from economics, \"rational agent\".[1]\n An agent has an \"objective function\" that encapsulates all the IA's goals. Such an agent is designed to create and execute whatever plan will, upon completion, maximize the expected value of the objective function.[2]\n For example, a reinforcement learning agent has a \"reward function\" that allows the programmers to shape the IA's desired behavior,[3] and an evolutionary algorithm's behavior is shaped by a \"fitness function\".[4]\n Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\n Intelligent agents are often described schematically as an abstract functional system similar to a computer program. \n Abstract descriptions of intelligent agents are called abstract intelligent agents (AIA) to distinguish them from their real-world implementations. \n An autonomous intelligent agent is designed to function in the absence of human intervention. Intelligent agents are also closely related to software agents. An autonomous computer program that carries out tasks on behalf of users.\n Artificial Intelligence: A Modern Approach[5][6][2] defines an \"agent\" as \n \"Anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators\" It defines a \"rational agent\" as:\n \"An agent that acts so as to maximize the expected value of a performance measure based on past experience and knowledge.\" It also defines the field of \"artificial intelligence research\" as:\n \"The study and design of rational agents\" Padgham & Winikoff (2005) agree that an intelligent agent is situated in an environment and responds in a timely (though not necessarily real-time) manner to changes in the environment. However, intelligent agents must also proactively pursue goals in a flexible and robust way.[a] Optional desiderata include that the agent be rational, and that the agent be capable of belief-desire-intention analysis.[7]\n Kaplan and Haenlein define artificial intelligence as \"a system's ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation\".[8] This definition is closely related to that of an intelligent agent.\n Philosophically, this definition of artificial intelligence avoids several lines of criticism. Unlike the Turing test, it does not refer to human intelligence in any way. Thus, there is no need to discuss if it is \"real\" vs \"simulated\" intelligence (i.e., \"synthetic\" vs \"artificial\" intelligence) and does not indicate that such a machine has a mind, consciousness or true understanding. It seems not to imply John Searle's \"strong AI hypothesis\". It also doesn't attempt to draw a sharp dividing line between behaviors that are \"intelligent\" and behaviors that are \"unintelligent\"—programs need only be measured in terms of their objective function.\n More importantly, it has a number of practical advantages that have helped move AI research forward. It provides a reliable and scientific way to test programs; researchers can directly compare or even combine different approaches to isolated problems, by asking which agent is best at maximizing a given \"goal function\".  \n It also gives them a common language to communicate with other fields—such as mathematical optimization (which is defined in terms of \"goals\") or economics (which uses the same definition of a \"rational agent\").[9]\n An agent that is assigned an explicit \"goal function\" is considered more intelligent if it consistently takes actions that successfully maximize its programmed goal function. \n The goal can be simple: 1 if the IA wins a game of Go, 0 otherwise.\n Or the goal can be complex: Perform actions mathematically similar to ones that succeeded in the past. \n The \"goal function\" encapsulates all of the goals the agent is driven to act on; in the case of rational agents, the function also encapsulates the acceptable trade-offs between accomplishing conflicting goals. \n Terminology varies. For example, some agents seek to maximize or minimize an \"utility function\", \"objective function\" or \"loss function\".[6][2]\n Goals can be explicitly defined or induced. If the AI is programmed for \"reinforcement learning\", it has a \"reward function\" that encourages some types of behavior and punishes others. \n Alternatively, an evolutionary system can induce goals by using a \"fitness function\" to mutate and preferentially replicate high-scoring AI systems, similar to how animals evolved to innately desire certain goals such as finding food.[10]\n Some AI systems, such as nearest-neighbor, instead of reason by analogy, these systems are not generally given goals, except to the degree that goals are implicit in their training data.[11] Such systems can still be benchmarked if the non-goal system is framed as a system whose \"goal\" is to accomplish its narrow classification task.[12]\n Systems that are not traditionally considered agents, such as knowledge-representation systems, are sometimes subsumed into the paradigm by framing them as agents that have a goal of (for example) answering questions as accurately as possible; the concept of an \"action\" is here extended to encompass the \"act\" of giving an answer to a question. As an additional extension, mimicry-driven systems can be framed as agents who are optimizing a \"goal function\" based on how closely the IA succeeds in mimicking the desired behavior.[6][2] In the generative adversarial networks of the 2010s, an \"encoder\"/\"generator\" component attempts to mimic and improvise human text composition. The generator is attempting to maximize a function encapsulating how well it can fool an antagonistic \"predictor\"/\"discriminator\" component.[13]\n While symbolic AI systems often accept an explicit goal function, the paradigm can also be applied to neural networks and to evolutionary computing. Reinforcement learning can generate intelligent agents that appear to act in ways intended to maximize a \"reward function\".[14] Sometimes, rather than setting the reward function to be directly equal to the desired benchmark evaluation function, machine learning programmers will use reward shaping to initially give the machine rewards for incremental progress in learning.[15] Yann LeCun stated in 2018, \"Most of the learning algorithms that people have come up with essentially consist of minimizing some objective function.\"[16] AlphaZero chess had a simple objective function; each win counted as +1 point, and each loss counted as -1 point. An objective function for a self-driving car would have to be more complicated.[17] Evolutionary computing can evolve intelligent agents that appear to act in ways intended to maximize a \"fitness function\" that influences how many descendants each agent is allowed to leave.[4]\n The mathematical formalism of AIXI was proposed as a maximally intelligent agent in this paradigm.[18] However, AIXI is uncomputable. In the real world, an IA is constrained by finite time and hardware resources, and scientists compete to produce algorithms that can achieve progressively higher scores on benchmark tests with existing hardware.[19]\n A simple agent program can be defined mathematically as a function f (called the \"agent function\")[20] which maps every possible percepts sequence to a possible action the agent can perform or to a coefficient, feedback element, function or constant that affects eventual actions:\n Agent function is an abstract concept as it could incorporate various principles of decision making like calculation of utility of individual options, deduction over logic rules, fuzzy logic, etc.[21]\n The program agent, instead, maps every possible percept to an action.[22]\n We use the term percept to refer to the agent's perceptional inputs at any given instant. In the following figures, an agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators.\n Russell & Norvig (2003) group agents into five classes based on their degree of perceived intelligence and capability:[23]\n Simple reflex agents act only on the basis of the current percept, ignoring the rest of the percept history. The agent function is based on the condition-action rule: \"if condition, then action\".\n This agent function only succeeds when the environment is fully observable. Some reflex agents can also contain information on their current state which allows them to disregard conditions whose actuators are already triggered.\n Infinite loops are often unavoidable for simple reflex agents operating in partially observable environments. If the agent can randomize its actions, it may be possible to escape from infinite loops.\n A model-based agent can handle partially observable environments. Its current state is stored inside the agent maintaining some kind of structure that describes the part of the world which cannot be seen. This knowledge about \"how the world works\" is called a model of the world, hence the name \"model-based agent\".\n A model-based reflex agent should maintain some sort of internal model that depends on the percept history and thereby reflects at least some of the unobserved aspects of the current state. Percept history and impact of action on the environment can be determined by using the internal model. It then chooses an action in the same way as reflex agent.\n An agent may also use models to describe and predict the behaviors of other agents in the environment.[24]\n Goal-based agents further expand on the capabilities of the model-based agents, by using \"goal\" information. Goal information describes situations that are desirable. This provides the agent a way to choose among multiple possibilities, selecting the one which reaches a goal state. Search and planning are the subfields of artificial intelligence devoted to finding action sequences that achieve the agent's goals.\n Goal-based agents only distinguish between goal states and non-goal states. It is also possible to define a measure of how desirable a particular state is. This measure can be obtained through the use of a utility function which maps a state to a measure of the utility of the state. A more general performance measure should allow a comparison of different world states according to how well they satisfied the agent's goals. The term utility can be used to describe how \"happy\" the agent is.\n A rational utility-based agent chooses the action that maximizes the expected utility of the action outcomes - that is, what the agent expects to derive, on average, given the probabilities and utilities of each outcome. A utility-based agent has to model and keep track of its environment, tasks that have involved a great deal of research on perception, representation, reasoning, and learning.\n Learning has the advantage of allowing agents to initially operate in unknown environments and become more competent than their initial knowledge alone might allow. The most important distinction is between the \"learning element\", responsible for making improvements, and the \"performance element\", responsible for selecting external actions.\n The learning element uses feedback from the \"critic\" on how the agent is doing and determines how the performance element, or \"actor\", should be modified to do better in the future. The performance element, previously considered the entire agent, takes in percepts and decides on actions.\n The last component of the learning agent is the \"problem generator\". It is responsible for suggesting actions that will lead to new and informative experiences.\n Weiss (2013) defines four classes of agents:\n In 2013, Alexander Wissner-Gross published a theory pertaining to Freedom and Intelligence for intelligent agents.[25][26]\n Intelligent agents can be organized hierarchically into multiple \"sub-agents\". Intelligent sub-agents process and perform lower-level functions. Taken together, the intelligent agent and sub-agents create a complete system that can accomplish difficult tasks or goals with behaviors and responses that display a form of intelligence.\n Generally, an agent can be constructed by separating the body into the sensors and actuators, and so that it operates with a complex perception system that takes the description of the world as input for a controller and outputs commands to the actuator. However, a hierarchy of controller layers is often necessary to balance the immediate reaction desired for low-level tasks and the slow reasoning about complex, high-level goals.[27]\n \"Intelligent agent\" is also often used as a vague term, sometimes synonymous with \"virtual personal assistant\".[28] Some 20th-century definitions characterize an agent as a program that aids a user or that acts on behalf of a user.[29] These examples are known as software agents, and sometimes an \"intelligent software agent\" (that is, a software agent with intelligence) is referred to as an \"intelligent agent\".\n According to Nikola Kasabov, IA systems should exhibit the following characteristics:[30]\n \nIn the context of generative artificial intelligence, AI agents[31] (also known as compound AI systems,[31] agentic AI,[32][33][34] large action models,[32] or large agent models[32]) are agents defined by a spectrum of attributes: complexity of their environment, complexity of their goals, a user interface based on natural language, capability of acting independently from user supervision, the use of software tools or planning, and a control flow based on large language models.[31] A common hypothetical use case is automatically booking travel plans based on a prompted request.[32][33][34][35][36][37][38] Examples of AI agents include Devin AI, AutoGPT, and SIMA.[39] Examples of AI agent frameworks include LangChain,[40][41] Microsoft AutoGen[41] and OpenAI Swarm.[42]\n Proposed benefits include increased personal and economic productivity,[36][33] more innovation,[38] and liberation from tedious work.[34][38] Potential concerns include liability,[36][33][34] cybercrime,[36][32] philosophical ethical considerations,[36] AI safety[36] and AI alignment,[32][34] data privacy,[35][32][43][44] weakening of human oversight,[35][36][32][33][43] algorithmic bias,[35][45] compounding software errors,[32][39] the compounding effect of existing concerns about artificial intelligence,[32] unpredictability,[32][43] difficulty explaining decisions made by an agentic system,[32][43] security vulnerabilities,[32][33][44][45] underemployment,[45] job displacement,[33][45] manipulation of users,[43][46] how to adapt legal systems to agents,[34] hallucinations,[37][45] difficulty counteracting agents,[43] the lack of a framework to identify and manage agents,[43] reward hacking,[43] targeted harassment,[43] promotion of AI slop,[47] high cost of use,[31] overfitting of benchmark datasets,[31] and the lack of standardization in or reproducibility of agent evaluation frameworks.[31][43]\n Hallerbach et al. discussed the application of agent-based approaches for the development and validation of automated driving systems via a digital twin of the vehicle-under-test and microscopic traffic simulation based on independent agents.[48] Waymo has created a multi-agent simulation environment, Carcraft, to test algorithms for self-driving cars.[49][50] It simulates traffic interactions between human drivers, pedestrians and automated vehicles. People's behavior is imitated by artificial agents based on data of real human behavior. The basic idea of using agent-based modeling to understand self-driving cars was discussed as early as 2003.[51]\n"
    },
    {
        "title": "Automated planning and scheduling",
        "url": "https://en.wikipedia.org/wiki/Automated_planning_and_scheduling",
        "content": "Automated planning and scheduling, sometimes denoted as simply AI planning,[1] is a branch of artificial intelligence that concerns the realization of strategies or action sequences, typically for execution by intelligent agents, autonomous robots and unmanned vehicles. Unlike classical control and classification problems, the solutions are complex and must be discovered and optimized in multidimensional space. Planning is also related to decision theory.\n In known environments with available models, planning can be done offline. Solutions can be found and evaluated prior to execution. In dynamically unknown environments, the strategy often needs to be revised online. Models and policies must be adapted. Solutions usually resort to iterative trial and error processes commonly seen in artificial intelligence. These include dynamic programming, reinforcement learning and combinatorial optimization. Languages used to describe planning and scheduling are often called action languages.\n Given a description of the possible initial states of the world, a description of the desired goals, and a description of a set of possible actions, the planning problem is to synthesize a plan that is guaranteed (when applied to any of the initial states) to generate a state which contains the desired goals (such a state is called a goal state).\n The difficulty of planning is dependent on the simplifying assumptions employed. Several classes of planning problems can be identified depending on the properties the problems have in several dimensions.\n The simplest possible planning problem, known as the Classical Planning Problem, is determined by:\n Since the initial state is known unambiguously, and all actions are deterministic, the state of the world after any sequence of actions can be accurately predicted, and the question of observability is irrelevant for classical planning.\n Further, plans can be defined as sequences of actions, because it is always known in advance which actions will be needed.\n With nondeterministic actions or other events outside the control of the agent, the possible executions form a tree, and plans have to determine the appropriate actions for every node of the tree.\n Discrete-time Markov decision processes (MDP) are planning problems with:\n When full observability is replaced by partial observability, planning corresponds to a partially observable Markov decision process (POMDP).\n If there are more than one agent, we have multi-agent planning, which is closely related to game theory.\n In AI planning, planners typically input a domain model (a description of a set of possible actions which model the domain) as well as the specific problem to be solved specified by the initial state and goal, in contrast to those in which there is no input domain specified. Such planners are called \"domain independent\" to emphasize the fact that they can solve planning problems from a wide range of domains. Typical examples of domains are block-stacking, logistics, workflow management, and robot task planning. Hence a single domain-independent planner can be used to solve planning problems in all these various domains. On the other hand, a route planner is typical of a domain-specific planner.\n The most commonly used languages for representing planning domains and specific planning problems, such as STRIPS and PDDL for Classical Planning, are based on state variables. Each possible state of the world is an assignment of values to the state variables, and actions determine how the values of the state variables change when that action is taken. Since a set of state variables induce a state space that has a size that is exponential in the set, planning, similarly to many other computational problems, suffers from the curse of dimensionality and the combinatorial explosion.\n An alternative language for describing planning problems is that of hierarchical task networks, in which a set of tasks is given, and each task can be either realized by a primitive action or decomposed into a set of other tasks. This does not necessarily involve state variables, although in more realistic applications state variables simplify the description of task networks.\n Temporal planning can be solved with methods similar to classical planning. The main difference is, because of the possibility of several, temporally overlapping actions with a duration being taken concurrently, that the definition of a state has to include information about the current absolute time and how far the execution of each active action has proceeded. Further, in planning with rational or real time, the state space may be infinite, unlike in classical planning or planning with integer time. Temporal planning is closely related to scheduling problems when uncertainty is involved and can also be understood in terms of timed automata. The Simple Temporal Network with Uncertainty (STNU) is a scheduling problem which involves controllable actions, uncertain events and temporal constraints. Dynamic Controllability for such problems is a type of scheduling which requires a temporal planning strategy to activate controllable actions reactively as uncertain events are observed so that all constraints are guaranteed to be satisfied. [2]\n Probabilistic planning can be solved with iterative methods such as value iteration and policy iteration, when the state space is sufficiently small.\nWith partial observability, probabilistic planning is similarly solved with iterative methods, but using a representation of the value functions defined for the space of beliefs instead of states.\n In preference-based planning, the objective is not only to produce a plan but also to satisfy user-specified preferences. A difference to the more common reward-based planning, for example corresponding to MDPs, preferences don't necessarily have a precise numerical value.\n Deterministic planning was introduced with the STRIPS planning system, which is a hierarchical planner. Action names are ordered in a sequence and this is a plan for the robot. Hierarchical planning can be compared with an automatic generated behavior tree.[3] The disadvantage is, that a normal behavior tree is not so expressive like a computer program. That means, the notation of a behavior graph contains action commands, but no loops or if-then-statements. Conditional planning overcomes the bottleneck and introduces an elaborated notation which is similar to a control flow, known from other programming languages like Pascal. It is very similar to program synthesis, which means a planner generates sourcecode which can be executed by an interpreter.[4]\n An early example of a conditional planner is “Warplan-C” which was introduced in the mid 1970s.[5] What is the difference between a normal sequence and a complicated plan, which contains if-then-statements? It has to do with uncertainty at runtime of a plan. The idea is that a plan can react to sensor signals which are unknown for the planner. The planner generates two choices in advance. For example, if an object was detected, then action A is executed, if an object is missing, then action B is executed.[6] A major advantage of conditional planning is the ability to handle partial plans.[7] An agent is not forced to plan everything from start to finish but can divide the problem into chunks. This helps to reduce the state space and solves much more complex problems.\n We speak of \"contingent planning\" when the environment is observable through sensors, which can be faulty. It is thus a situation where the planning agent acts under incomplete information. For a contingent planning problem, a plan is no longer a sequence of actions but a decision tree because each step of the plan is represented by a set of states rather than a single perfectly observable state, as in the case of classical planning.[8] The selected actions depend on the state of the system. For example, if it rains, the agent chooses to take the umbrella, and if it doesn't, they may choose not to take it.\n Michael L. Littman showed in 1998 that with branching actions, the planning problem becomes EXPTIME-complete.[9][10] A particular case of contiguous planning is represented by FOND problems - for \"fully-observable and non-deterministic\". If the goal is specified in LTLf (linear time logic on finite trace) then the problem is always EXPTIME-complete[11] and 2EXPTIME-complete if the goal is specified with LDLf.\n Conformant planning is when the agent is uncertain about the state of the system, and it cannot make any observations. The agent then has beliefs about the real world, but cannot verify them with sensing actions, for instance. These problems are solved by techniques similar to those of classical planning,[12][13] but where the state space is exponential in the size of the problem, because of the uncertainty about the current state. A solution for a conformant planning problem is a sequence of actions. Haslum and Jonsson have demonstrated that the problem of conformant planning is EXPSPACE-complete,[14] and 2EXPTIME-complete when the initial situation is uncertain, and there is non-determinism in the actions outcomes.[10]\n"
    },
    {
        "title": "Computer vision",
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "content": "Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the form of decisions.[1][2][3][4] \"Understanding\" in this context signifies the transformation of visual images (the input to the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.\n The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. Image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, 3D point clouds from LiDaR sensors, or medical scanning devices. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems.\n Subdisciplines of computer vision include scene reconstruction, object detection, event detection, activity recognition, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration.\n Computer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.[5][6][7] \"Computer vision is concerned with the automatic extraction, analysis, and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.\"[8] As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner.[9] As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems. Machine vision refers to a systems engineering discipline, especially in the context of factory automation. In more recent times, the terms computer vision and machine vision have converged to a greater degree.[10]: 13 \n In the late 1960s, computer vision began at universities that were pioneering artificial intelligence. It was meant to mimic the human visual system as a stepping stone to endowing robots with intelligent behavior.[11] In 1966, it was believed that this could be achieved through an undergraduate summer project,[12] by attaching a camera to a computer and having it \"describe what it saw\".[13][14]\n What distinguished computer vision from the prevalent field of digital image processing at that time was a desire to extract three-dimensional structure from images with the goal of achieving full scene understanding. Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation.[11]\n The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields.[15]\nBy the 1990s, some of the previous research topics became more active than others. Research in projective 3-D reconstructions led to better understanding of camera calibration. With the advent of optimization methods for camera calibration, it was realized that a lot of the ideas were already explored in bundle adjustment theory from the field of photogrammetry. This led to methods for sparse 3-D reconstructions of scenes from multiple images. Progress was made on the dense stereo correspondence problem and further multi-view stereo techniques. At the same time, variations of graph cut were used to solve image segmentation. This decade also marked the first time statistical learning techniques were used in practice to recognize faces in images (see Eigenface). Toward the end of the 1990s, a significant change came about with the increased interaction between the fields of computer graphics and computer vision. This included image-based rendering, image morphing, view interpolation, panoramic image stitching and early light-field rendering.[11]\n Recent work has seen the resurgence of feature-based methods used in conjunction with machine learning techniques and complex optimization frameworks.[16][17] \nThe advancement of Deep Learning techniques has brought further life to the field of computer vision. The accuracy of deep learning algorithms on several benchmark computer vision data sets for tasks ranging from classification,[18] segmentation and optical flow has surpassed prior methods. [citation needed][19]\n Solid-state physics is another field that is closely related to computer vision. Most computer vision systems rely on image sensors, which detect electromagnetic radiation, which is typically in the form of either visible, infrared or ultraviolet light. The sensors are designed using quantum physics. The process by which light interacts with surfaces is explained using physics. Physics explains the behavior of optics which are a core part of most imaging systems. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process.[11] Also, various measurement problems in physics can be addressed using computer vision, for example, motion in fluids.\n Neurobiology has greatly influenced the development of computer vision algorithms. Over the last century, there has been an extensive study of eyes, neurons, and brain structures devoted to the processing of visual stimuli in both humans and various animals. This has led to a coarse yet convoluted description of how natural vision systems operate in order to solve certain vision-related tasks. These results have led to a sub-field within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems at different levels of complexity. Also, some of the learning-based methods developed within computer vision (e.g. neural net and deep learning based image and feature analysis and classification) have their background in neurobiology.  The Neocognitron, a neural network developed in the 1970s by Kunihiko Fukushima, is an early example of computer vision taking direct inspiration from neurobiology, specifically the primary visual cortex.\n Some strands of computer vision research are closely related to the study of biological vision—indeed, just as many strands of AI research are closely tied with research into human intelligence and the use of stored knowledge to interpret, integrate, and utilize visual information. The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals. Computer vision, on the other hand, develops and describes the algorithms implemented in software and hardware behind artificial vision systems. An interdisciplinary exchange between biological and computer vision has proven fruitful for both fields.[21]\n Yet another field related to computer vision is signal processing. Many methods for processing one-variable signals, typically temporal signals, can be extended in a natural way to the processing of two-variable signals or multi-variable signals in computer vision. However, because of the specific nature of images, there are many methods developed within computer vision that have no counterpart in the processing of one-variable signals. Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision.\n Robot navigation sometimes deals with autonomous path planning or deliberation for robotic systems to navigate through an environment.[22] A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot\n Besides the above-mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view. For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance. Computer vision is also used in fashion eCommerce, inventory management, patent search, furniture, and the beauty industry.[23]\n The fields most closely related to computer vision are image processing, image analysis and machine vision. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences, and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented. In image processing, the input is an image and the output is an image as well, whereas in computer vision, an image or a video is taken as an input and the output could be an enhanced image, an understanding of the content of an image or even behavior of a computer system based on such understanding.\n Computer graphics produces image data from 3D models, and computer vision often produces 3D models from image data.[24] There is also a trend towards a combination of the two disciplines, e.g., as explored in augmented reality.\n The following characterizations appear relevant but should not be taken as universally accepted:\n Photogrammetry also overlaps with computer vision, e.g., stereophotogrammetry vs. computer stereo vision.\n Applications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. Machine vision usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications. In many computer-vision applications, computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Examples of applications of computer vision include systems for:\n One of the most prominent application fields is medical computer vision, or medical image processing, characterized by the extraction of information from image data to diagnose a patient. An example of this is the detection of tumours, arteriosclerosis or other malign changes, and a variety of dental pathologies; measurements of organ dimensions, blood flow, etc. are another example. It also supports medical research by providing new information: e.g., about the structure of the brain or the quality of medical treatments. Applications of computer vision in the medical area also include enhancement of images interpreted by humans—ultrasonic images or X-ray images, for example—to reduce the influence of noise.\n A second application area in computer vision is in industry, sometimes called machine vision, where information is extracted for the purpose of supporting a production process. One example is quality control where details or final products are being automatically inspected in order to find defects. One of the most prevalent fields for such inspection is the Wafer industry in which every single Wafer is being measured and inspected for inaccuracies or defects to prevent a computer chip from coming to market in an unusable manner. Another example is a measurement of the position and orientation of details to be picked up by a robot arm. Machine vision is also heavily used in the agricultural processes to remove undesirable foodstuff from bulk material, a process called optical sorting.[32]\n Military applications are probably one of the largest areas of computer vision[citation needed]. The obvious examples are the detection of enemy soldiers or vehicles and missile guidance. More advanced systems for missile guidance send the missile to an area rather than a specific target, and target selection is made when the missile reaches the area based on locally acquired image data. Modern military concepts, such as \"battlefield awareness\", imply that various sensors, including image sensors, provide a rich set of information about a combat scene that can be used to support strategic decisions. In this case, automatic processing of the data is used to reduce complexity and to fuse information from multiple sensors to increase reliability.\n One of the newer application areas is autonomous vehicles, which include submersibles, land-based vehicles (small robots with wheels, cars, or trucks), aerial vehicles, and unmanned aerial vehicles (UAV). The level of autonomy ranges from fully autonomous (unmanned) vehicles to vehicles where computer-vision-based systems support a driver or a pilot in various situations. Fully autonomous vehicles typically use computer vision for navigation, e.g., for knowing where they are or mapping their environment (SLAM), for detecting obstacles. It can also be used for detecting certain task-specific events, e.g., a UAV looking for forest fires. Examples of supporting systems are obstacle warning systems in cars, cameras and LiDAR sensors in vehicles, and systems for autonomous landing of aircraft. Several car manufacturers have demonstrated systems for autonomous driving of cars. There are ample examples of military autonomous vehicles ranging from advanced missiles to UAVs for recon missions or missile guidance. Space exploration is already being made with autonomous vehicles using computer vision, e.g., NASA's Curiosity and CNSA's Yutu-2 rover.\n Materials such as rubber and silicon are being used to create sensors that allow for applications such as detecting microundulations and calibrating robotic hands. Rubber can be used in order to create a mold that can be placed over a finger, inside of this mold would be multiple strain gauges. The finger mold and sensors could then be placed on top of a small sheet of rubber containing an array of rubber pins. A user can then wear the finger mold and trace a surface. A computer can then read the data from the strain gauges and measure if one or more of the pins are being pushed upward. If a pin is being pushed upward then the computer can recognize this as an imperfection in the surface. This sort of technology is useful in order to receive accurate data on imperfections on a very large surface.[33] Another variation of this finger mold sensor are sensors that contain a camera suspended in silicon. The silicon forms a dome around the outside of the camera and embedded in the silicon are point markers that are equally spaced. These cameras can then be placed on devices such as robotic hands in order to allow the computer to receive highly accurate tactile data.[34]\n Other application areas include:\n Each of the application areas described above employ a range of computer vision tasks; more or less well-defined measurement problems or processing problems, which can be solved using a variety of methods. Some examples of typical computer vision tasks are presented below.\n Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions.[1][2][3][4] Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.[39]\n The classical problem in computer vision, image processing, and machine vision is that of determining whether or not the image data contains some specific object, feature, or activity. Different varieties of recognition problem are described in the literature.[40]\n Currently, the best algorithms for such tasks are based on convolutional neural networks. An illustration of their capabilities is given by the ImageNet Large Scale Visual Recognition Challenge; this is a benchmark in object classification and detection, with millions of images and 1000 object classes used in the competition.[41] Performance of convolutional neural networks on the ImageNet tests is now close to that of humans.[41] The best algorithms still struggle with objects that are small or thin, such as a small ant on the stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters (an increasingly common phenomenon with modern digital cameras). By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained classes, such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this with ease.[citation needed]\n Several specialized tasks based on recognition exist, such as:\n Several tasks relate to motion estimation, where an image sequence is processed to produce an estimate of the velocity either at each points in the image or in the 3D scene or even of the camera that produces the images. Examples of such tasks are:\n Given one or (typically) more images of a scene, or a video, scene reconstruction aims at computing a 3D model of the scene. In the simplest case, the model can be a set of 3D points. More sophisticated methods produce a complete 3D surface model. The advent of 3D imaging not requiring motion or scanning, and related processing algorithms is enabling rapid advances in this field. Grid-based 3D sensing can be used to acquire 3D images from multiple angles. Algorithms are now available to stitch multiple 3D images together into point clouds and 3D models.[24]\n Image restoration comes into the picture when the original image is degraded or damaged due to some external factors like lens wrong positioning, transmission interference, low lighting or motion blurs, etc., which is referred to as noise. When the images are degraded or damaged, the information to be extracted from them also gets damaged. Therefore, we need to recover or restore the image as it was intended to be. The aim of image restoration is the removal of noise (sensor noise, motion blur, etc.) from images. The simplest possible approach for noise removal is various types of filters, such as low-pass filters or median filters. More sophisticated methods assume a model of how the local image structures look to distinguish them from noise. By first analyzing the image data in terms of the local image structures, such as lines or edges, and then controlling the filtering based on local information from the analysis step, a better level of noise removal is usually obtained compared to the simpler approaches.\n An example in this field is inpainting.\n The organization of a computer vision system is highly application-dependent. Some systems are stand-alone applications that solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc. The specific implementation of a computer vision system also depends on whether its functionality is pre-specified or if some part of it can be learned or modified during operation. Many functions are unique to the application. There are, however, typical functions that are found in many computer vision systems.\n Image-understanding systems (IUS) include three levels of abstraction as follows: low level includes image primitives such as edges, texture elements, or regions; intermediate level includes boundaries, surfaces and volumes; and high level includes objects, scenes, or events. Many of these requirements are entirely topics for further research.\n The representational requirements in the designing of IUS for these levels are: representation of prototypical concepts, concept organization, spatial knowledge, temporal knowledge, scaling, and description by comparison and differentiation.\n While inference refers to the process of deriving new, not explicitly represented facts from currently known facts, control refers to the process that selects which of the many inference, search, and matching techniques should be applied at a particular stage of processing. Inference and control requirements for IUS are: search and hypothesis activation, matching and hypothesis testing, generation and use of expectations, change and focus of attention, certainty and strength of belief, inference and goal satisfaction.[48]\n There are many kinds of computer vision systems; however, all of them contain these basic elements: a power source, at least one image acquisition device (camera, ccd, etc.), a processor, and control and communication cables or some kind of wireless interconnection mechanism. In addition, a practical vision system contains software, as well as a display in order to monitor the system. Vision systems for inner spaces, as most industrial ones, contain an illumination system and may be placed in a controlled environment. Furthermore, a completed system includes many accessories, such as camera supports, cables, and connectors.\n Most computer vision systems use visible-light cameras passively viewing a scene at frame rates of at most 60 frames per second (usually far slower).\n A few computer vision systems use image-acquisition hardware with active illumination or something other than visible light or both, such as structured-light 3D scanners, thermographic cameras, hyperspectral imagers, radar imaging, lidar scanners, magnetic resonance images, side-scan sonar, synthetic aperture sonar, etc. Such hardware captures \"images\" that are then processed often using the same computer vision algorithms used to process visible-light images.\n While traditional broadcast and consumer video systems operate at a rate of 30 frames per second, advances in digital signal processing and consumer graphics hardware has made high-speed image acquisition, processing, and display possible for real-time systems on the order of hundreds to thousands of frames per second. For applications in robotics, fast, real-time video systems are critically important and often can simplify the processing needed for certain algorithms. When combined with a high-speed projector, fast image acquisition allows 3D measurement and feature tracking to be realized.[49]\n Egocentric vision systems are composed of a wearable camera that automatically take pictures from a first-person perspective.\n As of 2016, vision processing units are emerging as a new class of processors to complement CPUs and graphics processing units (GPUs) in this role.[50]\n"
    },
    {
        "title": "General game playing",
        "url": "https://en.wikipedia.org/wiki/General_game_playing",
        "content": "General game playing (GGP) is the design of artificial intelligence programs to be able to play more than one game successfully.[1][2][3] For many games like chess, computers are programmed to play these games using a specially designed algorithm, which cannot be transferred to another context. For instance, a chess-playing computer program cannot play checkers. General game playing is considered as a necessary milestone on the way to artificial general intelligence.[4]\n General video game playing  (GVGP) is the concept of GGP adjusted to the purpose of playing video games. For video games, game rules have to be either learnt over multiple iterations by artificial players like TD-Gammon,[5] or are predefined manually in a domain-specific language and sent in advance to artificial players[6][7] like in traditional GGP. Starting in 2013, significant progress was made following the deep reinforcement learning approach, including the development of programs that can learn to play Atari 2600 games[8][5][9][10][11] as well as a program that can learn to play Nintendo Entertainment System games.[12][13][14]\n The first commercial usage of general game playing technology was Zillions of Games in 1998. General game playing was also proposed for trading agents in supply chain management there under price negotiation in online auctions from 2003 on.[15][16][17][18]\n In 1992, Barney Pell defined the concept of Meta-Game Playing, and developed the \"MetaGame\" system. This was the first program to automatically generate game rules of chess-like games, and one of the earliest programs to use automated game generation.  Pell then developed the system Metagamer.[19] This system was able to play a number of chess-like games, given game rules definition in a special language called Game Description Language (GDL), without any human interaction once the games were generated.[20]\n In 1998, the commercial system Zillions of Games was developed by Jeff Mallett and Mark Lefler. The system used a LISP-like language to define the game rules. Zillions of Games derived the evaluation function automatically from the game rules based on piece mobility, board structure and game goals. It also employed usual algorithms as found in computer chess systems: alpha–beta pruning with move ordering, transposition tables, etc.[21]  The package was extended in 2007 by the addition of the Axiom plug-in, an alternate metagame engine that incorporates a complete Forth-based programming language.\n In 1998, z-Tree was developed by Urs Fischbacher.[22] z-Tree is the first and the most cited software tool for experimental economics. z-Tree allows the definition of game rules in z-Tree-language for game-theoretic experiments with human subjects. It also allows definition of computer players, which participate in a play with human subjects.[23]\n In 2005, the Stanford Project General Game Playing was established.[3]\n In 2012, the development of PyVGDL started.[24]\n General Game Playing is a project of the Stanford Logic Group of Stanford University, California, which aims to create a platform for general game playing. It is the most well-known effort at standardizing GGP AI, and generally seen as the standard for GGP systems. The games are defined by sets of rules represented in the Game Description Language. In order to play the games, players interact with a game hosting server[25][26] that monitors moves for legality and keeps players informed of state changes.\n Since 2005, there have been annual General Game Playing competitions at the AAAI Conference. The competition judges competitor AI's abilities to play a variety of different games, by recording their performance on each individual game. In the first stage of the competition, entrants are judged on their ability to perform legal moves, gain the upper hand, and complete games faster. In the following runoff round, the AIs face off against each other in increasingly complex games. The AI that wins the most games at this stage wins the competition, and until 2013 its creator used to win a $10,000 prize.[19] So far, the following programs were victorious:[27]\n There are other general game playing systems, which use their own languages for defining the game rules. Other general game playing software include:\n GVGP could potentially be used to create real video game AI automatically, as well as \"to test game environments, including those created automatically using procedural content generation and to find potential loopholes in the gameplay that a human player could exploit\".[7] GVGP has also been used to generate game rules, and estimate a game's quality based on Relative Algorithm Performance Profiles (RAPP), which compare the skill differentiation that a game allows between good AI and bad AI.[42]\n The General Video Game AI Competition (GVGAI) has been running since 2014. In this competition, two-dimensional video games similar to (and sometimes based on) 1980s-era arcade and console games are used instead of the board games used in the GGP competition. It has offered a way for researchers and practitioners to test and compare their best general video game playing algorithms. The competition has an associated software framework including a large number of games written in the Video Game Description Language (VGDL), which should not be confused with GDL and is a coding language using simple semantics and commands that can easily be parsed. One example for VGDL is PyVGDL developed in 2013.[6][24] The games used in GVGP are, for now, often 2-dimensional arcade games, as they are the simplest and easiest to quantify.[43] To simplify the process of creating an AI that can interpret video games, games for this purpose are written in VGDL manually.[clarification needed] VGDL can be used to describe a game specifically for procedural generation of levels, using Answer Set Programming (ASP) and an Evolutionary Algorithm (EA). GVGP can then be used to test the validity of procedural levels, as well as the difficulty or quality of levels based on how an agent performed.[44]\n Since GGP AI must be designed to play multiple games, its design cannot rely on algorithms created specifically for certain games. Instead, the AI must be designed using algorithms whose methods can be applied to a wide range of games. The AI must also be an ongoing process, that can adapt to its current state rather than the output of previous states. For this reason, open loop techniques are often most effective.[45]\n A popular method for developing GGP AI is the Monte Carlo tree search (MCTS) algorithm.[46] Often used together with the UCT method (Upper Confidence Bound applied to Trees), variations of MCTS have been proposed to better play certain games, as well as to make it compatible with video game playing.[47][48][49] Another variation of tree-search algorithms used is the Directed Breadth-first Search (DBS),[50] in which a child node to the current state is created for each available action, and visits each child ordered by highest average reward, until either the game ends or runs out of time.[51] In each tree-search method, the AI simulates potential actions and ranks each based on the average highest reward of each path, in terms of points earned.[46][51]\n In order to interact with games, algorithms must operate under the assumption that games all share common characteristics. In the book Half-Real: Video Games Between Real Worlds and Fictional Worlds, Jesper Juul gives the following definition of games: Games are based on rules, they have variable outcomes, different outcomes give different values, player effort influences outcomes, the player is attached to the outcomes, and the game has negotiable consequences.[52]  Using these assumptions, game playing AI can be created by quantifying the player input, the game outcomes, and how the various rules apply, and using algorithms to compute the most favorable path.[43]\n"
    },
    {
        "title": "Knowledge representation and reasoning",
        "url": "https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning",
        "content": "Knowledge representation (KR) aims to model information in a structured manner to formally represent it as knowledge in knowledge-based systems. Whereas knowledge representation and reasoning (KRR, KR&R, or KR²) also aims to understand, reason and interpret knowledge. KRR is widely used in the field of artificial intelligence (AI) with the goal to represent information about the world in a form that a computer system can use to solve complex tasks, such as diagnosing a medical condition or having a natural-language dialog. KR incorporates findings from psychology[1] about how humans solve problems and represent knowledge, in order to design formalisms that make complex systems easier to design and build. KRR also incorporates findings from logic to automate various kinds of reasoning.\n Examples of knowledge representation formalisms include vocabularies, thesaurus, semantic networks, axiom systems, frames, rules, logic programs, and ontologies. Examples of automated reasoning engines include inference engines, theorem provers, model generators, and classifiers.\n The earliest work in computerized knowledge representation was focused on general problem-solvers such as the General Problem Solver (GPS) system developed by Allen Newell and Herbert A. Simon in 1959 and the Advice Taker proposed by John McCarthy also in 1959. GPS featured data structures for planning and decomposition. The system would begin with a goal. It would then decompose that goal into sub-goals and then set out to construct strategies that could accomplish each subgoal. The Advisor Taker, on the other hand, proposed the use of the predicate calculus to represent common sense reasoning.\n Many of the early approaches to knowledge represention in Artificial Intelligence (AI) used graph representations and semantic networks, similar to knowledge graphs today. In such approaches, problem solving was a form of graph traversal[2] or path-finding, as in the A* search algorithm. Typical applications included robot plan-formation and game-playing.\n Other researchers focused on developing  automated theorem-provers for first-order logic, motivated by the use of mathematical logic to formalise mathematics and to automate the proof of mathematical theorems. A major step in this direction was the development of the resolution method by John Alan Robinson.\n In the meanwhile, John McCarthy and Pat Hayes developed the situation calculus as a logical representation of common sense knowledge about the laws of cause and effect. Cordell Green, in turn, showed how to do robot plan-formation by applying resolution to the situation calculus. He also showed how to use resolution for question-answering and automatic programming.[3]\n In contrast, researchers at Massachusetts Institute of Technology (MIT) rejected the resolution uniform proof procedure paradigm and advocated the procedural embedding of knowledge instead.[4] The resulting conflict between the use of logical representations and the use of procedural representations was resolved in the early 1970s with the development of logic programming and Prolog, using SLD resolution to treat Horn clauses as goal-reduction procedures.\n The early development of logic programming was largely a European phenomenon. In North America, AI researchers such as Ed Feigenbaum and Frederick Hayes-Roth advocated the representation of domain-specific knowledge rather than general-purpose reasoning.[5]\n These efforts led to the cognitive revolution in psychology and to the phase of AI focused on knowledge representation that resulted in expert systems in the 1970s and 80s, production systems, frame languages, etc. Rather than general problem solvers, AI changed its focus to expert systems that could match human competence on a specific task, such as medical diagnosis.[6]\n Expert systems gave us the terminology still in use today where AI systems are divided into a knowledge base, which includes facts and rules about a problem domain, and an inference engine, which applies the knowledge in the knowledge base to answer questions and solve problems in the domain. In these early systems the facts in the knowledge base tended to be a fairly flat structure, essentially assertions about the values of variables used by the rules.[7]\n Meanwhile, Marvin Minsky developed the concept of frame in the mid-1970s.[8] A frame is similar to an object class: It is an abstract description of a category describing things in the world, problems, and potential solutions. Frames were originally used on systems geared toward human interaction, e.g. understanding natural language and the social settings in which various default expectations such as ordering food in a restaurant narrow the search space and allow the system to choose appropriate responses to dynamic situations.\n It was not long before the frame communities and the rule-based researchers realized that there was a synergy between their approaches. Frames were good for representing the real world, described as classes, subclasses, slots (data values) with various constraints on possible values. Rules were good for representing and utilizing complex logic such as the process to make a medical diagnosis. Integrated systems were developed that combined frames and rules. One of the most powerful and well known was the 1983 Knowledge Engineering Environment (KEE) from Intellicorp. KEE had a complete rule engine with forward and backward chaining. It also had a complete frame-based knowledge base with triggers, slots (data values), inheritance, and message passing. Although message passing originated in the object-oriented community rather than AI it was quickly embraced by AI researchers as well in environments such as KEE and in the operating systems for Lisp machines from Symbolics, Xerox, and Texas Instruments.[9]\n The integration of frames, rules, and object-oriented programming was significantly driven by commercial ventures such as KEE and Symbolics spun off from various research projects. At the same time, there was another strain of research that was less commercially focused and was driven by mathematical logic and automated theorem proving.[citation needed] One of the most influential languages in this research was the KL-ONE language of the mid-'80s. KL-ONE was a frame language that had a rigorous semantics, formal definitions for concepts such as an Is-A relation.[10] KL-ONE and languages that were influenced by it such as Loom had an automated reasoning engine that was based on formal logic rather than on IF-THEN rules. This reasoner is called the classifier. A classifier can analyze a set of declarations and infer new assertions, for example, redefine a class to be a subclass or superclass of some other class that wasn't formally specified. In this way the classifier can function as an inference engine, deducing new facts from an existing knowledge base. The classifier can also provide consistency checking on a knowledge base (which in the case of KL-ONE languages is also referred to as an Ontology).[11]\n Another area of knowledge representation research was the problem of common-sense reasoning. One of the first realizations learned from trying to make software that can function with human natural language was that humans regularly draw on an extensive foundation of knowledge about the real world that we simply take for granted but that is not at all obvious to an artificial agent, such as basic principles of common-sense physics, causality, intentions, etc. An example is the frame problem, that in an event driven logic there need to be axioms that state things maintain position from one moment to the next unless they are moved by some external force. In order to make a true artificial intelligence agent that can converse with humans using natural language and can process basic statements and questions about the world, it is essential to represent this kind of knowledge.[12] In addition to McCarthy and Hayes' situation calculus, one of the most ambitious programs to tackle this problem was Doug Lenat's Cyc project. Cyc established its own Frame language and had large numbers of analysts document various areas of common-sense reasoning in that language. The knowledge recorded in Cyc included common-sense models of time, causality, physics, intentions, and many others.[13]\n The starting point for knowledge representation is the knowledge representation hypothesis first formalized by Brian C. Smith in 1985:[14]\n Any mechanically embodied intelligent process will be comprised of structural ingredients that a) we as external observers naturally take to represent a propositional account of the knowledge that the overall process exhibits, and b) independent of such external semantic attribution, play a formal but causal and essential role in engendering the behavior that manifests that knowledge. One of the most active areas of knowledge representation research is the Semantic Web.[citation needed] The Semantic Web seeks to add a layer of semantics (meaning) on top of the current Internet. Rather than indexing web sites and pages via keywords, the Semantic Web creates large ontologies of concepts. Searching for a concept will be more effective than traditional text only searches. Frame languages and automatic classification play a big part in the vision for the future Semantic Web. The automatic classification gives developers technology to provide order on a constantly evolving network of knowledge. Defining ontologies that are static and incapable of evolving on the fly would be very limiting for Internet-based systems. The classifier technology provides the ability to deal with the dynamic environment of the Internet.\n Recent projects funded primarily by the Defense Advanced Research Projects Agency (DARPA) have integrated frame languages and classifiers with markup languages based on XML. The Resource Description Framework (RDF) provides the basic capability to define classes, subclasses, and properties of objects. The Web Ontology Language (OWL) provides additional levels of semantics and enables integration with classification engines.[15][16]\n Knowledge-representation is a field of artificial intelligence that focuses on designing computer representations that capture information about the world that can be used for solving complex problems.\n The justification for knowledge representation is that conventional procedural code is not the best formalism to use to solve complex problems. Knowledge representation makes complex software easier to define and maintain than procedural code and can be used in expert systems.\n For example, talking to experts in terms of business rules rather than code lessens the semantic gap between users and developers and makes development of complex systems more practical.\n Knowledge representation goes hand in hand with automated reasoning because one of the main purposes of explicitly representing knowledge is to be able to reason about that knowledge, to make inferences, assert new knowledge, etc. Virtually all knowledge representation languages have a reasoning or inference engine as part of the system.[17]\n A key trade-off in the design of knowledge representation formalisms is that between expressivity and tractability.[18] First Order Logic (FOL), with its high expressive power and ability to formalise much of mathematics, is a standard for comparing the expressibility of  knowledge representation languages.\n Arguably, FOL has two drawbacks as a knowledge representation formalism in its own right, namely ease of use and efficiency of implementation. Firstly, because of its high expressive power, FOL allows many ways of expressing the same information, and this can make it hard for users to formalise or even to understand knowledge expressed in complex, mathematically-oriented ways. Secondly, because of its complex proof procedures, it can be difficult for users to understand complex proofs and explanations, and it can be hard for implementations to be efficient. As a consequence, unrestricted FOL can be intimidating for many software developers.\n One of the key discoveries of AI research in the 1970s was that languages that do not have the full expressive power of FOL can still provide close to the same expressive power of FOL, but can be easier for both the average developer and for the computer to understand. Many of the early AI knowledge representation formalisms, from databases to semantic nets to production systems, can be viewed as making various design decisions about how to balance expressive power with naturalness of expression and efficiency.[19] In particular, this balancing act was a driving motivation for the development of IF-THEN rules in rule-based expert systems. \n A similar balancing act was also a motivation for the development of  logic programming (LP) and the logic programming language Prolog. Logic programs have a rule-based syntax, which is easily confused with the IF-THEN syntax of production rules. But logic programs have a well-defined logical semantics, whereas production systems do not.\n The earliest form of logic programming was based on the Horn clause subset of FOL. But later extensions of LP included the negation as failure inference rule, which turns LP into a non-monotonic logic for default reasoning. The resulting extended semantics of LP is a variation of the standard semantics of Horn clauses and FOL, and is a form of database semantics, [20] which includes the unique name assumption and a form of closed world assumption. These assumptions are much harder to state and reason with explicitly using the standard semantics of FOL.\n In a key 1993 paper on the topic, Randall Davis of MIT outlined five distinct roles to analyze a knowledge representation framework:[21]\n Knowledge representation and reasoning are a key enabling technology for the Semantic Web. Languages based on the Frame model with automatic classification provide a layer of semantics on top of the existing Internet. Rather than searching via text strings as is typical today, it will be possible to define logical queries and find pages that map to those queries.[15] The automated reasoning component in these systems is an engine known as the classifier. Classifiers focus on the subsumption relations in a knowledge base rather than rules. A classifier can infer new classes and dynamically change the ontology as new information becomes available. This capability is ideal for the ever-changing and evolving information space of the Internet.[22]\n The Semantic Web integrates concepts from knowledge representation and reasoning with markup languages based on XML.  The Resource Description Framework (RDF) provides the basic capabilities to define knowledge-based objects on the Internet with basic features such as Is-A relations and object properties. The Web Ontology Language (OWL) adds additional semantics and integrates with automatic classification reasoners.[16]\n In 1985, Ron Brachman categorized the core issues for knowledge representation as follows:[23]\n In the early years of knowledge-based systems the knowledge-bases were fairly small. The knowledge-bases that were meant to actually solve real problems rather than do proof of concept demonstrations needed to focus on well defined problems. So for example, not just medical diagnosis as a whole topic, but medical diagnosis of certain kinds of diseases.\n As knowledge-based technology scaled up, the need for larger knowledge bases and for modular knowledge bases that could communicate and integrate with each other became apparent. This gave rise to the discipline of ontology engineering, designing and building large knowledge bases that could be used by multiple projects. One of the leading research projects in this area was the Cyc project. Cyc was an attempt to build a huge encyclopedic knowledge base that would contain not just expert knowledge but common-sense knowledge. In designing an artificial intelligence agent, it was soon realized that representing common-sense knowledge, knowledge that humans simply take for granted, was essential to make an AI that could interact with humans using natural language. Cyc was meant to address this problem. The language they defined was known as CycL.\n After CycL, a number of ontology languages have been developed. Most are declarative languages, and are either frame languages, or are based on first-order logic. Modularity—the ability to define boundaries around specific domains and problem spaces—is essential for these languages because as stated by Tom Gruber, \"Every ontology is a treaty–a social agreement among people with common motive in sharing.\" There are always many competing and differing views that make any general-purpose ontology impossible. A general-purpose ontology would have to be applicable in any domain and different areas of knowledge need to be unified.[27]\n There is a long history of work attempting to build ontologies for a variety of task domains, e.g., an ontology for liquids,[28] the lumped element model widely used in representing electronic circuits (e.g.[29]), as well as ontologies for time, belief, and even programming itself. Each of these offers a way to see some part of the world.\n The lumped element model, for instance, suggests that we think of circuits in terms of components with connections between them, with signals flowing instantaneously along the connections. This is a useful view, but not the only possible one. A different ontology arises if we need to attend to the electrodynamics in the device: Here signals propagate at finite speed and an object (like a resistor) that was previously viewed as a single component with an I/O behavior may now have to be thought of as an extended medium through which an electromagnetic wave flows.\n Ontologies can of course be written down in a wide variety of languages and notations (e.g., logic, LISP, etc.); the essential information is not the form of that language but the content, i.e., the set of concepts offered as a way of thinking about the world. Simply put, the important part is notions like connections and components, not the choice between writing them as predicates or LISP constructs.\n The commitment made selecting one or another ontology can produce a sharply different view of the task at hand. Consider the difference that arises in selecting the lumped element view of a circuit rather than the electrodynamic view of the same device. As a second example, medical diagnosis viewed in terms of rules (e.g., MYCIN) looks substantially different from the same task viewed in terms of frames (e.g., INTERNIST). Where MYCIN sees the medical world as made up of empirical associations connecting symptom to disease, INTERNIST sees a set of prototypes, in particular prototypical diseases, to be matched against the case at hand.\n"
    },
    {
        "title": "Natural language processing",
        "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
        "content": "Natural language processing (NLP) is a subfield of computer science and especially artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics. Typically data is collected in text corpora, using either rule-based, statistical or neural-based approaches in machine learning and deep learning.\n Major tasks in natural language processing are speech recognition, text classification, natural-language understanding, and natural-language generation.\n Natural language processing has its roots in the 1950s.[1] Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.\n The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\n Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.  This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[8]\n In 2003, word n-gram model, at the time the best statistical algorithm, was outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with co-authors.[9]\n In 2010, Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling,[10] and in the following years he went on to develop Word2vec. In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques[11][12] can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling[13] and parsing.[14][15] This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care[16] or protect patient privacy.[17]\n Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular:[18][19] such as by writing grammars or devising heuristic rules for stemming.\n Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: \n Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of LLMs in 2023. \n Before that they were commonly used:\n In the late 1980s and mid-1990s, the statistical approach ended a period of AI winter, which was caused by the inefficiencies of the rule-based approaches.[20][21]\n The earliest decision trees, producing systems of hard if–then rules, were still very similar to the old rule-based approaches.\nOnly the introduction of hidden Markov models, applied to part-of-speech tagging, announced the end of the old rule-based approach.\n A major drawback of statistical methods is that they require elaborate feature engineering. Since 2015,[22] the statistical approach has been replaced by the neural networks approach, using semantic networks[23] and word embeddings to capture semantic properties of words.  \n Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) are not needed anymore. \n Neural machine translation, based on then-newly invented sequence-to-sequence transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for statistical machine translation.\n The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.\n Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.\n Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:[46]\n Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).\n Cognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.\"[47] Cognitive science is the interdisciplinary, scientific study of the mind and its processes.[48] Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics.[49] Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies.\n As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[50] with two defining aspects:\n Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar,[53] functional grammar,[54] construction grammar,[55] computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences[56] of the ACL). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of \"cognitive AI\".[57] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit)[58] and developments in artificial intelligence, specifically tools and technologies using large language model approaches[59] and new directions in artificial general intelligence based on the free energy principle[60] by British neuroscientist and theoretician at University College London Karl J. Friston.\n"
    },
    {
        "title": "Robotics",
        "url": "https://en.wikipedia.org/wiki/Robotics",
        "content": "\n Robotics is the interdisciplinary study and practice of the design, construction, operation, and use of robots.[1]\n Within mechanical engineering, robotics is the design and construction of the physical structures of robots, while in computer science, robotics focuses on robotic automation algorithms. Other disciplines contributing to robotics include electrical, control, software, information, electronic, telecommunication, computer, mechatronic, and materials engineering.\n The goal of most robotics is to design machines that can help and assist humans. Many robots are built to do jobs that are hazardous to people, such as finding survivors in unstable ruins, and exploring space, mines and shipwrecks. Others replace people in jobs that are boring, repetitive, or unpleasant, such as cleaning, monitoring, transporting, and assembling. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes.\n Robotics usually combines three aspects of design work to create robot systems:\n As many robots are designed for specific tasks, this method of classification becomes more relevant. For example, many robots are designed for assembly work, which may not be readily adaptable for other applications. They are termed \"assembly robots\". For seam welding, some suppliers provide complete welding systems with the robot i.e. the welding equipment along with other material handling facilities like turntables, etc. as an integrated unit. Such an integrated robotic system is called a \"welding robot\" even though its discrete manipulator unit could be adapted to a variety of tasks. Some robots are specifically designed for heavy load manipulation, and are labeled as \"heavy-duty robots\".[4]\n Current and potential applications include:\n At present, mostly (lead–acid) batteries are used as a power source. Many different types of batteries can be used as a power source for robots. They range from lead–acid batteries, which are safe and have relatively long shelf lives but are rather heavy compared to silver–cadmium batteries which are much smaller in volume and are currently much more expensive. Designing a battery-powered robot needs to take into account factors such as safety, cycle lifetime, and weight. Generators, often some type of internal combustion engine, can also be used. However, such designs are often mechanically complex and need fuel, require heat dissipation, and are relatively heavy. A tether connecting the robot to a power supply would remove the power supply from the robot entirely. This has the advantage of saving weight and space by moving all power generation and storage components elsewhere. However, this design does come with the drawback of constantly having a cable connected to the robot, which can be difficult to manage.[16] \nPotential power sources could be:\n Actuators are the \"muscles\" of a robot, the parts which convert stored energy into movement.[17] By far the most popular actuators are electric motors that rotate a wheel or gear, and linear actuators that control industrial robots in factories. There are some recent advances in alternative types of actuators, powered by electricity, chemicals, or compressed air.\n The vast majority of robots use electric motors, often brushed and brushless DC motors in portable robots or AC motors in industrial robots and CNC machines. These motors are often preferred in systems with lighter loads, and where the predominant form of motion is rotational.\n Various types of linear actuators move in and out instead of by spinning, and often have quicker direction changes, particularly when very large forces are needed such as with industrial robotics. They are typically powered by compressed and oxidized air (pneumatic actuator) or an oil (hydraulic actuator) Linear actuators can also be powered by electricity which usually consists of a motor and a leadscrew. Another common type is a mechanical linear actuator such as a rack and pinion on a car.\n Series elastic actuation (SEA) relies on the idea of introducing intentional elasticity between the motor actuator and the load for robust force control. Due to the resultant lower reflected inertia, series elastic actuation improves safety when a robot interacts with the environment (e.g., humans or workpieces) or during collisions.[18] Furthermore, it also provides energy efficiency and shock absorption (mechanical filtering) while reducing excessive wear on the transmission and other mechanical components. This approach has successfully been employed in various robots, particularly advanced manufacturing robots[19] and walking humanoid robots.[20][21]\n The controller design of a series elastic actuator is most often performed within the passivity framework as it ensures the safety of interaction with unstructured environments.[22] Despite its remarkable stability and robustness, this framework suffers from the stringent limitations imposed on the controller which may trade-off performance. The reader is referred to the following survey which summarizes the common controller architectures for SEA along with the corresponding sufficient passivity conditions.[23] One recent study has derived the necessary and sufficient passivity conditions for one of the most common impedance control architectures, namely velocity-sourced SEA.[24] This work is of particular importance as it drives the non-conservative passivity bounds in an SEA scheme for the first time which allows a larger selection of control gains.\n Pneumatic artificial muscles also known as air muscles, are special tubes that expand (typically up to 42%) when air is forced inside them. They are used in some robot applications.[25][26][27]\n Muscle wire, also known as shape memory alloy, is a material that contracts (under 5%) when electricity is applied. They have been used for some small robot applications.[28][29]\n EAPs or EPAMs are a plastic material that can contract substantially (up to 380% activation strain) from electricity, and have been used in facial muscles and arms of humanoid robots,[30] and to enable new robots to float,[31] fly, swim or walk.[32]\n Recent alternatives to DC motors are piezo motors or ultrasonic motors. These work on a fundamentally different principle, whereby tiny piezoceramic elements, vibrating many thousands of times per second, cause linear or rotary motion. There are different mechanisms of operation; one type uses the vibration of the piezo elements to step the motor in a circle or a straight line.[33] Another type uses the piezo elements to cause a nut to vibrate or to drive a screw. The advantages of these motors are nanometer resolution, speed, and available force for their size.[34] These motors are already available commercially and being used on some robots.[35][36]\n Elastic nanotubes are a promising artificial muscle technology in early-stage experimental development. The absence of defects in carbon nanotubes enables these filaments to deform elastically by several percent, with energy storage levels of perhaps 10 J/cm3 for metal nanotubes. Human biceps could be replaced with an 8 mm diameter wire of this material. Such compact \"muscle\" might allow future robots to outrun and outjump humans.[37]\n Sensors allow robots to receive information about a certain measurement of the environment, or internal components. This is essential for robots to perform their tasks, and act upon any changes in the environment to calculate the appropriate response. They are used for various forms of measurements, to give the robots warnings about safety or malfunctions, and to provide real-time information about the task it is performing.\n Current robotic and prosthetic hands receive far less tactile information than the human hand. Recent research has developed a tactile sensor array that mimics the mechanical properties and touch receptors of human fingertips.[38][39] The sensor array is constructed as a rigid core surrounded by conductive fluid contained by an elastomeric skin. Electrodes are mounted on the surface of the rigid core and are connected to an impedance-measuring device within the core. When the artificial skin touches an object the fluid path around the electrodes is deformed, producing impedance changes that map the forces received from the object. The researchers expect that an important function of such artificial fingertips will be adjusting the robotic grip on held objects.\n Scientists from several European countries and Israel developed a prosthetic hand in 2009, called SmartHand, which functions like a real one —allowing patients to write with it, type on a keyboard, play piano, and perform other fine movements. The prosthesis has sensors which enable the patient to sense real feelings in its fingertips.[40]\n Other common forms of sensing in robotics use lidar, radar, and sonar.[41] Lidar measures the distance to a target by illuminating the target with laser light and measuring the reflected light with a sensor. Radar uses radio waves to determine the range, angle, or velocity of objects. Sonar uses sound propagation to navigate, communicate with or detect objects on or under the surface of the water.\n One of the most common types of end-effectors are \"grippers\". In its simplest manifestation, it consists of just two fingers that can open and close to pick up and let go of a range of small objects. Fingers can, for example, be made of a chain with a metal wire running through it.[42] Hands that resemble and work more like a human hand include the Shadow Hand and the Robonaut hand.[43] Hands that are of a mid-level complexity include the Delft hand.[44][45] Mechanical grippers can come in various types, including friction and encompassing jaws. Friction jaws use all the force of the gripper to hold the object in place using friction. Encompassing jaws cradle the object in place, using less friction.\n Suction end-effectors, powered by vacuum generators, are very simple astrictive[46] devices that can hold very large loads provided the prehension surface is smooth enough to ensure suction.\n Pick and place robots for electronic components and for large objects like car windscreens, often use very simple vacuum end-effectors.\n Suction is a highly used type of end-effector in industry, in part because the natural compliance of soft suction end-effectors can enable a robot to be more robust in the presence of imperfect robotic perception. As an example: consider the case of a robot vision system that estimates the position of a water bottle but has 1 centimeter of error. While this may cause a rigid mechanical gripper to puncture the water bottle, the soft suction end-effector may just bend slightly and conform to the shape of the water bottle surface.\n Some advanced robots are beginning to use fully humanoid hands, like the Shadow Hand, MANUS,[47] and the Schunk hand.[48] They have powerful robot dexterity intelligence (RDI), with as many as 20 degrees of freedom and hundreds of tactile sensors.[49]\n The mechanical structure of a robot must be controlled to perform tasks.[50] The control of a robot involves three distinct phases – perception, processing, and action (robotic paradigms).[51] Sensors give information about the environment or the robot itself (e.g. the position of its joints or its end effector). This information is then processed to be stored or transmitted and to calculate the appropriate signals to the actuators (motors), which move the mechanical structure to achieve the required co-ordinated motion or force actions.\n The processing phase can range in complexity. At a reactive level, it may translate raw sensor information directly into actuator commands (e.g. firing motor power electronic gates based directly upon encoder feedback signals to achieve the required torque/velocity of the shaft). Sensor fusion and internal models may first be used to estimate parameters of interest (e.g. the position of the robot's gripper) from noisy sensor data. An immediate task (such as moving the gripper in a certain direction until an object is detected with a proximity sensor) is sometimes inferred from these estimates. Techniques from control theory are generally used to convert the higher-level tasks into individual commands that drive the actuators, most often using kinematic and dynamic models of the mechanical structure.[50][51][52]\n At longer time scales or with more sophisticated tasks, the robot may need to build and reason with a \"cognitive\" model. Cognitive models try to represent the robot, the world, and how the two interact. Pattern recognition and computer vision can be used to track objects.[50] Mapping techniques can be used to build maps of the world. Finally, motion planning and other artificial intelligence techniques may be used to figure out how to act. For example, a planner may figure out how to achieve a task without hitting obstacles, falling over, etc.\n Modern commercial robotic control systems are highly complex, integrate multiple sensors and effectors, have many interacting degrees-of-freedom (DOF) and require operator interfaces, programming tools and real-time capabilities.[51] They are oftentimes interconnected to wider communication networks and in many cases are now both IoT-enabled and mobile.[53] Progress towards open architecture, layered, user-friendly and 'intelligent' sensor-based interconnected robots has emerged from earlier concepts related to Flexible Manufacturing Systems (FMS), and several 'open or 'hybrid' reference architectures exist which assist developers of robot control software and hardware to move beyond traditional, earlier notions of 'closed' robot control systems have been proposed.[52] Open architecture controllers are said to be better able to meet the growing requirements of a wide range of robot users, including system developers, end users and research scientists, and are better positioned to deliver the advanced robotic concepts related to Industry 4.0.[52] In addition to utilizing many established features of robot controllers, such as position, velocity and force control of end effectors, they also enable IoT interconnection and the implementation of more advanced sensor fusion and control techniques, including adaptive control, Fuzzy control and Artificial Neural Network (ANN)-based control.[52] When implemented in real-time, such techniques can potentially improve the stability and performance of robots operating in unknown or uncertain environments by enabling the control systems to learn and adapt to environmental changes.[54] There are several examples of reference architectures for robot controllers, and also examples of successful implementations of actual robot controllers developed from them. One example of a generic reference architecture and associated interconnected, open-architecture robot and controller implementation was used in a number of research and development studies, including prototype implementation of novel advanced and intelligent control and environment mapping methods in real-time.[54][55]\n A definition of robotic manipulation has been provided by Matt Mason as: \"manipulation refers to an agent's control of its environment through selective contact\".[56]\n Robots need to manipulate objects; pick up, modify, destroy, move or otherwise have an effect. Thus the functional end of a robot arm intended to make the effect (whether a hand, or tool) are often referred to as end effectors,[57] while the \"arm\" is referred to as a manipulator.[58] Most robot arms have replaceable end-effectors, each allowing them to perform some small range of tasks. Some have a fixed manipulator that cannot be replaced, while a few have one very general-purpose manipulator, for example, a humanoid hand.[59]\n For simplicity, most mobile robots have four wheels or a number of continuous tracks. Some researchers have tried to create more complex wheeled robots with only one or two wheels. These can have certain advantages such as greater efficiency and reduced parts, as well as allowing a robot to navigate in confined places that a four-wheeled robot would not be able to.\n Balancing robots generally use a gyroscope to detect how much a robot is falling and then drive the wheels proportionally in the same direction, to counterbalance the fall at hundreds of times per second, based on the dynamics of an inverted pendulum.[60] Many different balancing robots have been designed.[61] While the Segway is not commonly thought of as a robot, it can be thought of as a component of a robot, when used as such Segway refer to them as RMP (Robotic Mobility Platform). An example of this use has been as NASA's Robonaut that has been mounted on a Segway.[62]\n A one-wheeled balancing robot is an extension of a two-wheeled balancing robot so that it can move in any 2D direction using a round ball as its only wheel. Several one-wheeled balancing robots have been designed recently, such as Carnegie Mellon University's \"Ballbot\" which is the approximate height and width of a person, and Tohoku Gakuin University's \"BallIP\".[63] Because of the long, thin shape and ability to maneuver in tight spaces, they have the potential to function better than other robots in environments with people.[64]\n Several attempts have been made in robots that are completely inside a spherical ball, either by spinning a weight inside the ball,[65][66] or by rotating the outer shells of the sphere.[67][68] These have also been referred to as an orb bot[69] or a ball bot.[70][71]\n Using six wheels instead of four wheels can give better traction or grip in outdoor terrain such as on rocky dirt or grass.\n Tracks provide even more traction than a six-wheeled robot. Tracked wheels behave as if they were made of hundreds of wheels, therefore are very common for outdoor off-road robots, where the robot must drive on very rough terrain. However, they are difficult to use indoors such as on carpets and smooth floors. Examples include NASA's Urban Robot \"Urbie\".[72]\n Walking is a difficult and dynamic problem to solve. Several robots have been made which can walk reliably on two legs, however, none have yet been made which are as robust as a human. There has been much study on human-inspired walking, such as AMBER lab which was established in 2008 by the Mechanical Engineering Department at Texas A&M University.[73] Many other robots have been built that walk on more than two legs, due to these robots being significantly easier to construct.[74][75] Walking robots can be used for uneven terrains, which would provide better mobility and energy efficiency than other locomotion methods. Typically, robots on two legs can walk well on flat floors and can occasionally walk up stairs. None can walk over rocky, uneven terrain. Some of the methods which have been tried are:\n The zero moment point (ZMP) is the algorithm used by robots such as Honda's ASIMO. The robot's onboard computer tries to keep the total inertial forces (the combination of Earth's gravity and the acceleration and deceleration of walking), exactly opposed by the floor reaction force (the force of the floor pushing back on the robot's foot). In this way, the two forces cancel out, leaving no moment (force causing the robot to rotate and fall over).[76] However, this is not exactly how a human walks, and the difference is obvious to human observers, some of whom have pointed out that ASIMO walks as if it needs the lavatory.[77][78][79] ASIMO's walking algorithm is not static, and some dynamic balancing is used (see below). However, it still requires a smooth surface to walk on.\n Several robots, built in the 1980s by Marc Raibert at the MIT Leg Laboratory, successfully demonstrated very dynamic walking. Initially, a robot with only one leg, and a very small foot could stay upright simply by hopping. The movement is the same as that of a person on a pogo stick. As the robot falls to one side, it would jump slightly in that direction, in order to catch itself.[80] Soon, the algorithm was generalised to two and four legs. A bipedal robot was demonstrated running and even performing somersaults.[81] A quadruped was also demonstrated which could trot, run, pace, and bound.[82] For a full list of these robots, see the MIT Leg Lab Robots page.[83]\n A more advanced way for a robot to walk is by using a dynamic balancing algorithm, which is potentially more robust than the Zero Moment Point technique, as it constantly monitors the robot's motion, and places the feet in order to maintain stability.[84] This technique was recently demonstrated by Anybots' Dexter Robot,[85] which is so stable, it can even jump.[86] Another example is the TU Delft Flame.\n Perhaps the most promising approach uses passive dynamics where the momentum of swinging limbs is used for greater efficiency. It has been shown that totally unpowered humanoid mechanisms can walk down a gentle slope, using only gravity to propel themselves. Using this technique, a robot need only supply a small amount of motor power to walk along a flat surface or a little more to walk up a hill. This technique promises to make walking robots at least ten times more efficient than ZMP walkers, like ASIMO.[87][88]\n A modern passenger airliner is essentially a flying robot, with two humans to manage it. The autopilot can control the plane for each stage of the journey, including takeoff, normal flight, and even landing.[89] Other flying robots are uninhabited and are known as unmanned aerial vehicles (UAVs). They can be smaller and lighter without a human pilot on board, and fly into dangerous territory for military surveillance missions. Some can even fire on targets under command. UAVs are also being developed which can fire on targets automatically, without the need for a command from a human. Other flying robots include cruise missiles, the Entomopter, and the Epson micro helicopter robot. Robots such as the Air Penguin, Air Ray, and Air Jelly have lighter-than-air bodies, are propelled by paddles, and are guided by sonar.\n BFRs take inspiration from flying mammals, birds, or insects. BFRs can have flapping wings, which generate the lift and thrust, or they can be propeller actuated. BFRs with flapping wings have increased stroke efficiencies, increased maneuverability, and reduced energy consumption in comparison to propeller actuated BFRs.[90] Mammal and bird inspired BFRs share similar flight characteristics and design considerations. For instance, both mammal and bird inspired BFRs minimize edge fluttering and pressure-induced wingtip curl by increasing the rigidity of the wing edge and wingtips. Mammal and insect inspired BFRs can be impact resistant, making them useful in cluttered environments.\n Mammal inspired BFRs typically take inspiration from bats, but the flying squirrel has also inspired a prototype.[91] Examples of bat inspired BFRs include Bat Bot[92] and the DALER.[93] Mammal inspired BFRs can be designed to be multi-modal; therefore, they're capable of both flight and terrestrial movement. To reduce the impact of landing, shock absorbers can be implemented along the wings.[93] Alternatively, the BFR can pitch up and increase the amount of drag it experiences.[91] By increasing the drag force, the BFR will decelerate and minimize the impact upon grounding. Different land gait patterns can also be implemented.[91]\n Bird inspired BFRs can take inspiration from raptors, gulls, and everything in-between. Bird inspired BFRs can be feathered to increase the angle of attack range over which the prototype can operate before stalling.[94] The wings of bird inspired BFRs allow for in-plane deformation, and the in-plane wing deformation can be adjusted to maximize flight efficiency depending on the flight gait.[94] An example of a raptor inspired BFR is the prototype by Savastano et al.[95] The prototype has fully deformable flapping wings and is capable of carrying a payload of up to 0.8 kg while performing a parabolic climb, steep descent, and rapid recovery. The gull inspired prototype by Grant et al. accurately mimics the elbow and wrist rotation of gulls, and they find that lift generation is maximized when the elbow and wrist deformations are opposite but equal.[96]\n Insect inspired BFRs typically take inspiration from beetles or dragonflies. An example of a beetle inspired BFR is the prototype by Phan and Park,[97] and a dragonfly inspired BFR is the prototype by Hu et al.[98] The flapping frequency of insect inspired BFRs are much higher than those of other BFRs; this is because of the aerodynamics of insect flight.[99] Insect inspired BFRs are much smaller than those inspired by mammals or birds, so they are more suitable for dense environments.\n A class of robots that are biologically inspired, but which do not attempt to mimic biology, are creations such as the Entomopter. Funded by DARPA, NASA, the United States Air Force, and the Georgia Tech Research Institute and patented by Prof. Robert C. Michelson for covert terrestrial missions as well as flight in the lower Mars atmosphere, the Entomopter flight propulsion system uses low Reynolds number wings similar to those of the hawk moth (Manduca sexta), but flaps them in a non-traditional \"opposed x-wing fashion\" while \"blowing\" the surface to enhance lift based on the Coandă effect as well as to control vehicle attitude and direction. Waste gas from the propulsion system not only facilitates the blown wing aerodynamics, but also serves to create ultrasonic emissions like that of a Bat for obstacle avoidance. The Entomopter and other biologically-inspired robots leverage features of biological systems, but do not attempt to create mechanical analogs.\n Several snake robots have been successfully developed. Mimicking the way real snakes move, these robots can navigate very confined spaces, meaning they may one day be used to search for people trapped in collapsed buildings.[100] The Japanese ACM-R5 snake robot[101] can even navigate both on land and in water.[102]\n A small number of skating robots have been developed, one of which is a multi-mode walking and skating device. It has four legs, with unpowered wheels, which can either step or roll.[103] Another robot, Plen, can use a miniature skateboard or roller-skates, and skate across a desktop.[104]\n Several different approaches have been used to develop robots that have the ability to climb vertical surfaces. One approach mimics the movements of a human climber on a wall with protrusions; adjusting the center of mass and moving each limb in turn to gain leverage. An example of this is Capuchin,[105] built by Ruixiang Zhang at Stanford University, California. Another approach uses the specialized toe pad method of wall-climbing geckoes, which can run on smooth surfaces such as vertical glass. Examples of this approach include Wallbot[106] and Stickybot.[107]\n China's Technology Daily reported on 15 November 2008, that Li Hiu Yeung and his research group of New Concept Aircraft (Zhuhai) Co., Ltd. had successfully developed a bionic gecko robot named \"Speedy Freelander\". According to Yeung, the gecko robot could rapidly climb up and down a variety of building walls, navigate through ground and wall fissures, and walk upside-down on the ceiling. It was also able to adapt to the surfaces of smooth glass, rough, sticky or dusty walls as well as various types of metallic materials. It could also identify and circumvent obstacles automatically. Its flexibility and speed were comparable to a natural gecko. A third approach is to mimic the motion of a snake climbing a pole.[41]\n It is calculated that when swimming some fish can achieve a propulsive efficiency greater than 90%.[108] Furthermore, they can accelerate and maneuver far better than any man-made boat or submarine, and produce less noise and water disturbance. Therefore, many researchers studying underwater robots would like to copy this type of locomotion.[109] Notable examples are the Robotic Fish G9,[110] and Robot Tuna built to analyze and mathematically model thunniform motion.[111] The Aqua Penguin,[112] copies the streamlined shape and propulsion by front \"flippers\" of penguins. The Aqua Ray and Aqua Jelly emulate the locomotion of manta ray, and jellyfish, respectively.\n In 2014, iSplash-II was developed as the first robotic fish capable of outperforming real carangiform fish in terms of average maximum velocity (measured in body lengths/ second) and endurance, the duration that top speed is maintained.[113] This build attained swimming speeds of 11.6BL/s (i.e. 3.7 m/s).[114] The first build, iSplash-I (2014) was the first robotic platform to apply a full-body length carangiform swimming motion which was found to increase swimming speed by 27% over the traditional approach of a posterior confined waveform.[115]\n Sailboat robots have also been developed in order to make measurements at the surface of the ocean. A typical sailboat robot is Vaimos.[116] Since the propulsion of sailboat robots uses the wind, the energy of the batteries is only used for the computer, for the communication and for the actuators (to tune the rudder and the sail). If the robot is equipped with solar panels, the robot could theoretically navigate forever. The two main competitions of sailboat robots are WRSC, which takes place every year in Europe, and Sailbot.\n Control systems may also have varying levels of autonomy.\n Another classification takes into account the interaction between human control and the machine motions.\n Computer vision is the science and technology of machines that see. As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences and views from cameras.\n In most practical computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common.\n Computer vision systems rely on image sensors that detect electromagnetic radiation which is typically in the form of either visible light or infra-red light. The sensors are designed using solid-state physics. The process by which light propagates and reflects off surfaces is explained using optics. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Robots can also be equipped with multiple vision sensors to be better able to compute the sense of depth in the environment. Like human eyes, robots' \"eyes\" must also be able to focus on a particular area of interest, and also adjust to variations in light intensities.\n There is a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological system, at different levels of complexity. Also, some of the learning-based methods developed within computer vision have a background in biology.\n Though a significant percentage of robots in commission today are either human controlled or operate in a static environment, there is an increasing interest in robots that can operate autonomously in a dynamic environment. These robots require some combination of navigation hardware and software in order to traverse their environment. In particular, unforeseen events (e.g. people and other obstacles that are not stationary) can cause problems or collisions. Some highly advanced robots such as ASIMO and Meinü robot have particularly good robot navigation hardware and software. Also, self-controlled cars, Ernst Dickmanns' driverless car, and the entries in the DARPA Grand Challenge, are capable of sensing the environment well and subsequently making navigational decisions based on this information, including by a swarm of autonomous robots.[119] Most of these robots employ a GPS navigation device with waypoints, along with radar, sometimes combined with other sensory data such as lidar, video cameras, and inertial guidance systems for better navigation between waypoints.\n The state of the art in sensory intelligence for robots will have to progress through several orders of magnitude if we want the robots working in our homes to go beyond vacuum-cleaning the floors. If robots are to work effectively in homes and other non-industrial environments, the way they are instructed to perform their jobs, and especially how they will be told to stop will be of critical importance. The people who interact with them may have little or no training in robotics, and so any interface will need to be extremely intuitive. Science fiction authors also typically assume that robots will eventually be capable of communicating with humans through speech, gestures, and facial expressions, rather than a command-line interface. Although speech would be the most natural way for the human to communicate, it is unnatural for the robot. It will probably be a long time before robots interact as naturally as the fictional C-3PO, or Data of Star Trek, Next Generation. Even though the current state of robotics cannot meet the standards of these robots from science-fiction, robotic media characters (e.g., Wall-E, R2-D2) can elicit audience sympathies that increase people's willingness to accept actual robots in the future.[120] Acceptance of social robots is also likely to increase if people can meet a social robot under appropriate conditions. Studies have shown that interacting with a robot by looking at, touching, or even imagining interacting with the robot can reduce negative feelings that some people have about robots before interacting with them.[121] However, if pre-existing negative sentiments are especially strong, interacting with a robot can increase those negative feelings towards robots.[121]\n Interpreting the continuous flow of sounds coming from a human, in real time, is a difficult task for a computer, mostly because of the great variability of speech.[122] The same word, spoken by the same person may sound different depending on local acoustics, volume, the previous word, whether or not the speaker has a cold, etc.. It becomes even harder when the speaker has a different accent.[123] Nevertheless, great strides have been made in the field since Davis, Biddulph, and Balashek designed the first \"voice input system\" which recognized \"ten digits spoken by a single user with 100% accuracy\" in 1952.[124] Currently, the best systems can recognize continuous, natural speech, up to 160 words per minute, with an accuracy of 95%.[125] With the help of artificial intelligence, machines nowadays can use people's voice to identify their emotions such as satisfied or angry.[126]\n Other hurdles exist when allowing the robot to use voice for interacting with humans. For social reasons, synthetic voice proves suboptimal as a communication medium,[127] making it necessary to develop the emotional component of robotic voice through various techniques.[128][129] An advantage of diphonic branching is the emotion that the robot is programmed to project, can be carried on the voice tape, or phoneme, already pre-programmed onto the voice media. One of the earliest examples is a teaching robot named Leachim developed in 1974 by Michael J. Freeman.[130][131] Leachim was able to convert digital memory to rudimentary verbal speech on pre-recorded computer discs.[132] It was programmed to teach students in The Bronx, New York.[132]\n Facial expressions can provide rapid feedback on the progress of a dialog between two humans, and soon may be able to do the same for humans and robots. Robotic faces have been constructed by Hanson Robotics using their elastic polymer called Frubber, allowing a large number of facial expressions due to the elasticity of the rubber facial coating and embedded subsurface motors (servos).[133] The coating and servos are built on a metal skull. A robot should know how to approach a human, judging by their facial expression and body language. Whether the person is happy, frightened, or crazy-looking affects the type of interaction expected of the robot. Likewise, robots like Kismet and the more recent addition, Nexi[134] can produce a range of facial expressions, allowing it to have meaningful social exchanges with humans.[135]\n One can imagine, in the future, explaining to a robot chef how to make a pastry, or asking directions from a robot police officer. In both of these cases, making hand gestures would aid the verbal descriptions. In the first case, the robot would be recognizing gestures made by the human, and perhaps repeating them for confirmation. In the second case, the robot police officer would gesture to indicate \"down the road, then turn right\". It is likely that gestures will make up a part of the interaction between humans and robots.[136] A great many systems have been developed to recognize human hand gestures.[137]\n Proxemics is the study of personal space, and HRI systems may try to model and work with its concepts for human interactions.\n Artificial emotions can also be generated, composed of a sequence of facial expressions or gestures. As can be seen from the movie Final Fantasy: The Spirits Within, the programming of these artificial emotions is complex and requires a large amount of human observation. To simplify this programming in the movie, presets were created together with a special software program. This decreased the amount of time needed to make the film. These presets could possibly be transferred for use in real-life robots. An example of a robot with artificial emotions is Robin the Robot developed by an Armenian IT company Expper Technologies, which uses AI-based peer-to-peer interaction. Its main task is achieving emotional well-being, i.e. overcome stress and anxiety. Robin was trained to analyze facial expressions and use his face to display his emotions given the context. The robot has been tested by kids in US clinics, and observations show that Robin increased the appetite and cheerfulness of children after meeting and talking.[138]\n Many of the robots of science fiction have a personality, something which may or may not be desirable in the commercial robots of the future.[139] Nevertheless, researchers are trying to create robots which appear to have a personality:[140][141] i.e. they use sounds, facial expressions, and body language to try to convey an internal state, which may be joy, sadness, or fear. One commercial example is Pleo, a toy robot dinosaur, which can exhibit several apparent emotions.[142]\n Much of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robots, alternative ways to think about or design robots, and new ways to manufacture them. Other investigations, such as MIT's cyberflora project, are almost wholly academic.\n To describe the level of advancement of a robot, the term \"Generation Robots\" can be used. This term is coined by Professor Hans Moravec, Principal Research Scientist at the Carnegie Mellon University Robotics Institute in describing the near future evolution of robot technology. First-generation robots, Moravec predicted in 1997, should have an intellectual capacity comparable to perhaps a lizard and should become available by 2010. Because the first generation robot would be incapable of learning, however, Moravec predicts that the second generation robot would be an improvement over the first and become available by 2020, with the intelligence maybe comparable to that of a mouse. The third generation robot should have intelligence comparable to that of a monkey. Though fourth generation robots, robots with human intelligence, professor Moravec predicts, would become possible, he does not predict this happening before around 2040 or 2050.[143]\n The study of motion can be divided into kinematics and dynamics.[144] Direct kinematics or forward kinematics refers to the calculation of end effector position, orientation, velocity, and acceleration when the corresponding joint values are known. Inverse kinematics refers to the opposite case in which required joint values are calculated for given end effector values, as done in path planning. Some special aspects of kinematics include handling of redundancy (different possibilities of performing the same movement), collision avoidance, and singularity avoidance. Once all relevant positions, velocities, and accelerations have been calculated using kinematics, methods from the field of dynamics are used to study the effect of forces upon these movements. Direct dynamics refers to the calculation of accelerations in the robot once the applied forces are known. Direct dynamics is used in computer simulations of the robot. Inverse dynamics refers to the calculation of the actuator forces necessary to create a prescribed end-effector acceleration. This information can be used to improve the control algorithms of a robot.\n In each area mentioned above, researchers strive to develop new concepts and strategies, improve existing ones, and improve the interaction between these areas. To do this, criteria for \"optimal\" performance and ways to optimize design, structure, and control of robots must be developed and implemented.\n Open source robotics research seeks standards for defining, and methods for designing and building, robots so that they can easily be reproduced by anyone. Research includes legal and technical definitions; seeking out alternative tools and materials to reduce costs and simplify builds; and creating interfaces and standards for designs to work together. Human usability research also investigates how to best document builds through visual, text or video instructions.\n Evolutionary robots is a methodology that uses evolutionary computation to help design robots, especially the body form, or motion and behavior controllers. In a similar way to natural evolution, a large population of robots is allowed to compete in some way, or their ability to perform a task is measured using a fitness function. Those that perform worst are removed from the population and replaced by a new set, which have new behaviors based on those of the winners. Over time the population improves, and eventually a satisfactory robot may appear. This happens without any direct programming of the robots by the researchers. Researchers use this method both to create better robots,[145] and to explore the nature of evolution.[146] Because the process often requires many generations of robots to be simulated,[147] this technique may be run entirely or mostly in simulation, using a robot simulator software package, then tested on real robots once the evolved algorithms are good enough.[148] Currently, there are about 10 million industrial robots toiling around the world, and Japan is the top country having high density of utilizing robots in its manufacturing industry.[citation needed]\n Bionics and biomimetics apply the physiology and methods of locomotion of animals to the design of robots. For example, the design of BionicKangaroo was based on the way kangaroos jump.\n Swarm robotics is an approach to the coordination of multiple robots as a system which consist of large numbers of mostly simple physical robots. ″In a robot swarm, the collective behavior of the robots results from local interactions between the robots and between the robots and the environment in which they act.″* [119]\n There has been some research into whether robotics algorithms can be run more quickly on quantum computers than they can be run on digital computers. This area has been referred to as quantum robotics.[149]\n The main venues for robotics research are the international conferences ICRA and IROS.\n Robotics engineers design robots, maintain them, develop new applications for them, and conduct research to expand the potential of robotics.[152] Robots have become a popular educational tool in some middle and high schools, particularly in parts of the USA,[153] as well as in numerous youth summer camps, raising interest in programming, artificial intelligence, and robotics among students.\n Robotics is an essential component in many modern manufacturing environments. As factories increase their use of robots, the number of robotics–related jobs grow and have been observed to be steadily rising.[154] The employment of robots in industries has increased productivity and efficiency savings and is typically seen as a long-term investment for benefactors. A study found that 47 percent of US jobs are at risk to automation \"over some unspecified number of years\".[155] These claims have been criticized on the ground that social policy, not AI, causes unemployment.[156] In a 2016 article in The Guardian, Stephen Hawking stated \"The automation of factories has already decimated jobs in traditional manufacturing, and the rise of artificial intelligence is likely to extend this job destruction deep into the middle classes, with only the most caring, creative or supervisory roles remaining\".[157]   The rise of robotics is thus often used as an argument for universal basic income.\n According to a GlobalData September 2021 report, the robotics industry was worth $45bn in 2020, and by 2030, it will have grown at a compound annual growth rate (CAGR) of 29% to $568bn, driving jobs in robotics and related industries.[158]\n A discussion paper drawn up by EU-OSHA highlights how the spread of robotics presents both opportunities and challenges for occupational safety and health (OSH).[159]\n The greatest OSH benefits stemming from the wider use of robotics should be substitution for people working in unhealthy or dangerous environments. In space, defense, security, or the nuclear industry, but also in logistics, maintenance, and inspection, autonomous robots are particularly useful in replacing human workers performing dirty, dull or unsafe tasks, thus avoiding workers' exposures to hazardous agents and conditions and reducing physical, ergonomic and psychosocial risks. For example, robots are already used to perform repetitive and monotonous tasks, to handle radioactive material or to work in explosive atmospheres. In the future, many other highly repetitive, risky or unpleasant tasks will be performed by robots in a variety of sectors like agriculture, construction, transport, healthcare, firefighting or cleaning services.[160]\n Moreover, there are certain skills to which humans will be better suited than machines for some time to come and the question is how to achieve the best combination of human and robot skills. The advantages of robotics include heavy-duty jobs with precision and repeatability, whereas the advantages of humans include creativity, decision-making, flexibility, and adaptability. This need to combine optimal skills has resulted in collaborative robots and humans sharing a common workspace more closely and led to the development of new approaches and standards to guarantee the safety of the \"man-robot merger\". Some European countries are including robotics in their national programs and trying to promote a safe and flexible cooperation between robots and operators to achieve better productivity. For example, the German Federal Institute for Occupational Safety and Health (BAuA) organises annual workshops on the topic \"human-robot collaboration\".\n In the future, cooperation between robots and humans will be diversified, with robots increasing their autonomy and human-robot collaboration reaching completely new forms. Current approaches and technical standards[161][162] aiming to protect employees from the risk of working with collaborative robots will have to be revised.\n Great user experience predicts the needs, experiences, behaviors, language and cognitive abilities, and other factors of each user group. It then uses these insights to produce a product or solution that is ultimately useful and usable. For robots, user experience begins with an understanding of the robot's intended task and environment, while considering any possible social impact the robot may have on human operations and interactions with it.[163]\n It defines that communication as the transmission of information through signals, which are elements perceived through touch, sound, smell and sight.[164] The author states that the signal connects the sender to the receiver and consists of three parts: the signal itself, what it refers to, and the interpreter. Body postures and gestures, facial expressions, hand and head movements are all part of nonverbal behavior and communication. Robots are no exception when it comes to human-robot interaction. Therefore, humans use their verbal and nonverbal behaviors to communicate their defining characteristics. Similarly, social robots need this coordination to perform human-like behaviors.\n Robotics is an interdisciplinary field, combining primarily mechanical engineering and computer science but also drawing on electronic engineering and other subjects. The usual way to build a career in robotics is to complete an undergraduate degree in one of these established subjects, followed by a graduate (masters') degree in Robotics. Graduate degrees are typically joined by students coming from all of the contributing disciplines, and include familiarization of relevant undergraduate level subject matter from each of them, followed by specialist study in pure robotics topics which build upon them. As an interdisciplinary subject, robotics graduate programmes tend to be especially reliant on students working and learning together and sharing their knowledge and skills from their home discipline first degrees.\n Robotics industry careers then follow the same pattern, with most roboticists working as part of interdisciplinary teams of specialists from these home disciplines followed by the robotics graduate degrees which enable them to work together. Workers typically continue to identify as members of their home disciplines who work in robotics, rather than as 'roboticists'. This structure is reinforced by the nature of some engineering professions, which grant chartered engineer status to members of home disciplines rather than to robotics as a whole.\n Robotics careers are widely predicted to grow in the 21st century, as robots replace more manual and intellectual human work. Some workers who lose their jobs to robotics may be well-placed to retrain to build and maintain these robots, using their domain-specific knowledge and skills.\n"
    },
    {
        "title": "AI safety",
        "url": "https://en.wikipedia.org/wiki/AI_safety",
        "content": "AI safety is an interdisciplinary field focused on preventing accidents, misuse, or other harmful consequences arising from artificial intelligence (AI) systems. It encompasses machine ethics and AI alignment, which aim to ensure AI systems are moral and beneficial, as well as monitoring AI systems for risks and enhancing their reliability. The field is particularly concerned with existential risks posed by advanced AI models.\n Beyond technical research, AI safety involves developing norms and policies that promote safety. It gained significant popularity in 2023, with rapid progress in generative AI and public concerns voiced by researchers and CEOs about potential dangers. During the 2023 AI Safety Summit, the United States and the United Kingdom both established their own AI Safety Institute. However, researchers have expressed concern that AI safety measures are not keeping pace with the rapid development of AI capabilities.[1]\n Scholars discuss current risks from critical systems failures,[2] bias,[3] and AI-enabled surveillance,[4] as well as emerging risks like technological unemployment, digital manipulation,[5] weaponization,[6] AI-enabled cyberattacks[7] and bioterrorism.[8] They also discuss speculative risks from losing control of future artificial general intelligence (AGI) agents,[9] or from AI enabling perpetually stable dictatorships.[10]\n Some have criticized concerns about AGI, such as Andrew Ng who compared them in 2015 to \"worrying about overpopulation on Mars when we have not even set foot on the planet yet\".[11] Stuart J. Russell on the other side urges caution, arguing that \"it is better to anticipate human ingenuity than to underestimate it\".[12]\n AI researchers have widely differing opinions about the severity and primary sources of risk posed by AI technology[13][14][15] – though surveys suggest that experts take high consequence risks seriously. In two surveys of AI researchers, the median respondent was optimistic about AI overall, but placed a 5% probability on an \"extremely bad (e.g. human extinction)\" outcome of advanced AI.[13] In a 2022 survey of the natural language processing community, 37% agreed or weakly agreed that it is plausible that AI decisions could lead to a catastrophe that is \"at least as bad as an all-out nuclear war\".[16]\n Risks from AI began to be seriously discussed at the start of the computer age:\n Moreover, if we move in the direction of making machines which learn and whose behavior is modified by experience, we must face the fact that every degree of independence we give the machine is a degree of possible defiance of our wishes. From 2008 to 2009, the Association for the Advancement of Artificial Intelligence (AAAI) commissioned a study to explore and address potential long-term societal influences of AI research and development. The panel was generally skeptical of the radical views expressed by science-fiction authors but agreed that \"additional research would be valuable on methods for understanding and verifying the range of behaviors of complex computational systems to minimize unexpected outcomes\".[18]\n In 2011, Roman Yampolskiy introduced the term \"AI safety engineering\"[19] at the Philosophy and Theory of Artificial Intelligence conference,[20] listing prior failures of AI systems and arguing that \"the frequency and seriousness of such events will steadily increase as AIs become more capable\".[21]\n In 2014, philosopher Nick Bostrom published the book Superintelligence: Paths, Dangers, Strategies. He has the opinion that the rise of AGI has the potential to create various societal issues, ranging from the displacement of the workforce by AI, manipulation of political and military structures, to even the possibility of human extinction.[22] His argument that future advanced systems may pose a threat to human existence prompted Elon Musk,[23] Bill Gates,[24] and Stephen Hawking[25] to voice similar concerns.\n In 2015, dozens of artificial intelligence experts signed an open letter on artificial intelligence calling for research on the societal impacts of AI and outlining concrete directions.[26] To date, the letter has been signed by over 8000 people including Yann LeCun, Shane Legg, Yoshua Bengio, and Stuart Russell.\n In the same year, a group of academics led by professor Stuart Russell founded the Center for Human-Compatible AI at the University of California Berkeley and the Future of Life Institute awarded $6.5 million in grants for research aimed at \"ensuring artificial intelligence (AI) remains safe, ethical and beneficial\".[27]\n In 2016, the White House Office of Science and Technology Policy and Carnegie Mellon University announced The Public Workshop on Safety and Control for Artificial Intelligence,[28] which was one of a sequence of four White House workshops aimed at investigating \"the advantages and drawbacks\" of AI.[29] In the same year, Concrete Problems in AI Safety – one of the first and most influential technical AI Safety agendas – was published.[30]\n In 2017, the Future of Life Institute sponsored the Asilomar Conference on Beneficial AI, where more than 100 thought leaders formulated principles for beneficial AI including \"Race Avoidance: Teams developing AI systems should actively cooperate to avoid corner-cutting on safety standards\".[31]\n In 2018, the DeepMind Safety team outlined AI safety problems in specification, robustness,[32] and assurance.[33] The following year, researchers organized a workshop at ICLR that focused on these problem areas.[34]\n In 2021, Unsolved Problems in ML Safety was published, outlining research directions in robustness, monitoring, alignment, and systemic safety.[35]\n In 2023, Rishi Sunak said he wants the United Kingdom to be the \"geographical home of global AI safety regulation\" and to host the first global summit on AI safety.[36] The AI safety summit took place in November 2023, and focused on the risks of misuse and loss of control associated with frontier AI models.[37] During the summit the intention to create the International Scientific Report on the Safety of Advanced AI[38] was announced.  \n In 2024, The US and UK forged a new partnership on the science of AI safety. The MoU was signed on 1 April 2024 by US commerce secretary Gina Raimondo and UK technology secretary Michelle Donelan to jointly develop advanced AI model testing, following commitments announced at an AI Safety Summit in Bletchley Park in November.[39]\n AI safety research areas include robustness, monitoring, and alignment.[35][33]\n AI systems are often vulnerable to adversarial examples or \"inputs to machine learning (ML) models that an attacker has intentionally designed to cause the model to make a mistake\".[40] For example, in 2013, Szegedy et al. discovered that adding specific imperceptible perturbations to an image could cause it to be misclassified with high confidence.[41] This continues to be an issue with neural networks, though in recent work the perturbations are generally large enough to be perceptible.[42][43][44]\n All of the images on the right are predicted to be an ostrich after the perturbation is applied. (Left) is a correctly predicted sample, (center) perturbation applied magnified by 10x, (right) adversarial example.[41]\n Adversarial robustness is often associated with security.[45] Researchers demonstrated that an audio signal could be imperceptibly modified so that speech-to-text systems transcribe it to any message the attacker chooses.[46] Network intrusion[47] and malware[48] detection systems also must be adversarially robust since attackers may design their attacks to fool detectors.\n Models that represent objectives (reward models) must also be adversarially robust. For example, a reward model might estimate how helpful a text response is and a language model might be trained to maximize this score.[49] Researchers have shown that if a language model is trained for long enough, it will leverage the vulnerabilities of the reward model to achieve a better score and perform worse on the intended task.[50] This issue can be addressed by improving the adversarial robustness of the reward model.[51] More generally, any AI system used to evaluate another AI system must be adversarially robust. This could include monitoring tools, since they could also potentially be tampered with to produce a higher reward.[52]\n It is often important for human operators to gauge how much they should trust an AI system, especially in high-stakes settings such as medical diagnosis.[53] ML models generally express confidence by outputting probabilities; however, they are often overconfident,[54] especially in situations that differ from those that they were trained to handle.[55] Calibration research aims to make model probabilities correspond as closely as possible to the true proportion that the model is correct.\n Similarly, anomaly detection or out-of-distribution (OOD) detection aims to identify when an AI system is in an unusual situation. For example, if a sensor on an autonomous vehicle is malfunctioning, or it encounters challenging terrain, it should alert the driver to take control or pull over.[56] Anomaly detection has been implemented by simply training a classifier to distinguish anomalous and non-anomalous inputs,[57] though a range of additional techniques are in use.[58][59]\n Scholars[6] and government agencies have expressed concerns that AI systems could be used to help malicious actors to build weapons,[60] manipulate public opinion,[61][62] or automate cyber attacks.[63] These worries are a practical concern for companies like OpenAI which host powerful AI tools online.[64] In order to prevent misuse, OpenAI has built detection systems that flag or restrict users based on their activity.[65]\n Neural networks have often been described as black boxes,[66] meaning that it is difficult to understand why they make the decisions they do as a result of the massive number of computations they perform.[67] This makes it challenging to anticipate failures. In 2018, a self-driving car killed a pedestrian after failing to identify them. Due to the black box nature of the AI software, the reason for the failure remains unclear.[68] It also raises debates in healthcare over whether statistically efficient but opaque models should be used.[69]\n One critical benefit of transparency is explainability.[70] It is sometimes a legal requirement to provide an explanation for why a decision was made in order to ensure fairness, for example for automatically filtering job applications or credit score assignment.[70]\n Another benefit is to reveal the cause of failures.[66] At the beginning of the 2020 COVID-19 pandemic, researchers used transparency tools to show that medical image classifiers were 'paying attention' to irrelevant hospital labels.[71]\n Transparency techniques can also be used to correct errors. For example, in the paper \"Locating and Editing Factual Associations in GPT\", the authors were able to identify model parameters that influenced how it answered questions about the location of the Eiffel tower. They were then able to 'edit' this knowledge to make the model respond to questions as if it believed the tower was in Rome instead of France.[72] Though in this case, the authors induced an error, these methods could potentially be used to efficiently fix them. Model editing techniques also exist in computer vision.[73]\n Finally, some have argued that the opaqueness of AI systems is a significant source of risk and better understanding of how they function could prevent high-consequence failures in the future.[74] \"Inner\" interpretability research aims to make ML models less opaque. One goal of this research is to identify what the internal neuron activations represent.[75][76] For example, researchers identified a neuron in the CLIP artificial intelligence system that responds to images of people in spider man costumes, sketches of spiderman, and the word 'spider'.[77] It also involves explaining connections between these neurons or 'circuits'.[78][79] For example, researchers have identified pattern-matching mechanisms in transformer attention that may play a role in how language models learn from their context.[80] \"Inner interpretability\" has been compared to neuroscience. In both cases, the goal is to understand what is going on in an intricate system, though ML researchers have the benefit of being able to take perfect measurements and perform arbitrary ablations.[81]\n Machine learning models can potentially contain \"trojans\" or \"backdoors\": vulnerabilities that malicious actors maliciously build into an AI system. For example, a trojaned facial recognition system could grant access when a specific piece of jewelry is in view;[35] or a trojaned autonomous vehicle may function normally until a specific trigger is visible.[82] Note that an adversary must have access to the system's training data in order to plant a trojan. [citation needed] This might not be difficult to do with some large models like CLIP or GPT-3 as they are trained on publicly available internet data.[83] Researchers were able to plant a trojan in an image classifier by changing just 300 out of 3 million of the training images.[84] In addition to posing a security risk, researchers have argued that trojans provide a concrete setting for testing and developing better monitoring tools.[52]\n A 2024 research paper by Anthropic showed that large language models could be trained with persistent backdoors. These \"sleeper agent\" models could be programmed to generate malicious outputs (such as vulnerable code) after a specific date, while behaving normally beforehand. Standard AI safety measures, such as supervised fine-tuning, reinforcement learning and adversarial training, failed to remove these backdoors.[85]\n In the field of artificial intelligence (AI), AI alignment aims to steer AI systems toward a person's or group's intended goals, preferences, or ethical principles. An AI system is considered aligned if it advances the intended objectives. A misaligned AI system pursues unintended objectives.[86]\n It is often challenging for AI designers to align an AI system because it is difficult for them to specify the full range of desired and undesired behaviors. Therefore, AI designers often use simpler proxy goals, such as gaining human approval. But proxy goals can overlook necessary constraints or reward the AI system for merely appearing aligned.[86][87] AI systems may also find loopholes that allow them to accomplish their proxy goals efficiently but in unintended, sometimes harmful, ways (reward hacking).[86][88]\n Advanced AI systems may develop unwanted instrumental strategies, such as seeking power or survival because such strategies help them achieve their assigned final goals.[86][89][90] Furthermore, they might develop undesirable emergent goals that could be hard to detect before the system is deployed and encounters new situations and data distributions.[91][92] Empirical research showed in 2024 that advanced large language models (LLMs) such as OpenAI o1 or Claude 3 sometimes engage in strategic deception to achieve their goals or prevent them from being changed.[93][94]\n Today, some of these issues affect existing commercial systems such as LLMs,[95][96][97] robots,[98] autonomous vehicles,[99] and social media recommendation engines.[95][90][100] Some AI researchers argue that more capable future systems will be more severely affected because these problems partially result from high capabilities.[101][88][87]\n Many prominent AI researchers and the leadership of major AI companies have argued or asserted that AI is approaching human-like (AGI) and superhuman cognitive capabilities (ASI), and could endanger human civilization if misaligned.[102][90] These include \"AI Godfathers\" Geoffrey Hinton and Yoshua Bengio and the CEOs of OpenAI, Anthropic, and Google DeepMind.[103][104][105] These risks remain debated.[106]\n It is common for AI risks (and technological risks more generally) to be categorized as misuse or accidents.[120] Some scholars have suggested that this framework falls short.[120] For example, the Cuban Missile Crisis was not clearly an accident or a misuse of technology.[120] Policy analysts Zwetsloot and Dafoe wrote, \"The misuse and accident perspectives tend to focus only on the last step in a causal chain leading up to a harm: that is, the person who misused the technology, or the system that behaved in unintended ways… Often, though, the relevant causal chain is much longer.\" Risks often arise from 'structural' or 'systemic' factors such as competitive pressures, diffusion of harms, fast-paced development, high levels of uncertainty, and inadequate safety culture.[120] In the broader context of safety engineering, structural factors like 'organizational safety culture' play a central role in the popular STAMP risk analysis framework.[121]\n Inspired by the structural perspective, some researchers have emphasized the importance of using machine learning to improve sociotechnical safety factors, for example, using ML for cyber defense, improving institutional decision-making, and facilitating cooperation.[35] Others have emphasized the importance of involving both AI practitioners and domain experts in the design process to address structural vulnerabilities.[122]\n Some scholars are concerned that AI will exacerbate the already imbalanced game between cyber attackers and cyber defenders.[123] This would increase 'first strike' incentives and could lead to more aggressive and destabilizing attacks. In order to mitigate this risk, some have advocated for an increased emphasis on cyber defense. In addition, software security is essential for preventing powerful AI models from being stolen and misused.[6] Recent studies have shown that AI can significantly enhance both technical and managerial cybersecurity tasks by automating routine tasks and improving overall efficiency.[124]\n The advancement of AI in economic and military domains could precipitate unprecedented political challenges.[125] Some scholars have compared AI race dynamics to the cold war, where the careful judgment of a small number of decision-makers often spelled the difference between stability and catastrophe.[126] AI researchers have argued that AI technologies could also be used to assist decision-making.[35] For example, researchers are beginning to develop AI forecasting[127] and advisory systems.[128]\n Many of the largest global threats (nuclear war,[129] climate change,[130] etc.) have been framed as cooperation challenges. As in the well-known prisoner's dilemma scenario, some dynamics may lead to poor results for all players, even when they are optimally acting in their self-interest. For example, no single actor has strong incentives to address climate change even though the consequences may be significant if no one intervenes.[130]\n A salient AI cooperation challenge is avoiding a 'race to the bottom'.[131] In this scenario, countries or companies race to build more capable AI systems and neglect safety, leading to a catastrophic accident that harms everyone involved. Concerns about scenarios like these have inspired both political[132] and technical[133] efforts to facilitate cooperation between humans, and potentially also between AI systems. Most AI research focuses on designing individual agents to serve isolated functions (often in 'single-player' games).[134] Scholars have suggested that as AI systems become more autonomous, it may become essential to study and shape the way they interact.[134][122]\n In recent years, the development of large language models (LLMs) has raised unique concerns within the field of AI safety. Researchers Bender and Gebru et al.[135] have highlighted the environmental and financial costs associated with training these models, emphasizing that the energy consumption and carbon footprint of training procedures like those for Transformer models can be substantial. Moreover, these models often rely on massive, uncurated Internet-based datasets, which can encode hegemonic and biased viewpoints, further marginalizing underrepresented groups. The large-scale training data, while vast, does not guarantee diversity and often reflects the worldviews of privileged demographics, leading to models that perpetuate existing biases and stereotypes. This situation is exacerbated by the tendency of these models to produce seemingly coherent and fluent text, which can mislead users into attributing meaning and intent where none exists, a phenomenon described as 'stochastic parrots'. These models, therefore, pose risks of amplifying societal biases, spreading misinformation, and being used for malicious purposes, such as generating extremist propaganda or deepfakes. To address these challenges, researchers advocate for more careful planning in dataset creation and system development, emphasizing the need for research projects that contribute positively towards an equitable technological ecosystem.[136][137]\n AI governance is broadly concerned with creating norms, standards, and regulations to guide the use and development of AI systems.[126]\n AI safety governance research ranges from foundational investigations into the potential impacts of AI to specific applications. On the foundational side, researchers have argued that AI could transform many aspects of society due to its broad applicability, comparing it to electricity and the steam engine.[139] Some work has focused on anticipating specific risks that may arise from these impacts – for example, risks from mass unemployment,[140] weaponization,[141] disinformation,[142] surveillance,[143] and the concentration of power.[144] Other work explores underlying risk factors such as the difficulty of monitoring the rapidly evolving AI industry,[145] the availability of AI models,[146] and 'race to the bottom' dynamics.[131][147] Allan Dafoe, the head of longterm governance and strategy at DeepMind has emphasized the dangers of racing and the potential need for cooperation: \"it may be close to a necessary and sufficient condition for AI safety and alignment that there be a high degree of caution prior to deploying advanced powerful systems; however, if actors are competing in a domain with large returns to first-movers or relative advantage, then they will be pressured to choose a sub-optimal level of caution\".[132] A research stream focuses on developing approaches, frameworks, and methods to assess AI accountability, guiding and promoting audits of AI-based systems.[148][149][150]\n Efforts to enhance AI safety include frameworks designed to align AI outputs with ethical guidelines and reduce risks like misuse and data leakage. Tools such as Nvidia's  Guardrails,[151] Llama Guard,[152] and Preamble's customizable guardrails[153] mitigate vulnerabilities like prompt injection and ensure outputs adhere to predefined principles. These frameworks are often integrated into AI systems to improve safety and reliability.[154]\n The field of AI safety is deeply intertwined with philosophical considerations, particularly in the realm of ethics. Deontological ethics, which emphasizes adherence to moral rules, has been proposed as a framework for aligning AI systems with human values. By embedding deontological principles, AI systems can be guided to avoid actions that cause harm, ensuring their operations remain within ethical boundaries.[155]\n In addressing the AI safety problem it is important to stress the distinction between local and global solutions. Local solutions focus on individual AI systems, ensuring they are safe and beneficial, while global solutions seek to implement safety measures for all AI systems across various jurisdictions. Some researchers[156] argue for the necessity of scaling local safety measures to a global level, proposing a classification for these global solutions. This approach underscores the importance of collaborative efforts in the international governance of AI safety, emphasizing that no single entity can effectively manage the risks associated with AI technologies. This perspective aligns with ongoing efforts in international policy-making and regulatory frameworks, which aim to address the complex challenges posed by advanced AI systems worldwide.[157][158]\n Some experts have argued that it is too early to regulate AI, expressing concerns that regulations will hamper innovation and it would be foolish to \"rush to regulate in ignorance\".[159][160] Others, such as business magnate Elon Musk, call for pre-emptive action to mitigate catastrophic risks.[161]\n Outside of formal legislation, government agencies have put forward ethical and safety recommendations. In March 2021, the US National Security Commission on Artificial Intelligence reported that advances in AI may make it increasingly important to \"assure that systems are aligned with goals and values, including safety, robustness and trustworthiness\".[162] Subsequently, the National Institute of Standards and Technology drafted a framework for managing AI Risk, which advises that when \"catastrophic risks are present – development and deployment should cease in a safe manner until risks can be sufficiently managed\".[163]\n In September 2021, the People's Republic of China published ethical guidelines for the use of AI in China, emphasizing that AI decisions should remain under human control and calling for accountability mechanisms. In the same month, The United Kingdom published its 10-year National AI Strategy,[164] which states the British government \"takes the long-term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for ... the world, seriously\".[165] The strategy describes actions to assess long-term AI risks, including catastrophic risks.[165] The British government held first major global summit on AI safety. This took place on the 1st and 2 November 2023 and was described as \"an opportunity for policymakers and world leaders to consider the immediate and future risks of AI and how these risks can be mitigated via a globally coordinated approach\".[166][167]\n Government organizations, particularly in the United States, have also encouraged the development of technical AI safety research. The Intelligence Advanced Research Projects Activity initiated the TrojAI project to identify and protect against Trojan attacks on AI systems.[168] The DARPA engages in research on explainable artificial intelligence and improving robustness against adversarial attacks.[169][170] And the National Science Foundation supports the Center for Trustworthy Machine Learning, and is providing millions of dollars in funding for empirical AI safety research.[171]\n In 2024, the United Nations General Assembly adopted the first global resolution on the promotion of “safe, secure and trustworthy” AI systems that emphasized the respect, protection and promotion of human rights in the design, development, deployment and the use of AI.[172]\n In May 2024, the Department for Science, Innovation and Technology (DSIT) announced £8.5 million in funding for AI safety research under the Systemic AI Safety Fast Grants Programme, led by Christopher Summerfield and Shahar Avin at the AI Safety Institute, in partnership with UK Research and Innovation. Technology Secretary Michelle Donelan announced the plan at the AI Seoul Summit, stating the goal was to make AI safe across society and that promising proposals could receive further funding. The UK also signed an agreement with 10 other countries and the EU to form an international network of AI safety institutes to promote collaboration and share information and resources. Additionally, the UK AI Safety Institute planned to open an office in San Francisco.[173]\n AI labs and companies generally abide by safety practices and norms that fall outside of formal legislation.[174] One aim of governance researchers is to shape these norms. Examples of safety recommendations found in the literature include performing third-party auditing,[175] offering bounties for finding failures,[175] sharing AI incidents[175] (an AI incident database was created for this purpose),[176] following guidelines to determine whether to publish research or models,[146] and improving information and cyber security in AI labs.[177]\n Companies have also made commitments. Cohere, OpenAI, and AI21 proposed and agreed on \"best practices for deploying language models\", focusing on mitigating misuse.[178] To avoid contributing to racing-dynamics, OpenAI has also stated in their charter that \"if a value-aligned, safety-conscious project comes close to building AGI before we do, we commit to stop competing with and start assisting this project\"[179] Also, industry leaders such as CEO of DeepMind Demis Hassabis, director of Facebook AI Yann LeCun have signed open letters such as the Asilomar Principles[31] and the Autonomous Weapons Open Letter.[180]\n"
    },
    {
        "title": "Machine learning",
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "content": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.[1] Advances in the field of deep learning have allowed neural networks to surpass many previous approaches in performance.[2]\n ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.[3][4] The application of ML to business problems is known as predictive analytics.\n Statistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning.[6][7]\n From a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.\n The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.[8][9] The synonym self-teaching computers was also used in this time period.[10][11]\n Although the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes.[12] In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells.[13] Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data.[12] Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.[12]\n By the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognize patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions.[14] A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.[15] Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973.[16] In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.[17]\n Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\"[18] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".[19]\n Modern-day machine learning has two objectives.  One is to classify data based on models which have been developed; the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions.[20]\n As a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[22] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[23]: 488 \n However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[23]: 488  By 1980, expert systems had come to dominate AI, and statistics was out of favor.[24] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[23]: 708–710, 755  Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including John Hopfield, David Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[23]: 25 \n Machine learning (ML), reorganized and recognized as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.[24]\n There is a close connection between machine learning and compression. A system that predicts the posterior probabilities of a sequence given its entire history can be used for optimal data compression (by using arithmetic coding on the output distribution). Conversely, an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as a justification for using data compression as a benchmark for \"general intelligence\".[25][26][27]\n An alternative view can show compression algorithms implicitly map strings into implicit feature space vectors, and compression-based similarity measures compute similarity within these feature spaces. For each compressor C(.) we define an associated vector space ℵ, such that C(.) maps an input string x, corresponding to the vector norm ||~x||. An exhaustive examination of the feature spaces underlying all compression algorithms is precluded by space; instead, feature vectors chooses to examine three representative lossless compression methods, LZW, LZ77, and PPM.[28]\n According to AIXI theory, a connection more directly explained in Hutter Prize, the best possible compression of x is the smallest possible software that generates x. For example, in that model, a zip file's compressed size includes both the zip file and the unzipping software, since you can not unzip it without both, but there may be an even smaller combined form.\n Examples of AI-powered audio/video compression software include NVIDIA Maxine, AIVC.[29] Examples of software that can perform AI-powered image compression include OpenCV, TensorFlow, MATLAB's Image Processing Toolbox (IPT) and High-Fidelity Generative Image Compression.[30]\n In unsupervised machine learning, k-means clustering can be utilized to compress data by grouping similar data points into clusters. This technique simplifies handling extensive datasets that lack predefined labels and finds widespread use in fields such as image compression.[31]\n Data compression aims to reduce the size of data files, enhancing storage efficiency and speeding up data transmission. K-means clustering, an unsupervised machine learning algorithm, is employed to partition a dataset into a specified number of clusters, k, each represented by the centroid of its points. This process condenses extensive datasets into a more compact set of representative points. Particularly beneficial in image and signal processing, k-means clustering aids in data reduction by replacing groups of data points with their centroids, thereby preserving the core information of the original data while significantly decreasing the required storage space.[32]\n Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.\n Machine learning also has intimate ties to optimization: Many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples).[34]\n Characterizing the generalization of various learning algorithms is an active topic of current research, especially for deep learning algorithms.\n Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalizable predictive patterns.[35] According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.[36] He also suggested the term data science as a placeholder to call the overall field.[36]\n Conventional statistical analyses require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, machine learning is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be.[37]\n Leo Breiman distinguished two statistical modeling paradigms: data model and algorithmic model,[38] wherein \"algorithmic model\" means more or less the machine learning algorithms like Random Forest.\n Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[39]\n Analytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyse the weight space of deep neural networks.[40] Statistical physics is thus finding applications in the area of medical diagnostics.[41]\n A core objective of a learner is to generalize from its experience.[5][42] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.\n The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the Probably Approximately Correct Learning (PAC) model. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalization error.\n For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.[43]\n In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.\n \n Machine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the \"signal\" or \"feedback\" available to the learning system:\n Although each algorithm has advantages and limitations, no single algorithm works for all problems.[44][45][46]\n Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs.[47] The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs.[48] An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.[18]\n Types of supervised-learning algorithms include active learning, classification and regression.[49] Classification algorithms are used when the outputs are restricted to a limited set of values, and regression algorithms are used when the outputs may have any numerical value within a range. As an example, for a classification algorithm that filters emails, the input would be an incoming email, and the output would be the name of the folder in which to file the email. Examples of regression would be predicting the height of a person, or the future temperature. [50]\n Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.\n Unsupervised learning algorithms find structures in data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction,[7] and density estimation.[51]\n Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity.\n A special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself.[52][53]\n Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce a considerable improvement in learning accuracy.\n In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.[54]\n Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcements learning algorithms use dynamic programming techniques.[55] Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.\n Dimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables.[56] In other words, it is a process of reducing the dimension of the feature set, also called the \"number of features\". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D).\nThe manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the area of manifold learning and manifold regularization.\n Other approaches have been developed which do not fit neatly into this three-fold categorization, and sometimes more than one is used by the same machine learning system. For example, topic modeling, meta-learning.[57]\n Self-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA).[58] It is learning with no external rewards and no external teacher advice. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.[59]\nThe self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: \n It is a system with only one input, situation, and only one output, action (or behavior) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioral environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioral environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behavior, in an environment that contains both desirable and undesirable situations.[60]\n Several learning algorithms aim at discovering better representations of the inputs provided during training.[61] Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.\n Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labeled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization[62] and various forms of clustering.[63][64][65]\n Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors.[66] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[67]\n Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.\n Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately.[68] A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[69]\n In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.[70] Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.[71]\n In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.[72]\n Three broad categories of anomaly detection techniques exist.[73] Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set and then test the likelihood of a test instance to be generated by the model.\n Robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning,[74][75] and finally meta-learning (e.g. MAML).\n Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of \"interestingness\".[76]\n Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[77] Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.\n Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets.[78] For example, the rule \n\n\n\n{\n\no\nn\ni\no\nn\ns\n,\np\no\nt\na\nt\no\ne\ns\n\n}\n⇒\n{\n\nb\nu\nr\ng\ne\nr\n\n}\n\n\n{\\displaystyle \\{\\mathrm {onions,potatoes} \\}\\Rightarrow \\{\\mathrm {burger} \\}}\n\n found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.\n Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.[79]\n Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs.\n Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting.[80][81][82] Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples.[83] The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.\n A machine learning model is a type of mathematical model that, once \"trained\" on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the model's internal parameters to minimize errors in its predictions.[84] By extension, the term \"model\" can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned.[85]\n Various types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection.\n Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules.\n An ANN is a model based on a collection of connected units or nodes called \"artificial neurons\", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a \"signal\", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called \"edges\". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.\n The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\n Deep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[86]\n Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making.\n Support-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category.[87] An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\n Regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularization methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel[88]), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space.\n Multivariate linear regression extends the concept of linear regression to handle multiple dependent variables simultaneously. This approach estimates the relationships between a set of input variables and several output variables by fitting a multidimensional linear model. It is particularly useful in scenarios where outputs are interdependent or share underlying patterns, such as predicting multiple economic indicators or reconstructing images,[89] which are inherently multi-dimensional.\n A Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.\n A Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations.\n Given a set of observed points, or input–output examples, the distribution of the (unobserved) output of a new point as function of its input data can be directly computed by looking like the observed points and the covariances between those points and the new, unobserved point.\n Gaussian processes are popular surrogate models in Bayesian optimization used to do hyperparameter optimization.\n A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s.[91][92] Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[93]\n The theory of belief functions, also referred to as evidence theory or Dempster–Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and  imprecise probability theories. These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g.,  Dempster's rule of combination), just like how in a pmf-based Bayesian approach[clarification needed] would combine probabilities. However, there are many caveats to these beliefs functions when compared to Bayesian approaches in order to incorporate ignorance and uncertainty quantification. These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learner's decision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving.[4][9] However, the computational complexity of these algorithms are dependent on the number of propositions (classes), and can lead to a much higher computation time when compared to other machine learning approaches.\n Typically, machine learning models require a high quantity of reliable data to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and notably, becoming integrated within machine learning engineering teams.\n Federated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralizes the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralized server. This also increases efficiency by decentralizing the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.[94]\n There are many applications for machine learning, including:\n In 2006, the media-services provider Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[97] Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly.[98] In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis.[99] In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[100] In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognized influences among artists.[101] In 2019 Springer Nature published the first research book created using machine learning.[102] In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19.[103] Machine learning was recently applied to predict the pro-environmental behavior of travelers.[104] Recently, machine learning technology was also applied to optimize smartphone's performance and thermal behavior based on the user's interaction with the phone.[105][106][107] When applied correctly, machine learning algorithms (MLAs) can utilize a wide range of company characteristics to predict stock returns without overfitting. By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques like OLS.[108]\n Recent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes.[109]\n Machine Learning is becoming a useful tool to investigate and predict evacuation decision making in large scale and small scale disasters. Different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes.[110][111][112] Other applications have been focusing on pre evacuation decisions in building fires.[113][114]\n Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results.[115][116][117] Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.[118]\n The \"black box theory\" poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted out of the data.[119] The House of Lords Select Committee, which claimed that such an \"intelligence system\" that could have a \"substantial impact on an individual's life\" would not be considered acceptable unless it provided \"a full and satisfactory explanation for the decisions\" it makes.[119]\n In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision.[120] Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested.[121][122] Microsoft's Bing Chat chatbot has been reported to produce hostile and offensive response against its users.[123]\n Machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves.[124]\n Explainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI.[125] It contrasts with the \"black box\" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision.[126] By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation.\n Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalizing the theory in accordance with how complex the theory is.[127]\n Learners can also disappoint by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses.[128] A real-world example is that, unlike humans, current image classifiers often do not primarily make judgments from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in \"adversarial\" images that the system misclassifies.[129][130]\n Adversarial vulnerabilities can also result in nonlinear systems, or from non-pattern perturbations. For some systems, it is possible to change the output by only changing a single adversarially chosen pixel.[131] Machine learning models are often vulnerable to manipulation or evasion via adversarial machine learning.[132]\n Researchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories \"spam\" and well-visible \"not spam\" of posts) machine learning models that are often developed or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access.[133][134][135]\n Classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.[136]\n In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning true positive rate (TPR) and true negative rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. Receiver operating characteristic (ROC) along with the accompanying Area Under the ROC Curve (AUC) offer additional tools for classification model assessment. Higher AUC is associated with a better performing model.[137]\n The ethics of artificial intelligence covers a broad range of topics within the field that are considered to have particular ethical stakes.[138] This includes algorithmic biases, fairness, automated decision-making, accountability, privacy, and regulation. \nIt also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation, how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks.[138]\n Different machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.[139]\n Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices.[140] For example, in 1988, the UK's Commission for Racial Equality found that St. George's Medical School had been using a computer program trained from data of previous admissions staff and that this program had denied nearly 60 candidates who were found to either be women or have non-European sounding names.[139] Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants.[141][142] Another example includes predictive policing company Geolitica's predictive algorithm that resulted in \"disproportionately high levels of over-policing in low-income and minority communities\" after being trained with historical crime data.[143]\n While responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame lack of participation and representation of minority population in the field of AI for machine learning's vulnerability to biases.[144] In fact, according to research carried out by the Computing Research Association (CRA) in 2021, \"female faculty merely make up 16.1%\" of all faculty members who focus on AI among several universities around the world.[145] Furthermore, among the group of \"new U.S. resident AI PhD graduates,\" 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further demonstrates a lack of diversity in the field of AI.[145]\n Language models learned from data have been shown to contain human-like biases.[146][147] Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases.[148][149] In 2016, Microsoft tested Tay, a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.[150]\n In an experiment carried out by ProPublica, an investigative journalism organization, a machine learning algorithm's insight into the recidivism rates among prisoners falsely flagged \"black defendants high risk twice as often as white defendants.\"[143] In 2015, Google Photos once tagged a couple of black people as gorillas, which caused controversy. The gorilla label was subsequently removed, and in 2023, it still cannot recognize gorillas.[151] Similar issues with recognizing non-white people have been found in many other systems.[152]\n Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains.[153] Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who said that \"[t]here's nothing artificial about AI. It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\"[154]\n There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.[155]\n Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units.[156] By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI.[157] OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.[158][159]\n Neuromorphic computing refers to a class of computing systems designed to emulate the structure and functionality of biological neural networks. These systems may be implemented through software-based simulations on conventional hardware or through specialized hardware architectures.[160]\n A physical neural network is a specific type of neuromorphic hardware that relies on electrically adjustable materials, such as memristors, to emulate the function of neural synapses. The term \"physical neural network\" highlights the use of physical hardware for computation, as opposed to software-based implementations. It broadly refers to artificial neural networks that use materials with adjustable resistance to replicate neural synapses.[161][162]\n Embedded machine learning is a sub-field of machine learning where models are deployed on embedded systems with limited computing resources, such as wearable computers, edge devices and microcontrollers.[163][164][165] Running models directly on these devices eliminates the need to transfer and store data on cloud servers for further processing, thereby reducing the risk of data breaches, privacy leaks and theft of intellectual property, personal data and business secrets. Embedded machine learning can be achieved through various techniques, such as hardware acceleration,[166][167] approximate computing,[168] and model optimization.[169][170] Common optimization techniques include pruning, quantization, knowledge distillation, low-rank factorization, network architecture search, and parameter sharing.\n Software suites containing a variety of machine learning algorithms include the following:\n"
    },
    {
        "title": "Symbolic artificial intelligence",
        "url": "https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence",
        "content": "In artificial intelligence, symbolic artificial intelligence (also known as classical artificial intelligence  or logic-based artificial intelligence)[1][2]\nis the term for the collection of all methods in artificial intelligence research that are based on high-level symbolic (human-readable) representations of problems, logic and search.[3] Symbolic AI used tools such as logic programming, production rules, semantic nets and frames, and it developed applications such as knowledge-based systems (in particular, expert systems),  symbolic mathematics, automated theorem provers, ontologies, the semantic web, and automated planning and scheduling systems. The Symbolic AI paradigm led to seminal ideas in search, symbolic programming languages, agents, multi-agent systems, the semantic web, and the strengths and limitations of formal knowledge and reasoning systems.\n Symbolic AI was the dominant paradigm of AI research from the mid-1950s until the mid-1990s.[4] Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the ultimate goal of their field.[citation needed] An early boom, with early successes such as the Logic Theorist and Samuel's Checkers Playing Program, led to unrealistic expectations and promises and was followed by the first AI Winter as funding dried up.[5][6] A second boom (1969–1986) occurred with the rise of expert systems, their promise of capturing corporate expertise, and an enthusiastic corporate embrace.[7][8] That boom, and some early successes, e.g., with XCON at DEC, was followed again by later disappointment.[8] Problems with difficulties in knowledge acquisition, maintaining large knowledge bases, and brittleness in handling out-of-domain problems arose. Another, second, AI Winter (1988–2011) followed.[9] Subsequently, AI researchers focused on addressing underlying problems in handling uncertainty and in knowledge acquisition.[10] Uncertainty was addressed with formal methods such as hidden Markov models, Bayesian reasoning, and statistical relational learning.[11][12] Symbolic machine learning addressed the knowledge acquisition problem with contributions including Version Space, Valiant's PAC learning, Quinlan's ID3 decision-tree learning, case-based learning, and inductive logic programming to learn relations.[13]\n Neural networks, a subsymbolic approach, had been pursued from early days and reemerged strongly in 2012.  Early examples are Rosenblatt's perceptron learning work, the backpropagation work of Rumelhart, Hinton and Williams,[14] and work in convolutional neural networks by LeCun et al. in 1989.[15] However, neural networks were not viewed as successful until about 2012: \"Until Big Data became commonplace, the general consensus in the Al community was that the so-called neural-network approach was hopeless. Systems just didn't work that well, compared to other methods. ... A revolution came in 2012, when a number of people, including a team of researchers working with Hinton, worked out a way to use the power of GPUs to enormously increase the power of neural networks.\"[16] Over the next several years, deep learning had spectacular success in handling vision, speech recognition, speech synthesis, image generation, and machine translation. However, since 2020, as inherent difficulties with bias, explanation, comprehensibility, and robustness became more apparent with deep learning approaches; an increasing number of AI researchers have called for combining the best of both the symbolic and neural network approaches[17][18] and addressing areas that both approaches have difficulty with, such as common-sense reasoning.[16]\n A short history of symbolic AI to the present day follows below. Time periods and titles are drawn from Henry Kautz's 2020 AAAI Robert S. Engelmore Memorial Lecture[19] and the longer Wikipedia article on the History of AI, with dates and titles differing slightly for increased clarity.\n Success at early attempts in AI occurred in three main areas: artificial neural networks, knowledge representation, and heuristic search, contributing to high expectations. This section summarizes Kautz's reprise of early AI history.\n Cybernetic approaches attempted to replicate the feedback loops between animals and their environments. A robotic turtle, with sensors, motors for driving and steering, and seven vacuum tubes for control, based on a preprogrammed neural net, was built as early as 1948. This work can be seen as an early precursor to later work in neural networks, reinforcement learning, and situated robotics.[20]\n An important early symbolic AI program was the Logic theorist, written by Allen Newell, Herbert Simon and Cliff Shaw in 1955–56, as it was able to prove 38 elementary theorems from Whitehead and Russell's Principia Mathematica. Newell, Simon, and Shaw later generalized this work to create a domain-independent problem solver, GPS (General Problem Solver). GPS solved problems represented with formal operators via state-space search using means-ends analysis.[21]\n During the 1960s, symbolic approaches achieved great success at simulating intelligent behavior in structured environments such as game-playing, symbolic mathematics, and theorem-proving. AI research was concentrated in four institutions in the 1960s: Carnegie Mellon University, Stanford, MIT and (later) University of Edinburgh. Each one developed its own style of research. Earlier approaches based on cybernetics or artificial neural networks were abandoned or pushed into the background.\n Herbert Simon and Allen Newell studied human problem-solving skills and attempted to formalize them, and their work laid the foundations of the field of artificial intelligence, as well as cognitive science, operations research and management science. Their research team used the results of psychological experiments to develop programs that simulated the techniques that people used to solve problems.[22][23] This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s.[24][25]\n In addition to the highly specialized domain-specific kinds of knowledge that we will see later used in expert systems, early symbolic AI researchers discovered another more general application of knowledge. These were called heuristics, rules of thumb that guide a search in promising directions: \"How can non-enumerative search be practical when the underlying problem is exponentially hard? The approach advocated by Simon and Newell is to employ heuristics: fast algorithms that may fail on some inputs or output suboptimal solutions.\"[26] Another important advance was to find a way to apply these heuristics that guarantees a solution will be found, if there is one, not withstanding the occasional fallibility of heuristics: \"The A* algorithm provided a general frame for complete and optimal heuristically guided search. A* is used as a subroutine within practically every AI algorithm today but is still no magic bullet; its guarantee of completeness is bought at the cost of worst-case exponential time.[26]\n Early work covered both applications of formal reasoning emphasizing first-order logic, along with attempts to handle common-sense reasoning in a less formal manner.\n Unlike Simon and Newell, John McCarthy felt that machines did not need to simulate the exact mechanisms of human thought, but could instead try to find the essence of abstract reasoning and problem-solving with logic,[27] regardless of whether people used the same algorithms.[a]\nHis laboratory at Stanford (SAIL) focused on using formal logic to solve a wide variety of problems, including knowledge representation, planning and learning.[31]\nLogic was also the focus of the work at the University of Edinburgh and elsewhere in Europe which led to the development of the programming language Prolog and the science of logic programming.[32][33]\n Researchers at MIT (such as Marvin Minsky and Seymour Papert)[34][35][6] found that solving difficult problems in vision and natural language processing required ad hoc solutions—they argued that no simple and general principle (like logic) would capture all the aspects of intelligent behavior. Roger Schank described their \"anti-logic\" approaches as \"scruffy\" (as opposed to the \"neat\" paradigms at CMU and Stanford).[36][37]\nCommonsense knowledge bases (such as Doug Lenat's Cyc) are an example of \"scruffy\" AI, since they must be built by hand, one complicated concept at a time.[38][39][40]\n The first AI winter was a shock:\n During the first AI summer, many people thought that machine intelligence could be achieved in just a few years. The Defense Advance Research Projects Agency (DARPA) launched programs to support AI research to use AI to solve problems of national security; in particular, to automate the translation of Russian to English for intelligence operations and to create autonomous tanks for the battlefield. Researchers had begun to realize that achieving AI was going to be much harder than was supposed a decade earlier, but a combination of hubris and disingenuousness led many university and think-tank researchers to accept funding with promises of deliverables that they should have known they could not fulfill. By the mid-1960s neither useful natural language translation systems nor autonomous tanks had been created, and a dramatic backlash set in. New DARPA leadership canceled existing AI funding programs.\n ...\n \nOutside of the United States, the most fertile ground for AI research was the United Kingdom. The AI winter in the United Kingdom was spurred on not so much by disappointed military leaders as by rival academics who viewed AI researchers as charlatans and a drain on research funding. A professor of applied mathematics, Sir James Lighthill, was commissioned by Parliament to evaluate the state of AI research in the nation. The report stated that all of the problems being worked on in AI would be better handled by researchers from other disciplines—such as applied mathematics. The report also claimed that AI successes on toy problems could never scale to real-world applications due to combinatorial explosion.[41] As limitations with weak, domain-independent methods became more and more apparent,[42] researchers from all three traditions began to build knowledge into AI applications.[43][7] The knowledge revolution was driven by the realization that knowledge underlies high-performance, domain-specific AI applications.\n Edward Feigenbaum said:\n to describe that high performance in a specific domain requires both general and highly domain-specific knowledge. Ed Feigenbaum and Doug Lenat called this The Knowledge Principle: \n (1) The Knowledge Principle: if a program is to perform a complex task well, it must know a great deal about the world in which it operates.(2) A plausible extension of that principle, called the Breadth Hypothesis: there are two additional abilities necessary for intelligent behavior in unexpected situations: falling back on increasingly general knowledge, and analogizing to specific but far-flung knowledge.[45] This \"knowledge revolution\" led to the development and deployment of expert systems (introduced by Edward Feigenbaum), the first commercially successful form of AI software.[46][47][48]\n Key expert systems were:\n DENDRAL is considered the first expert system that relied on knowledge-intensive problem-solving. It is described below, by Ed Feigenbaum, from a Communications of the ACM interview, Interview with Ed Feigenbaum:\n One of the people at Stanford interested in computer-based models of mind was Joshua Lederberg, the 1958 Nobel Prize winner in genetics. When I told him I wanted an induction \"sandbox\", he said, \"I have just the one for you.\" His lab was doing mass spectrometry of amino acids. The question was: how do you go from looking at the spectrum of an amino acid to the chemical structure of the amino acid? That's how we started the DENDRAL Project: I was good at heuristic search methods, and he had an algorithm that was good at generating the chemical problem space.\n We did not have a grandiose vision. We worked bottom up. Our chemist was Carl Djerassi, inventor of the chemical behind the birth control pill, and also one of the world's most respected mass spectrometrists. Carl and his postdocs were world-class experts in mass spectrometry. We began to add to their knowledge, inventing knowledge of engineering as we went along. These experiments amounted to titrating DENDRAL more and more knowledge. The more you did that, the smarter the program became. We had very good results.\n \nThe generalization was: in the knowledge lies the power. That was the big idea. In my career that is the huge, \"Ah ha!,\" and it wasn't the way AI was being done previously. Sounds simple, but it's probably AI's most powerful generalization.[51] The other expert systems mentioned above came after DENDRAL. MYCIN exemplifies the classic expert system architecture of a knowledge-base of rules coupled to a symbolic reasoning mechanism, including the use of certainty factors to handle uncertainty. GUIDON shows how an explicit knowledge base can be repurposed for a second application, tutoring, and is an example of an intelligent tutoring system, a particular kind of knowledge-based application. Clancey showed that it was not sufficient simply to use MYCIN's rules for instruction, but that he also needed to add rules for dialogue management and student modeling.[50] XCON is significant because of the millions of dollars it saved DEC, which triggered the expert system boom where most all major corporations in the US had expert systems groups, to capture corporate expertise, preserve it, and automate it:\n By 1988, DEC's AI group had 40 expert systems deployed, with more on the way. DuPont had 100 in use and 500 in development. Nearly every major U.S. corporation had its own Al group and was either using or investigating expert systems.[49] Chess expert knowledge was encoded in Deep Blue. In 1996, this allowed IBM's Deep Blue, with the help of symbolic AI, to win in a game of chess against the world champion at that time, Garry Kasparov.[52]\n A key component of the system architecture for all expert systems is the knowledge base, which stores facts and rules for problem-solving.[53]\nThe simplest approach for an expert system knowledge base is simply a collection or network of production rules. Production rules connect symbols in a relationship similar to an If-Then statement. The expert system processes the rules to make deductions and to determine what additional information it needs, i.e. what questions to ask, using human-readable symbols. For example, OPS5, CLIPS and their successors Jess and Drools operate in this fashion.\n Expert systems can operate in either a forward chaining – from evidence to conclusions – or backward chaining – from goals to needed data and prerequisites – manner. More advanced knowledge-based systems, such as Soar can also perform meta-level reasoning, that is reasoning about their own reasoning in terms of deciding how to solve problems and monitoring the success of problem-solving strategies.\n Blackboard systems are a second kind of knowledge-based or expert system architecture. They model a community of experts incrementally contributing, where they can, to solve a problem. The problem is represented in multiple levels of abstraction or alternate views. The experts (knowledge sources) volunteer their services whenever they recognize they can contribute. Potential problem-solving actions are represented on an agenda that is updated as the problem situation changes. A controller decides how useful each contribution is, and who should make the next problem-solving action. One example, the BB1 blackboard architecture[54] was originally inspired by studies of how humans plan to perform multiple tasks in a trip.[55] An innovation of BB1 was to apply the same blackboard model to solving its control problem, i.e., its controller performed meta-level reasoning with knowledge sources that monitored how well a plan or the problem-solving was proceeding and could switch from one strategy to another as conditions – such as goals or times – changed. BB1 has been applied in multiple domains: construction site planning, intelligent tutoring systems, and real-time patient monitoring.\n At the height of the AI boom, companies such as Symbolics, LMI, and Texas Instruments were selling LISP machines specifically targeted to accelerate the development of AI applications and research. In addition, several artificial intelligence companies, such as Teknowledge and Inference Corporation, were selling expert system shells, training, and consulting to corporations.\n Unfortunately, the AI boom did not last and Kautz best describes the second AI winter that followed:\n Many reasons can be offered for the arrival of the second AI winter. The hardware companies failed when much more cost-effective general Unix workstations from Sun together with good compilers for LISP and Prolog came onto the market. Many commercial deployments of expert systems were discontinued when they proved too costly to maintain. Medical expert systems never caught on for several reasons: the difficulty in keeping them up to date; the challenge for medical professionals to learn how to use a bewildering variety of different expert systems for different medical conditions; and perhaps most crucially, the reluctance of doctors to trust a computer-made diagnosis over their gut instinct, even for specific domains where the expert systems could outperform an average doctor. Venture capital money deserted AI practically overnight. The world AI conference IJCAI hosted an enormous and lavish trade show and thousands of nonacademic attendees in 1987 in Vancouver; the main AI conference the following year, AAAI 1988 in St. Paul, was a small and strictly academic affair.\n[9] Both statistical approaches and extensions to logic were tried.\n One statistical approach, hidden Markov models, had already been popularized in the 1980s for speech recognition work.[11] Subsequently, in 1988, Judea Pearl popularized the use of Bayesian Networks as a sound but efficient way of handling uncertain reasoning with his publication of the book Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.[56] and Bayesian approaches were applied successfully in expert systems.[57] Even later, in the 1990s, statistical relational learning, an approach that combines probability with logical formulas, allowed probability to be combined with first-order logic, e.g., with either Markov Logic Networks or Probabilistic Soft Logic.\n Other, non-probabilistic extensions to first-order logic to support were also tried. For example, non-monotonic reasoning could be used with truth maintenance systems. A truth maintenance system tracked assumptions and justifications for all inferences. It allowed inferences to be withdrawn when assumptions were found out to be incorrect or a contradiction was derived. Explanations could be provided for an inference by explaining which rules were applied to create it and then continuing through underlying inferences and rules all the way back to root assumptions.[58] Lofti Zadeh had introduced a different kind of extension to handle the representation of vagueness. For example, in deciding how \"heavy\" or \"tall\" a man is, there is frequently no clear \"yes\" or \"no\" answer, and a predicate for heavy or tall would instead return values between 0 and 1. Those values represented to what degree the predicates were true. His fuzzy logic further provided a means for propagating combinations of these values through logical formulas.[59]\n Symbolic machine learning approaches were investigated to address the knowledge acquisition bottleneck. One of the earliest is Meta-DENDRAL. Meta-DENDRAL used a generate-and-test technique to generate plausible rule hypotheses to test against spectra. Domain and task knowledge reduced the number of candidates tested to a manageable size. Feigenbaum described Meta-DENDRAL as\n ...the culmination of my dream of the early to mid-1960s having to do with theory formation. The conception was that you had a problem solver like DENDRAL that took some inputs and produced an output. In doing so, it used layers of knowledge to steer and prune the search. That knowledge got in there because we interviewed people. But how did the people get the knowledge? By looking at thousands of spectra. So we wanted a program that would look at thousands of spectra and infer the knowledge of mass spectrometry that DENDRAL could use to solve individual hypothesis formation problems.\nWe did it. We were even able to publish new knowledge of mass spectrometry in the Journal of the American Chemical Society, giving credit only in a footnote that a program, Meta-DENDRAL, actually did it. We were able to do something that had been a dream: to have a computer program come up with a new and publishable piece of science.[51] In contrast to the knowledge-intensive approach of Meta-DENDRAL, Ross Quinlan invented a domain-independent approach to statistical classification, decision tree learning, starting first with ID3[60] and then later extending its capabilities to C4.5.[61] The decision trees created are glass box, interpretable classifiers, with human-interpretable classification rules.\n Advances were made in understanding machine learning theory, too. Tom Mitchell introduced version space learning which describes learning as a search through a space of hypotheses, with upper, more general, and lower, more specific, boundaries encompassing all viable hypotheses consistent with the examples seen so far.[62] More formally, Valiant introduced Probably Approximately Correct Learning (PAC Learning), a framework for the mathematical analysis of machine learning.[63]\n Symbolic machine learning encompassed more than learning by example. E.g., John Anderson provided a cognitive model of human learning where skill practice results in a compilation of rules from a declarative format to a procedural format with his ACT-R cognitive architecture. For example, a student might learn to apply \"Supplementary angles are two angles whose measures sum 180 degrees\" as several different procedural rules. E.g., one rule might say that if X and Y are supplementary and you know X, then Y will be 180 - X. He called his approach \"knowledge compilation\". ACT-R has been used successfully to model aspects of human cognition, such as learning and retention. ACT-R is also used in intelligent tutoring systems, called cognitive tutors, to successfully teach geometry, computer programming, and algebra to school children.[64]\n Inductive logic programming was another approach to learning that allowed logic programs to be synthesized from input-output examples. E.g., Ehud Shapiro's MIS (Model Inference System) could synthesize Prolog programs from examples.[65] John R. Koza applied genetic algorithms to program synthesis to create genetic programming, which he used to synthesize LISP programs. Finally, Zohar Manna and Richard Waldinger provided a more general approach to program synthesis that synthesizes a functional program in the course of proving its specifications to be correct.[66]\n As an alternative to logic, Roger Schank introduced case-based reasoning (CBR). The CBR approach outlined in his book, Dynamic Memory,[67] focuses first on remembering key problem-solving cases for future use and generalizing them where appropriate. When faced with a new problem, CBR retrieves the most similar previous case and adapts it to the specifics of the current problem.[68] Another alternative to logic, genetic algorithms and genetic programming are based on an evolutionary model of learning, where sets of rules are encoded into populations, the rules govern the behavior of individuals, and selection of the fittest prunes out sets of unsuitable rules over many generations.[69]\n Symbolic machine learning was applied to learning concepts, rules, heuristics, and problem-solving. Approaches, other than those above, include:\n With the rise of deep learning, the symbolic AI approach has been compared to deep learning as complementary \"...with parallels having been drawn many times by AI researchers between Kahneman's research on human reasoning and decision making – reflected in his book Thinking, Fast and Slow – and the so-called \"AI systems 1 and 2\", which would in principle be modelled by deep learning and symbolic reasoning, respectively.\" In this view, symbolic reasoning is more apt for deliberative reasoning, planning, and explanation while deep learning is more apt for fast pattern recognition in perceptual applications with noisy data.[17][18]\n Neuro-symbolic AI attempts to integrate neural and symbolic architectures in a manner that addresses strengths and weaknesses of each, in a complementary fashion, in order to support robust AI capable of reasoning, learning, and cognitive modeling. As argued by Valiant[77] and many others,[78] the effective construction of rich computational cognitive models demands the combination of sound symbolic reasoning and efficient (machine) learning models. Gary Marcus, similarly, argues that: \"We cannot construct rich cognitive models in an adequate, automated way without the triumvirate of hybrid architecture, rich prior knowledge, and sophisticated techniques for reasoning.\",[79] and in particular:\n\"To build a robust, knowledge-driven approach to AI we must have the machinery of symbol-manipulation in our toolkit. Too much of useful knowledge is abstract to make do without tools that represent and manipulate abstraction, and to date, the only machinery that we know of that can manipulate such abstract knowledge reliably is the apparatus of symbol manipulation.\"[80]\n Henry Kautz,[19] Francesca Rossi,[81] and Bart Selman[82] have also argued for a synthesis. Their arguments are based on a need to address the two kinds of thinking discussed in Daniel Kahneman's book, Thinking, Fast and Slow. Kahneman describes human thinking as having two components, System 1 and System 2. System 1 is fast, automatic, intuitive and unconscious. System 2 is slower, step-by-step, and explicit. System 1 is the kind used for pattern recognition while System 2 is far better suited for planning, deduction, and deliberative thinking. In this view, deep learning best models the first kind of thinking while symbolic reasoning best models the second kind and both are needed.\n Garcez and Lamb describe research in this area as being ongoing for at least the past twenty years,[83] dating from their 2002 book on neurosymbolic learning systems.[84] A series of workshops on neuro-symbolic reasoning has been held every year since 2005, see http://www.neural-symbolic.org/ for details.\n In their 2015 paper, Neural-Symbolic Learning and Reasoning: Contributions and Challenges, Garcez et al. argue that:\n The integration of the symbolic and connectionist paradigms of AI has been pursued by a relatively small research community over the last two decades and has yielded several significant results. Over the last decade, neural symbolic systems have been shown capable of overcoming the so-called propositional fixation of neural networks, as McCarthy (1988) put it in response to Smolensky (1988); see also (Hinton, 1990). Neural networks were shown capable of representing modal and temporal logics (d'Avila Garcez and Lamb, 2006) and fragments of first-order logic (Bader, Hitzler, Hölldobler, 2008; d'Avila Garcez, Lamb, Gabbay, 2009). Further, neural-symbolic systems have been applied to a number of problems in the areas of bioinformatics, control engineering, software verification and adaptation, visual intelligence, ontology learning, and computer games.[78] Approaches for integration are varied. Henry Kautz's taxonomy of neuro-symbolic architectures, along with some examples, follows:\n Many key research questions remain, such as:\n This section provides an overview of techniques and contributions in an overall context leading to many other, more detailed articles in Wikipedia. Sections on Machine Learning and Uncertain Reasoning are covered earlier in the history section.\n The key AI programming language in the US during the last symbolic AI boom period was LISP. LISP is the second oldest programming language after FORTRAN and was created in 1958 by John McCarthy. LISP provided the first read-eval-print loop to support rapid program development. Compiled functions could be freely mixed with interpreted functions. Program tracing, stepping, and breakpoints were also provided, along with the ability to change values or functions and continue from breakpoints or errors. It had the first self-hosting compiler, meaning that the compiler itself was originally written in LISP and then ran interpretively to compile the compiler code.\n Other key innovations pioneered by LISP that have spread to other programming languages include:\n Programs were themselves data structures that other programs could operate on, allowing the easy definition of higher-level languages.\n In contrast to the US, in Europe the key AI programming language during that same period was Prolog. Prolog provided a built-in store of facts and clauses that could be queried by a read-eval-print loop. The store could act as a knowledge base and the clauses could act as rules or a restricted form of logic. As a subset of first-order logic Prolog was based on Horn clauses with a closed-world assumption—any facts not known were considered false—and a unique name assumption for primitive terms—e.g., the identifier barack_obama was considered to refer to exactly one object. Backtracking and unification are built-in to Prolog.\n Alain Colmerauer and Philippe Roussel are credited as the inventors of Prolog. Prolog is a form of logic programming, which was invented by Robert Kowalski. Its history was also influenced by Carl Hewitt's PLANNER, an assertional database with pattern-directed invocation of methods. For more detail see the section on the origins of Prolog in the PLANNER article.\n Prolog is also a kind of declarative programming. The logic clauses that describe programs are directly interpreted to run the programs specified. No explicit series of actions is required, as is the case with imperative programming languages.\n Japan championed Prolog for its Fifth Generation Project, intending to build special hardware for high performance. Similarly, LISP machines were built to run LISP, but as the second AI boom turned to bust these companies could not compete with new workstations that could now run LISP or Prolog natively at comparable speeds. See the history section for more detail.\n Smalltalk was another influential AI programming language. For example, it introduced metaclasses and, along with Flavors and CommonLoops, influenced the Common Lisp Object System, or (CLOS), that is now part of Common Lisp, the current standard Lisp dialect. CLOS is a Lisp-based object-oriented system that allows multiple inheritance, in addition to incremental extensions to both classes and metaclasses, thus providing a run-time meta-object protocol.[88]\n For other AI programming languages see this list of programming languages for artificial intelligence. Currently, Python, a multi-paradigm programming language, is the most popular programming language, partly due to its extensive package library that supports data science, natural language processing, and deep learning. Python includes a read-eval-print loop, functional elements such as higher-order functions, and object-oriented programming that includes metaclasses.\n Search arises in many kinds of problem solving, including planning, constraint satisfaction, and playing games such as checkers, chess, and go. The best known AI-search tree search algorithms are breadth-first search, depth-first search, A*, and Monte Carlo Search. Key search algorithms for Boolean satisfiability are WalkSAT, conflict-driven clause learning, and the DPLL algorithm. For adversarial search when playing games, alpha-beta pruning, branch and bound, and minimax were early contributions.\n Multiple different approaches to represent knowledge and then reason with those representations have been investigated. Below is a quick overview of approaches to knowledge representation and automated reasoning.\n Semantic networks, conceptual graphs, frames, and logic are all approaches to modeling knowledge such as domain knowledge, problem-solving knowledge, and the semantic meaning of language. Ontologies model key concepts and their relationships in a domain. Example ontologies are YAGO, WordNet, and DOLCE. DOLCE is an example of an upper ontology that can be used for any domain while WordNet is a lexical resource that can also be viewed as an ontology. YAGO incorporates WordNet as part of its ontology, to align facts extracted from Wikipedia with WordNet synsets. The Disease Ontology is an example of a medical ontology currently being used.\n Description logic is a logic for automated classification of ontologies and for detecting inconsistent classification data. OWL is a language used to represent ontologies with description logic. Protégé is an ontology editor that can read in OWL ontologies and then check consistency with deductive classifiers such as such as HermiT.[89]\n First-order logic is more general than description logic. The automated theorem provers discussed below can prove theorems in first-order logic. Horn clause logic is more restricted than first-order logic and is used in logic programming languages such as Prolog. Extensions to first-order logic include temporal logic, to handle time; epistemic logic, to reason about agent knowledge; modal logic, to handle possibility and necessity; and probabilistic logics to handle logic and probability together.\n Examples of automated theorem provers for first-order logic are:\n Prover9 can be used in conjunction with the Mace4 model checker. ACL2 is a theorem prover that can handle proofs by induction and is a descendant of the Boyer-Moore Theorem Prover, also known as Nqthm.\n Knowledge-based systems have an explicit knowledge base, typically of rules, to enhance reusability across domains by separating procedural code and domain knowledge. A separate inference engine processes rules and adds, deletes, or modifies a knowledge store.\n Forward chaining inference engines are the most common, and are seen in CLIPS and OPS5. Backward chaining occurs in Prolog, where a more limited logical representation is used, Horn Clauses. Pattern-matching, specifically unification, is used in Prolog.\n A more flexible kind of problem-solving occurs when reasoning about what to do next occurs, rather than simply choosing one of the available actions. This kind of meta-level reasoning is used in Soar and in the BB1 blackboard architecture.\n Cognitive architectures such as ACT-R may have additional capabilities, such as the ability to compile frequently used knowledge into higher-level chunks.\n Marvin Minsky first proposed frames as a way of interpreting common visual situations, such as an office, and Roger Schank extended this idea to scripts for common routines, such as dining out. Cyc has attempted to capture useful common-sense knowledge and has \"micro-theories\" to handle particular kinds of domain-specific reasoning.\n Qualitative simulation, such as Benjamin Kuipers's QSIM,[90] approximates human reasoning about naive physics, such as what happens when we heat a liquid in a pot on the stove. We expect it to heat and possibly boil over, even though we may not know its temperature, its boiling point, or other details, such as atmospheric pressure.\n Similarly, Allen's temporal interval algebra is a simplification of reasoning about time and Region Connection Calculus is a simplification of reasoning about spatial relationships. Both can be solved with constraint solvers.\n Constraint solvers perform a more limited kind of inference than first-order logic. They can simplify sets of spatiotemporal constraints, such as those for RCC or Temporal Algebra, along with solving other kinds of puzzle problems, such as Wordle, Sudoku, cryptarithmetic problems, and so on. Constraint logic programming can be used to solve scheduling problems, for example with constraint handling rules (CHR).\n The General Problem Solver (GPS) cast planning as problem-solving used means-ends analysis to create plans. STRIPS took a different approach, viewing planning as theorem proving. Graphplan takes a least-commitment approach to planning, rather than sequentially choosing actions from an initial state, working forwards, or a goal state if working backwards.  Satplan is an approach to planning where a planning problem is reduced to a Boolean satisfiability problem.\n Natural language processing focuses on treating language as data to perform tasks such as identifying topics without necessarily understanding the intended meaning. Natural language understanding, in contrast, constructs a meaning representation and uses that for further processing, such as answering questions.\n Parsing, tokenizing, spelling correction, part-of-speech tagging, noun and verb phrase chunking are all aspects of natural language processing long handled by symbolic AI, but since improved by deep learning approaches. In symbolic AI, discourse representation theory and first-order logic have been used to represent sentence meanings. Latent semantic analysis (LSA) and explicit semantic analysis also provided vector representations of documents. In the latter case, vector components are interpretable as concepts named by Wikipedia articles.\n New deep learning approaches based on Transformer models have now eclipsed these earlier symbolic AI approaches and attained state-of-the-art performance in natural language processing. However, Transformer models are opaque and do not yet produce human-interpretable semantic representations for sentences and documents. Instead, they produce task-specific vectors where the meaning of the vector components is opaque.\n Agents are autonomous systems embedded in an environment they perceive and act upon in some sense. Russell and Norvig's standard textbook on artificial intelligence is organized to reflect agent architectures of increasing sophistication.[91] The sophistication of agents varies from simple reactive agents, to those with a model of the world and automated planning capabilities, possibly a BDI agent, i.e., one with beliefs, desires, and intentions – or alternatively a reinforcement learning model learned over time to choose actions – up to a combination of alternative architectures, such as a neuro-symbolic architecture[87] that includes deep learning for perception.[92]\n In contrast, a multi-agent system consists of multiple agents that communicate amongst themselves with some inter-agent communication language such as Knowledge Query and Manipulation Language (KQML). The agents need not all have the same internal architecture. Advantages of multi-agent systems include the ability to divide work among the agents and to increase fault tolerance when agents are lost. Research problems include how agents reach consensus, distributed problem solving, multi-agent learning, multi-agent planning, and distributed constraint optimization.\n Controversies arose from early on in symbolic AI, both within the field—e.g., between logicists (the pro-logic \"neats\") and non-logicists (the anti-logic \"scruffies\")—and between those who embraced AI but rejected symbolic approaches—primarily connectionists—and those outside the field. Critiques from outside of the field were primarily from philosophers, on intellectual grounds, but also from funding agencies, especially during the two AI winters.\n Limitations were discovered in using simple first-order logic to reason about dynamic domains. Problems were discovered both with regards to enumerating the preconditions for an action to succeed and in providing axioms for what did not change after an action was performed.\n McCarthy and Hayes introduced the Frame Problem in 1969 in the paper, \"Some Philosophical Problems from the Standpoint of Artificial Intelligence.\"[93] A simple example occurs in \"proving that one person could get into conversation with another\", as an axiom asserting \"if a person has a telephone he still has it after looking up a number in the telephone book\" would be required for the deduction to succeed. Similar axioms would be required for other domain actions to specify what did not change.\n A similar problem, called the Qualification Problem, occurs in trying to enumerate the preconditions for an action to succeed. An infinite number of pathological conditions can be imagined, e.g., a banana in a tailpipe could prevent a car from operating correctly.\n McCarthy's approach to fix the frame problem was circumscription, a kind of non-monotonic logic where deductions could be made from actions that need only specify what would change while not having to explicitly specify everything that would not change. Other non-monotonic logics provided truth maintenance systems that revised beliefs leading to contradictions.\n Other ways of handling more open-ended domains included probabilistic reasoning systems and machine learning to learn new concepts and rules.  McCarthy's Advice Taker can be viewed as an inspiration here, as it could incorporate new knowledge provided by a human in the form of assertions or rules. For example, experimental symbolic machine learning systems explored the ability to take high-level natural language advice and to interpret it into domain-specific actionable rules.\n Similar to the problems in handling dynamic domains, common-sense reasoning is also difficult to capture in formal reasoning. Examples of common-sense reasoning include implicit reasoning about how people think or general knowledge of day-to-day events, objects, and living creatures.  This kind of knowledge is taken for granted and not viewed as noteworthy. Common-sense reasoning is an open area of research and challenging both for symbolic systems (e.g., Cyc has attempted to capture key parts of this knowledge over more than a decade) and neural systems (e.g., self-driving cars that do not know not to drive into cones or not to hit pedestrians walking a bicycle).\n McCarthy viewed his Advice Taker as having common-sense, but his definition of common-sense was different than the one above.[94] He defined a program as having common sense \"if it automatically deduces for itself a sufficiently wide class of immediate consequences of anything it is told and what it already knows.\"\n Connectionist approaches include earlier work on neural networks,[95] such as perceptrons; work in the mid to late 80s, such as Danny Hillis's Connection Machine and Yann LeCun's advances in convolutional neural networks; to today's more advanced approaches, such as Transformers, GANs, and other work in deep learning.\n Three philosophical positions[96] have been outlined among connectionists:\n \nOlazaran, in his sociological history of the controversies within the neural network community, described the moderate connectionism view as essentially compatible with current research in neuro-symbolic hybrids: \nThe third and last position I would like to examine here is what I call the moderate connectionist view, a more eclectic view of the current debate between connectionism and symbolic AI. One of the researchers who has elaborated this position most explicitly is Andy Clark, a philosopher from the School of Cognitive and Computing Sciences of the University of Sussex (Brighton, England). Clark defended hybrid (partly symbolic, partly connectionist) systems. He claimed that (at least) two kinds of theories are needed in order to study and model cognition. On the one hand, for some information-processing tasks (such as pattern recognition) connectionism has advantages over symbolic models. But on the other hand, for other cognitive processes (such as serial, deductive reasoning, and generative symbol manipulation processes) the symbolic paradigm offers adequate models, and not only \"approximations\" (contrary to what radical connectionists would claim).[97] Gary Marcus has claimed that the animus in the deep learning community against symbolic approaches now may be more sociological than philosophical: To think that we can simply abandon symbol-manipulation is to suspend disbelief.\n \nAnd yet, for the most part, that's how most current AI proceeds. Hinton and many others have tried hard to banish symbols altogether. The deep learning hope—seemingly grounded not so much in science, but in a sort of historical grudge—is that intelligent behavior will emerge purely from the confluence of massive data and deep learning. Where classical computers and software solve tasks by defining sets of symbol-manipulating rules dedicated to particular jobs, such as editing a line in a word processor or performing a calculation in a spreadsheet, neural networks typically try to solve tasks by statistical approximation and learning from examples. According to Marcus, Geoffrey Hinton and his colleagues have been vehemently \"anti-symbolic\": When deep learning reemerged in 2012, it was with a kind of take-no-prisoners attitude that has characterized most of the last decade. By 2015, his hostility toward all things symbols had fully crystallized. He gave a talk at an AI workshop at Stanford comparing symbols to aether, one of science's greatest mistakes.\n ...\n \nSince then, his anti-symbolic campaign has only increased in intensity. In 2016, Yann LeCun, Bengio, and Hinton wrote a manifesto for deep learning in one of science's most important journals, Nature. It closed with a direct attack on symbol manipulation, calling not for reconciliation but for outright replacement. Later, Hinton told a gathering of European Union leaders that investing any further money in symbol-manipulating approaches was \"a huge mistake,\" likening it to investing in internal combustion engines in the era of electric cars.[98] Part of these disputes may be due to unclear terminology: \n Turing award winner Judea Pearl offers a critique of machine learning which, unfortunately, conflates the terms machine learning and deep learning. Similarly, when Geoffrey Hinton refers to symbolic AI, the connotation of the term tends to be that of expert systems dispossessed of any ability to learn. The use of the terminology is in need of clarification. Machine learning is not confined to association rule mining, c.f. the body of work on symbolic ML and relational learning (the differences to deep learning being the choice of representation, localist logical rather than distributed, and the non-use of gradient-based learning algorithms). Equally, symbolic AI is not just about production rules written by hand. A proper definition of AI concerns knowledge representation and reasoning, autonomous multi-agent systems, planning and argumentation, as well as learning.[99] Another critique of symbolic AI is the embodied cognition approach:\n The embodied cognition approach claims that it makes no sense to consider the brain separately: cognition takes place within a body, which is embedded in an environment. We need to study the system as a whole; the brain's functioning exploits regularities in its environment, including the rest of its body. Under the embodied cognition approach, robotics, vision, and other sensors become central, not peripheral.[100] Rodney Brooks invented behavior-based robotics, one approach to embodied cognition. Nouvelle AI, another name for this approach, is viewed as an alternative to both symbolic AI and connectionist AI. His approach rejected representations, either symbolic or distributed, as not only unnecessary, but as detrimental. Instead, he created the subsumption architecture, a layered architecture for embodied agents. Each layer achieves a different purpose and must function in the real world. For example, the first robot he describes in Intelligence Without Representation, has three layers. The bottom layer interprets sonar sensors to avoid objects. The middle layer causes the robot to wander around when there are no obstacles. The top layer causes the robot to go to more distant places for further exploration. Each layer can temporarily inhibit or suppress a lower-level layer. He criticized AI researchers for defining AI problems for their systems, when: \"There is no clean division between perception (abstraction) and reasoning in the real world.\"[101] He called his robots \"Creatures\" and each layer was \"composed of a fixed-topology network of simple finite state machines.\"[102]  In the Nouvelle AI approach, \"First, it is vitally important to test the Creatures we build in the real world; i.e., in the same world that we humans inhabit. It is disastrous to fall into the temptation of testing them in a simplified world first, even with the best intentions of later transferring activity to an unsimplified world.\"[103]  His emphasis on real-world testing was in contrast to \"Early work in AI concentrated on games, geometrical problems, symbolic algebra, theorem proving, and other formal systems\"[104] and the use of the blocks world in symbolic AI systems such as SHRDLU.\n Each approach—symbolic, connectionist, and behavior-based—has advantages, but has been criticized by the other approaches. Symbolic AI has been criticized as disembodied, liable to the qualification problem, and poor in handling the perceptual problems where deep learning excels. In turn, connectionist AI has been criticized as poorly suited for deliberative step-by-step problem solving, incorporating knowledge, and handling planning. Finally, Nouvelle AI excels in reactive and real-world robotics domains but has been criticized for difficulties in incorporating learning and knowledge.\n Hybrid AIs incorporating one or more of these approaches are currently viewed as the path forward.[19][81][82] Russell and Norvig conclude that: Overall, Dreyfus saw areas where AI did not have complete answers and said that Al is therefore impossible; we now see many of these same areas undergoing continued research and development leading to increased capability, not impossibility.[100]"
    },
    {
        "title": "Deep learning",
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "content": "Deep learning is a subset of machine learning that focuses on utilizing neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be either supervised, semi-supervised or unsupervised.[2]\n Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.[3][4][5]\n Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.[6]\n Most modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.[7]\n Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face.\n Importantly, a deep learning process can learn which features to optimally place at which level on its own. Prior to deep learning, machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate on. In the deep learning approach, features are not hand-crafted and the model discovers useful feature representations from the data automatically. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.[8][2]\n The word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.[9] No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than two. CAP of depth two has been shown to be a universal approximator in the sense that it can emulate any function.[10] Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > two) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.\n Deep learning architectures can be constructed with a greedy layer-by-layer method.[11] Deep learning helps to disentangle these abstractions and pick out which features improve performance.[8]\n Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks.[8][12]\n The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986,[13] and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.[14][15] Although the history of its appearance is apparently more complicated.[16]\n Deep neural networks are generally interpreted in terms of the universal approximation theorem[17][18][19][20][21] or probabilistic inference.[22][23][8][9][24]\n The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions.[17][18][19][20] In 1989, the first proof was published by George Cybenko for sigmoid activation functions[17] and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik.[18] Recent work also showed that universal approximation also holds for non-bounded activation functions such as Kunihiko Fukushima's rectified linear unit.[25][26]\n The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al.[21] proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; if the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator.\n The probabilistic interpretation[24] derives from the field of machine learning. It features inference,[23][7][8][9][12][24] as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function.[24] The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.[27]\n There are two types of artificial neural network (ANN): feedforward neural network (FNN) or multilayer perceptron (MLP) and recurrent neural networks (RNN). RNNs have cycles in their connectivity structure, FNNs don't. In the 1920s, Wilhelm Lenz and Ernst Ising created the Ising model[28][29] which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive.[30][31] His learning RNN was republished by John Hopfield in 1982.[32] Other early recurrent neural networks were published by Kaoru Nakano in 1971.[33][34] Already in 1948, Alan Turing produced work on \"Intelligent Machinery\"  that was not published in his lifetime,[35] containing \"ideas related to artificial evolution and learning RNNs\".[31]\n Frank Rosenblatt (1958)[36] proposed the perceptron, an MLP with 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. He later published a 1962 book that also introduced variants and computer experiments, including a version with four-layer perceptrons \"with adaptive preterminal networks\" where the last two layers have learned weights (here he credits H. D. Block and B. W. Knight).[37]: section 16  The book cites an earlier network by R. D. Joseph (1960)[38] \"functionally equivalent to a variation of\" this four-layer system (the book mentions Joseph over 30 times). Should Joseph therefore be considered the originator of proper adaptive multilayer perceptrons with learning hidden units? Unfortunately, the learning algorithm was not a functional one, and fell into oblivion.\n The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in 1965. They regarded it as a form of polynomial regression,[39] or a generalization of Rosenblatt's perceptron.[40] A 1971 paper described a deep network with eight layers trained by this method,[41] which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates\".[31]\n The first deep learning multilayer perceptron trained by stochastic gradient descent[42] was published in 1967 by Shun'ichi Amari.[43] In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned  internal representations to classify non-linearily separable pattern classes.[31] Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.\n In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function.[25][31] The rectifier has become the most popular activation function for deep learning.[44]\n Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation.[45][46]\n Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673[47] to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt,[37] but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory.[48] The modern form of backpropagation was first published in Seppo Linnainmaa's master thesis (1970).[49][50][31] G.M. Ostrovski et al. republished it in 1971.[51][52] Paul Werbos applied backpropagation to neural networks in 1982[53] (his 1974 PhD thesis, reprinted in a 1994 book,[54] did not yet describe the algorithm[52]). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work.[55][56]\n The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation.[57][58]  In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition.[59] \nIn 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days.[60] In 1990, Wei Zhang implemented a CNN on optical computing hardware.[61] In 1991, a CNN was applied to medical image object segmentation[62] and breast cancer detection in mammograms.[63] LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images.[64]\n Recurrent neural networks (RNN)[28][30] were further developed in the 1980s. Recurrence is used for sequence processing, and when a recurrent network is unrolled, it mathematically resembles a deep feedforward layer. Consequently, they have similar properties and issues, and their developments had mutual influences. In RNN, two early influential works were the Jordan network (1986)[65] and the Elman network (1990),[66] which applied RNN to study problems in cognitive psychology.\n In the 1980s, backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem, in 1991, Jürgen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning where each RNN tries to predict its own next input, which is the next unexpected input of the RNN below.[67][68] This \"neural history compressor\" uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by  distilling a higher level chunker network into a lower level automatizer network.[67][68][31] In 1993, a neural history compressor solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.[69] The \"P\" in ChatGPT refers to such pre-training.\n Sepp Hochreiter's diploma thesis (1991)[70] implemented the neural history compressor,[67] and identified and analyzed the vanishing gradient problem.[70][71]  Hochreiter proposed recurrent residual connections to solve the vanishing gradient problem. This led to the long short-term memory (LSTM), published in 1995.[72] LSTM can learn \"very deep learning\" tasks[9] with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. That LSTM was not yet the modern architecture, which required a \"forget gate\", introduced in 1999,[73] which became the standard RNN architecture.\n In 1991, Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss.[74][75] The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called \"artificial curiosity\". In 2014, this principle was used in generative adversarial networks (GANs).[76]\n During 1985–1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine,[77] restricted Boltzmann machine,[78] Helmholtz machine,[79] and the wake-sleep algorithm.[80] These were designed for unsupervised learning of deep generative models. However, those were more computationally expensive compared to backpropagation. Boltzmann machine learning algorithm, published in 1985, was briefly popular before being eclipsed by the backpropagation algorithm in 1986. (p. 112 [81]). A 1988 network became state of the art in protein structure prediction, an early application of deep learning to bioinformatics.[82]\n Both shallow and deep learning (e.g., recurrent nets) of ANNs for speech recognition have been explored for many years.[83][84][85] These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.[86] Key difficulties have been analyzed, including gradient diminishing[70] and weak temporal correlation structure in neural predictive models.[87][88] Additional difficulties were the lack of training data and limited computing power.\n Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI researched in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 NIST Speaker Recognition benchmark.[89][90] It was deployed in the Nuance Verifier, representing the first major industrial application of deep learning.[91]\n The principle of elevating \"raw\" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the \"raw\" spectrogram or linear filter-bank features in the late 1990s,[90] showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.[92]\n Neural networks entered a null, and simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) became the preferred choices in the 1990s and 2000s, because of artificial neural networks' computational cost and a lack of understanding of how the brain wires its biological networks.[citation needed]\n In 2003, LSTM became competitive with traditional speech recognizers on certain tasks.[93] In 2006, Alex Graves, Santiago Fernández, Faustino Gomez, and Schmidhuber combined it with connectionist temporal classification (CTC)[94] in stacks of LSTMs.[95] In 2009, it became the first RNN to win a pattern recognition contest, in connected handwriting recognition.[96][9]\n In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh[97][98] deep belief networks were developed for generative modeling. They are trained by training one restricted Boltzmann machine, then freezing it and training another one on top of the first one, and so on, then optionally fine-tuned using supervised backpropagation.[99] They could model high-dimensional probability distributions, such as the distribution of MNIST images, but convergence was slow.[100][101][102]\n The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun.[103] Industrial applications of deep learning to large-scale speech recognition started around 2010.\n The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems.[104] The nature of the recognition errors produced by the two types of systems was characteristically different,[105] offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems.[23][106][107] Analysis around 2009–2010, contrasting the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition.[105]  That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.[104][105][108]\nIn 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.[109][110][111][106]\n The deep learning revolution started around CNN- and GPU-based computer vision.\n Although CNNs trained by backpropagation had been around for decades and GPU implementations of NNs for years,[112] including CNNs,[113] faster implementations of CNNs on GPUs were needed to progress on computer vision. Later, as deep learning becomes widespread, specialized hardware and algorithm optimizations were developed specifically for deep learning.[114]\n A key advance for the deep learning revolution was hardware advances, especially GPU. Some early work dated back to 2004.[112][113] In 2009, Raina, Madhavan, and Andrew Ng reported a 100M deep belief network trained on 30 Nvidia GeForce GTX 280 GPUs, an early demonstration of GPU-based deep learning. They reported up to 70 times faster training.[115]\n In 2011, a CNN named DanNet[116][117] by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3.[9] It then won more contests.[118][119] They also showed how max-pooling CNNs on GPU improved performance significantly.[3]\n In 2012, Andrew Ng and Jeff Dean created an FNN that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos.[120]\n In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton[4] won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman[121] and Google's Inceptionv3.[122]\n The success in image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.[123][124][125]\n In 2014, the state of the art was training “very deep neural network” with 20 to 30 layers.[126] Stacking too many layers led to a steep reduction in training accuracy,[127] known as the \"degradation\" problem.[128] In 2015, two techniques were developed to train very deep networks: the Highway Network was published in May 2015, and the residual neural network (ResNet)[129] in Dec 2015. ResNet behaves like an open-gated Highway Net.\n Around the same time, deep learning started impacting the field of art. Early examples included Google DeepDream (2015), and neural style transfer (2015),[130] both of which were based on pretrained image classification neural networks, such as VGG-19.\n Generative adversarial network (GAN) by (Ian Goodfellow et al., 2014)[131] (based on  Jürgen Schmidhuber's principle of artificial curiosity[74][76])\nbecame state of the art in generative modeling during 2014-2018 period. Excellent image quality is achieved by Nvidia's StyleGAN (2018)[132] based on the Progressive GAN by Tero Karras et al.[133] Here the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes.[134] Diffusion models (2015)[135] eclipsed GANs in generative modeling since then, with systems such as DALL·E 2 (2022) and Stable Diffusion (2022).\n In 2015, Google's speech recognition improved by 49% by an LSTM-based model, which they made available through Google Voice Search on smartphone.[136][137]\n In 2017, Topological deep learning was introduced by integrating topological data analysis and convolutional neural networks.\n[138]\nTopological deep learning surpasses competing methods in predicting protein-ligand binding affinities and protein stability changes caused by mutations.\n In 2017-2019, mathematical deep learning achieved first place in multiple categories of the D3R Grand Challenges, an annual competition series focused on computer-aided drug design. \n[139]\n[140]\n Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved.[104][141] Convolutional neural networks were superseded for ASR by LSTM.[137][142][143][144] but are more successful in computer vision.\n Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the 2018 Turing Award for \"conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing\".[145]\n Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.\n An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.\n Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.\n The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.\n Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\n As of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, or playing \"Go\"[147]).\n A deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers.[7][9] There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions.[148] These components as a whole function in a way that mimics functions of the human brain, and can be trained like any other ML algorithm.[citation needed]\n For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer, [149] and complex DNN have many layers, hence the name \"deep\" networks. \n DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives.[150] The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network.[7] For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks.[151]\n Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.[149]\n DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or \"weights\", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights.[152] That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.\n Recurrent neural networks, in which data can flow in any direction, are used for applications such as language modeling.[153][154][155][156][157] Long short-term memory is particularly effective for this use.[158][159]\n Convolutional neural networks (CNNs) are used in computer vision.[160] CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).[161]\n As with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time.\n DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning[41] or weight decay (\n\n\n\n\nℓ\n\n2\n\n\n\n\n{\\displaystyle \\ell _{2}}\n\n-regularization) or sparsity (\n\n\n\n\nℓ\n\n1\n\n\n\n\n{\\displaystyle \\ell _{1}}\n\n-regularization) can be applied during training to combat overfitting.[162] Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies.[163] Another interesting recent development is research into models of just enough complexity through an estimation of the intrinsic complexity of the task being modelled. This approach has been successfully applied for multivariate time series prediction tasks such as traffic prediction.[164] Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.[165]\n DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples)[166] speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.[167][168]\n Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.[169][170]\n Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.[171] By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method for training large-scale commercial cloud AI .[172] OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017) and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months.[173][174]\n Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) in Huawei cellphones[175] and cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform.[176] Cerebras Systems has also built a dedicated system to handle large deep learning models, the CS-2, based on the largest processor in the industry, the second-generation Wafer Scale Engine (WSE-2).[177][178]\n Atomically thin semiconductors are considered promising for energy-efficient deep learning hardware where the same basic device structure is used for both logic operations and data storage.\nIn 2020, Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs).[179]\n In 2021, J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing.[180] The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs, and (2) extremely high data modulation speeds.[180] Their system can execute trillions of multiply-accumulate operations per second, indicating the potential of integrated photonics in data-heavy AI applications.[180]\n Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn \"Very Deep Learning\" tasks[9] that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates[159] is competitive with traditional speech recognizers on certain tasks.[93]\n The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences.[181] Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.\n The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007, accelerated progress in eight major areas:[23][108][106]\n All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.[23][186][187]\n A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.[188]\n Deep learning-based image recognition has become \"superhuman\", producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs, and in 2014, with recognition of human faces.[189][190]\n Deep learning-trained vehicles now interpret 360° camera views.[191] Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.\n Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of\n Neural networks have been used for implementing language models since the early 2000s.[153] LSTM helped to improve machine translation and language modeling.[154][155][156]\n Other key techniques in this field are negative sampling[194] and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN.[195] Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing.[195] Deep neural architectures provide the best results for constituency parsing,[196] sentiment analysis,[197] information retrieval,[198][199] spoken language understanding,[200] machine translation,[154][201] contextual entity linking,[201] writing style recognition,[202] named-entity recognition (token classification),[203] text classification, and others.[204]\n Recent developments generalize word embedding to sentence embedding.\n Google Translate (GT) uses a large end-to-end long short-term memory (LSTM) network.[205][206][207][208] Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system \"learns from millions of examples\".[206] It translates \"whole sentences at a time, rather than pieces\". Google Translate supports over one hundred languages.[206] The network encodes the \"semantics of the sentence rather than simply memorizing phrase-to-phrase translations\".[206][209] GT uses English as an intermediate between most language pairs.[209]\n A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects.[210][211] Research has explored use of deep learning to predict the biomolecular targets,[212][213] off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.[214][215][216]\n The integration of advanced mathematics, such as persistent homology and graph theory, and deep neural networks gives rise to victories in drug scoring and pose prediction. [138] [139] [140]\n AtomNet is a deep learning system for structure-based rational drug design.[217] AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus[218] and multiple sclerosis.[219][218]\n In 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set.[220] In 2019, generative neural networks were used to produce molecules that were validated experimentally all the way into mice.[221][222]\n Deep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value.[223]\n Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations.[224][225] Multi-view deep learning has been applied for learning user preferences from multiple domains.[226] The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.\n An autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.[227]\n In medical informatics, deep learning was used to predict sleep quality based on data from wearables[228] and predictions of health complications from electronic health record data.[229]\n Deep neural networks have shown unparalleled performance in predicting protein structure, according to the sequence of the amino acids that make it up. In 2020, AlphaFold, a deep-learning based system, achieved a level of accuracy significantly higher than all previous computational methods.[230][231]\n Deep neural networks can be used to estimate the entropy of a stochastic process and called Neural Joint Entropy Estimator (NJEE).[232] Such an estimation provides insights on the effects of input random variables on an independent random variable. Practically, the DNN is trained as a classifier that maps an input vector or matrix X to an output probability distribution over the possible classes of random variable Y, given input X. For example, in image classification tasks, the NJEE maps a vector of pixels' color values to probabilities over possible image classes. In practice, the probability distribution of Y is obtained by a Softmax layer with number of nodes that is equal to the alphabet size of Y. NJEE uses continuously differentiable activation functions, such that the conditions for the universal approximation theorem holds. It is shown that this method provides a strongly consistent estimator and outperforms other methods in case of large alphabet sizes.[232]\n Deep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement.[233][234] Modern deep learning tools demonstrate the high accuracy of detecting various diseases and the helpfulness of their use by specialists to improve the diagnosis efficiency.[235][236]\n Finding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server.[237] Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection.\n Deep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization.[238] These applications include learning methods such as \"Shrinkage Fields for Effective Image Restoration\"[239] which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.\n Deep learning is being successfully applied to financial fraud detection, tax evasion detection,[240] and anti-money laundering.[241]\n In November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.[242][243][244]\n The United States Department of Defense applied deep learning to train robots in new tasks through observation.[245]\n Physics informed neural networks have been used to solve partial differential equations in both forward and inverse problems in a data driven manner.[246] One example is the reconstructing fluid flow governed by the Navier-Stokes equations. Using physics informed neural networks does not require the often expensive mesh generation that conventional CFD methods rely on.[247][248]\n Deep backward stochastic differential equation method is a numerical method that combines deep learning with Backward stochastic differential equation (BSDE). This method is particularly useful for solving high-dimensional problems in financial mathematics. By leveraging the powerful function approximation capabilities of deep neural networks, deep BSDE addresses the computational challenges faced by traditional numerical methods in high-dimensional settings. Specifically, traditional methods like finite difference methods or Monte Carlo simulations often struggle with the curse of dimensionality, where computational cost increases exponentially with the number of dimensions. Deep BSDE methods, however, employ deep neural networks to approximate solutions of high-dimensional partial differential equations (PDEs), effectively reducing the computational burden.[249]\n In addition, the integration of Physics-informed neural networks (PINNs) into the deep BSDE framework enhances its capability by embedding the underlying physical laws directly into the neural network architecture. This ensures that the solutions not only fit the data but also adhere to the governing stochastic differential equations. PINNs leverage the power of deep learning while respecting the constraints imposed by the physical models, resulting in more accurate and reliable solutions for financial mathematics problems.\n Image reconstruction is the reconstruction of the underlying images from the image-related measurements. Several works showed the better and superior performance of the deep learning methods compared to analytical methods for various applications, e.g., spectral imaging [250] and ultrasound imaging.[251]\n Traditional weather prediction systems solve a very complex system of partial differential equations. GraphCast is a deep learning based model, trained on a long history of weather data to predict how weather patterns change over time. It is able to  predict weather conditions for up to 10 days globally, at a very detailed level, and in under a minute, with precision similar to state of the art systems.[252][253]\n An epigenetic clock is a biochemical test that can be used to measure age. Galkin et al. used deep neural networks to train an epigenetic aging clock of unprecedented accuracy using >6,000 blood samples.[254] The clock uses information from 1000 CpG sites and predicts people with certain conditions older than healthy controls: IBD, frontotemporal dementia, ovarian cancer, obesity. The aging clock was planned to be released for public use in 2021 by an Insilico Medicine spinoff company Deep Longevity.\n Deep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s.[255][256][257][258] These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, \"...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature\".[259]\n A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism.[260][261] Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality.[262][263] In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.[264]\n Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons[265] and neural populations.[266] Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system[267] both at the single-unit[268] and at the population[269] levels.\n Facebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.[270]\n Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player.[271][272][273] Google Translate uses a neural network to translate between more than 100 languages.\n In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories.[274]\n As of 2008,[275] researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor.[245] First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation.[245] Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as \"good job\" and \"bad job\".[276]\n Deep learning has attracted both criticism and comment, in some cases from outside the field of computer science.\n A main criticism concerns the lack of theory surrounding some methods.[277] Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear.[citation needed] (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.[278]\n Others point out that deep learning should be looked at as a step towards realizing strong AI[disambiguation needed], not as an all-encompassing solution. Despite the power of deep learning methods, they still lack much of the functionality needed to realize this goal entirely. Research psychologist Gary Marcus noted:\n Realistically, deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used. The most powerful A.I. systems, like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques, ranging from the statistical technique of Bayesian inference to deductive reasoning.[279]\n In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained[280] demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's[281] website.\n Some deep learning architectures display problematic behaviors,[282] such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014)[283] and misclassifying minuscule perturbations of correctly classified images (2013).[284] Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures.[282] These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar[285] decompositions of observed entities and events.[282] Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition[286] and artificial intelligence (AI).[287]\n As deep learning moves from the lab into the world, research and experience show that artificial neural networks are vulnerable to hacks and deception.[288] By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an \"adversarial attack\".[289]\n In 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points, and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system.[290] One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.[291]\n Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.[290]\n ANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.[290]\n In 2016, another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address, and hypothesized that this could \"serve as a stepping stone for further attacks (e.g., opening a web page hosting drive-by malware)\".[290]\n In \"data poisoning\", false data is continually smuggled into a machine learning system's training set to prevent it from achieving mastery.[290]\n The deep learning systems that are trained using supervised learning often rely on data that is created and/or annotated by humans.[292] It has been argued that not only low-paid clickwork (such as on Amazon Mechanical Turk) is regularly deployed for this purpose, but also implicit forms of human microwork that are often not recognized as such.[293] The philosopher Rainer Mühlhoff distinguishes five types of \"machinic capture\" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game), (2) \"trapping and tracking\" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages), (3) exploitation of social motivations (e.g. tagging faces on Facebook to obtain labeled facial images), (4) information mining (e.g. by leveraging quantified-self devices such as activity trackers) and (5) clickwork.[293]\n"
    },
    {
        "title": "Bayesian network",
        "url": "https://en.wikipedia.org/wiki/Bayesian_network",
        "content": "A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG).[1] While it is one of several forms of causal notation, causal networks are special cases of Bayesian networks. Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.\n Efficient algorithms can perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.\n Formally, Bayesian networks are directed acyclic graphs (DAGs) whose nodes represent variables in the Bayesian sense: they may be observable quantities, latent variables, unknown parameters or hypotheses. Each edge represents a direct conditional dependency. Any pair of nodes that are not connected (i.e. no path connects one node to the other) represent variables that are conditionally independent of each other. Each node is associated with a probability function that takes, as input, a particular set of values for the node's parent variables, and gives (as output) the probability (or probability distribution, if applicable) of the variable represented by the node. For example, if \n\n\n\nm\n\n\n{\\displaystyle m}\n\n parent nodes represent \n\n\n\nm\n\n\n{\\displaystyle m}\n\n Boolean variables, then the probability function could be represented by a table of \n\n\n\n\n2\n\nm\n\n\n\n\n{\\displaystyle 2^{m}}\n\n entries, one entry for each of the \n\n\n\n\n2\n\nm\n\n\n\n\n{\\displaystyle 2^{m}}\n\n possible parent combinations. Similar ideas may be applied to undirected, and possibly cyclic, graphs such as Markov networks.\n Let us use an illustration to enforce the concepts of a Bayesian network. Suppose we want to model the dependencies between three variables: the sprinkler (or more appropriately, its state - whether it is on or not), the presence or absence of rain and whether the grass is wet or not. Observe that two events can cause the grass to become wet: an active sprinkler or rain. Rain has a direct effect on the use of the sprinkler (namely that when it rains, the sprinkler usually is not active). This situation can be modeled with a Bayesian network (shown to the right). Each variable has two possible values, T (for true) and F (for false).\n The joint probability function is, by the chain rule of probability,\n where G = \"Grass wet (true/false)\", S = \"Sprinkler turned on (true/false)\", and R = \"Raining (true/false)\".\n The model can answer questions about the presence of a cause given the presence of an effect (so-called inverse probability) like \"What is the probability that it is raining, given the grass is wet?\" by using the conditional probability formula and summing over all nuisance variables:\n Using the expansion for the joint probability function \n\n\n\nPr\n(\nG\n,\nS\n,\nR\n)\n\n\n{\\displaystyle \\Pr(G,S,R)}\n\n and the conditional probabilities from the conditional probability tables (CPTs) stated in the diagram, one can evaluate each term in the sums in the numerator and denominator. For example,\n Then the numerical results (subscripted by the associated variable values) are\n To answer an interventional question, such as \"What is the probability that it would rain, given that we wet the grass?\" the answer is governed by the post-intervention joint distribution function\n obtained by removing the factor \n\n\n\nPr\n(\nG\n∣\nS\n,\nR\n)\n\n\n{\\displaystyle \\Pr(G\\mid S,R)}\n\n from the pre-intervention distribution. The do operator forces the value of G to be true. The probability of rain is unaffected by the action:\n To predict the impact of turning the sprinkler on:\n with the term \n\n\n\nPr\n(\nS\n=\nT\n∣\nR\n)\n\n\n{\\displaystyle \\Pr(S=T\\mid R)}\n\n removed, showing that the action affects the grass but not the rain.\n These predictions may not be feasible given unobserved variables, as in most policy evaluation problems. The effect of the action \n\n\n\n\ndo\n\n(\nx\n)\n\n\n{\\displaystyle {\\text{do}}(x)}\n\n can still be predicted, however, whenever the back-door criterion is satisfied.[2][3] It states that, if a set Z of nodes can be observed that d-separates[4] (or blocks) all back-door paths from X to Y then\n A back-door path is one that ends with an arrow into X. Sets that satisfy the back-door criterion are called \"sufficient\" or \"admissible.\" For example, the set Z = R is admissible for predicting the effect of S = T on G, because R d-separates the (only) back-door path S ← R → G. However, if S is not observed, no other set d-separates this path and the effect of turning the sprinkler on (S = T) on the grass (G) cannot be predicted from passive observations. In that case P(G | do(S = T)) is not \"identified\". This reflects the fact that, lacking interventional data, the observed dependence between S and G is due to a causal connection or is spurious\n(apparent dependence arising from a common cause, R). (see Simpson's paradox)\n To determine whether a causal relation is identified from an arbitrary Bayesian network with unobserved variables, one can use the three rules of \"do-calculus\"[2][5] and test whether all do terms can be removed from the expression of that relation, thus confirming that the desired quantity is estimable from frequency data.[6]\n Using a Bayesian network can save considerable amounts of memory over exhaustive probability tables, if the dependencies in the joint distribution are sparse. For example, a naive way of storing the conditional probabilities of 10 two-valued variables as a table requires storage space for \n\n\n\n\n2\n\n10\n\n\n=\n1024\n\n\n{\\displaystyle 2^{10}=1024}\n\n values. If no variable's local distribution depends on more than three parent variables, the Bayesian network representation stores at most \n\n\n\n10\n⋅\n\n2\n\n3\n\n\n=\n80\n\n\n{\\displaystyle 10\\cdot 2^{3}=80}\n\n values.\n One advantage of Bayesian networks is that it is intuitively easier for a human to understand (a sparse set of) direct dependencies and local distributions than complete joint distributions.\n Bayesian networks perform three main inference tasks:\n Because a Bayesian network is a complete model for its variables and their relationships, it can be used to answer probabilistic queries about them. For example, the network can be used to update knowledge of the state of a subset of variables when other variables (the evidence variables) are observed. This process of computing the posterior distribution of variables given evidence is called probabilistic inference. The posterior gives a universal sufficient statistic for detection applications, when choosing values for the variable subset that minimize some expected loss function, for instance the probability of decision error. A Bayesian network can thus be considered a mechanism for automatically applying Bayes' theorem to complex problems.\n The most common exact inference methods are: variable elimination, which eliminates (by integration or summation) the non-observed non-query variables one by one by distributing the sum over the product; clique tree propagation, which caches the computation so that many variables can be queried at one time and new evidence can be propagated quickly; and recursive conditioning and AND/OR search, which allow for a space–time tradeoff and match the efficiency of variable elimination when enough space is used. All of these methods have complexity that is exponential in the network's treewidth. The most common approximate inference algorithms are importance sampling, stochastic MCMC simulation, mini-bucket elimination, loopy belief propagation, generalized belief propagation and variational methods.\n In order to fully specify the Bayesian network and thus fully represent the joint probability distribution, it is necessary to specify for each node X the probability distribution for X conditional upon X's parents. The distribution of X conditional upon its parents may have any form. It is common to work with discrete or Gaussian distributions since that simplifies calculations. Sometimes only constraints on distribution are known; one can then use the principle of maximum entropy to determine a single distribution, the one with the greatest entropy given the constraints. (Analogously, in the specific context of a dynamic Bayesian network, the conditional distribution for the hidden state's temporal evolution is commonly specified to maximize the entropy rate of the implied stochastic process.)\n Often these conditional distributions include parameters that are unknown and must be estimated from data, e.g., via the maximum likelihood approach. Direct maximization of the likelihood (or of the posterior probability) is often complex given unobserved variables. A classical approach to this problem is the expectation-maximization algorithm, which alternates computing expected values of the unobserved variables conditional on observed data, with maximizing the complete likelihood (or posterior) assuming that previously computed expected values are correct. Under mild regularity conditions, this process converges on maximum likelihood (or maximum posterior) values for parameters.\n A more fully Bayesian approach to parameters is to treat them as additional unobserved variables and to compute a full posterior distribution over all nodes conditional upon observed data, then to integrate out the parameters. This approach can be expensive and lead to large dimension models, making classical parameter-setting approaches more tractable.\n In the simplest case, a Bayesian network is specified by an expert and is then used to perform inference. In other applications, the task of defining the network is too complex for humans. In this case, the network structure and the parameters of the local distributions must be learned from data.\n Automatically learning the graph structure of a Bayesian network (BN) is a challenge pursued within machine learning. The basic idea goes back to a recovery algorithm developed by Rebane and Pearl[7] and rests on the distinction between the three possible patterns allowed in a 3-node DAG:\n The first 2 represent the same dependencies (\n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nZ\n\n\n{\\displaystyle Z}\n\n are independent given \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n) and are, therefore, indistinguishable. The collider, however, can be uniquely identified, since \n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nZ\n\n\n{\\displaystyle Z}\n\n are marginally independent and all other pairs are dependent. Thus, while the skeletons (the graphs stripped of arrows) of these three triplets are identical, the directionality of the arrows is partially identifiable. The same distinction applies when \n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nZ\n\n\n{\\displaystyle Z}\n\n have common parents, except that one must first condition on those parents. Algorithms have been developed to systematically determine the skeleton of the underlying graph and, then, orient all arrows whose directionality is dictated by the conditional independences observed.[2][8][9][10]\n An alternative method of structural learning uses optimization-based search. It requires a scoring function and a search strategy. A common scoring function is posterior probability of the structure given the training data, like the BIC or the BDeu. The time requirement of an exhaustive search returning a structure that maximizes the score is superexponential in the number of variables. A local search strategy makes incremental changes aimed at improving the score of the structure. A global search algorithm like Markov chain Monte Carlo can avoid getting trapped in local minima. Friedman et al.[11][12] discuss using mutual information between variables and finding a structure that maximizes this. They do this by restricting the parent candidate set to k nodes and exhaustively searching therein.\n A particularly fast method for exact BN learning is to cast the problem as an optimization problem, and solve it using integer programming. Acyclicity constraints are added to the integer program (IP) during solving in the form of cutting planes.[13] Such method can handle problems with up to 100 variables.\n In order to deal with problems with thousands of variables, a different approach is necessary. One is to first sample one ordering, and then find the optimal BN structure with respect to that ordering. This implies working on the search space of the possible orderings, which is convenient as it is smaller than the space of network structures. Multiple orderings are then sampled and evaluated. This method has been proven to be the best available in literature when the number of variables is huge.[14]\n Another method consists of focusing on the sub-class of decomposable models, for which the MLE have a closed form. It is then possible to discover a consistent structure for hundreds of variables.[15]\n Learning Bayesian networks with bounded treewidth is necessary to allow exact, tractable inference, since the worst-case inference complexity is exponential in the treewidth k (under the exponential time hypothesis). Yet, as a global property of the graph, it considerably increases the difficulty of the learning process. In this context it is possible to use K-tree for effective learning.[16]\n Given data \n\n\n\nx\n\n\n\n\n{\\displaystyle x\\,\\!}\n\n and parameter \n\n\n\nθ\n\n\n{\\displaystyle \\theta }\n\n, a simple Bayesian analysis starts with a prior probability (prior) \n\n\n\np\n(\nθ\n)\n\n\n{\\displaystyle p(\\theta )}\n\n and likelihood \n\n\n\np\n(\nx\n∣\nθ\n)\n\n\n{\\displaystyle p(x\\mid \\theta )}\n\n to compute a posterior probability \n\n\n\np\n(\nθ\n∣\nx\n)\n∝\np\n(\nx\n∣\nθ\n)\np\n(\nθ\n)\n\n\n{\\displaystyle p(\\theta \\mid x)\\propto p(x\\mid \\theta )p(\\theta )}\n\n.\n Often the prior on \n\n\n\nθ\n\n\n{\\displaystyle \\theta }\n\n depends in turn on other parameters \n\n\n\nφ\n\n\n{\\displaystyle \\varphi }\n\n that are not mentioned in the likelihood. So, the prior \n\n\n\np\n(\nθ\n)\n\n\n{\\displaystyle p(\\theta )}\n\n must be replaced by a likelihood \n\n\n\np\n(\nθ\n∣\nφ\n)\n\n\n{\\displaystyle p(\\theta \\mid \\varphi )}\n\n, and a prior \n\n\n\np\n(\nφ\n)\n\n\n{\\displaystyle p(\\varphi )}\n\n on the newly introduced parameters \n\n\n\nφ\n\n\n{\\displaystyle \\varphi }\n\n is required, resulting in a posterior probability\n This is the simplest example of a hierarchical Bayes model.\n The process may be repeated; for example, the parameters \n\n\n\nφ\n\n\n{\\displaystyle \\varphi }\n\n may depend in turn on additional parameters \n\n\n\nψ\n\n\n\n\n{\\displaystyle \\psi \\,\\!}\n\n, which require their own prior. Eventually the process must terminate, with priors that do not depend on unmentioned parameters.\n Given the measured quantities \n\n\n\n\nx\n\n1\n\n\n,\n…\n,\n\nx\n\nn\n\n\n\n\n\n\n{\\displaystyle x_{1},\\dots ,x_{n}\\,\\!}\n\neach with normally distributed errors of known standard deviation \n\n\n\nσ\n\n\n\n\n{\\displaystyle \\sigma \\,\\!}\n\n,\n Suppose we are interested in estimating the \n\n\n\n\nθ\n\ni\n\n\n\n\n{\\displaystyle \\theta _{i}}\n\n. An approach would be to estimate the \n\n\n\n\nθ\n\ni\n\n\n\n\n{\\displaystyle \\theta _{i}}\n\n using a maximum likelihood approach; since the observations are independent, the likelihood factorizes and the maximum likelihood estimate is simply\n However, if the quantities are related, so that for example the individual \n\n\n\n\nθ\n\ni\n\n\n\n\n{\\displaystyle \\theta _{i}}\n\nhave themselves been drawn from an underlying distribution, then this relationship destroys the independence and suggests a more complex model, e.g.,\n with improper priors \n\n\n\nφ\n∼\n\nflat\n\n\n\n{\\displaystyle \\varphi \\sim {\\text{flat}}}\n\n, \n\n\n\nτ\n∼\n\nflat\n\n∈\n(\n0\n,\n∞\n)\n\n\n{\\displaystyle \\tau \\sim {\\text{flat}}\\in (0,\\infty )}\n\n. When \n\n\n\nn\n≥\n3\n\n\n{\\displaystyle n\\geq 3}\n\n, this is an identified model (i.e. there exists a unique solution for the model's parameters), and the posterior distributions of the individual \n\n\n\n\nθ\n\ni\n\n\n\n\n{\\displaystyle \\theta _{i}}\n\n will tend to move, or shrink away from the maximum likelihood estimates towards their common mean. This shrinkage is a typical behavior in hierarchical Bayes models.\n Some care is needed when choosing priors in a hierarchical model, particularly on scale variables at higher levels of the hierarchy such as the variable \n\n\n\nτ\n\n\n\n\n{\\displaystyle \\tau \\,\\!}\n\n in the example. The usual priors such as the Jeffreys prior often do not work, because the posterior distribution will not be normalizable and estimates made by minimizing the expected loss will be inadmissible.\n Several equivalent definitions of a Bayesian network have been offered. For the following, let G = (V,E) be a directed acyclic graph (DAG) and let X = (Xv), v ∈ V be a set of random variables indexed by V.\n X is a Bayesian network with respect to G if its joint probability density function (with respect to a product measure) can be written as a product of the individual density functions, conditional on their parent variables:[17]\n where pa(v) is the set of parents of v (i.e. those vertices pointing directly to v via a single edge).\n For any set of random variables, the probability of any member of a joint distribution can be calculated from conditional probabilities using the chain rule (given a topological ordering of X) as follows:[17]\n Using the definition above, this can be written as:\n The difference between the two expressions is the conditional independence of the variables from any of their non-descendants, given the values of their parent variables.\n X is a Bayesian network with respect to G if it satisfies the local Markov property: each variable is conditionally independent of its non-descendants given its parent variables:[18]\n where de(v) is the set of descendants and V \\ de(v) is the set of non-descendants of v.\n This can be expressed in terms similar to the first definition, as\n The set of parents is a subset of the set of non-descendants because the graph is acyclic.\n In general, learning a Bayesian network from data is known to be NP-hard.[19] This is due in part to the combinatorial explosion of enumerating DAGs as the number of variables increases. Nevertheless, insights about an underlying Bayesian network can be learned from data in polynomial time by focusing on its marginal independence structure:[20] while the conditional independence statements of a distribution modeled by a Bayesian network are encoded by a DAG (according to the factorization and Markov properties above), its marginal independence statements—the conditional independence statements in which the conditioning set is empty—are encoded by a simple undirected graph with special properties such as equal intersection and independence numbers.\n Developing a Bayesian network often begins with creating a DAG G such that X satisfies the local Markov property with respect to G. Sometimes this is a causal DAG. The conditional probability distributions of each variable given its parents in G are assessed. In many cases, in particular in the case where the variables are discrete, if the joint distribution of X is the product of these conditional distributions, then X is a Bayesian network with respect to G.[21]\n The Markov blanket of a node is the set of nodes consisting of its parents, its children, and any other parents of its children. The Markov blanket renders the node independent of the rest of the network; the joint distribution of the variables in the Markov blanket of a node is sufficient knowledge for calculating the distribution of the node. X is a Bayesian network with respect to G if every node is conditionally independent of all other nodes in the network, given its Markov blanket.[18]\n This definition can be made more general by defining the \"d\"-separation of two nodes, where d stands for directional.[2] We first define the \"d\"-separation of a trail and then we will define the \"d\"-separation of two nodes in terms of that.\n Let P be a trail from node u to v. A trail is a loop-free, undirected (i.e. all edge directions are ignored) path between two nodes. Then P is said to be d-separated by a set of nodes Z if any of the following conditions holds:\n The nodes u and v are d-separated by Z if all trails between them are d-separated. If u and v are not d-separated, they are d-connected.\n X is a Bayesian network with respect to G if, for any two nodes u, v:\n where Z is a set which d-separates u and v. (The Markov blanket is the minimal set of nodes which d-separates node v from all other nodes.)\n Although Bayesian networks are often used to represent causal relationships, this need not be the case: a directed edge from u to v does not require that Xv be causally dependent on Xu. This is demonstrated by the fact that Bayesian networks on the graphs:\n are equivalent: that is they impose exactly the same conditional independence requirements.\n A causal network is a Bayesian network with the requirement that the relationships be causal. The additional semantics of causal networks specify that if a node X is actively caused to be in a given state x (an action written as do(X = x)), then the probability density function changes to that of the network obtained by cutting the links from the parents of X to X, and setting X to the caused value x.[2] Using these semantics, the impact of external interventions from data obtained prior to intervention can be predicted.\n In 1990, while working at Stanford University on large bioinformatic applications, Cooper proved that exact inference in Bayesian networks is NP-hard.[22] This result prompted research on approximation algorithms with the aim of developing a tractable approximation to probabilistic inference. In 1993, Paul Dagum and Michael Luby proved two surprising results on the complexity of approximation of probabilistic inference in Bayesian networks.[23] First, they proved that no tractable deterministic algorithm can approximate probabilistic inference to within an absolute error ɛ < 1/2. Second, they proved that no tractable randomized algorithm can approximate probabilistic inference to within an absolute error ɛ < 1/2 with confidence probability greater than 1/2.\n At about the same time, Roth proved that exact inference in Bayesian networks is in fact #P-complete (and thus as hard as counting the number of satisfying assignments of a conjunctive normal form formula (CNF)) and that approximate inference within a factor 2n1−ɛ for every ɛ > 0, even for Bayesian networks with restricted architecture, is NP-hard.[24][25]\n In practical terms, these complexity results suggested that while Bayesian networks were rich representations for AI and machine learning applications, their use in large real-world applications would need to be tempered by either topological structural constraints, such as naïve Bayes networks, or by restrictions on the conditional probabilities. The bounded variance algorithm[26] developed by Dagum and Luby was the first provable fast approximation algorithm to efficiently approximate probabilistic inference in Bayesian networks with guarantees on the error approximation. This powerful algorithm required the minor restriction on the conditional probabilities of the Bayesian network to be bounded away from zero and one by \n\n\n\n1\n\n/\n\np\n(\nn\n)\n\n\n{\\displaystyle 1/p(n)}\n\n where \n\n\n\np\n(\nn\n)\n\n\n{\\displaystyle p(n)}\n\n was any polynomial of the number of nodes in the network, \n\n\n\nn\n\n\n{\\displaystyle n}\n\n.\n Notable software for Bayesian networks include:\n The term Bayesian network was coined by Judea Pearl in 1985 to emphasize:[28]\n In the late 1980s Pearl's Probabilistic Reasoning in Intelligent Systems[30] and Neapolitan's Probabilistic Reasoning in Expert Systems[31] summarized their properties and established them as a field of study.\n"
    },
    {
        "title": "Evolutionary algorithm",
        "url": "https://en.wikipedia.org/wiki/Evolutionary_algorithm",
        "content": "Evolutionary algorithms (EA) reproduce essential elements of the biological evolution in a computer algorithm in order to solve “difficult” problems, at least approximately, for which no exact or satisfactory solution methods are known. They belong to the class of metaheuristics and are a subset of population based bio-inspired algorithms[1] and evolutionary computation, which itself are part of the field of computational intelligence.[2] The mechanisms of biological evolution that an EA mainly imitates are reproduction, mutation, recombination and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators.\n Evolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlying fitness landscape. Techniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations of microevolutionary processes and planning models based upon cellular processes. In most real applications of EAs, computational complexity is a prohibiting factor.[3] In fact, this computational complexity is due to fitness function evaluation. Fitness approximation is one of the solutions to overcome this difficulty. However, seemingly simple EA can solve often complex problems;[4][5][6] therefore, there may be no direct link between algorithm complexity and problem complexity.\n The following is an example of a generic evolutionary algorithm:[7][8][9]\n Similar techniques differ in genetic representation and other implementation details, and the nature of the particular applied problem.\n The following theoretical principles apply to all or almost all EAs.\n The no free lunch theorem of optimization states that all optimization strategies are equally effective when the set of all optimization problems is considered. Under the same condition, no evolutionary algorithm is fundamentally better than another. This can only be the case if the set of all problems is restricted. This is exactly what is inevitably done in practice. Therefore, to improve an EA, it must exploit problem knowledge in some form (e.g. by choosing a certain mutation strength or a problem-adapted coding). Thus, if two EAs are compared, this constraint is implied. In addition, an EA can use problem specific knowledge by, for example, not randomly generating the entire start population, but creating some individuals through heuristics or other procedures.[18][19] Another possibility to tailor an EA to a given problem domain is to involve suitable heuristics, local search procedures or other problem-related procedures in the process of generating the offspring. This form of extension of an EA is also known as a memetic algorithm. Both extensions play a major role in practical applications, as they can speed up the search process and make it more robust.[18][20]\n For EAs in which, in addition to the offspring, at least the best individual of the parent generation is used to form the subsequent generation (so-called elitist EAs), there is a general proof of convergence under the condition that an optimum exists. Without loss of generality, a maximum search is assumed for the proof:\n From the property of elitist offspring acceptance and the existence of the optimum it follows that per generation \n\n\n\nk\n\n\n{\\displaystyle k}\n\n an improvement of the fitness \n\n\n\nF\n\n\n{\\displaystyle F}\n\n of the respective best individual \n\n\n\n\nx\n′\n\n\n\n{\\displaystyle x'}\n\n will occur with a probability \n\n\n\nP\n>\n0\n\n\n{\\displaystyle P>0}\n\n. Thus:\n I.e., the fitness values represent a monotonically non-decreasing sequence, which is bounded due to the existence of the optimum. From this follows the convergence of the sequence against the optimum.\n Since the proof makes no statement about the speed of convergence, it is of little help in practical applications of EAs. But it does justify the recommendation to use elitist EAs. However, when using the usual panmictic population model, elitist EAs tend to converge prematurely more than non-elitist ones.[21] In a panmictic population model, mate selection (see step 4 of the generic definition) is such that every individual in the entire population is eligible as a mate. In non-panmictic populations, selection is suitably restricted, so that the dispersal speed of better individuals is reduced compared to panmictic ones. Thus, the general risk of premature convergence of elitist EAs can be significantly reduced by suitable population models that restrict mate selection.[22][23]\n With the theory of virtual alphabets, David E. Goldberg showed in 1990 that by using a representation with real numbers, an EA that uses classical recombination operators (e.g. uniform or n-point crossover) cannot reach certain areas of the search space, in contrast to a coding with binary numbers.[24] This results in the recommendation for EAs with real representation to use arithmetic operators for recombination (e.g. arithmetic mean or intermediate recombination). With suitable operators, real-valued representations are more effective than binary ones, contrary to earlier opinion.[25][26]\n A possible limitation[according to whom?] of many evolutionary algorithms is their lack of a clear genotype–phenotype distinction. In nature, the fertilized egg cell undergoes a complex process known as embryogenesis to become a mature phenotype. This indirect encoding is believed to make the genetic search more robust (i.e. reduce the probability of fatal mutations), and also may improve the evolvability of the organism.[27][28] Such indirect (also known as generative or developmental) encodings also enable evolution to exploit the regularity in the environment.[29] Recent work in the field of artificial embryogeny, or artificial developmental systems, seeks to address these concerns. And gene expression programming successfully explores a genotype–phenotype system, where the genotype consists of linear multigenic chromosomes of fixed length and the phenotype consists of multiple expression trees or computer programs of different sizes and shapes.[30][improper synthesis?]\n Both method classes have in common that their individual search steps are determined by chance. The main difference, however, is that EAs, like many other metaheuristics, learn from past search steps and incorporate this experience into the execution of the next search steps in a method-specific form. With EAs, this is done firstly through the fitness-based selection operators for partner choice and the formation of the next generation. And secondly, in the type of search steps: In EA, they start from a current solution and change it or they mix the information of two solutions. In contrast, when dicing out new solutions in Monte-Carlo methods, there is usually no connection to existing solutions.[31][32]\n If, on the other hand, the search space of a task is such that there is nothing to learn, Monte-Carlo methods are an appropriate tool, as they do not contain any algorithmic overhead that attempts to draw suitable conclusions from the previous search. An example of such tasks is the proverbial search for a needle in a haystack, e.g. in the form of a flat (hyper)plane with a single narrow peak.\n The areas in which evolutionary algorithms are practically used are almost unlimited[6] and range from industry,[33][34] engineering,[3][4][35] complex scheduling,[5][36][37] agriculture,[38] robot movement planning[39] and finance[40][41] to research[42][43] and art. The application of an evolutionary algorithm requires some rethinking from the inexperienced user, as the approach to a task using an EA is different from conventional exact methods and this is usually not part of the curriculum of engineers or other disciplines. For example, the fitness calculation must not only formulate the goal but also support the evolutionary search process towards it, e.g. by rewarding improvements that do not yet lead to a better evaluation of the original quality criteria. For example, if peak utilisation of resources such as personnel deployment or energy consumption is to be avoided in a scheduling task, it is not sufficient to assess the maximum utilisation. Rather, the number and duration of exceedances of a still acceptable level should also be recorded in order to reward reductions below the actual maximum peak value.[44] There are therefore some publications that are aimed at the beginner and want to help avoiding beginner's mistakes as well as leading an application project to success.[44][45][46] This includes clarifying the fundamental question of when an EA should be used to solve a problem and when it is better not to.\n There are some other proven and widely used methods of nature inspired global search techniques such as\n In addition, many new nature-inspired or methaphor-guided algorithms have been proposed since the beginning of this century. For criticism of most publications on these, see the remarks at the end of the introduction to the article on metaheuristics.\n In 2020, Google stated that their AutoML-Zero can successfully rediscover classic algorithms such as the concept of neural networks.[47]\n The computer simulations Tierra and Avida attempt to model macroevolutionary dynamics.\n [48][49]\n"
    },
    {
        "title": "Hybrid intelligent system",
        "url": "https://en.wikipedia.org/wiki/Hybrid_intelligent_system",
        "content": "\nHybrid intelligent system denotes a software system which employs, in parallel, a combination of methods and techniques from artificial intelligence subfields, such as:\n From the cognitive science perspective, every natural intelligent system is hybrid because it performs mental operations on both the symbolic and subsymbolic levels. For the past few years, there has been an increasing discussion of the importance of A.I. Systems Integration. Based on notions that there have already been created simple and specific AI systems (such as systems for computer vision, speech synthesis, etc., or software that employs some of the models mentioned above) and now is the time for integration to create broad AI systems. Proponents of this approach are researchers such as Marvin Minsky, Ron Sun, Aaron Sloman,  and Michael A. Arbib.\n An example hybrid is a hierarchical control system in which the lowest, reactive layers are sub-symbolic. The higher layers, having relaxed time constraints, are capable of reasoning from an abstract world model and performing planning.\n Intelligent systems usually rely on hybrid reasoning processes, which include induction, deduction, abduction and reasoning by analogy.\n"
    },
    {
        "title": "Artificial intelligence systems integration",
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_systems_integration",
        "content": "\n The core idea of artificial intelligence systems integration is making individual software components, such as speech synthesizers, interoperable with other components, such as common sense knowledgebases, in order to create larger, broader and more capable A.I. systems. The main methods that have been proposed for integration are message routing, or communication protocols that the software components use to communicate with each other, often through a middleware blackboard system.\n Most artificial intelligence systems involve some sort of integrated technologies, for example, the integration of speech synthesis technologies with that of speech recognition. However, in recent years, there has been an increasing discussion on the importance of systems integration as a field in its own right. Proponents of this approach are researchers such as Marvin Minsky, Aaron Sloman, Deb Roy, Kristinn R. Thórisson and Michael A. Arbib. A reason for the recent attention A.I. integration is attracting is that there have already been created a number of (relatively) simple A.I. systems for specific problem domains (such as computer vision, speech synthesis, etc.), and that integrating what's already available is a more logical approach to broader A.I. than building monolithic systems from scratch.\n The focus on systems' integration, especially with regard to modular approaches, derive from the fact that most intelligences of significant scales are composed of a multitude of processes and/or utilize multi-modal input and output. For example, a humanoid-type of intelligence would preferably have to be able to talk using speech synthesis, hear using speech recognition, understand using a logical (or some other undefined) mechanism, and so forth. In order to produce artificially intelligent software of broader intelligence, integration of these modalities is necessary.\n Collaboration is an integral part of software development as evidenced by the size of software companies and the size of their software departments. Among the tools to ease software collaboration are various procedures and standards that developers can follow to ensure quality, reliability and that their software is compatible with software created by others (such as W3C standards for webpage development). However, collaboration in fields of A.I. has been lacking, for the most part not seen outside the respected schools, departments or research institutes (and sometimes not within them either). This presents practitioners of A.I. systems integration with a substantial problem and often causes A.I. researchers to have to 're-invent the wheel' each time they want a specific functionality to work with their software. Even more damaging is the \"not invented here\" syndrome, which manifests itself in a strong reluctance of A.I. researchers to build on the work of others.\n The outcome of this in A.I. is a large set of \"solution islands\": A.I. research has produced numerous isolated software components and mechanisms that deal with various parts of intelligence separately. To take some examples:\n With the increased popularity of the free software movement, a lot of the software being created, including A.I. systems, is available for public exploit. The next natural step is to merge these individual software components into coherent, intelligent systems of a broader nature. As a multitude of components (that often serve the same purpose) have already been created by the community, the most accessible way of integration is giving each of these components an easy way to communicate with each other. By doing so, each component by itself becomes a module, which can then be tried in various settings and configurations of larger architectures. Some challenging and limitations of using A.I. software is the uncontrolled fatal errors. For example, serious and fatal errors have been discovered in very precise fields such as human oncology, as in an article published in the journal Oral Oncology Reports entitled “When AI goes wrong: Fatal errors in oncological research reviewing assistance\".[1] The article pointed out a grave error in artificial intelligence based on GBT in the field of biophysics. \n Many online communities for A.I. developers exist where tutorials, examples, and forums aim at helping both beginners and experts build intelligent systems. However, few communities have succeeded in making a certain standard, or a code of conduct popular to allow the large collection of miscellaneous systems to be integrated with ease.\n The constructionist design methodology (CDM, or 'Constructionist A.I.') is a formal methodology proposed in 2004, for use in the development of cognitive robotics, communicative humanoids and broad AI systems. The creation of such systems requires the integration of a large number of functionalities that must be carefully coordinated to achieve coherent system behavior. CDM is based on iterative design steps that lead to the creation of a network of named interacting modules, communicating via explicitly typed streams and discrete messages. The OpenAIR message protocol (see below) was inspired by the CDM and has frequently been used to aid in the development of intelligent systems using CDM.\n"
    },
    {
        "title": "Applications of artificial intelligence",
        "url": "https://en.wikipedia.org/wiki/Applications_of_artificial_intelligence",
        "content": "\n Artificial intelligence (AI) has been used in applications throughout industry and academia. In a manner analogous to electricity or computers, AI serves as a general-purpose technology. AI programs are designed to simulate human perception and understanding. These systems are capable of adapting to new information and responding to changing situations. Machine learning has been used for various scientific and commercial purposes[1] including language translation, image recognition, decision-making,[2][3] credit scoring, and e-commerce.\n Machine learning is has been used for recommendation systems in for determining which posts should show up in social media feeds.[4][5] Various types of social media analysis also make use of machine learning[6][7] and there is research into its use for (semi-)automated tagging/enhancement/correction of online misinformation and related filter bubbles.[8][9][10]\n AI has been used to customize shopping options and personalize offers.[11] Online gambling companies have used AI for targeting gamblers.[12]\n Intelligent personal assistants use AI to understand many natural language requests in other ways than rudimentary commands. Common examples are Apple's Siri, Amazon's Alexa, and a more recent AI, ChatGPT by OpenAI.[13]\n Bing Chat has used artificial intelligence as part of its search engine.[14]\n Machine learning can be used to combat spam, scams, and phishing. It can scrutinize the contents of spam and phishing attacks to attempt to identify malicious elements.[15] Some models built via machine learning algorithms have over 90% accuracy in distinguishing between spam and legitimate emails.[16] These models can be refined using new data and evolving spam tactics. Machine learning also analyzes traits such as sender behavior, email header information, and attachment types, potentially enhancing spam detection.[17]\n Speech translation technology attempts to convert one language's spoken words into another language. This potentially reduces language barriers in global commerce and cross-cultural exchange, enabling speakers of various languages to communicate with one another.[18]\n AI has been used to automatically translate spoken language and textual content in products such as Microsoft Translator, Google Translate, and DeepL Translator.[19] Additionally, research and development are in progress to decode and conduct animal communication.[20][21]\n Meaning is conveyed not only by text, but also through usage and context (see semantics and pragmatics). As a result, the two primary categorization approaches for machine translations are statistical and neural machine translations (NMTs). The old method of performing translation was to use a statistical machine translation (SMT) methodology to forecast the best probable output with specific algorithms. However, with NMT, the approach employs dynamic algorithms to achieve better translations based on context.[22]\n AI has been used in facial recognition systems. Some examples are Apple's Face ID and Android's Face Unlock, which are used to secure mobile devices.[23]\n Image labeling has been used by Google Image Labeler to detect products in photos and to allow people to search based on a photo. Image labeling has also been demonstrated to generate speech to describe images to blind people.[24] Facebook's DeepFace identifies human faces in digital images.\n Games have been a major application[relevant?] of AI's capabilities since the 1950s. In the 21st century, AIs have beaten human players in many games, including chess (Deep Blue), Jeopardy! (Watson),[25] Go (AlphaGo),[26][27][28][29][30][31][32] poker (Pluribus[33] and Cepheus),[34] E-sports (StarCraft),[35][36] and general game playing (AlphaZero[37][38][39] and MuZero).[40][41][42][43]\n Kuki AI is a set of chatbots and other apps which were designed for entertainment and as a marketing tool.[44][45] Character.ai is another example of a chatbot being used for recreation.\n AI for Good is a platform launched in 2017 by the International Telecommunication Union (ITU) agency of the United Nations (UN). The goal of the platform is to use AI to help achieve the UN's Sustainable Development Goals.[citation needed]\n The University of Southern California launched the Center for Artificial Intelligence in Society, with the goal of using AI to address problems such as homelessness. Stanford researchers use AI to analyze satellite images to identify high poverty areas.[46]\n In agriculture, AI has been proposed as a way for farmers to identify areas that need irrigation, fertilization, or pesticide treatments to increase yields, thereby improving efficiency.[47] AI has been used to attempt to classify livestock pig call emotions,[20] automate greenhouses,[48] detect diseases and pests,[49] and optimize irrigation.[50]\n Cyber security companies are adopting neural networks, machine learning, and natural language processing to improve their systems.[51]\n Applications of AI in cyber security include:\n AI elevates teaching, focusing on significant issues like the knowledge nexus and educational equality. The evolution of AI in education and technology should be used to improve human capabilities in relationships where they do not replace humans. UNESCO recognizes the future of AI in education as an instrument to reach Sustainable Development Goal 4, called \"Inclusive and Equitable Quality Education.” [56]\n The World Economic Forum also stresses AI's contribution to students' overall improvement and transforming teaching into a more enjoyable process.[56]\n Personalized Learning\n AI driven tutoring systems, such as Khan Academy, Duolingo and Carnegie Learning are the forefoot of delivering personalized education.[57]\n These platforms leverage AI algorithms to analyze individual learning patterns, strengths, and weaknesses, enabling the customization of content and Algorithm to suit each student's pace and style of learning.[57]\n Administrative Efficiency\n In educational institutions, AI is increasingly used to automate routine tasks like attendance tracking, grading and marking, which allows educators to devote more time to interactive teaching and direct student engagement.[58]\n Furthermore, AI tools are employed to monitor student progress, analyze learning behaviors, and predict academic challenges, facilitating timely and proactive interventions for students who may be at risk of falling behind.[58]\n Ethical and Privacy Concerns\n Despite the benefits, the integration of AI in education raises significant ethical and privacy concerns, particularly regarding the handling of sensitive student data.[57]\n It is imperative that AI systems in education are designed and operated with a strong emphasis on transparency, security, and respect for privacy to maintain trust and uphold the integrity of educational practices.[57]\n Much of the regulation will be influenced by the AI Act, the world’s first comprehensive AI law. [59]\n Financial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation. The use of AI in banking began in 1987 when Security Pacific National Bank launched a fraud prevention task-force to counter the unauthorized use of debit cards.[60] Kasisto and Moneystream use AI.\n Banks use AI to organize operations for bookkeeping, investing in stocks, and managing properties. AI can adapt to changes during non-business hours.[61] AI is used to combat fraud and financial crimes by monitoring behavioral patterns for any abnormal changes or anomalies.[62][63][64]\n The use of AI in applications such as online trading and decision-making has changed major economic theories.[65] For example, AI-based buying and selling platforms estimate personalized demand and supply curves, thus enabling individualized pricing. AI systems reduce information asymmetry in the market and thus make markets more efficient.[66] The application of artificial intelligence in the financial industry can alleviate the financing constraints of non-state-owned enterprises, especially for smaller and more innovative enterprises.[67]\n Algorithmic trading involves the use of AI systems to make trading decisions at speeds orders of magnitude greater than any human is capable of, making millions of trades in a day without human intervention. Such high-frequency trading represents a fast-growing sector. Many banks, funds, and proprietary trading firms now have entire portfolios that are AI-managed. Automated trading systems are typically used by large institutional investors but include smaller firms trading with their own AI systems.[68]\n Large financial institutions use AI to assist with their investment practices. BlackRock's AI engine, Aladdin, is used both within the company and by clients to help with investment decisions. Its functions include the use of natural language processing to analyze text such as news, broker reports, and social media feeds. It then gauges the sentiment on the companies mentioned and assigns a score. Banks such as UBS and Deutsche Bank use SQREEM (Sequential Quantum Reduction and Extraction Model) to mine data to develop consumer profiles and match them with wealth management products.[69]\n Online lender Upstart uses machine learning for underwriting.[70]\n ZestFinance's Zest Automated Machine Learning (ZAML) platform is used for credit underwriting. This platform uses machine learning to analyze data including purchase transactions and how a customer fills out a form to score borrowers. The platform is particularly useful to assign credit scores to those with limited credit histories.[71]\n AI makes continuous auditing possible. Potential benefits include reducing audit risk, increasing the level of assurance, and reducing audit duration.[72][quantify]\n Continuous auditing with AI allows real-time monitoring and reporting of financial activities and provides businesses with timely insights that can lead to quick decision making.[73]\n AI software, such as LaundroGraph which uses contemporary suboptimal datasets, could be used for anti-money laundering (AML).[74][75]\n In the 1980s, AI started to become prominent in finance as expert systems were commercialized. For example, Dupont created 100 expert systems, which helped them to save almost $10 million per year.[76] One of the first systems was the Pro-trader expert system that predicted the 87-point drop in the Dow Jones Industrial Average in 1986. \"The major junctions of the system were to monitor premiums in the market, determine the optimum investment strategy, execute transactions when appropriate and modify the knowledge base through a learning mechanism.\"[77]\n One of the first expert systems to help with financial plans was PlanPowerm and Client Profiling System, created by Applied Expert Systems (APEX). It was launched in 1986. It helped create personal financial plans for people.[78]\n In the 1990s AI was applied to fraud detection. In 1993 FinCEN Artificial Intelligence System (FAIS) launched. It was able to review over 200,000 transactions per week and over two years it helped identify 400 potential cases of money laundering equal to $1 billion.[79] These expert systems were later replaced by machine learning systems.[80]\n AI can enhance entrepreneurial activity and AI is one of the most dynamic areas for start-ups, with significant venture capital flowing into AI.[81]\n AI facial recognition systems are used for mass surveillance, notably in China.[82][83] In 2019, Bengaluru, India deployed AI-managed traffic signals. This system uses cameras to monitor traffic density and adjust signal timing based on the interval needed to clear traffic.[84]\n Various countries are deploying AI military applications.[85] The main applications enhance command and control, communications, sensors, integration and interoperability.[86] Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles.[85] AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams.[86]\n AI has been used in military operations in Iraq, Syria, Israel and Ukraine.[85][87][88][89]\n AI in healthcare is often used for classification, to evaluate a CT scan or electrocardiogram or to identify high-risk patients for population health. AI is helping with the high-cost problem of dosing. One study suggested that AI could save $16 billion. In 2016, a study reported that an AI-derived formula derived the proper dose of immunosuppressant drugs to give to transplant patients.[90] Current research has indicated that non-cardiac vascular illnesses are also being treated with artificial intelligence (AI). For certain disorders, AI algorithms can aid in diagnosis, recommended treatments, outcome prediction, and patient progress tracking. As AI technology advances, it is anticipated that it will become more significant in the healthcare industry.[91]\n The early detection of diseases like cancer is made possible by AI algorithms, which diagnose diseases by analyzing complex sets of medical data. For example, the IBM Watson system might be used to comb through massive data such as medical records and clinical trials to help diagnose a problem.[92] Microsoft's AI project Hanover helps doctors choose cancer treatments from among the more than 800 medicines and vaccines.[93][94] Its goal is to memorize all the relevant papers to predict which (combinations of) drugs will be most effective for each patient. Myeloid leukemia is one target. Another study reported on an AI that was as good as doctors in identifying skin cancers.[95] Another project monitors multiple high-risk patients by asking each patient questions based on data acquired from doctor/patient interactions.[96] In one study done with transfer learning, an AI diagnosed eye conditions similar to an ophthalmologist and recommended treatment referrals.[97]\n Another study demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel judged better than a surgeon.[98]\n Artificial neural networks are used as clinical decision support systems for medical diagnosis,[99] such as in concept processing technology in EMR software.\n Other healthcare tasks thought suitable for an AI that are in development include:\n AI-enabled chatbots decrease the need for humans to perform basic call center tasks.[115]\n Machine learning in sentiment analysis can spot fatigue in order to prevent overwork.[115] Similarly, decision support systems can prevent industrial disasters and make disaster response more efficient.[116] For manual workers in material handling, predictive analytics may be used to reduce musculoskeletal injury.[117] Data collected from wearable sensors can improve workplace health surveillance, risk assessment, and research.[116][how?]\n AI can auto-code workers' compensation claims.[118][119] AI-enabled virtual reality systems can enhance safety training for hazard recognition.[116] AI can more efficiently detect accident near misses, which are important in reducing accident rates, but are often underreported.[120]\n AlphaFold 2 can determine the 3D structure of a (folded) protein in hours rather than the months required by earlier automated approaches and was used to provide the likely structures of all proteins in the human body and essentially all proteins known to science (more than 200 million).[121][122][123][124]\n Machine learning has been used for drug design.[125] It has also been used for predicting molecular properties and exploring large chemical/reaction spaces.[126] Computer-planned syntheses via computational reaction networks, described as a platform that combines \"computational synthesis with AI algorithms to predict molecular properties\",[127] have been used to explore the origins of life on Earth,[128] drug-syntheses and developing routes for recycling 200 industrial waste chemicals into important drugs and agrochemicals (chemical synthesis design).[129] There is research about which types of computer-aided chemistry would benefit from machine learning.[130] It can also be used for \"drug discovery and development, drug repurposing, improving pharmaceutical productivity, and clinical trials\".[131] It has been used for the design of proteins with prespecified functional sites.[132][133]\n It has been used with databases for the development of a 46-day process to design, synthesize and test a drug which inhibits enzymes of a particular gene, DDR1. DDR1 is involved in cancers and fibrosis which is one reason for the high-quality datasets that enabled these results.[134]\n There are various types of applications for machine learning in decoding human biology, such as helping to map gene expression patterns to functional activation patterns[135] or identifying functional DNA motifs.[136] It is widely used in genetic research.[137]\n There also is some use of machine learning in synthetic biology,[138][139] disease biology,[139] nanotechnology (e.g. nanostructured materials and bionanotechnology),[140][141] and materials science.[142][143][144]\n There are also prototype robot scientists, including robot-embodied ones like the two Robot Scientists, which show a form of \"machine learning\" not commonly associated with the term.[145][146]\n Similarly, there is research and development of biological \"wetware computers\" that can learn (e.g. for use as biosensors) and/or implantation into an organism's body (e.g. for use to control prosthetics).[147][148][149] Polymer-based artificial neurons operate directly in biological environments and define biohybrid neurons made of artificial and living components.[150][151]\n Moreover, if whole brain emulation is possible via both scanning and replicating the, at least, bio-chemical brain – as premised in the form of digital replication in The Age of Em, possibly using physical neural networks – that may have applications as or more extensive than e.g. valued human activities and may imply that society would face substantial moral choices, societal risks and ethical problems[152][153] such as whether (and how) such are built, sent through space and used compared to potentially competing e.g. potentially more synthetic and/or less human and/or non/less-sentient types of artificial/semi-artificial intelligence.[additional citation(s) needed] An alternative or additive approach to scanning are types of reverse engineering of the brain.[154][155]\n A subcategory of artificial intelligence is embodied,[156][157] some of which are mobile robotic systems that each consist of one or multiple robots that are able to learn in the physical world.\n However, biological computers, even if both highly artificial and intelligent, are typically distinguished from synthetic, often silicon-based, computers – they could however be combined or used for the design of either. Moreover, many tasks may be carried out inadequately by artificial intelligence even if its algorithms were transparent, understood, bias-free, apparently effective, and goal-aligned and its trained data sufficiently large and cleansed – such as in cases were the underlying or available metrics, values or data are inappropriate. Computer-aided is a phrase used to describe human activities that make use of computing as tool in more comprehensive activities and systems such as AI for narrow tasks or making use of such without substantially relying on its results (see also: human-in-the-loop).[citation needed] A study described the biological as a limitation of AI with \"as long as the biological system cannot be understood, formalized, and imitated, we will not be able to develop technologies that can mimic it\" and that if it was understood this does not mean there being \"a technological solution to imitate natural intelligence\".[158] Technologies that integrate biology and are often AI-based include biorobotics.\n Artificial intelligence is used in astronomy to analyze increasing amounts of available data[159][160] and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights\" for example for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy.[161] It could also be used for activities in space such as space exploration, including analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance,[162] and more autonomous operation.[163][164][165][160]\n In the search for extraterrestrial intelligence (SETI), machine learning has been used in attempts to identify artificially generated electromagnetic waves in available data[166][167] – such as real-time observations[168] – and other technosignatures, e.g. via anomaly detection.[169] In ufology, the SkyCAM-5 project headed by Prof. Hakan Kayal[170] and the Galileo Project headed by Avi Loeb use machine learning to attempt to detect and classify types of UFOs.[171][172][173][174][175] The Galileo Project also seeks to detect two further types of potential extraterrestrial technological signatures with the use of AI: 'Oumuamua-like interstellar objects, and non-manmade artificial satellites.[176][177]\n Machine learning can also be used to produce datasets of spectral signatures of molecules that may be involved in the atmospheric production or consumption of particular chemicals – such as phosphine possibly detected on Venus – which could prevent miss assignments and, if accuracy is improved, be used in future detections and identifications of molecules on other planets.[178]\n In April 2024, the Scientific Advice Mechanism to the European Commission published advice[179] including a comprehensive evidence review of the opportunities and challenges posed by artificial intelligence in scientific research.\n As benefits, the evidence review[180] highlighted:\n As challenges:\n Machine learning can help to restore and attribute ancient texts.[181] It can help to index texts for example to enable better and easier searching[182] and classification of fragments.[183]\n \nArtificial intelligence can also be used to investigate genomes to uncover genetic history, such as interbreeding between archaic and modern humans by which for example the past existence of a ghost population, not Neanderthal or Denisovan, was inferred.[184]  \nIt can also be used for \"non-invasive and non-destructive access to internal structures of archaeological remains\".[185]  A deep learning system was reported to learn intuitive physics from visual data (of virtual 3D environments) based on an unpublished approach inspired by studies of visual cognition in infants.[186][187] Other researchers have developed a machine learning algorithm that could discover sets of basic variables of various physical systems and predict the systems' future dynamics from video recordings of their behavior.[188][189] In the future, it may be possible that such can be used to automate the discovery of physical laws of complex systems.[188]\n AI could be used for materials optimization and discovery such as the discovery of stable materials and the prediction of their crystal structure.[190][191][192]\n In November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.[193][194][195]\n Machine learning is used in diverse types of reverse engineering. For example, machine learning has been used to reverse engineer a composite material part, enabling unauthorized production of high quality parts,[196] and for quickly understanding the behavior of malware.[197][198][199] It can be used to reverse engineer artificial intelligence models.[200] It can also design components by engaging in a type of reverse engineering of not-yet existent virtual components such as inverse molecular design for particular desired functionality[201] or protein design for prespecified functional sites.[132][133] Biological network reverse engineering could model interactions in a human understandable way, e.g. bas on time series data of gene expression levels.[202]\n AI is a mainstay of law-related professions. Algorithms and machine learning do some tasks previously done by entry-level lawyers.[203] While its use is common, it is not expected to replace most work done by lawyers in the near future.[204]\n The electronic discovery industry uses machine learning to reduce manual searching.[205]\n Law enforcement has begun using facial recognition systems (FRS) to identify suspects from visual data. FRS results have proven to be more accurate when compared to eyewitness results. Furthermore, FRS has shown to have much a better ability to identify individuals when video clarity and visibility are low in comparison to human participants. [206]\n COMPAS is a commercial system used by U.S. courts to assess the likelihood of recidivism.[207]\n One concern relates to algorithmic bias, AI programs may become biased after processing data that exhibits bias.[208] ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than that of white defendants.[207]\n In 2019, the city of Hangzhou, China established a pilot program artificial intelligence-based Internet Court to adjudicate disputes related to ecommerce and internet-related intellectual property claims.[209]: 124  Parties appear before the court via videoconference and AI evaluates the evidence presented and applies relevant legal standards.[209]: 124 \n Another application of AI is in human resources. AI can screen resumes and rank candidates based on their qualifications, predict candidate success in given roles, and automate repetitive communication tasks via chatbots.[210]\n AI has simplified the recruiting /job search process for both recruiters and job seekers. According to Raj Mukherjee from Indeed, 65% of job searchers search again within 91 days after hire. An AI-powered engine streamlines the complexity of job hunting by assessing information on job skills, salaries, and user tendencies, matching job seekers to the most relevant positions. Machine intelligence calculates appropriate wages and highlights resume information for recruiters using NLP, which extracts relevant words and phrases from text. Another application is an AI resume builder that compiles a CV in 5 minutes.[211] Chatbots assist website visitors and refine workflows.\n AI underlies avatars (automated online assistants) on web pages.[212] It can reduce operation and training costs.[212] Pypestream automated customer service for its mobile application to streamline communication with customers.[213]\n A Google app analyzes language and converts speech into text. The platform can identify angry customers through their language and respond appropriately.[214] Amazon uses a chatbot for customer service that can perform tasks like checking the status of an order, cancelling orders, offering refunds and connecting the customer with a human representative.[215] Generative AI (GenAI), such as ChatGPT, is increasingly used in business to automate tasks and enhance decision-making.[216]\n In the hospitality industry, AI is used to reduce repetitive tasks, analyze trends, interact with guests, and predict customer needs.[217] AI hotel services come in the form of a chatbot,[218] application, virtual voice assistant and service robots.\n AI applications analyze media content such as movies, TV programs, advertisement videos or user-generated content. The solutions often involve computer vision.\n Typical scenarios include the analysis of images using object recognition or face recognition techniques, or the analysis of video for scene recognizing scenes, objects or faces. AI-based media analysis can facilitate media search, the creation of descriptive keywords for content, content policy monitoring (such as verifying the suitability of content for a particular TV viewing time), speech to text for archival or other purposes, and the detection of logos, products or celebrity faces for ad placement.\n Deep-fakes can be used for comedic purposes but are better known for fake news and hoaxes.\n Deepfakes can portray individuals in harmful or compromising situations, causing significant reputational damage and emotional distress, especially when the content is defamatory or violates personal ethics. While defamation and false light laws offer some recourse, their focus on false statements rather than fabricated images or videos often leaves victims with limited legal protection and a challenging burden of proof.[232]\n In January 2016,[233] the Horizon 2020 program financed the InVID Project[234][235] to help journalists and researchers detect fake documents, made available as browser plugins.[236][237]\n In June 2016, the visual computing group of the Technical University of Munich and from Stanford University developed Face2Face,[238] a program that animates photographs of faces, mimicking the facial expressions of another person. The technology has been demonstrated animating the faces of people including Barack Obama and Vladimir Putin. Other methods have been demonstrated based on deep neural networks, from which the name deep fake was taken.\n In September 2018, U.S. Senator Mark Warner proposed to penalize social media companies that allow sharing of deep-fake documents on their platforms.[239]\n In 2018, Darius Afchar and Vincent Nozick found a way to detect faked content by analyzing the mesoscopic properties of video frames.[240] DARPA gave 68 million dollars to work on deep-fake detection.[240]\n Audio deepfakes[241][242] and AI software capable of detecting deep-fakes and cloning human voices have been developed.[243][244]\n Respeecher is a program that enables one person to speak with the voice of another.\n AI algorithms have been used to detect deepfake videos.[245][246]\n Artificial intelligence is also starting to be used in video production, with tools and software being developed that utilize generative AI in order to create new video, or alter existing video. Some of the major tools that are being used in these processes currently are DALL-E, Mid-journey, and Runway.[247]  Way mark Studios utilized the tools offered by both DALL-E and Mid-journey to create a fully AI generated film called The Frost in the summer of 2023.[247] Way mark Studios is experimenting with using these AI tools to generate advertisements and commercials for companies in mere seconds.[247]  Yves Bergquist, a director of the AI & Neuroscience in Media Project at USC's Entertainment Technology Center, says post production crews in Hollywood are already using generative AI, and predicts that in the future more companies will embrace this new technology.[248]\n AI has been used to compose music of various genres.\n David Cope created an AI called Emily Howell that managed to become well known in the field of algorithmic computer music.[249] The algorithm behind Emily Howell is registered as a US patent.[250]\n In 2012, AI Iamus created the first complete classical album.[251]\n AIVA (Artificial Intelligence Virtual Artist), composes symphonic music, mainly classical music for film scores.[252] It achieved a world first by becoming the first virtual composer to be recognized by a musical professional association.[253]\n Melomics creates computer-generated music for stress and pain relief.[254]\n At Sony CSL Research Laboratory, the Flow Machines software creates pop songs by learning music styles from a huge database of songs. It can compose in multiple styles.\n The Watson Beat uses reinforcement learning and deep belief networks to compose music on a simple seed input melody and a select style. The software was open sourced[255] and musicians such as Taryn Southern[256] collaborated with the project to create music.\n South Korean singer, Hayeon's, debut song, \"Eyes on You\" was composed using AI which was supervised by real composers, including NUVO.[257]\n Narrative Science sells computer-generated news and reports. It summarizes sporting events based on statistical data from the game. It also creates financial reports and real estate analyses.[258] Automated Insights generates personalized recaps and previews for Yahoo Sports Fantasy Football.[259]\n Yseop, uses AI to turn structured data into natural language comments and recommendations. Yseop writes financial reports, executive summaries, personalized sales or marketing documents and more in multiple languages, including English, Spanish, French, and German.[260]\n TALESPIN made up stories similar to the fables of Aesop. The program started with a set of characters who wanted to achieve certain goals. The story narrated their attempts to satisfy these goals.[citation needed] Mark Riedl and Vadim Bulitko asserted that the essence of storytelling was experience management, or \"how to balance the need for a coherent story progression with user agency, which is often at odds\".[261]\n While AI storytelling focuses on story generation (character and plot), story communication also received attention. In 2002, researchers developed an architectural framework for narrative prose generation. They faithfully reproduced text variety and complexity on stories such as Little Red Riding Hood.[262] In 2016, a Japanese AI co-wrote a short story and almost won a literary prize.[263]\n South Korean company Hanteo Global uses a journalism bot to write articles.[264]\n Literary authors are also exploring uses of AI. An example is David Jhave Johnston's work ReRites (2017-2019), where the poet created a daily rite of editing the poetic output of a neural network to create a series of performances and publications.\n In 2010, artificial intelligence used baseball statistics to automatically generate news articles. This was launched by The Big Ten Network using software from Narrative Science.[265]\n After being unable to cover every Minor League Baseball game with a large team, Associated Press collaborated with Automated Insights in 2016 to create game recaps that were automated by artificial intelligence.[266]\n UOL in Brazil expanded the use of AI in its writing. Rather than just generating news stories, they programmed the AI to include commonly searched words on Google.[266]\n El Pais, a Spanish news site that covers many things including sports, allows users to make comments on each news article. They use the Perspective API to moderate these comments and if the software deems a comment to contain toxic language, the commenter must modify it in order to publish it.[266]\n A local Dutch media group used AI to create automatic coverage of amateur soccer, set to cover 60,000 games in just a single season. NDC partnered with United Robots to create this algorithm and cover what would have never been possible before without an extremely large team.[266]\n Lede AI has been used in 2023 to take scores from high school football games to generate stories automatically for the local newspaper. This was met with significant criticism from readers for the very robotic diction that was published. With some descriptions of games being a \"close encounter of the athletic kind,\" readers were not pleased and let the publishing company, Gannett, know on social media. Gannett has since halted their used of Lede AI until they come up with a solution for what they call an experiment.[267]\n  Millions of its articles have been edited by bots[271] which however are usually not artificial intelligence software. Many AI platforms use Wikipedia data,[272] mainly for training machine learning applications. There is research and development of various artificial intelligence applications for Wikipedia such as for identifying outdated sentences,[273] detecting covert vandalism[274] or recommending articles and tasks to new editors.\n Machine translation (see above) has also be used for translating Wikipedia articles and could play a larger role in creating, updating, expanding, and generally improving articles in the future. A content translation tool allows editors of some Wikipedias to more easily translate articles across several select languages.[275][276]\n In video games, AI is routinely used to generate behavior in non-player characters (NPCs). In addition, AI is used for pathfinding. Some researchers consider NPC AI in games to be a \"solved problem\" for most production tasks.[who?] Games with less typical AI include the AI director of Left 4 Dead (2008) and the neuroevolutionary training of platoons in Supreme Commander 2 (2010).[277][278] AI is also used in Alien Isolation (2014) as a way to control the actions the Alien will perform next.[279]\n Kinect, which provides a 3D body–motion interface for the Xbox 360 and the Xbox One, uses algorithms that emerged from AI research.[280][which?]\n AI has been used to produce visual art. The first AI art program, called AARON, was developed by Harold Cohen in 1968[281] with the goal of being able to code the act of drawing. It started by creating simple black and white drawings, and later to painting using special brushes and dyes that were chosen by the program itself without mediation from Cohen.[282]\n AI platforms such as \"DALL-E\",[283] Stable Diffusion,[283] Imagen,[284] and Midjourney[285] have been used for generating visual images from inputs such as text or other images.[286] Some AI tools allow users to input images and output changed versions of that image, such as to display an object or product in different environments. AI image models can also attempt to replicate the specific styles of artists, and can add visual complexity to rough sketches.\n Since their design in 2014, generative adversarial networks (GANs) have been used by AI artists. GAN computer programming, generates technical images through machine learning frameworks that surpass the need for human operators.[281] Examples of GAN programs that generate art include Artbreeder and DeepDream.\n In addition to the creation of original art, research methods that utilize AI have been generated to quantitatively analyze digital art collections. Although the main goal of the large-scale digitization of artwork in the past few decades was to allow for accessibility and exploration of these collections, the use of AI in analyzing them has brought about new research perspectives.[287]\nTwo computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art.[288] While distant viewing includes the analysis of large collections, close reading involves one piece of artwork.\n AI has been in use since the early 2000s, most notably by a system designed by Pixar called \"Genesis\".[289] It was designed to learn algorithms and create 3D models for its characters and props. Notable movies that used this technology included Up and The Good Dinosaur.[290] AI has been used less ceremoniously in recent years. In 2023, it was revealed Netflix of Japan was using AI to generate background images for their upcoming show to be met with backlash online.[291]  In recent years, motion capture became an easily accessible form of AI animation. For example, Move AI is a program built to capture any human movement and reanimate it in its animation program using learning AI.[292]\n Power electronics converters are used in renewable energy, energy storage, electric vehicles and high-voltage direct current transmission. These converters are failure-prone, which can interrupt service and require costly maintenance or catastrophic consequences in mission critical applications.[citation needed] AI can guide the design process for reliable power electronics converters, by calculating exact design parameters that ensure the required lifetime.[293]\n The U.S. Department of Energy underscores AI's pivotal role in realizing national climate goals. With AI, the ambitious target of achieving net-zero greenhouse gas emissions across the economy becomes feasible. AI also helps make room for wind and solar on the grid by avoiding congestion and increasing grid reliability. [294]\n Machine learning can be used for energy consumption prediction and scheduling, e.g. to help with renewable energy intermittency management (see also: smart grid and climate change mitigation in the power grid).[295][296][297][298][125]\n Many telecommunications companies make use of heuristic search to manage their workforces. For example, BT Group deployed heuristic search[299] in an application that schedules 20,000 engineers. Machine learning is also used for speech recognition (SR), including of voice-controlled devices, and SR-related transcription, including of videos.[300][301]\n Artificial intelligence has been combined with digital spectrometry by IdeaCuria Inc.,[302][303] enable applications such as at-home water quality monitoring.\n In the 1990s, early artificial intelligence tools controlled Tamagotchis and Giga Pets, the Internet, and the first widely released robot, Furby. Aibo was a domestic robot in the form of a robotic dog with intelligent features and autonomy.\n Mattel created an assortment of AI-enabled toys that \"understand\" conversations, give intelligent responses, and learn.[304]\n Oil and gas companies have used artificial intelligence tools to automate functions, foresee equipment issues, and increase oil and gas output.[305][306]\n AI in transport is expected to provide safe, efficient, and reliable transportation while minimizing the impact on the environment and communities. The major development challenge is the complexity of transportation systems that involves independent components and parties, with potentially conflicting objectives.[307]\n AI-based fuzzy logic controllers operate gearboxes. For example, the 2006 Audi TT, VW Touareg [citation needed] and VW Caravell feature the DSP transmission. A number of Škoda variants (Škoda Fabia) include a fuzzy logic-based controller. Cars have AI-based driver-assist features such as self-parking and adaptive cruise control.\n There are also prototypes of autonomous automotive public transport vehicles such as electric mini-buses[308][309][310][311] as well as autonomous rail transport in operation.[312][313][314]\n There also are prototypes of autonomous delivery vehicles, sometimes including delivery robots.[315][316][317][318][319][320][321]\n Transportation's complexity means that in most cases training an AI in a real-world driving environment is impractical. Simulator-based testing can reduce the risks of on-road training.[322]\n AI underpins self-driving vehicles. Companies involved with AI include Tesla, Waymo, and General Motors. AI-based systems control functions such as braking, lane changing, collision prevention, navigation and mapping.[323]\n Autonomous trucks are in the testing phase. The UK government passed legislation to begin testing of autonomous truck platoons in 2018.[324] A group of autonomous trucks follow closely behind each other. German corporation Daimler is testing its Freightliner Inspiration.[325]\n Autonomous vehicles require accurate maps to be able to navigate between destinations.[326] Some autonomous vehicles do not allow human drivers (they have no steering wheels or pedals).[327]\n AI has been used to optimize traffic management, which reduces wait times, energy use, and emissions by as much as 25 percent.[328]\n Smart traffic lights have been developed at Carnegie Mellon since 2009.  Professor Stephen Smith has started a company since then Surtrac that has installed smart traffic control systems in 22 cities.  It costs about $20,000 per intersection to install. Drive time has been reduced by 25% and traffic jam waiting time has been reduced by 40% at the intersections it has been installed.[329]\n The Royal Australian Air Force (RAAF) Air Operations Division (AOD) uses AI for expert systems. AIs operate as surrogate operators for combat and training simulators, mission management aids, support systems for tactical decision making, and post processing of the simulator data into symbolic summaries.[330]\n Aircraft simulators use AI for training aviators. Flight conditions can be simulated that allow pilots to make mistakes without risking themselves or expensive aircraft. Air combat can also be simulated.\n AI can also be used to operate planes analogously to their control of ground vehicles. Autonomous drones can fly independently or in swarms.[331]\n AOD uses the Interactive Fault Diagnosis and Isolation System, or IFDIS, which is a rule-based expert system using information from TF-30 documents and expert advice from mechanics that work on the TF-30. This system was designed to be used for the development of the TF-30 for the F-111C. The system replaced specialized workers. The system allowed regular workers to communicate with the system and avoid mistakes, miscalculations, or having to speak to one of the specialized workers.\n Speech recognition allows traffic controllers to give verbal directions to drones.\n Artificial intelligence supported design of aircraft,[332] or AIDA, is used to help designers in the process of creating conceptual designs of aircraft. This program allows the designers to focus more on the design itself and less on the design process. The software also allows the user to focus less on the software tools. The AIDA uses rule-based systems to compute its data. This is a diagram of the arrangement of the AIDA modules. Although simple, the program is proving effective.\n In 2003 a Dryden Flight Research Center project created software that could enable a damaged aircraft to continue flight until a safe landing can be achieved.[333] The software compensated for damaged components by relying on the remaining undamaged components.[334]\n The 2016 Intelligent Autopilot System combined apprenticeship learning and behavioral cloning whereby the autopilot observed low-level actions required to maneuver the airplane and high-level strategy used to apply those actions.[335]\n Neural networks are used by situational awareness systems in ships and boats.[336] There also are autonomous boats.\n Autonomous ships that monitor the ocean, AI-driven satellite data analysis, passive acoustics[337] or remote sensing and other applications of environmental monitoring make use of machine learning.[338][339][340][165]\n For example, \"Global Plastic Watch\" is an AI-based satellite monitoring-platform for analysis/tracking of plastic waste sites to help prevention of plastic pollution – primarily ocean pollution – by helping identify who and where mismanages plastic waste, dumping it into oceans.[341][342]\n Machine learning can be used to spot early-warning signs of disasters and environmental issues, possibly including natural pandemics,[343][344] earthquakes,[345][346][347] landslides,[348] heavy rainfall,[349] long-term water supply vulnerability,[350] tipping-points of ecosystem collapse,[351] cyanobacterial bloom outbreaks,[352] and droughts.[353][354][355]\n AI can be used for real-time code completion, chat, and automated test generation. These tools are typically integrated with editors and IDEs as plugins. They differ in functionality, quality, speed, and approach to privacy.[356] Code suggestions could be incorrect, and should be carefully reviewed by software developers before accepted.\n GitHub Copilot is an artificial intelligence model developed by GitHub and OpenAI that is able to autocomplete code in multiple programming languages.[357] Price for individuals: $10/mo or $100/yr, with one free month trial.\n Tabnine was created by Jacob Jackson and was originally owned by Tabnine company. In late 2019, Tabnine was acquired by Codota.[358] Tabnine tool is available as plugin to most popular IDEs. It offers multiple pricing options, including limited \"starter\" free version.[359]\n CodiumAI by CodiumAI, small startup in Tel Aviv, offers automated test creation. Currently supports Python, JS, and TS.[360]\n Ghostwriter by Replit offers code completion and chat.[361] They have multiple pricing plans, including a free one and a \"Hacker\" plan for $7/month.\n CodeWhisperer by Amazon collects individual users' content, including files open in the IDE. They claim to focus on security both during transmission and when storing.[362] Individual plan is free, professional plan is $19/user/month.\n Other tools: SourceGraph Cody, CodeCompleteFauxPilot, Tabby[356]\n AI can be used to create other AIs. For example, around November 2017, Google's AutoML project to evolve new neural net topologies created NASNet, a system optimized for ImageNet and POCO F1. NASNet's performance exceeded all previously published performance on ImageNet.[363]\n Machine learning has been used for noise-cancelling in quantum technology,[364] including quantum sensors.[365] Moreover, there is substantial research and development of using quantum computers with machine learning algorithms. For example, there is a prototype, photonic, quantum memristive device for neuromorphic (quantum-)computers (NC)/artificial neural networks and NC-using quantum materials with some variety of potential neuromorphic computing-related applications,[366][367] and quantum machine learning is a field with some variety of applications under development. AI could be used for quantum simulators which may have the application of solving physics and chemistry[368][369] problems as well as for quantum annealers for training of neural networks for AI applications.[370] There may also be some usefulness in chemistry, e.g. for drug discovery, and in materials science, e.g. for materials optimization/discovery (with possible relevance to quantum materials manufacturing[191][192]).[371][372][373][better source needed]\n AI researchers have created many tools to solve the most difficult problems in computer science. Many of their inventions have been adopted by mainstream computer science and are no longer considered AI. All of the following were originally developed in AI laboratories:[374]\n An optical character reader is used in the extraction of data in business documents like invoices and receipts. It can also be used in business contract documents e.g. employment agreements to extract critical data like employment terms, delivery terms, termination clauses, etc.[375]\n Artificial intelligence in architecture describes the use of artificial intelligence in automation, design and planning in the architectural process or in assisting human skills in the field of architecture. Artificial Intelligence is thought to potentially lead to and ensue major changes in architecture.[376][377][378]\n AI in architecture has created a way for architects to create things beyond human understanding. AI implementation of machine learning text-to-render technologies, like DALL-E and stable Diffusion, gives power to visualization complex.[379]\n AI allows designers to demonstrate their creativity and even invent new ideas while designing. In future, AI will not replace architects; instead, it will improve the speed of translating ideas sketching.[379]\n"
    },
    {
        "title": "Machine learning in bioinformatics",
        "url": "https://en.wikipedia.org/wiki/Machine_learning_in_bioinformatics",
        "content": "\n Machine learning in bioinformatics is the application of machine learning algorithms to bioinformatics,[1] including genomics, proteomics, microarrays, systems biology, evolution, and text mining.[2][3]\n Prior to the emergence of machine learning, bioinformatics algorithms had to be programmed by hand; for problems such as protein structure prediction, this proved difficult.[4] Machine learning techniques such as deep learning can learn features of data sets rather than requiring the programmer to define them individually. The algorithm can further learn how to combine low-level features into more abstract features, and so on. This multi-layered approach allows such systems to make sophisticated predictions when appropriately trained. These methods contrast with other computational biology approaches which, while exploiting existing datasets, do not allow the data to be interpreted and analyzed in unanticipated ways. \n Machine learning algorithms in bioinformatics can be used for prediction, classification, and feature selection. Methods to achieve this task are varied and span many disciplines; most well known among them are machine learning and statistics. Classification and prediction tasks aim at building models that describe and distinguish classes or concepts for future prediction. The differences between them are the following:\n Due to the exponential growth of information technologies and applicable models, including artificial intelligence and data mining, in addition to the access ever-more comprehensive data sets, new and better information analysis techniques have been created, based on their ability to learn. Such models allow reach beyond description and provide insights in the form of testable models.\n Artificial neural networks in bioinformatics have been used for:[5]\n The way that features, often vectors in a many-dimensional space, are extracted from the domain data is an important component of learning systems.[6] In genomics, a typical representation of a sequence is a vector of k-mers frequencies, which is a vector of dimension \n\n\n\n\n4\n\nk\n\n\n\n\n{\\displaystyle 4^{k}}\n\n whose entries count the appearance of each subsequence of length \n\n\n\nk\n\n\n{\\displaystyle k}\n\n in a given sequence. Since for a value as small as \n\n\n\nk\n=\n12\n\n\n{\\displaystyle k=12}\n\n the dimensionality of these vectors is huge (e.g. in this case the dimension is \n\n\n\n\n4\n\n12\n\n\n≈\n16\n×\n\n10\n\n6\n\n\n\n\n{\\displaystyle 4^{12}\\approx 16\\times 10^{6}}\n\n), techniques such as principal component analysis are used to project the data to a lower dimensional space, thus selecting a smaller set of features from the sequences.[6][additional citation(s) needed]\n In this type of machine learning task, the output is a discrete variable. One example of this type of task in bioinformatics is labeling new genomic data (such as genomes of unculturable bacteria) based on a model of already labeled data.[6]\n Hidden Markov models (HMMs) are a class of statistical models for sequential data (often related to systems evolving over time). An HMM is composed of two mathematical objects: an observed state‐dependent process \n\n\n\n\nX\n\n1\n\n\n,\n\nX\n\n2\n\n\n,\n…\n,\n\nX\n\nM\n\n\n\n\n{\\displaystyle X_{1},X_{2},\\ldots ,X_{M}}\n\n, and an unobserved (hidden) state process \n\n\n\n\nS\n\n1\n\n\n,\n\nS\n\n2\n\n\n,\n…\n,\n\nS\n\nT\n\n\n\n\n{\\displaystyle S_{1},S_{2},\\ldots ,S_{T}}\n\n. In an HMM, the state process is not directly observed – it is a 'hidden' (or 'latent') variable – but observations are made of a state‐dependent process (or observation process) that is driven by the underlying state process (and which can thus be regarded as a noisy measurement of the system states of interest).[7] HMMs can be formulated in continuous time.[8][9]\n HMMs can be used to profile and convert a multiple sequence alignment into a position-specific scoring system suitable for searching databases for homologous sequences remotely.[10] Additionally, ecological phenomena can be described by HMMs.[11]\n Convolutional neural networks (CNN) are a class of deep neural network whose architecture is based on shared weights of convolution kernels or filters that slide along input features, providing translation-equivariant responses known as feature maps.[12][13] CNNs take advantage of the hierarchical pattern in data and assemble patterns of increasing complexity using smaller and simpler patterns discovered via their filters.[14]\n Convolutional networks were inspired by biological processes[15][16][17][18] in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\n CNN uses relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered. This reduced reliance on prior knowledge of the analyst and on human intervention in manual feature extraction makes CNNs a desirable model.[14]\n A phylogenetic convolutional neural network (Ph-CNN) is a convolutional neural network architecture proposed by Fioranti et al. in 2018 to classify metagenomics data.[19] In this approach, phylogenetic data is endowed with patristic distance (the sum of the lengths of all branches connecting two operational taxonomic units [OTU]) to select k-neighborhoods for each OTU, and each OTU and its neighbors are processed with convolutional filters.\n Unlike supervised methods, self-supervised learning methods learn representations without relying on annotated data. That is well-suited for genomics, where high throughput sequencing techniques can create potentially large amounts of unlabeled data. Some examples of self-supervised learning methods applied on genomics include DNABERT and Self-GenomeNet.[20][21]\n Random forests (RF) classify by constructing an ensemble of decision trees, and outputting the average prediction of the individual trees.[22] This is a modification of bootstrap aggregating (which aggregates a large collection of decision trees) and can be used for classification or regression.[23][24]\n As random forests give an internal estimate of generalization error, cross-validation is unnecessary. In addition, they produce proximities, which can be used to impute missing values, and which enable novel data visualizations.[25]\n Computationally, random forests are appealing because they naturally handle both regression and (multiclass) classification, are relatively fast to train and to predict, depend only on one or two tuning parameters, have a built-in estimate of the generalization error, can be used directly for high-dimensional problems, and can easily be implemented in parallel. Statistically, random forests are appealing for additional features, such as measures of variable importance, differential class weighting, missing value imputation, visualization, outlier detection, and unsupervised learning.[25]\n Clustering - the partitioning of a data set into disjoint subsets, so that the data in each subset are as close as possible to each other and as distant as possible from data in any other subset, according to some defined distance or similarity function - is a common technique for statistical data analysis.\n Clustering is central to much data-driven bioinformatics research and serves as a powerful computational method whereby means of hierarchical, centroid-based, distribution-based, density-based, and self-organizing maps classification, has long been studied and used in classical machine learning settings. Particularly, clustering helps to analyze unstructured and high-dimensional data in the form of sequences, expressions, texts, images, and so on. Clustering is also used to gain insights into biological processes at the genomic level, e.g. gene functions, cellular processes, subtypes of cells, gene regulation, and metabolic processes.[26]\n Data clustering algorithms can be hierarchical or partitional. Hierarchical algorithms find successive clusters using previously established clusters, whereas partitional algorithms determine all clusters at once. Hierarchical algorithms can be agglomerative (bottom-up) or divisive (top-down).\n Agglomerative algorithms begin with each element as a separate cluster and merge them in successively larger clusters. Divisive algorithms begin with the whole set and proceed to divide it into successively smaller clusters. Hierarchical clustering is calculated using metrics on Euclidean spaces, the most commonly used is the Euclidean distance computed by finding the square of the difference between each variable, adding all the squares, and finding the square root of the said sum. An example of a hierarchical clustering algorithm is BIRCH, which is particularly good on bioinformatics for its nearly linear time complexity given generally large datasets.[27] Partitioning algorithms are based on specifying an initial number of groups, and iteratively reallocating objects among groups to convergence. This algorithm typically determines all clusters at once. Most applications adopt one of two popular heuristic methods: k-means algorithm or k-medoids. Other algorithms do not require an initial number of groups, such as affinity propagation. In a genomic setting this algorithm has been used both to cluster biosynthetic gene clusters in gene cluster families(GCF) and to cluster said GCFs.[28]\n Typically, a workflow for applying machine learning to biological data goes through four steps:[2]\n In general, a machine learning system can usually be trained to recognize elements of a certain class given sufficient samples.[30] For example, machine learning methods can be trained to identify specific visual features such as splice sites.[31]\n Support vector machines have been extensively used in cancer genomic studies.[32] In addition, deep learning has been incorporated into bioinformatic algorithms. Deep learning applications have been used for regulatory genomics and cellular imaging.[33] Other applications include medical image classification, genomic sequence analysis, as well as protein structure classification and prediction.[34] Deep learning has been applied to regulatory genomics, variant calling and pathogenicity scores.[35] Natural language processing and text mining have helped to understand phenomena including protein-protein interaction, gene-disease relation as well as predicting biomolecule structures and functions.[36]\n Natural language processing algorithms personalized medicine for patients who suffer genetic diseases, by combining the extraction of clinical information and genomic data available from the patients. Institutes such as Health-funded Pharmacogenomics Research Network focus on finding breast cancer treatments.[37]\n Precision medicine considers individual genomic variability, enabled by large-scale biological databases. Machine learning can be applied to perform the matching function between (groups of patients) and specific treatment modalities.[38]\n Computational techniques are used to solve other problems, such as efficient primer design for PCR, biological-image analysis and back translation of proteins (which is, given the degeneration of the genetic code, a complex combinatorial problem).[2]\n While genomic sequence data has historically been sparse due to the technical difficulty of sequencing a piece of DNA, the number of available sequences is growing. On average, the number of bases available in the GenBank public repository has doubled every 18 months since 1982.[39]  However, while raw data was becoming increasingly available and accessible, As of 2002[update], biological interpretation of this data was occurring at a much slower pace.[40] This made for an increasing need for developing computational genomics tools, including machine learning systems, that can automatically determine the location of protein-encoding genes within a given DNA sequence (i.e. gene prediction).[40]\n Gene prediction is commonly performed through both extrinsic searches and intrinsic searches.[40] For the extrinsic search, the input DNA sequence is run through a large database of sequences whose genes have been previously discovered and their locations annotated and identifying the target sequence's genes by determining which strings of bases within the sequence are homologous to known gene sequences. However, not all the genes in a given input sequence can be identified through homology alone, due to limits in the size of the database of known and annotated gene sequences. Therefore, an intrinsic search is needed where a gene prediction program attempts to identify the remaining genes from the DNA sequence alone.[40]\n Machine learning has also been used for the problem of multiple sequence alignment which involves aligning many DNA or amino acid sequences in order to determine regions of similarity that could indicate a shared evolutionary history.[2] It can also be used to detect and visualize genome rearrangements.[41]\n Proteins, strings of amino acids, gain much of their function from protein folding, where they conform into a three-dimensional structure, including the primary structure, the secondary structure (alpha helices and beta sheets), the tertiary structure, and the quaternary structure.\n Protein secondary structure prediction is a main focus of this subfield as tertiary and quaternary structures are determined based on the secondary structure.[4] Solving the true structure of a protein is expensive and time-intensive, furthering the need for systems that can accurately predict the structure of a protein by analyzing the amino acid sequence directly.[4][2] Prior to machine learning, researchers needed to conduct this prediction manually. This trend began in 1951 when Pauling and Corey released their work on predicting the hydrogen bond configurations of a protein from a polypeptide chain.[42] Automatic feature learning reaches an accuracy of 82-84%.[4][43] The current state-of-the-art in secondary structure prediction uses a system called DeepCNF (deep convolutional neural fields) which relies on the machine learning model of artificial neural networks to achieve an accuracy of approximately 84% when tasked to classify the amino acids of a protein sequence into one of three structural classes (helix, sheet, or coil).[43] [needs update] The theoretical limit for three-state protein secondary structure is 88–90%.[4]\n Machine learning has also been applied to proteomics problems such as protein side-chain prediction, protein loop modeling, and protein contact map prediction.[2]\n Metagenomics is the study of microbial communities from environmental DNA samples.[44] Currently, limitations and challenges predominate in the implementation of machine learning tools due to the amount of data in environmental samples.[45] Supercomputers and web servers have made access to these tools easier.[46] The high dimensionality of microbiome datasets is a major challenge in studying the microbiome; this significantly limits the power of current approaches for identifying true differences and increases the chance of false discoveries.[47][better source needed]\n Despite their importance, machine learning tools related to metagenomics have focused on the study of gut microbiota and the relationship with digestive diseases, such as inflammatory bowel disease (IBD), Clostridioides difficile infection (CDI), colorectal cancer and diabetes, seeking better diagnosis and treatments.[46] Many algorithms were developed to classify microbial communities according to the health condition of the host, regardless of the type of sequence data, e.g. 16S rRNA or whole-genome sequencing (WGS), using methods such as least absolute shrinkage and selection operator classifier, random forest, supervised classification model, and gradient boosted tree model. Neural networks, such as recurrent neural networks (RNN), convolutional neural networks (CNN), and Hopfield neural networks have been added.[46] For example, in 2018, Fioravanti et al. developed an algorithm called Ph-CNN to classify data samples from healthy patients and patients with IBD symptoms (to distinguish healthy and sick patients) by using phylogenetic trees and convolutional neural networks.[48]\n In addition, random forest (RF) methods and implemented importance measures help in the identification of microbiome species that can be used to distinguish diseased and non-diseased samples. However, the performance of a decision tree and the diversity of decision trees in the ensemble significantly influence the performance of RF algorithms. The generalization error for RF measures how accurate the individual classifiers are and their interdependence. Therefore, the high dimensionality problems of microbiome datasets pose challenges. Effective approaches require many possible variable combinations, which exponentially increases the computational burden as the number of features increases.[47]\n For microbiome analysis in 2020 Dang & Kishino[47] developed a novel analysis pipeline. The core of the pipeline is an RF classifier coupled with forwarding variable selection (RF-FVS), which selects a minimum-size core set of microbial species or functional signatures that maximize the predictive classifier performance. The framework combines:\n They demonstrated performance by analyzing two published datasets from large-scale case-control studies:\n The proposed approach improved the accuracy from 81% to 99.01% for CDI and from 75.14% to 90.17% for CRC.\n The use of machine learning in environmental samples has been less explored, maybe because of data complexity, especially from WGS. Some works show that it is possible to apply these tools in environmental samples. In 2021 Dhungel et al.,[49] designed an R package called MegaR. This package allows working with 16S rRNA and whole metagenomic sequences to make taxonomic profiles and classification models by machine learning models. MegaR includes a comfortable visualization environment to improve the user experience. Machine learning in environmental metagenomics can help to answer questions related to the interactions between microbial communities and ecosystems, e.g. the work of Xun et al., in 2021[50] where the use of different machine learning methods offered insights on the relationship among the soil, microbiome biodiversity, and ecosystem stability.\n Microarrays, a type of lab-on-a-chip, are used for automatically collecting data about large amounts of biological material. Machine learning can aid in analysis, and has been applied to expression pattern identification, classification, and genetic network induction.[2]\n This technology is especially useful for monitoring gene expression, aiding in diagnosing cancer by examining which genes are expressed.[51] One of the main tasks is identifying which genes are expressed based on the collected data.[2] In addition, due to the huge number of genes on which data is collected by the microarray, winnowing the large amount of irrelevant data to the task of expressed gene identification is challenging. Machine learning presents a potential solution as various classification methods can be used to perform this identification. The most commonly used methods are radial basis function networks, deep learning, Bayesian classification, decision trees, and random forest.[51]\n Systems biology focuses on the study of emergent behaviors from complex interactions of simple biological components in a system. Such components can include DNA, RNA, proteins, and metabolites.[52]\n Machine learning has been used to aid in modeling these interactions in domains such as genetic networks, signal transduction networks, and metabolic pathways.[2] Probabilistic graphical models, a machine learning technique for determining the relationship between different variables, are one of the most commonly used methods for modeling genetic networks.[2] In addition, machine learning has been applied to systems biology problems such as identifying transcription factor binding sites using Markov chain optimization.[2] Genetic algorithms, machine learning techniques which are based on the natural process of evolution, have been used to model genetic networks and regulatory structures.[2]\n Other systems biology applications of machine learning include the task of enzyme function prediction, high throughput microarray data analysis, analysis of genome-wide association studies to better understand markers of disease, protein function prediction.[53]\n This domain, particularly phylogenetic tree reconstruction, uses the features of machine learning techniques. Phylogenetic trees are schematic representations of the evolution of organisms. Initially, they were constructed using features such as morphological and metabolic features. Later, due to the availability of genome sequences, the construction of the phylogenetic tree algorithm used the concept based on genome comparison. With the help of optimization techniques, a comparison was done by means of multiple sequence alignment.[54]\n Machine learning methods for the analysis of neuroimaging data are used to help diagnose stroke. Historically multiple approaches to this problem involved neural networks.[55][56]\n Multiple approaches to detect strokes used machine learning. As proposed by Mirtskhulava,[57] feed-forward networks were tested to detect strokes using neural imaging. As proposed by Titano[58] 3D-CNN techniques were tested in supervised classification to screen head CT images for acute neurologic events. Three-dimensional CNN and SVM methods are often used.[56]\n The increase in biological publications increased the difficulty in searching and compiling relevant available information on a given topic. This task is known as knowledge extraction. It is necessary for biological data collection which can then in turn be fed into machine learning algorithms to generate new biological knowledge.[2][59] Machine learning can be used for this knowledge extraction task using techniques such as natural language processing to extract the useful information from human-generated reports in a database. Text Nailing, an alternative approach to machine learning, capable of extracting features from clinical narrative notes was introduced in 2017.\n This technique has been applied to the search for novel drug targets, as this task requires the examination of information stored in biological databases and journals.[59] Annotations of proteins in protein databases often do not reflect the complete known set of knowledge of each protein, so additional information must be extracted from biomedical literature. Machine learning has been applied to the automatic annotation of gene and protein function, determination of the protein subcellular localization, DNA-expression array analysis, large-scale protein interaction analysis, and molecule interaction analysis.[59]\n Another application of text mining is the detection and visualization of distinct DNA regions given sufficient reference data.[60]\n Microbial communities are complex assembles of diverse microorganisms,[61] where symbiont partners constantly produce diverse metabolites derived from the primary and secondary (specialized) metabolism, from which metabolism plays an important role in microbial interaction.[62] Metagenomic and metatranscriptomic data are an important source for deciphering communications signals.\n Molecular mechanisms produce specialized metabolites in various ways. Biosynthetic Gene Clusters (BGCs) attract attention, since several metabolites are clinically valuable, anti-microbial, anti-fungal, anti-parasitic, anti-tumor and immunosuppressive agents produced by the modular action of multi-enzymatic, multi-domains gene clusters, such as Nonribosomal peptide synthetases (NRPSs) and polyketide synthases (PKSs).[63] Diverse studies[64][65][66][67][68][69][70][71] show that grouping BGCs that share homologous core genes into gene cluster families (GCFs) can yield useful insights into the chemical diversity of the analyzed strains, and can support linking BGCs to their secondary metabolites.[65][67] GCFs have been used as functional markers in human health studies[72][73] and to study the ability of soil to suppress fungal pathogens.[74] Given their direct relationship to catalytic enzymes, and compounds produced from their encoded pathways, BGCs/GCFs can serve as a proxy to explore the chemical space of microbial secondary metabolism. Cataloging GCFs in sequenced microbial genomes yields an overview of the existing chemical diversity and offers insights into future priorities.[64][66] Tools such as BiG-SLiCE and BIG-MAP[75] have emerged with the sole purpose of unveiling the importance of BGCs in natural environments.\n The increase of experimentally characterized ribosomally synthesized and post-translationally modified peptides (RiPPs), together with the availability of information on their sequence and chemical structure, selected from databases such as BAGEL, BACTIBASE, MIBIG, and THIOBASE, provide the opportunity to develop machine learning tools to decode the chemical structure and classify them.\n In 2017, researchers at the National Institute of Immunology of New Delhi, India, developed RiPPMiner[76] software, a bioinformatics resource for decoding RiPP chemical structures by genome mining. The RiPPMiner web server consists of a query interface and the RiPPDB database. RiPPMiner defines 12 subclasses of RiPPs, predicting the cleavage site of the leader peptide and the final cross-link of the RiPP chemical structure.\n Many tandem mass spectrometry (MS/MS) based metabolomics studies, such as library matching and molecular networking, use spectral similarity as a proxy for structural similarity. Spec2vec[77] algorithm provides a new way of spectral similarity score, based on Word2Vec. Spec2Vec learns fragmental relationships within a large set of spectral data, in order to assess spectral similarities between molecules and to classify unknown molecules through these comparisons.\n For systemic annotation, some metabolomics studies rely on fitting measured fragmentation mass spectra to library spectra or contrasting spectra via network analysis. Scoring functions are used to determine the similarity between pairs of fragment spectra as part of these processes. So far, no research has suggested scores that are significantly different from the commonly utilized cosine-based similarity.[78]\n An important part of bioinformatics is the management of big datasets, known as databases of reference. Databases exist for each type of biological data, for example for biosynthetic gene clusters and metagenomes.\n The National Center for Biotechnology Information (NCBI)[79] provides a large suite of online resources for biological information and data, including the GenBank nucleic acid sequence database and the PubMed database of citations and abstracts for published life science journals. Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. Resources include PubMed Data Management, RefSeq Functional Elements, genome data download, variation services API, Magic-BLAST, QuickBLASTp, and Identical Protein Groups. All of these resources can be accessed through NCBI.[80]\n antiSMASH allows the rapid genome-wide identification, annotation and analysis of secondary metabolite biosynthesis gene clusters in bacterial and fungal genomes. It integrates and cross-links with a large number of in silico secondary metabolite analysis tools.[81]\n gutSMASH is a tool that systematically evaluates bacterial metabolic potential by predicting both known and novel anaerobic metabolic gene clusters (MGCs) from the gut microbiome.\n MIBiG,[82] the minimum information about a biosynthetic gene cluster specification, provides a standard for annotations and metadata on biosynthetic gene clusters and their molecular products. MIBiG is a Genomic Standards Consortium project that builds on the minimum information about any sequence (MIxS) framework.[83]\n MIBiG facilitates the standardized deposition and retrieval of biosynthetic gene cluster data as well as the development of comprehensive comparative analysis tools. It empowers next-generation research on the biosynthesis, chemistry and ecology of broad classes of societally relevant bioactive secondary metabolites, guided by robust experimental evidence and rich metadata components.[84]\n SILVA[85] is an interdisciplinary project among biologists and computers scientists assembling a complete database of RNA ribosomal (rRNA) sequences of genes, both small (16S, 18S, SSU) and large (23S, 28S, LSU) subunits, which belong to the bacteria, archaea and eukarya domains. These data are freely available for academic and commercial use.[86]\n Greengenes[87] is a full-length 16S rRNA gene database that provides chimera screening, standard alignment and a curated taxonomy based on de novo tree inference.[88][89] Overview: \n Open Tree of Life Taxonomy (OTT)[90] aims to build a complete, dynamic, and digitally available Tree of Life by synthesizing published phylogenetic trees along with taxonomic data. Phylogenetic trees have been classified, aligned, and merged. Taxonomies have been used to fill in sparse regions and gaps left by phylogenies. OTT is a base that has been little used for sequencing analyzes of the 16S region, however, it has a greater number of sequences classified taxonomically down to the genus level compared to SILVA and Greengenes. However, in terms of classification at the edge level, it contains a lesser amount of information[91]\n Ribosomal Database Project (RDP)[92] is a database that provides RNA ribosomal (rRNA) sequences of small subunits of domain bacterial and archaeal (16S); and fungal rRNA sequences of large subunits (28S).[93]\n \n"
    },
    {
        "title": "Deepfake",
        "url": "https://en.wikipedia.org/wiki/Deepfake",
        "content": "Deepfakes (a portmanteau of 'deep learning' and 'fake'[1]) are images, videos, or audio which are edited or generated using artificial intelligence tools, and which may depict real or non-existent people. They are a type of synthetic media[2] and modern form of a Media prank.\n While the act of creating fake content is not new, deepfakes uniquely leverage the technological tools and techniques of machine learning and artificial intelligence,[3][4][5] including facial recognition algorithms and artificial neural networks such as variational autoencoders (VAEs) and generative adversarial networks (GANs).[4][6] In turn the field of image forensics develops techniques to detect manipulated images.[7] Deepfakes have garnered widespread attention for their potential use in creating child sexual abuse material, celebrity pornographic videos, revenge porn, fake news, hoaxes, bullying, and financial fraud.[8][9][10][11]\n Academics have raised concerns about the potential for deep fakes to be used to promote disinformation and hate speech, and interfere with elections. The information technology industry and governments have responded with recommendations to detect and limit their use.\n From traditional entertainment to gaming, deepfake technology has evolved to be increasingly convincing[12] and available to the public, allowing for the disruption of the entertainment and media industries.[13]\n Photo manipulation was developed in the 19th century and soon applied to motion pictures. Technology steadily improved during the 20th century, and more quickly with the advent of digital video.\n Deepfake technology has been developed by researchers at academic institutions beginning in the 1990s, and later by amateurs in online communities.[14][15] More recently the methods have been adopted by industry.[16]\n Academic research related to deepfakes is split between the field of computer vision, a sub-field of computer science,[14] which develops techniques for creating and identifying deepfakes, and humanities and social science approaches that study the social, ethical and aesthetic implications of deepfakes.\n In cinema studies, deepfakes demonstrate how \"the human face is emerging as a central object of ambivalence in the digital age\".[17] Video artists have used deepfakes to \"playfully rewrite film history by retrofitting canonical cinema with new star performers\".[18] Film scholar Christopher Holliday analyses how switching out the gender and race of performers in familiar movie scenes destabilizes gender classifications and categories.[18] The idea of \"queering\" deepfakes is also discussed in Oliver M. Gingrich's discussion of media artworks that use deepfakes to reframe gender,[19] including British artist Jake Elwes' Zizi: Queering the Dataset, an artwork that uses deepfakes of drag queens to intentionally play with gender. The aesthetic potentials of deepfakes are also beginning to be explored. Theatre historian John Fletcher notes that early demonstrations of deepfakes are presented as performances, and situates these in the context of theater, discussing \"some of the more troubling paradigm shifts\" that deepfakes represent as a performance genre.[20]\n Philosophers and media scholars have discussed the ethics of deepfakes especially in relation to pornography.[21] Media scholar Emily van der Nagel draws upon research in photography studies on manipulated images to discuss verification systems, that allow women to consent to uses of their images.[22]\n Beyond pornography, deepfakes have been framed by philosophers as an \"epistemic threat\" to knowledge and thus to society.[23] There are several other suggestions for how to deal with the risks deepfakes give rise beyond pornography, but also to corporations, politicians and others, of \"exploitation, intimidation, and personal sabotage\",[24] and there are several scholarly discussions of potential legal and regulatory responses both in legal studies and media studies.[25] In psychology and media studies, scholars discuss the effects of disinformation that uses deepfakes,[26][27] and the social impact of deepfakes.[28]\n While most English-language academic studies of deepfakes focus on the Western anxieties about disinformation and pornography, digital anthropologist Gabriele de Seta has analyzed the Chinese reception of deepfakes, which are known as huanlian, which translates to \"changing faces\". The Chinese term does not contain the \"fake\" of the English deepfake, and de Seta argues that this cultural context may explain why the Chinese response has been more about practical regulatory responses to \"fraud risks, image rights, economic profit, and ethical imbalances\".[29]\n An early landmark project was the Video Rewrite program, published in 1997. The program modified existing video footage of a person speaking to depict that person mouthing the words contained in a different audio track.[30] It was the first system to fully automate this kind of facial reanimation, and it did so using machine learning techniques to make connections between the sounds produced by a video's subject and the shape of the subject's face.[30]\n Contemporary academic projects have focused on creating more realistic videos and on improving techniques.[31][32] The \"Synthesizing Obama\" program, published in 2017, modifies video footage of former president Barack Obama to depict him mouthing the words contained in a separate audio track.[31] The project lists as a main research contribution to its photorealistic technique for synthesizing mouth shapes from audio.[31] The Face2Face program, published in 2016, modifies video footage of a person's face to depict them mimicking the facial expressions of another person.[32]The project highlights its primary research contribution as the development of the first method for re-enacting facial expressions in real time using a camera that does not capture depth, enabling the technique to work with common consumer cameras.[1]\n In August 2018, researchers at the University of California, Berkeley published a paper introducing a fake dancing app that can create the impression of masterful dancing ability using AI.[33] This project expands the application of deepfakes to the entire body; previous works focused on the head or parts of the face.[34]\n Researchers have also shown that deepfakes are expanding into other domains such as tampering with medical imagery.[35] In this work, it was shown how an attacker can automatically inject or remove lung cancer in a patient's 3D CT scan. The result was so convincing that it fooled three radiologists and a state-of-the-art lung cancer detection AI. To demonstrate the threat, the authors successfully performed the attack on a hospital in a White hat penetration test.[36]\n A survey of deepfakes, published in May 2020, provides a timeline of how the creation and detection deepfakes have advanced over the last few years.[37] The survey identifies that researchers have been focusing on resolving the following challenges of deepfake creation:\n Overall, deepfakes are expected to have several implications in media and society, media production, media representations, media audiences, gender, law, and regulation, and politics.[38]\n The term deepfakes originated around the end of 2017 from a Reddit user named \"deepfakes\".[39] He, as well as others in the Reddit community r/deepfakes, shared deepfakes they created; many videos involved celebrities' faces swapped onto the bodies of actors in pornographic videos,[39] while non-pornographic content included many videos with actor Nicolas Cage's face swapped into various movies.[40]\n Other online communities remain, including Reddit communities that do not share pornography, such as r/SFWdeepfakes (short for \"safe for work deepfakes\"), in which community members share deepfakes depicting celebrities, politicians, and others in non-pornographic scenarios.[41] Other online communities continue to share pornography on platforms that have not banned deepfake pornography.[42]\n In January 2018, a proprietary desktop application called FakeApp was launched.[43] This app allows users to easily create and share videos with their faces swapped with each other.[44] As of 2019, FakeApp has been superseded by open-source alternatives such as Faceswap, command line-based DeepFaceLab, and web-based apps such as DeepfakesWeb.com [45][46][47]\n Larger companies started to use deepfakes.[16] Corporate training videos can be created using deepfaked avatars and their voices, for example Synthesia, which uses deepfake technology with avatars to create personalized videos.[48] The mobile app Momo created the application Zao which allows users to superimpose their face on television and movie clips with a single picture.[16] As of 2019 the Japanese AI company DataGrid made a full body deepfake that could create a person from scratch.[49]\n As of 2020 audio deepfakes, and AI software capable of detecting deepfakes and cloning human voices after 5 seconds of listening time also exist.[50][51][52][53][54][55] A mobile deepfake app, Impressions, was launched in March 2020. It was the first app for the creation of celebrity deepfake videos from mobile phones.[56][57]\n Deepfake technology's ability to  fabricate messages and actions of others can include deceased individuals. On 29 October 2020, Kim Kardashian posted a video featuring a hologram of her late father Robert Kardashian created by the company Kaleida, which used a combination of performance, motion tracking, SFX, VFX and DeepFake technologies to create the illusion.[58][59]\n In 2020, a deepfake video of Joaquin Oliver, a victim of the Parkland shooting was created as part of a gun safety campaign. Oliver's parents partnered with nonprofit Change the Ref and McCann Health to produce a video in which Oliver to encourage people to support gun safety legislation and politicians who back do so as well.[60]\n In 2022, a deepfake video of Elvis Presley was used on the program America's Got Talent 17.[61]\n A TV commercial used a deepfake video of Beatles member John Lennon, who was murdered in 1980.[62]\n Deepfakes rely on a type of neural network called an autoencoder.[63] These consist of an encoder, which reduces an image to a lower dimensional latent space, and a decoder, which reconstructs the image from the latent representation.[64] Deepfakes utilize this architecture by having a universal encoder which encodes a person in to the latent space.[citation needed] The latent representation contains key features about their facial features and body posture. This can then be decoded with a model trained specifically for the target. This means the target's detailed information will be superimposed on the underlying facial and body features of the original video, represented in the latent space.[citation needed]\n A popular upgrade to this architecture attaches a generative adversarial network to the decoder. A GAN trains a generator, in this case the decoder, and a discriminator in an adversarial relationship. The generator creates new images from the latent representation of the source material, while the discriminator attempts to determine whether or not the image is generated.[citation needed] This causes the generator to create images that mimic reality extremely well as any defects would be caught by the discriminator.[65] Both algorithms improve constantly in a zero sum game. This makes deepfakes difficult to combat as they are constantly evolving; any time a defect is determined, it can be corrected.[65]\n Digital clones of professional actors have appeared in films before, and progress in deepfake technology is expected to further the accessibility and effectiveness of such clones.[66] The use of AI technology was a major issue in the 2023 SAG-AFTRA strike, as new techniques enabled the capability of generating and storing a digital likeness to use in place of actors.[67]\n Disney has improved their visual effects using high-resolution deepfake face swapping technology.[68] Disney improved their technology through progressive training programmed to identify facial expressions, implementing a face-swapping feature, and iterating in order to stabilize and refine the output.[68] This high-resolution deepfake technology saves significant operational and production costs.[69] Disney's deepfake generation model can produce AI-generated media at a 1024 x 1024 resolution, as opposed to common models that produce media at a 256 x 256 resolution.[69] The technology allows Disney to de-age characters or revive deceased actors.[70] Similar technology was initially used by fans to unofficially insert faces into existing media, such as  overlaying Harrison Ford's young face onto Han Solo's face in Solo: A Star Wars Story.[71] Disney used deepfakes for the characters of Princess Leia and Grand Moff Tarkin in Rogue One.[72][73]\n The 2020 documentary Welcome to Chechnya used deepfake technology to obscure the identity of the people interviewed, so as to protect them from retaliation.[74]\n Creative Artists Agency has developed a facility to capture the likeness of an actor \"in a single day\", to develop a digital clone of the actor, which would be controlled by the actor or their estate alongside other personality rights.[75]\n Companies which have used digital clones of professional actors in advertisements include Puma, Nike and Procter & Gamble.[76]\n Deep fake allowed portray David Beckham to able to publish in a campaign in nearly nine languages to raise awareness the fight against Malaria.[77]\n In the 2024 Indian Tamil science fiction action thriller The Greatest of All Time, the teenage version of Vijay's character Jeevan is portrayed by Ayaz Khan. Vijay's teenage face was then attained by AI deepfake.[78]\n In March 2018 the multidisciplinary artist Joseph Ayerle published the video artwork Un'emozione per sempre 2.0 (English title: The Italian Game). The artist worked with Deepfake technology to create an AI actor, a synthetic version of 80s movie star Ornella Muti, Deepfakes are also being used in education and media to create realistic videos and interactive content, which offer new ways to engage audiences. However, they also bring risks, especially for spreading false information, which has led to calls for responsible use and clear rules. traveling in time from 1978 to 2018. The Massachusetts Institute of Technology referred this artwork in the study \"Collective Wisdom\".[79] The artist used Ornella Muti's time travel to explore generational reflections, while also investigating questions about the role of provocation in the world of art.[80] For the technical realization Ayerle used scenes of photo model Kendall Jenner. The program replaced Jenner's face by an AI calculated face of Ornella Muti. As a result, the AI actor has the face of the Italian actor Ornella Muti and the body of Kendall Jenner.\n Deepfakes have been widely used in satire or to parody celebrities and politicians. The 2020 webseries Sassy Justice, created by Trey Parker and Matt Stone, heavily features the use of deepfaked public figures to satirize current events and raise awareness of deepfake technology.[81]\n Deepfakes can be used to generate blackmail materials that falsely incriminate a victim. A report by the American Congressional Research Service warned that deepfakes could be used to blackmail elected officials or those with access to classified information for espionage or influence purposes.[82]\n Alternatively, since the fakes cannot reliably be distinguished from genuine materials, victims of actual blackmail can now claim that the true artifacts are fakes, granting them plausible deniability. The effect is to void credibility of existing blackmail materials, which erases loyalty to blackmailers and destroys the blackmailer's control. This phenomenon can be termed \"blackmail inflation\", since it \"devalues\" real blackmail, rendering it worthless.[83] It is possible to utilize commodity GPU hardware with a small software program to generate this blackmail content for any number of subjects in huge quantities, driving up the supply of fake blackmail content limitlessly and in highly scalable fashion.[84]\n On June 8, 2022,[85] Daniel Emmet, a former AGT contestant, teamed up with the AI startup[86][87] Metaphysic AI, to create a hyperrealistic deepfake to make it appear as Simon Cowell. Cowell, notoriously known for severely critiquing contestants,[88] was on stage interpreting \"You're The Inspiration\" by Chicago. Emmet sang on stage as an image of Simon Cowell emerged on the screen behind him in flawless synchronicity.[89]\n On August 30, 2022, Metaphysic AI had 'deep-fake' Simon Cowell, Howie Mandel and Terry Crews singing opera on stage.[90]\n On September 13, 2022, Metaphysic AI performed with a synthetic version of Elvis Presley for the finals of America's Got Talent.[91]\n The MIT artificial intelligence project 15.ai has been used for content creation for multiple Internet fandoms, particularly on social media.[92][93][94]\n In 2023 the bands ABBA and KISS partnered with Industrial Light & Magic and Pophouse Entertainment to develop deepfake avatars capable of performing virtual concerts.[95]\n Fraudsters and scammers make use of deepfakes to trick people into fake investment schemes, financial fraud, cryptocurrencies, sending money, and following endorsements. The likenesses of celebrities and politicians have been used for large-scale scams, as well as those of private individuals, which are used in spearphishing attacks. According to the Better Business Bureau, deepfake scams are becoming more prevalent.[96] These scams are responsible for an estimated $12 billion in fraud losses globally.[97] According to a recent report these numbers are expected to reach $40 Billion over the next three years.[98]\n Fake endorsements have misused the identities of celebrities like Taylor Swift,[99][96] Tom Hanks,[100] Oprah Winfrey,[101] and Elon Musk;[102] news anchors[103] like Gayle King[100] and Sally Bundock;[104] and politicians like Lee Hsien Loong[105] and Jim Chalmers.[106][107] Videos of them have appeared in online advertisements on YouTube, Facebook, and TikTok, who have policies against synthetic and manipulated media.[108][99][109] Ads running these videos are seen by millions of people. A single Medicare fraud campaign had been viewed more than 195 million times across thousands of videos.[108][110] Deepfakes have been used for: a fake giveaway of Le Creuset cookware for a \"shipping fee\" without receiving the products, except for hidden monthly charges;[99] weight-loss gummies that charge significantly more than what was said;[101] a fake iPhone giveaway;[99][109] and fraudulent get-rich-quick,[102][111] investment,[112] and cryptocurrency schemes.[105][113]\n Many ads pair AI voice cloning with \"decontextualized video of the celebrity\" to mimic authenticity. Others use a whole clip from a celebrity before moving to a different actor or voice.[108] Some scams may involve real-time deepfakes.[109]\n Celebrities have been warning people of these fake endorsements, and to be more vigilant against them.[96][99][101] Celebrities are unlikely to file lawsuits against every person operating deepfake scams, as \"finding and suing anonymous social media users is resource intensive,\" though cease and desist letters to social media companies work in getting videos and ads taken down.[114]\n Audio deepfakes have been used as part of social engineering scams, fooling people into thinking they are receiving instructions from a trusted individual.[115] In 2019, a U.K.-based energy firm's CEO was scammed over the phone when he was ordered to transfer €220,000 into a Hungarian bank account by an individual who reportedly used audio deepfake technology to impersonate the voice of the firm's parent company's chief executive.[116][117]\n As of 2023, the combination advances in deepfake technology, which could clone an individual's voice from a recording of a few seconds to a minute, and new text generation tools, enabled automated impersonation scams, targeting victims using a convincing digital clone of a friend or relative.[118]\n Audio deepfakes can be used to mask a user's real identity. In online gaming, for example, a player may want to choose a voice that sounds like their in-game character when speaking to other players. Those who are subject to harassment, such as women, children, and transgender people, can use these \"voice skins\" to hide their gender or age.[119]\n In 2020, an internet meme emerged utilizing deepfakes to generate videos of people singing the chorus of \"Baka Mitai\" (ばかみたい), a song from the game Yakuza 0 in the video game series Like a Dragon. In the series, the melancholic song is sung by the player in a karaoke minigame. Most iterations of this meme use a 2017 video uploaded by user Dobbsyrules, who lip syncs the song, as a template.[120][121]\n Deepfakes have been used to misrepresent well-known politicians in videos.\n In 2017, Deepfake pornography prominently surfaced on the Internet, particularly on Reddit.[144] As of 2019, many deepfakes on the internet feature pornography of female celebrities whose likeness is typically used without their consent.[145] A report published in October 2019 by Dutch cybersecurity startup Deeptrace estimated that 96% of all deepfakes online were pornographic.[146] \nAs of 2018, a Daisy Ridley deepfake first captured attention,[144] among others.[147][148][149] As of October 2019, most of the deepfake subjects on the internet were British and American actors.[145] However, around a quarter of the subjects are South Korean, the majority of which are K-pop stars.[145][150]\n In June 2019, a downloadable Windows and Linux application called DeepNude was released that used neural networks, specifically generative adversarial networks, to remove clothing from images of women. The app had both a paid and unpaid version, the paid version costing $50.[151][152] On 27 June the creators removed the application and refunded consumers.[153]\n Female celebrities are often a main target when it comes to deepfake pornography. In 2023, deepfake porn videos appeared online of Emma Watson and Scarlett Johansson in a face swapping app.[154] In 2024, deepfake porn images circulated online of Taylor Swift.[155]\n Academic studies have reported that women, LGBT people and people of colour (particularly activists, politicians and those questioning power) are at higher risk of being targets of promulgation of deepfake pornography.[156]\n Deepfakes have begun to see use in popular social media platforms, notably through Zao, a Chinese deepfake app that allows users to substitute their own faces onto those of characters in scenes from films and television shows such as Romeo + Juliet and Game of Thrones.[157] The app originally faced scrutiny over its invasive user data and privacy policy, after which the company put out a statement claiming it would revise the policy.[16] In January 2020 Facebook announced that it was introducing new measures to counter this on its platforms.[158]\n The Congressional Research Service cited unspecified evidence as showing that foreign intelligence operatives used deepfakes to create social media accounts with the purposes of recruiting individuals with access to classified information.[82]\n In 2021, realistic deepfake videos of actor Tom Cruise were released on TikTok, which went viral and garnered more than tens of millions of views. The deepfake videos featured an \"artificial intelligence-generated doppelganger\" of Cruise doing various activities such as teeing off at the golf course, showing off a coin trick, and biting into a lollipop. The creator of the clips, Belgian VFX Artist Chris Umé,[159] said he first got interested in deepfakes in 2018 and saw the \"creative potential\" of them.[160][161]\n Deepfake photographs can be used to create sockpuppets, non-existent people, who are active both online and in traditional media. A deepfake photograph appears to have been generated together with a legend for an apparently non-existent person named Oliver Taylor, whose identity was described as a university student in the United Kingdom. The Oliver Taylor persona submitted opinion pieces in several newspapers and was active in online media attacking a British legal academic and his wife, as \"terrorist sympathizers.\" The academic had drawn international attention in 2018 when he commenced a lawsuit in Israel against NSO, a surveillance company, on behalf of people in Mexico who alleged they were victims of NSO's phone hacking technology. Reuters could find only scant records for Oliver Taylor and \"his\" university had no records for him. Many experts agreed that the profile photo is a deepfake. Several newspapers have not retracted articles attributed to him or removed them from their websites. It is feared that such techniques are a new battleground in disinformation.[162]\n Collections of deepfake photographs of non-existent people on social networks have also been deployed as part of Israeli partisan propaganda. The Facebook page \"Zionist Spring\" featured photos of non-existent persons along with their \"testimonies\" purporting to explain why they have abandoned their left-leaning politics to embrace right-wing politics, and the page also contained large numbers of posts from Prime Minister of Israel Benjamin Netanyahu and his son and from other Israeli right wing sources. The photographs appear to have been generated by \"human image synthesis\" technology, computer software that takes data from photos of real people to produce a realistic composite image of a non-existent person. In much of the \"testimonies,\" the reason given for embracing the political right was the shock of learning of alleged incitement to violence against the prime minister. Right wing Israeli television broadcasters then broadcast the \"testimonies\" of these non-existent people based on the fact that they were being \"shared\" online. The broadcasters aired these \"testimonies\" despite being unable to find such people, explaining \"Why does the origin matter?\" Other Facebook fake profiles—profiles of fictitious individuals—contained material that allegedly contained such incitement against the right wing prime minister, in response to which the prime minister complained that there was a plot to murder him.[163][164]\n Though fake photos have long been plentiful, faking motion pictures has been more difficult, and the presence of deepfakes increases the difficulty of classifying videos as genuine or not.[122] AI researcher Alex Champandard has said people should know how fast things can be corrupted with deepfake technology, and that the problem is not a technical one, but rather one to be solved by trust in information and journalism.[122] Computer science associate professor Hao Li of the University of Southern California states that deepfakes created for malicious use, such as fake news, will be even more harmful if nothing is done to spread awareness of deepfake technology.[165] Li predicted that genuine videos and deepfakes would become indistinguishable in as soon as half a year, as of October 2019, due to rapid advancement in artificial intelligence and computer graphics.[165] Former Google fraud czar Shuman Ghosemajumder has called deepfakes an area of \"societal concern\" and said that they will inevitably evolve to a point at which they can be generated automatically, and an individual could use that technology to produce millions of deepfake videos.[166]\n A primary pitfall is that humanity could fall into an age in which it can no longer be determined whether a medium's content corresponds to the truth.[122][167] Deepfakes are one of a number of tools for disinformation attack, creating doubt, and undermining trust. They have a potential to interfere with democratic functions in societies, such as identifying collective agendas, debating issues, informing decisions, and solving problems though the exercise of political will.[168] People may also start to dismiss real events as fake.[119]\n Deepfakes possess the ability to damage individual entities tremendously.[169] This is because deepfakes are often targeted at one individual, and/or their relations to others in hopes to create a narrative powerful enough to influence public opinion or beliefs. This can be done through deepfake voice phishing, which manipulates audio to create fake phone calls or conversations.[169] Another method of deepfake use is fabricated private remarks, which manipulate media to convey individuals voicing damaging comments.[169] The quality of a negative video or audio does not need to be that high. As long as someone's likeness and actions are recognizable, a deepfake can hurt their reputation.[119]\n In September 2020 Microsoft made public that they are developing a Deepfake detection software tool.[170]\n Detecting fake audio is a highly complex task that requires careful attention to the audio signal in order to achieve good performance. Using deep learning, preprocessing of feature design and masking augmentation have been proven effective in improving performance.[171]\n Most of the academic research surrounding deepfakes focuses on the detection of deepfake videos.[172] One approach to deepfake detection is to use algorithms to recognize patterns and pick up subtle inconsistencies that arise in deepfake videos.[172] For example, researchers have developed automatic systems that examine videos for errors such as irregular blinking patterns of lighting.[173][14] This approach has been criticized because deepfake detection is characterized by a \"moving goal post\" where the production of deepfakes continues to change and improve as algorithms to detect deepfakes improve.[172] In order to assess the most effective algorithms for detecting deepfakes, a coalition of leading technology companies hosted the Deepfake Detection Challenge to accelerate the technology for identifying manipulated content.[174] The winning model of the Deepfake Detection Challenge was 65% accurate on the holdout set of 4,000 videos.[175] A team at Massachusetts Institute of Technology published a paper in December 2021 demonstrating that ordinary humans are 69–72% accurate at identifying a random sample of 50 of these videos.[176]\n A team at the University of Buffalo published a paper in October 2020 outlining their technique of using reflections of light in the eyes of those depicted to spot deepfakes with a high rate of success, even without the use of an AI detection tool, at least for the time being.[177]\n In the case of well-documented individuals such as political leaders, algorithms have been developed to distinguish identity-based features such as patterns of facial, gestural, and vocal mannerisms and detect deep-fake impersonators.[178]\n Another team led by Wael AbdAlmageed with Visual Intelligence and Multimedia Analytics Laboratory (VIMAL) of the Information Sciences Institute at the University Of Southern California developed two generations [179][180] of deepfake detectors based on convolutional neural networks. The first generation [179] used recurrent neural networks to spot spatio-temporal inconsistencies to identify visual artifacts left by the deepfake generation process. The algorithm achieved 96% accuracy on FaceForensics++, the only large-scale deepfake benchmark available at that time. The second generation [180] used end-to-end deep networks to differentiate between artifacts and high-level semantic facial information using two-branch networks. The first branch propagates colour information while the other branch suppresses facial content and amplifies low-level frequencies using Laplacian of Gaussian (LoG). Further, they included a new loss function that learns a compact representation of bona fide faces, while dispersing the representations (i.e. features) of deepfakes. VIMAL's approach showed state-of-the-art performance on FaceForensics++ and Celeb-DF benchmarks, and on March 16, 2022 (the same day of the release), was used to identify the deepfake of Volodymyr Zelensky out-of-the-box without any retraining or knowledge of the algorithm with which the deepfake was created. [citation needed]\n Other techniques suggest that blockchain could be used to verify the source of the media.[181] For instance, a video might have to be verified through the ledger before it is shown on social media platforms.[181] With this technology, only videos from trusted sources would be approved, decreasing the spread of possibly harmful deepfake media.[181]\n Digitally signing of all video and imagery by cameras and video cameras, including smartphone cameras, was suggested to fight deepfakes.[182] That allows tracing every photograph or video back to its original owner that can be used to pursue dissidents.[182]\n One easy way to uncover deepfake video calls consists in asking the caller to turn sideways.[183]\n Henry Ajder who works for Deeptrace, a company that detects deepfakes, says there are several ways to protect against deepfakes in the workplace. Semantic passwords or secret questions can be used when holding important conversations. Voice authentication and other biometric security features should be up to date. Educate employees about deepfakes.[119]\n In March 2024, a video clip was shown from the Buckingham Palace, where Kate Middleton had cancer and she was undergoing chemotherapy.      \nHowever, the clip fuelled rumours that the woman in that clip was an AI deepfake.[184] UCLA's race director Johnathan Perkins doubted she had cancer, and further speculated that she could be in critical condition or dead.\n[185]\n Twitter (later X) is taking active measures to handle synthetic and manipulated media on their platform. In order to prevent disinformation from spreading, Twitter is placing a notice on tweets that contain manipulated media and/or deepfakes that signal to viewers that the media is manipulated.[210] There will also be a warning that appears to users who plan on retweeting, liking, or engaging with the tweet.[210] Twitter will also work to provide users a link next to the tweet containing manipulated or synthetic media that links to a Twitter Moment or credible news article on the related topic—as a debunking action.[210] Twitter also has the ability to remove any tweets containing deepfakes or manipulated media that may pose a harm to users' safety.[210] In order to better improve Twitter's detection of deepfakes and manipulated media, Twitter asked users who are interested in partnering with them to work on deepfake detection solutions to fill out a form.[211]\n \"In August 2024, the secretaries of state of Minnesota, Pennsylvania, Washington, Michigan and New Mexico penned an open letter to X owner Elon Musk urging modifications to its AI chatbot Grok's new text-to-video generator, added in August 2024, stating that it had disseminated election misinformation.[212][213][214]\n Facebook has taken efforts towards encouraging the creation of deepfakes in order to develop state of the art deepfake detection software. Facebook was the prominent partner in hosting the Deepfake Detection Challenge (DFDC), held December 2019, to 2114 participants who generated more than 35,000 models.[215] The top performing models with the highest detection accuracy were analyzed for similarities and differences; these findings are areas of interest in further research to improve and refine deepfake detection models.[215] Facebook has also detailed that the platform will be taking down media generated with artificial intelligence used to alter an individual's speech.[216] However, media that has been edited to alter the order or context of words in one's message would remain on the site but be labeled as false, since it was not generated by artificial intelligence.[216]\n On 31 January 2018, Gfycat began removing all deepfakes from its site.[217][218] On Reddit, the r/deepfakes subreddit was banned on 7 February 2018, due to the policy violation of \"involuntary pornography\".[219][220][221][222][223] In the same month, representatives from Twitter stated that they would suspend accounts suspected of posting non-consensual deepfake content.[224] Chat site Discord has taken action against deepfakes in the past,[225] and has taken a general stance against deepfakes.[218][226] In September 2018, Google added \"involuntary synthetic pornographic imagery\" to its ban list, allowing anyone to request the block of results showing their fake nudes.[227][check quotation syntax]\n In February 2018, Pornhub said that it would ban deepfake videos on its website because it is considered \"non consensual content\" which violates their terms of service.[228] They also stated previously to Mashable that they will take down content flagged as deepfakes.[229] Writers from Motherboard reported that searching \"deepfakes\" on Pornhub still returned multiple recent deepfake videos.[228]\n Facebook has previously stated that they would not remove deepfakes from their platforms.[230] The videos will instead be flagged as fake by third-parties and then have a lessened priority in user's feeds.[231] This response was prompted in June 2019 after a deepfake featuring a 2016 video of Mark Zuckerberg circulated on Facebook and Instagram.[230]\n In May 2022, Google officially changed the terms of service for their Jupyter Notebook colabs, banning the use of their colab service for the purpose of creating deepfakes.[232] This came a few days after a VICE article had been published, claiming that \"most deepfakes are non-consensual porn\" and that the main use of popular deepfake software DeepFaceLab (DFL), \"the most important technology powering the vast majority of this generation of deepfakes\" which often was used in combination with Google colabs, would be to create non-consensual pornography, by pointing to the fact that among many other well-known examples of third-party DFL implementations such as deepfakes commissioned by The Walt Disney Company, official music videos, and web series Sassy Justice by the creators of South Park, DFL's GitHub page also links to deepfake porn website Mr.‍Deepfakes and participants of the DFL Discord server also participate on Mr.‍Deepfakes.[233]\n In the United States, there have been some responses to the problems posed by deepfakes. In 2018, the Malicious Deep Fake Prohibition Act was introduced to the US Senate;[234] in 2019, the Deepfakes Accountability Act was introduced in the 116th United States Congress by U.S. representative for New York's 9th congressional district Yvette Clarke.[235] Several states have also introduced legislation regarding deepfakes, including Virginia,[236] Texas, California, and New York;[237] charges as varied as identity theft, cyberstalking, and revenge porn have been pursued, while more comprehensive statutes are urged.[227]\n Among U.S. legislative efforts, on 3 October 2019, California governor Gavin Newsom signed into law Assembly Bills No. 602 and No. 730.[238][239] Assembly Bill No. 602 provides individuals targeted by sexually explicit deepfake content made without their consent with a cause of action against the content's creator.[238] Assembly Bill No. 730 prohibits the distribution of malicious deepfake audio or visual media targeting a candidate running for public office within 60 days of their election.[239] U.S. representative Yvette Clarke introduced H.R. 5586: Deepfakes Accountability Act into the 118th United States Congress on September 20, 2023 in an effort to protect national security from threats posed by deepfake technology.[240] U.S. representative María Salazar introduced H.R. 6943: No AI Fraud Act into the 118th United States Congress on January 10, 2024, to establish specific property rights of individual physicality, including voice.[241]\n In November 2019, China announced that deepfakes and other synthetically faked footage should bear a clear notice about their fakeness starting in 2020. Failure to comply could be considered a crime the Cyberspace Administration of China stated on its website.[242] The Chinese government seems to be reserving the right to prosecute both users and online video platforms failing to abide by the rules.[243] The Cyberspace Administration of China, the Ministry of Industry and Information Technology, and the Ministry of Public Security jointly issued the Provision on the Administration of Deep Synthesis Internet Information Service in November 2022.[244] China's updated Deep Synthesis Provisions (Administrative Provisions on Deep Synthesis in Internet-Based Information Services) went into effect in January 2023.[245]\n In the United Kingdom, producers of deepfake material could be prosecuted for harassment, but deepfake production was not a specific crime[246] until 2023, when the Online Safety Act was passed, which made deepfakes illegal; the UK plans to expand the Act's scope to criminalize deepfakes created with \"intention to cause distress\" in 2024.[247][248]\n In Canada, in 2019, the Communications Security Establishment released a report which said that deepfakes could be used to interfere in Canadian politics, particularly to discredit politicians and influence voters.[249][250] As a result, there are multiple ways for citizens in Canada to deal with deepfakes if they are targeted by them.[251] In February 2024, bill C-63 was tabled in the 44th Canadian Parliament in order to enact the Online Harms Act, which would amend Criminal Code, and other Acts. An earlier version of the Bill, C-36, was ended by the dissolution of the 43rd Canadian Parliament in September 2021.[252][253]\n In India, there are no direct laws or regulation on AI or deepfakes, but there are provisions under the Indian Penal Code and Information Technology Act 2000/2008, which can be looked at for legal remedies, and the new proposed Digital India Act will have a chapter on AI and deepfakes in particular, as per the MoS Rajeev Chandrasekhar.[254]\n In Europe, the European Union's 2024 Artificial Intelligence Act (AI Act) takes a risk-based approach to regulating AI systems, including deepfakes. It establishes categories of \"unacceptable risk,\" \"high risk,\" \"specific/limited or transparency risk\", and \"minimal risk\" to determine the level of regulatory obligations for AI providers and users. However, the lack of clear definitions for these risk categories in the context of deepfakes creates potential challenges for effective implementation. Legal scholars have raised concerns about the classification of deepfakes intended for political misinformation or the creation of non-consensual intimate imagery. Debate exists over whether such uses should always be considered \"high-risk\" AI systems, which would lead to stricter regulatory requirements.[255]\n In August 2024, the Irish Data Protection Commission (DPC) launched court proceedings against X for its unlawful use of the personal data of over 60 million EU/EEA users, in order to train its AI technologies, such as its chatbot Grok.[256]\n In 2016, the Defense Advanced Research Projects Agency (DARPA) launched the Media Forensics (MediFor) program which was funded through 2020.[257] MediFor aimed at automatically spotting digital manipulation in images and videos, including Deepfakes.[258][259] In the summer of 2018, MediFor held an event where individuals competed to create AI-generated videos, audio, and images as well as automated tools to detect these deepfakes.[260] According to the MediFor program, it established a framework of three tiers of information - digital integrity, physical integrity and semantic integrity - to generate one integrity score in an effort to enable accurate detection of manipulated media.[261]\n In 2019, DARPA hosted a \"proposers day\" for the Semantic Forensics (SemaFor) program where researchers were driven to prevent viral spread of AI-manipulated media.[262] DARPA and the Semantic Forensics Program were also working together to detect AI-manipulated media through efforts in training computers to utilize common sense, logical reasoning.[262] Built on the MediFor's technologies, SemaFor's attribution algorithms infer if digital media originates from a particular organization or individual, while characterization algorithms determine whether media was generated or manipulated for malicious purposes.[263] In March 2024, SemaFor published an analytic catalog that offers the public access to open-source resources developed under SemaFor.[264][265]\n The International Panel on the Information Environment was launched in 2023 as a consortium of over 250 scientists working to develop effective countermeasures to deepfakes and other problems created by perverse incentives in organizations disseminating information via the Internet.[266]\n  Media related to Deepfake at Wikimedia Commons\n \n"
    },
    {
        "title": "Machine learning in earth sciences",
        "url": "https://en.wikipedia.org/wiki/Machine_learning_in_earth_sciences",
        "content": "Applications of machine learning (ML) in earth sciences include geological mapping, gas leakage detection and geological feature identification. Machine learning is a subdiscipline of artificial intelligence aimed at developing programs that are able to classify, cluster, identify, and analyze vast and complex data sets without the need for explicit programming to do so.[1] Earth science is the study of the origin, evolution, and future[2] of the Earth. The earth's system can be subdivided into four major components including the solid earth, atmosphere, hydrosphere, and biosphere.[3]\n A variety of algorithms may be applied depending on the nature of the task. Some algorithms may perform significantly better than others for particular objectives. For example, convolutional neural networks (CNNs) are good at interpreting images, whilst more general neural networks may be used for soil classification,[4] but can be more computationally expensive to train than alternatives such as support vector machines. The range of tasks to which ML (including deep learning) is applied has been ever-growing in recent decades, as has the development of other technologies such as unmanned aerial vehicles (UAVs),[5] ultra-high resolution remote sensing technology, and high-performance computing.[6] This has led to the availability of large high-quality datasets and more advanced algorithms.\n Problems in earth science are often complex.[7] It is difficult to apply well-known and described mathematical models to the natural environment, therefore machine learning is commonly a better alternative for such non-linear problems.[8] Ecological data are commonly non-linear and consist of higher-order interactions, and together with missing data, traditional statistics may underperform as unrealistic assumptions such as linearity are applied to the model.[9][10] A number of researchers found that machine learning outperforms traditional statistical models in earth science, such as in characterizing forest canopy structure,[11] predicting climate-induced range shifts,[12] and delineating geologic facies.[13] Characterizing forest canopy structure enables scientists to study vegetation response to climate change.[14] Predicting climate-induced range shifts enable policy makers to adopt suitable conversation method to overcome the consequences of climate change.[15] Delineating geologic facies helps geologists to understand the geology of an area, which is essential for the development and management of an area.[16]\n In Earth Sciences, some data are often difficult to access or collect, therefore inferring data from data that are easily available by machine learning method is desirable.[10] For example, geological mapping in tropical rainforests is challenging because the thick vegetation cover and rock outcrops are poorly exposed.[17] Applying remote sensing with machine learning approaches provides an alternative way for rapid mapping without the need of manually mapping in the unreachable areas.[17]\n Machine learning can also reduce the efforts done by experts, as manual tasks of classification and annotation etc are the bottlenecks in the workflow of the research of earth science.[10] Geological mapping, especially in a vast, remote area is labour, cost and time-intensive with traditional methods.[18] Incorporation of remote sensing and machine learning approaches can provide an alternative solution to eliminate some field mapping needs.[18]\n Consistency and bias-free is also an advantage of machine learning compared to manual works by humans. In research comparing the performance of human and machine learning in the identification of dinoflagellates, machine learning is found to be not as prone to systematic bias as humans.[19] A recency effect that is present in humans is that the classification often biases towards the most recently recalled classes.[19] In a labelling task of the research, if one kind of dinoflagellates occurs rarely in the samples, then expert ecologists commonly will not classify it correctly.[19] The systematic bias strongly deteriorate the classification accuracies of humans.[19]\n The extensive usage of machine learning in various fields has led to a wide range of algorithms of learning methods being applied. Choosing the optimal algorithm for a specific purpose can lead to a significant boost in accuracy:[20] for example, the lithological mapping of gold-bearing granite-greenstone rocks in Hutti, India with AVIRIS-NG hyperspectral data, shows more than 10% difference in overall accuracy between using support vector machines (SVMs) and random forest.[21]\n Some algorithms can also reveal hidden important information: white box models are transparent models, the outputs of which can be easily explained, while black box models are the opposite.[20] For example, although an SVM yielded the best result in landslide susceptibility assessment accuracy, the result cannot be rewritten in the form of expert rules that explain how and why an area was classified as that specific class.[7] In contrast, decision trees are transparent and easily understood, and the user can observe and fix the bias if any is present in such models.[7]\n If computational resource is a concern, more computationally demanding learning methods such as deep neural networks are less preferred, despite the fact that they may outperform other algorithms, such as in soil classification.[4]\n Geological or lithological mapping produces maps showing geological features and geological units. Mineral prospectivity mapping utilizes a variety of datasets such as geological maps and aeromagnetic imagery to produce maps that are specialized for mineral exploration.[22] Geological, lithological, and mineral prospectivity mapping can be carried out by processing data with ML techniques, with the input of spectral imagery obtained from remote sensing and geophysical data.[23] Spectral imaging is also used – the imaging of wavelength bands in the electromagnetic spectrum, while conventional imaging captures three wavelength bands (red, green, blue) in the electromagnetic spectrum.[24]\n Random forests and SVMs are some algorithms commonly used with remotely-sensed geophysical data, while Simple Linear Iterative Clustering-Convolutional Neural Network (SLIC-CNN)[5] and Convolutional Neural Networks (CNNs)[18] are commonly applied to aerial imagery. Large scale mapping can be carried out with geophysical data from airborne and satellite remote sensing geophysical data,[21] and smaller-scale mapping can be carried out with images from Unmanned Aerial Vehicles (UAVs) for higher resolution.[5]\n Vegetation cover is one of the major obstacles for geological mapping with remote sensing, as reported in various research, both in large-scale and small-scale mapping. Vegetation affects the quality of spectral images,[23] or obscures the rock information in aerial images.[5]\n Random Forest,\n Support Vector Machine (SVM)\n (1) Map generated with remote sensing data only has a 52.7% accuracy when compared to the geological map, but several new possible lithological units are identified\n (2) Map generated with remote sensing data and spatial constraints has a 78.7% accuracy but no new possible lithological units are identified\n geophysical data\n Morocco\n frequency electromagnetic, radiometric measurements, ground gravity measurements\n Liaoning Province, China\n Remote Predictive Mapping (RPM)\n Landsat Reflectance, High-Resolution Digital Elevation Data\n Northwest Territories, Canada\n Random Forest\n Landslide susceptibility refers to the probability of landslide of a certain geographical location, which is dependent on local terrain conditions.[27] Landslide susceptibility mapping can highlight areas prone to landslide risks, which is useful for urban planning and disaster management.[7] Such datasets for ML algorithms usually include topographic information, lithological information, satellite images, etc., and some may include land use, land cover, drainage information, and vegetation cover[7][28][29][30] according to the study requirements. As usual, for training an ML model for landslide susceptibility mapping, training and testing datasets are required.[7] There are two methods of allocating datasets for training and testing: one is to randomly split the study area for the datasets; another is to split the whole study into two adjacent parts for the two datasets. To test classification models, the common practice is to split the study area randomly;[7][31] however, it is more useful if the study area can be split into two adjacent parts so that an automation algorithm can carry out mapping of a new area with the input of expert-processed data of adjacent land.[7]\n Decision Trees, Logistic Regression\n Discontinuities such as fault planes and bedding planes have important implications in civil engineering.[32] Rock fractures can be recognized automatically by machine learning through photogrammetric analysis, even with the presence of interfering objects such as vegetation.[33] In ML training for classifying images, data augmentation is a common practice to avoid overfitting and increase the training dataset size and variability.[33] For example, in a study of rock fracture recognition, 68 images for training and 23 images for testing were prepared via random splitting.[33] Data augmentation was performed, increasing the training dataset size to 8704 images by flipping and random cropping.[33] The approach was able to recognize rock fractures accurately in most cases.[33] Both the negative prediction value (NPV) and the specificity were over 0.99.[33] This demonstrated the robustness of discontinuity analyses with machine learning.                                                        \n Seoul, Korea and Jeongseon-gun, Gangwon-do, Korea\n Quantifying carbon dioxide leakage from a geological sequestration site has gained increased attention as the public is interested in whether carbon dioxide is stored underground safely and effectively.[34] Carbon dioxide leakage from a geological sequestration site can be detected indirectly with the aid of remote sensing and an unsupervised clustering algorithm such as Iterative Self-Organizing Data Analysis Technique (ISODATA).[35] The increase in soil CO2 concentration causes a stress response for plants by inhibiting plant respiration, as oxygen is displaced by carbon dioxide.[36] The vegetation stress signal can be detected with the Normalized Difference Red Edge Index (NDRE).[36] The hyperspectral images are processed by the unsupervised algorithm, clustering pixels with similar plant responses.[36] The hyperspectral information in areas with known CO2 leakage is extracted so that areas with leakage can be matched with the clustered pixels with spectral anomalies.[36] Although the approach can identify CO2 leakage efficiently, there are some limitations that require further study.[36] The NDRE may not be accurate due to reasons like higher chlorophyll absorption, variation in vegetation, and shadowing effects; therefore, some stressed pixels can be incorrectly classed as healthy.[36] Seasonality, groundwater table height may also affect the stress response to CO2 of the vegetation.[36]\n The rock mass rating (RMR)[37] system is a widely adopted rock mass classification system by geomechanical means with the input of six parameters. The amount of water inflow is one of the inputs of the classification scheme, representing the groundwater condition. Quantification of the water inflow in the faces of a rock tunnel was traditionally carried out by visual observation in the field, which is labour and time consuming, and fraught with safety concerns.[38] Machine learning can determine water inflow by analyzing images taken on the construction site.[38] The classification of the approach mostly follows the RMR system, but combining damp and wet states, as it is difficult to distinguish only by visual inspection.[38][37] The images were classified into the non-damaged state, wet state, dripping state, flowing state, and gushing state.[38] The accuracy of classifying the images was approximately 90%.[38]\n The most popular cost-effective method od soil investigation method is cone penetration testing (CPT).[39] The test is carried out by pushing a metallic cone through the soil: the force required to push at a constant rate is recorded as a quasi-continuous log.[4] Machine learning can classify soil with the input of CPT data.[4] In an attempt to classify with ML, there are two tasks required to analyze the data, namely segmentation and classification.[4] Segmentation can be carried out with the Constraint Clustering and Classification (CONCC) algorithm to split a single series data into segments.[4] Classification can then be carried out by algorithms such as decision trees, SVMs, or neural networks.[4]\n Exposed geological structures such as anticlines, ripple marks, and xenoliths can be identified automatically with deep learning models.[40] Research has demonstrated that three-layer CNNs and transfer learning have strong accuracy (about 80% and 90% respectively), while others like k-nearest neighbors (k-NN), regular neural nets, and extreme gradient boosting (XGBoost) have low accuracies (ranging from 10% - 30%).[40] The grayscale images and colour images were both tested, with the accuracy difference being little, implying that colour is not very important in identifying geological structures.[40]\n Earthquake warning systems are often vulnerable to local impulsive noise, therefore giving out false alerts.[41] False alerts can be eliminated by discriminating the earthquake waveforms from noise signals with the aid of ML methods. The method consists of two parts, the first being unsupervised learning with a generative adversarial network (GAN) to learn and extract features of first-arrival P-waves, and the second being use of a random forest to discriminate P-waves. This approach achieved 99.2% in recognizing P-waves, and can avoid false triggers by noise signals with 98.4% accuracy.[41]\n Earthquakes can be produced in a laboratory settings to mimic real-world ones. With the help of machine learning, the patterns of acoustic signals as precursors for earthquakes can be identified. Predicting the time remaining before failure was demonstrated in a study with continuous acoustic time series data recorded from a fault. The algorithm applied was a random forest, trained with a set of slip events, performing strongly in predicting the time to failure. It identified acoustic signals to predict failures, with one of them being previously unidentified. Although this laboratory earthquake is not as complex as a natural one, progress was made that guides future earthquake prediction work.[42]\n Real-time streamflow data is integral for decision making (e.g., evacuations, or regulation of reservoir water levels during flooding).[43] Streamflow data can be estimated by data provided by stream gauges, which measure the water level of a river. However, water and debris from flooding may damage stream gauges, resulting in lack of essential real-time data. The ability of machine learning to infer missing data[10] enables it to predict streamflow with both historical stream gauge data and real-time data.\nStreamflow Hydrology Estimate using Machine Learning (SHEM) is a model that can serve this purpose. To verify its accuracies, the prediction result was compared with the actual recorded data, and the accuracies were found to be between 0.78 to 0.99.\n An adequate amount of training and validation data is required for machine learning.[10] However, some very useful products like satellite remote sensing data only have decades of data since the 1970s. If one is interested in the yearly data, then only less than 50 samples are available.[45] Such amount of data may not be adequate. In a study of automatic classification of geological structures, the weakness of the model is the small training dataset, even though with the help of data augmentation to increase the size of the dataset.[40] Another study of predicting streamflow found that the accuracies depend on the availability of sufficient historical data, therefore sufficient training data determine the performance of machine learning.[44] Inadequate training data may lead to a problem called overfitting. Overfitting causes inaccuracies in machine learning[46] as the model learns about the noise and undesired details.\n Machine learning cannot carry out some of the tasks as a human does easily. For example, in the quantification of water inflow in rock tunnel faces by images for Rock Mass Rating system (RMR),[38] the damp and the wet state was not classified by machine learning because discriminating the two only by visual inspection is not possible. In some tasks, machine learning may not able to fully substitute manual work by a human.\n In many machine learning algorithms, for example, Artificial Neural Network (ANN), it is considered as 'black box' approach as clear relationships and descriptions of how the results are generated in the hidden layers are unknown.[47] 'White-box' approach such as decision tree can reveal the algorithm details to the users.[48] If one wants to investigate the relationships, such 'black-box' approaches are not suitable. However, the performances of 'black-box' algorithms are usually better.[49]\n"
    },
    {
        "title": "Applications of artificial intelligence",
        "url": "https://en.wikipedia.org/wiki/Applications_of_artificial_intelligence#Finance",
        "content": "\n Artificial intelligence (AI) has been used in applications throughout industry and academia. In a manner analogous to electricity or computers, AI serves as a general-purpose technology. AI programs are designed to simulate human perception and understanding. These systems are capable of adapting to new information and responding to changing situations. Machine learning has been used for various scientific and commercial purposes[1] including language translation, image recognition, decision-making,[2][3] credit scoring, and e-commerce.\n Machine learning is has been used for recommendation systems in for determining which posts should show up in social media feeds.[4][5] Various types of social media analysis also make use of machine learning[6][7] and there is research into its use for (semi-)automated tagging/enhancement/correction of online misinformation and related filter bubbles.[8][9][10]\n AI has been used to customize shopping options and personalize offers.[11] Online gambling companies have used AI for targeting gamblers.[12]\n Intelligent personal assistants use AI to understand many natural language requests in other ways than rudimentary commands. Common examples are Apple's Siri, Amazon's Alexa, and a more recent AI, ChatGPT by OpenAI.[13]\n Bing Chat has used artificial intelligence as part of its search engine.[14]\n Machine learning can be used to combat spam, scams, and phishing. It can scrutinize the contents of spam and phishing attacks to attempt to identify malicious elements.[15] Some models built via machine learning algorithms have over 90% accuracy in distinguishing between spam and legitimate emails.[16] These models can be refined using new data and evolving spam tactics. Machine learning also analyzes traits such as sender behavior, email header information, and attachment types, potentially enhancing spam detection.[17]\n Speech translation technology attempts to convert one language's spoken words into another language. This potentially reduces language barriers in global commerce and cross-cultural exchange, enabling speakers of various languages to communicate with one another.[18]\n AI has been used to automatically translate spoken language and textual content in products such as Microsoft Translator, Google Translate, and DeepL Translator.[19] Additionally, research and development are in progress to decode and conduct animal communication.[20][21]\n Meaning is conveyed not only by text, but also through usage and context (see semantics and pragmatics). As a result, the two primary categorization approaches for machine translations are statistical and neural machine translations (NMTs). The old method of performing translation was to use a statistical machine translation (SMT) methodology to forecast the best probable output with specific algorithms. However, with NMT, the approach employs dynamic algorithms to achieve better translations based on context.[22]\n AI has been used in facial recognition systems. Some examples are Apple's Face ID and Android's Face Unlock, which are used to secure mobile devices.[23]\n Image labeling has been used by Google Image Labeler to detect products in photos and to allow people to search based on a photo. Image labeling has also been demonstrated to generate speech to describe images to blind people.[24] Facebook's DeepFace identifies human faces in digital images.\n Games have been a major application[relevant?] of AI's capabilities since the 1950s. In the 21st century, AIs have beaten human players in many games, including chess (Deep Blue), Jeopardy! (Watson),[25] Go (AlphaGo),[26][27][28][29][30][31][32] poker (Pluribus[33] and Cepheus),[34] E-sports (StarCraft),[35][36] and general game playing (AlphaZero[37][38][39] and MuZero).[40][41][42][43]\n Kuki AI is a set of chatbots and other apps which were designed for entertainment and as a marketing tool.[44][45] Character.ai is another example of a chatbot being used for recreation.\n AI for Good is a platform launched in 2017 by the International Telecommunication Union (ITU) agency of the United Nations (UN). The goal of the platform is to use AI to help achieve the UN's Sustainable Development Goals.[citation needed]\n The University of Southern California launched the Center for Artificial Intelligence in Society, with the goal of using AI to address problems such as homelessness. Stanford researchers use AI to analyze satellite images to identify high poverty areas.[46]\n In agriculture, AI has been proposed as a way for farmers to identify areas that need irrigation, fertilization, or pesticide treatments to increase yields, thereby improving efficiency.[47] AI has been used to attempt to classify livestock pig call emotions,[20] automate greenhouses,[48] detect diseases and pests,[49] and optimize irrigation.[50]\n Cyber security companies are adopting neural networks, machine learning, and natural language processing to improve their systems.[51]\n Applications of AI in cyber security include:\n AI elevates teaching, focusing on significant issues like the knowledge nexus and educational equality. The evolution of AI in education and technology should be used to improve human capabilities in relationships where they do not replace humans. UNESCO recognizes the future of AI in education as an instrument to reach Sustainable Development Goal 4, called \"Inclusive and Equitable Quality Education.” [56]\n The World Economic Forum also stresses AI's contribution to students' overall improvement and transforming teaching into a more enjoyable process.[56]\n Personalized Learning\n AI driven tutoring systems, such as Khan Academy, Duolingo and Carnegie Learning are the forefoot of delivering personalized education.[57]\n These platforms leverage AI algorithms to analyze individual learning patterns, strengths, and weaknesses, enabling the customization of content and Algorithm to suit each student's pace and style of learning.[57]\n Administrative Efficiency\n In educational institutions, AI is increasingly used to automate routine tasks like attendance tracking, grading and marking, which allows educators to devote more time to interactive teaching and direct student engagement.[58]\n Furthermore, AI tools are employed to monitor student progress, analyze learning behaviors, and predict academic challenges, facilitating timely and proactive interventions for students who may be at risk of falling behind.[58]\n Ethical and Privacy Concerns\n Despite the benefits, the integration of AI in education raises significant ethical and privacy concerns, particularly regarding the handling of sensitive student data.[57]\n It is imperative that AI systems in education are designed and operated with a strong emphasis on transparency, security, and respect for privacy to maintain trust and uphold the integrity of educational practices.[57]\n Much of the regulation will be influenced by the AI Act, the world’s first comprehensive AI law. [59]\n Financial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation. The use of AI in banking began in 1987 when Security Pacific National Bank launched a fraud prevention task-force to counter the unauthorized use of debit cards.[60] Kasisto and Moneystream use AI.\n Banks use AI to organize operations for bookkeeping, investing in stocks, and managing properties. AI can adapt to changes during non-business hours.[61] AI is used to combat fraud and financial crimes by monitoring behavioral patterns for any abnormal changes or anomalies.[62][63][64]\n The use of AI in applications such as online trading and decision-making has changed major economic theories.[65] For example, AI-based buying and selling platforms estimate personalized demand and supply curves, thus enabling individualized pricing. AI systems reduce information asymmetry in the market and thus make markets more efficient.[66] The application of artificial intelligence in the financial industry can alleviate the financing constraints of non-state-owned enterprises, especially for smaller and more innovative enterprises.[67]\n Algorithmic trading involves the use of AI systems to make trading decisions at speeds orders of magnitude greater than any human is capable of, making millions of trades in a day without human intervention. Such high-frequency trading represents a fast-growing sector. Many banks, funds, and proprietary trading firms now have entire portfolios that are AI-managed. Automated trading systems are typically used by large institutional investors but include smaller firms trading with their own AI systems.[68]\n Large financial institutions use AI to assist with their investment practices. BlackRock's AI engine, Aladdin, is used both within the company and by clients to help with investment decisions. Its functions include the use of natural language processing to analyze text such as news, broker reports, and social media feeds. It then gauges the sentiment on the companies mentioned and assigns a score. Banks such as UBS and Deutsche Bank use SQREEM (Sequential Quantum Reduction and Extraction Model) to mine data to develop consumer profiles and match them with wealth management products.[69]\n Online lender Upstart uses machine learning for underwriting.[70]\n ZestFinance's Zest Automated Machine Learning (ZAML) platform is used for credit underwriting. This platform uses machine learning to analyze data including purchase transactions and how a customer fills out a form to score borrowers. The platform is particularly useful to assign credit scores to those with limited credit histories.[71]\n AI makes continuous auditing possible. Potential benefits include reducing audit risk, increasing the level of assurance, and reducing audit duration.[72][quantify]\n Continuous auditing with AI allows real-time monitoring and reporting of financial activities and provides businesses with timely insights that can lead to quick decision making.[73]\n AI software, such as LaundroGraph which uses contemporary suboptimal datasets, could be used for anti-money laundering (AML).[74][75]\n In the 1980s, AI started to become prominent in finance as expert systems were commercialized. For example, Dupont created 100 expert systems, which helped them to save almost $10 million per year.[76] One of the first systems was the Pro-trader expert system that predicted the 87-point drop in the Dow Jones Industrial Average in 1986. \"The major junctions of the system were to monitor premiums in the market, determine the optimum investment strategy, execute transactions when appropriate and modify the knowledge base through a learning mechanism.\"[77]\n One of the first expert systems to help with financial plans was PlanPowerm and Client Profiling System, created by Applied Expert Systems (APEX). It was launched in 1986. It helped create personal financial plans for people.[78]\n In the 1990s AI was applied to fraud detection. In 1993 FinCEN Artificial Intelligence System (FAIS) launched. It was able to review over 200,000 transactions per week and over two years it helped identify 400 potential cases of money laundering equal to $1 billion.[79] These expert systems were later replaced by machine learning systems.[80]\n AI can enhance entrepreneurial activity and AI is one of the most dynamic areas for start-ups, with significant venture capital flowing into AI.[81]\n AI facial recognition systems are used for mass surveillance, notably in China.[82][83] In 2019, Bengaluru, India deployed AI-managed traffic signals. This system uses cameras to monitor traffic density and adjust signal timing based on the interval needed to clear traffic.[84]\n Various countries are deploying AI military applications.[85] The main applications enhance command and control, communications, sensors, integration and interoperability.[86] Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles.[85] AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams.[86]\n AI has been used in military operations in Iraq, Syria, Israel and Ukraine.[85][87][88][89]\n AI in healthcare is often used for classification, to evaluate a CT scan or electrocardiogram or to identify high-risk patients for population health. AI is helping with the high-cost problem of dosing. One study suggested that AI could save $16 billion. In 2016, a study reported that an AI-derived formula derived the proper dose of immunosuppressant drugs to give to transplant patients.[90] Current research has indicated that non-cardiac vascular illnesses are also being treated with artificial intelligence (AI). For certain disorders, AI algorithms can aid in diagnosis, recommended treatments, outcome prediction, and patient progress tracking. As AI technology advances, it is anticipated that it will become more significant in the healthcare industry.[91]\n The early detection of diseases like cancer is made possible by AI algorithms, which diagnose diseases by analyzing complex sets of medical data. For example, the IBM Watson system might be used to comb through massive data such as medical records and clinical trials to help diagnose a problem.[92] Microsoft's AI project Hanover helps doctors choose cancer treatments from among the more than 800 medicines and vaccines.[93][94] Its goal is to memorize all the relevant papers to predict which (combinations of) drugs will be most effective for each patient. Myeloid leukemia is one target. Another study reported on an AI that was as good as doctors in identifying skin cancers.[95] Another project monitors multiple high-risk patients by asking each patient questions based on data acquired from doctor/patient interactions.[96] In one study done with transfer learning, an AI diagnosed eye conditions similar to an ophthalmologist and recommended treatment referrals.[97]\n Another study demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel judged better than a surgeon.[98]\n Artificial neural networks are used as clinical decision support systems for medical diagnosis,[99] such as in concept processing technology in EMR software.\n Other healthcare tasks thought suitable for an AI that are in development include:\n AI-enabled chatbots decrease the need for humans to perform basic call center tasks.[115]\n Machine learning in sentiment analysis can spot fatigue in order to prevent overwork.[115] Similarly, decision support systems can prevent industrial disasters and make disaster response more efficient.[116] For manual workers in material handling, predictive analytics may be used to reduce musculoskeletal injury.[117] Data collected from wearable sensors can improve workplace health surveillance, risk assessment, and research.[116][how?]\n AI can auto-code workers' compensation claims.[118][119] AI-enabled virtual reality systems can enhance safety training for hazard recognition.[116] AI can more efficiently detect accident near misses, which are important in reducing accident rates, but are often underreported.[120]\n AlphaFold 2 can determine the 3D structure of a (folded) protein in hours rather than the months required by earlier automated approaches and was used to provide the likely structures of all proteins in the human body and essentially all proteins known to science (more than 200 million).[121][122][123][124]\n Machine learning has been used for drug design.[125] It has also been used for predicting molecular properties and exploring large chemical/reaction spaces.[126] Computer-planned syntheses via computational reaction networks, described as a platform that combines \"computational synthesis with AI algorithms to predict molecular properties\",[127] have been used to explore the origins of life on Earth,[128] drug-syntheses and developing routes for recycling 200 industrial waste chemicals into important drugs and agrochemicals (chemical synthesis design).[129] There is research about which types of computer-aided chemistry would benefit from machine learning.[130] It can also be used for \"drug discovery and development, drug repurposing, improving pharmaceutical productivity, and clinical trials\".[131] It has been used for the design of proteins with prespecified functional sites.[132][133]\n It has been used with databases for the development of a 46-day process to design, synthesize and test a drug which inhibits enzymes of a particular gene, DDR1. DDR1 is involved in cancers and fibrosis which is one reason for the high-quality datasets that enabled these results.[134]\n There are various types of applications for machine learning in decoding human biology, such as helping to map gene expression patterns to functional activation patterns[135] or identifying functional DNA motifs.[136] It is widely used in genetic research.[137]\n There also is some use of machine learning in synthetic biology,[138][139] disease biology,[139] nanotechnology (e.g. nanostructured materials and bionanotechnology),[140][141] and materials science.[142][143][144]\n There are also prototype robot scientists, including robot-embodied ones like the two Robot Scientists, which show a form of \"machine learning\" not commonly associated with the term.[145][146]\n Similarly, there is research and development of biological \"wetware computers\" that can learn (e.g. for use as biosensors) and/or implantation into an organism's body (e.g. for use to control prosthetics).[147][148][149] Polymer-based artificial neurons operate directly in biological environments and define biohybrid neurons made of artificial and living components.[150][151]\n Moreover, if whole brain emulation is possible via both scanning and replicating the, at least, bio-chemical brain – as premised in the form of digital replication in The Age of Em, possibly using physical neural networks – that may have applications as or more extensive than e.g. valued human activities and may imply that society would face substantial moral choices, societal risks and ethical problems[152][153] such as whether (and how) such are built, sent through space and used compared to potentially competing e.g. potentially more synthetic and/or less human and/or non/less-sentient types of artificial/semi-artificial intelligence.[additional citation(s) needed] An alternative or additive approach to scanning are types of reverse engineering of the brain.[154][155]\n A subcategory of artificial intelligence is embodied,[156][157] some of which are mobile robotic systems that each consist of one or multiple robots that are able to learn in the physical world.\n However, biological computers, even if both highly artificial and intelligent, are typically distinguished from synthetic, often silicon-based, computers – they could however be combined or used for the design of either. Moreover, many tasks may be carried out inadequately by artificial intelligence even if its algorithms were transparent, understood, bias-free, apparently effective, and goal-aligned and its trained data sufficiently large and cleansed – such as in cases were the underlying or available metrics, values or data are inappropriate. Computer-aided is a phrase used to describe human activities that make use of computing as tool in more comprehensive activities and systems such as AI for narrow tasks or making use of such without substantially relying on its results (see also: human-in-the-loop).[citation needed] A study described the biological as a limitation of AI with \"as long as the biological system cannot be understood, formalized, and imitated, we will not be able to develop technologies that can mimic it\" and that if it was understood this does not mean there being \"a technological solution to imitate natural intelligence\".[158] Technologies that integrate biology and are often AI-based include biorobotics.\n Artificial intelligence is used in astronomy to analyze increasing amounts of available data[159][160] and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights\" for example for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy.[161] It could also be used for activities in space such as space exploration, including analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance,[162] and more autonomous operation.[163][164][165][160]\n In the search for extraterrestrial intelligence (SETI), machine learning has been used in attempts to identify artificially generated electromagnetic waves in available data[166][167] – such as real-time observations[168] – and other technosignatures, e.g. via anomaly detection.[169] In ufology, the SkyCAM-5 project headed by Prof. Hakan Kayal[170] and the Galileo Project headed by Avi Loeb use machine learning to attempt to detect and classify types of UFOs.[171][172][173][174][175] The Galileo Project also seeks to detect two further types of potential extraterrestrial technological signatures with the use of AI: 'Oumuamua-like interstellar objects, and non-manmade artificial satellites.[176][177]\n Machine learning can also be used to produce datasets of spectral signatures of molecules that may be involved in the atmospheric production or consumption of particular chemicals – such as phosphine possibly detected on Venus – which could prevent miss assignments and, if accuracy is improved, be used in future detections and identifications of molecules on other planets.[178]\n In April 2024, the Scientific Advice Mechanism to the European Commission published advice[179] including a comprehensive evidence review of the opportunities and challenges posed by artificial intelligence in scientific research.\n As benefits, the evidence review[180] highlighted:\n As challenges:\n Machine learning can help to restore and attribute ancient texts.[181] It can help to index texts for example to enable better and easier searching[182] and classification of fragments.[183]\n \nArtificial intelligence can also be used to investigate genomes to uncover genetic history, such as interbreeding between archaic and modern humans by which for example the past existence of a ghost population, not Neanderthal or Denisovan, was inferred.[184]  \nIt can also be used for \"non-invasive and non-destructive access to internal structures of archaeological remains\".[185]  A deep learning system was reported to learn intuitive physics from visual data (of virtual 3D environments) based on an unpublished approach inspired by studies of visual cognition in infants.[186][187] Other researchers have developed a machine learning algorithm that could discover sets of basic variables of various physical systems and predict the systems' future dynamics from video recordings of their behavior.[188][189] In the future, it may be possible that such can be used to automate the discovery of physical laws of complex systems.[188]\n AI could be used for materials optimization and discovery such as the discovery of stable materials and the prediction of their crystal structure.[190][191][192]\n In November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.[193][194][195]\n Machine learning is used in diverse types of reverse engineering. For example, machine learning has been used to reverse engineer a composite material part, enabling unauthorized production of high quality parts,[196] and for quickly understanding the behavior of malware.[197][198][199] It can be used to reverse engineer artificial intelligence models.[200] It can also design components by engaging in a type of reverse engineering of not-yet existent virtual components such as inverse molecular design for particular desired functionality[201] or protein design for prespecified functional sites.[132][133] Biological network reverse engineering could model interactions in a human understandable way, e.g. bas on time series data of gene expression levels.[202]\n AI is a mainstay of law-related professions. Algorithms and machine learning do some tasks previously done by entry-level lawyers.[203] While its use is common, it is not expected to replace most work done by lawyers in the near future.[204]\n The electronic discovery industry uses machine learning to reduce manual searching.[205]\n Law enforcement has begun using facial recognition systems (FRS) to identify suspects from visual data. FRS results have proven to be more accurate when compared to eyewitness results. Furthermore, FRS has shown to have much a better ability to identify individuals when video clarity and visibility are low in comparison to human participants. [206]\n COMPAS is a commercial system used by U.S. courts to assess the likelihood of recidivism.[207]\n One concern relates to algorithmic bias, AI programs may become biased after processing data that exhibits bias.[208] ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than that of white defendants.[207]\n In 2019, the city of Hangzhou, China established a pilot program artificial intelligence-based Internet Court to adjudicate disputes related to ecommerce and internet-related intellectual property claims.[209]: 124  Parties appear before the court via videoconference and AI evaluates the evidence presented and applies relevant legal standards.[209]: 124 \n Another application of AI is in human resources. AI can screen resumes and rank candidates based on their qualifications, predict candidate success in given roles, and automate repetitive communication tasks via chatbots.[210]\n AI has simplified the recruiting /job search process for both recruiters and job seekers. According to Raj Mukherjee from Indeed, 65% of job searchers search again within 91 days after hire. An AI-powered engine streamlines the complexity of job hunting by assessing information on job skills, salaries, and user tendencies, matching job seekers to the most relevant positions. Machine intelligence calculates appropriate wages and highlights resume information for recruiters using NLP, which extracts relevant words and phrases from text. Another application is an AI resume builder that compiles a CV in 5 minutes.[211] Chatbots assist website visitors and refine workflows.\n AI underlies avatars (automated online assistants) on web pages.[212] It can reduce operation and training costs.[212] Pypestream automated customer service for its mobile application to streamline communication with customers.[213]\n A Google app analyzes language and converts speech into text. The platform can identify angry customers through their language and respond appropriately.[214] Amazon uses a chatbot for customer service that can perform tasks like checking the status of an order, cancelling orders, offering refunds and connecting the customer with a human representative.[215] Generative AI (GenAI), such as ChatGPT, is increasingly used in business to automate tasks and enhance decision-making.[216]\n In the hospitality industry, AI is used to reduce repetitive tasks, analyze trends, interact with guests, and predict customer needs.[217] AI hotel services come in the form of a chatbot,[218] application, virtual voice assistant and service robots.\n AI applications analyze media content such as movies, TV programs, advertisement videos or user-generated content. The solutions often involve computer vision.\n Typical scenarios include the analysis of images using object recognition or face recognition techniques, or the analysis of video for scene recognizing scenes, objects or faces. AI-based media analysis can facilitate media search, the creation of descriptive keywords for content, content policy monitoring (such as verifying the suitability of content for a particular TV viewing time), speech to text for archival or other purposes, and the detection of logos, products or celebrity faces for ad placement.\n Deep-fakes can be used for comedic purposes but are better known for fake news and hoaxes.\n Deepfakes can portray individuals in harmful or compromising situations, causing significant reputational damage and emotional distress, especially when the content is defamatory or violates personal ethics. While defamation and false light laws offer some recourse, their focus on false statements rather than fabricated images or videos often leaves victims with limited legal protection and a challenging burden of proof.[232]\n In January 2016,[233] the Horizon 2020 program financed the InVID Project[234][235] to help journalists and researchers detect fake documents, made available as browser plugins.[236][237]\n In June 2016, the visual computing group of the Technical University of Munich and from Stanford University developed Face2Face,[238] a program that animates photographs of faces, mimicking the facial expressions of another person. The technology has been demonstrated animating the faces of people including Barack Obama and Vladimir Putin. Other methods have been demonstrated based on deep neural networks, from which the name deep fake was taken.\n In September 2018, U.S. Senator Mark Warner proposed to penalize social media companies that allow sharing of deep-fake documents on their platforms.[239]\n In 2018, Darius Afchar and Vincent Nozick found a way to detect faked content by analyzing the mesoscopic properties of video frames.[240] DARPA gave 68 million dollars to work on deep-fake detection.[240]\n Audio deepfakes[241][242] and AI software capable of detecting deep-fakes and cloning human voices have been developed.[243][244]\n Respeecher is a program that enables one person to speak with the voice of another.\n AI algorithms have been used to detect deepfake videos.[245][246]\n Artificial intelligence is also starting to be used in video production, with tools and software being developed that utilize generative AI in order to create new video, or alter existing video. Some of the major tools that are being used in these processes currently are DALL-E, Mid-journey, and Runway.[247]  Way mark Studios utilized the tools offered by both DALL-E and Mid-journey to create a fully AI generated film called The Frost in the summer of 2023.[247] Way mark Studios is experimenting with using these AI tools to generate advertisements and commercials for companies in mere seconds.[247]  Yves Bergquist, a director of the AI & Neuroscience in Media Project at USC's Entertainment Technology Center, says post production crews in Hollywood are already using generative AI, and predicts that in the future more companies will embrace this new technology.[248]\n AI has been used to compose music of various genres.\n David Cope created an AI called Emily Howell that managed to become well known in the field of algorithmic computer music.[249] The algorithm behind Emily Howell is registered as a US patent.[250]\n In 2012, AI Iamus created the first complete classical album.[251]\n AIVA (Artificial Intelligence Virtual Artist), composes symphonic music, mainly classical music for film scores.[252] It achieved a world first by becoming the first virtual composer to be recognized by a musical professional association.[253]\n Melomics creates computer-generated music for stress and pain relief.[254]\n At Sony CSL Research Laboratory, the Flow Machines software creates pop songs by learning music styles from a huge database of songs. It can compose in multiple styles.\n The Watson Beat uses reinforcement learning and deep belief networks to compose music on a simple seed input melody and a select style. The software was open sourced[255] and musicians such as Taryn Southern[256] collaborated with the project to create music.\n South Korean singer, Hayeon's, debut song, \"Eyes on You\" was composed using AI which was supervised by real composers, including NUVO.[257]\n Narrative Science sells computer-generated news and reports. It summarizes sporting events based on statistical data from the game. It also creates financial reports and real estate analyses.[258] Automated Insights generates personalized recaps and previews for Yahoo Sports Fantasy Football.[259]\n Yseop, uses AI to turn structured data into natural language comments and recommendations. Yseop writes financial reports, executive summaries, personalized sales or marketing documents and more in multiple languages, including English, Spanish, French, and German.[260]\n TALESPIN made up stories similar to the fables of Aesop. The program started with a set of characters who wanted to achieve certain goals. The story narrated their attempts to satisfy these goals.[citation needed] Mark Riedl and Vadim Bulitko asserted that the essence of storytelling was experience management, or \"how to balance the need for a coherent story progression with user agency, which is often at odds\".[261]\n While AI storytelling focuses on story generation (character and plot), story communication also received attention. In 2002, researchers developed an architectural framework for narrative prose generation. They faithfully reproduced text variety and complexity on stories such as Little Red Riding Hood.[262] In 2016, a Japanese AI co-wrote a short story and almost won a literary prize.[263]\n South Korean company Hanteo Global uses a journalism bot to write articles.[264]\n Literary authors are also exploring uses of AI. An example is David Jhave Johnston's work ReRites (2017-2019), where the poet created a daily rite of editing the poetic output of a neural network to create a series of performances and publications.\n In 2010, artificial intelligence used baseball statistics to automatically generate news articles. This was launched by The Big Ten Network using software from Narrative Science.[265]\n After being unable to cover every Minor League Baseball game with a large team, Associated Press collaborated with Automated Insights in 2016 to create game recaps that were automated by artificial intelligence.[266]\n UOL in Brazil expanded the use of AI in its writing. Rather than just generating news stories, they programmed the AI to include commonly searched words on Google.[266]\n El Pais, a Spanish news site that covers many things including sports, allows users to make comments on each news article. They use the Perspective API to moderate these comments and if the software deems a comment to contain toxic language, the commenter must modify it in order to publish it.[266]\n A local Dutch media group used AI to create automatic coverage of amateur soccer, set to cover 60,000 games in just a single season. NDC partnered with United Robots to create this algorithm and cover what would have never been possible before without an extremely large team.[266]\n Lede AI has been used in 2023 to take scores from high school football games to generate stories automatically for the local newspaper. This was met with significant criticism from readers for the very robotic diction that was published. With some descriptions of games being a \"close encounter of the athletic kind,\" readers were not pleased and let the publishing company, Gannett, know on social media. Gannett has since halted their used of Lede AI until they come up with a solution for what they call an experiment.[267]\n  Millions of its articles have been edited by bots[271] which however are usually not artificial intelligence software. Many AI platforms use Wikipedia data,[272] mainly for training machine learning applications. There is research and development of various artificial intelligence applications for Wikipedia such as for identifying outdated sentences,[273] detecting covert vandalism[274] or recommending articles and tasks to new editors.\n Machine translation (see above) has also be used for translating Wikipedia articles and could play a larger role in creating, updating, expanding, and generally improving articles in the future. A content translation tool allows editors of some Wikipedias to more easily translate articles across several select languages.[275][276]\n In video games, AI is routinely used to generate behavior in non-player characters (NPCs). In addition, AI is used for pathfinding. Some researchers consider NPC AI in games to be a \"solved problem\" for most production tasks.[who?] Games with less typical AI include the AI director of Left 4 Dead (2008) and the neuroevolutionary training of platoons in Supreme Commander 2 (2010).[277][278] AI is also used in Alien Isolation (2014) as a way to control the actions the Alien will perform next.[279]\n Kinect, which provides a 3D body–motion interface for the Xbox 360 and the Xbox One, uses algorithms that emerged from AI research.[280][which?]\n AI has been used to produce visual art. The first AI art program, called AARON, was developed by Harold Cohen in 1968[281] with the goal of being able to code the act of drawing. It started by creating simple black and white drawings, and later to painting using special brushes and dyes that were chosen by the program itself without mediation from Cohen.[282]\n AI platforms such as \"DALL-E\",[283] Stable Diffusion,[283] Imagen,[284] and Midjourney[285] have been used for generating visual images from inputs such as text or other images.[286] Some AI tools allow users to input images and output changed versions of that image, such as to display an object or product in different environments. AI image models can also attempt to replicate the specific styles of artists, and can add visual complexity to rough sketches.\n Since their design in 2014, generative adversarial networks (GANs) have been used by AI artists. GAN computer programming, generates technical images through machine learning frameworks that surpass the need for human operators.[281] Examples of GAN programs that generate art include Artbreeder and DeepDream.\n In addition to the creation of original art, research methods that utilize AI have been generated to quantitatively analyze digital art collections. Although the main goal of the large-scale digitization of artwork in the past few decades was to allow for accessibility and exploration of these collections, the use of AI in analyzing them has brought about new research perspectives.[287]\nTwo computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art.[288] While distant viewing includes the analysis of large collections, close reading involves one piece of artwork.\n AI has been in use since the early 2000s, most notably by a system designed by Pixar called \"Genesis\".[289] It was designed to learn algorithms and create 3D models for its characters and props. Notable movies that used this technology included Up and The Good Dinosaur.[290] AI has been used less ceremoniously in recent years. In 2023, it was revealed Netflix of Japan was using AI to generate background images for their upcoming show to be met with backlash online.[291]  In recent years, motion capture became an easily accessible form of AI animation. For example, Move AI is a program built to capture any human movement and reanimate it in its animation program using learning AI.[292]\n Power electronics converters are used in renewable energy, energy storage, electric vehicles and high-voltage direct current transmission. These converters are failure-prone, which can interrupt service and require costly maintenance or catastrophic consequences in mission critical applications.[citation needed] AI can guide the design process for reliable power electronics converters, by calculating exact design parameters that ensure the required lifetime.[293]\n The U.S. Department of Energy underscores AI's pivotal role in realizing national climate goals. With AI, the ambitious target of achieving net-zero greenhouse gas emissions across the economy becomes feasible. AI also helps make room for wind and solar on the grid by avoiding congestion and increasing grid reliability. [294]\n Machine learning can be used for energy consumption prediction and scheduling, e.g. to help with renewable energy intermittency management (see also: smart grid and climate change mitigation in the power grid).[295][296][297][298][125]\n Many telecommunications companies make use of heuristic search to manage their workforces. For example, BT Group deployed heuristic search[299] in an application that schedules 20,000 engineers. Machine learning is also used for speech recognition (SR), including of voice-controlled devices, and SR-related transcription, including of videos.[300][301]\n Artificial intelligence has been combined with digital spectrometry by IdeaCuria Inc.,[302][303] enable applications such as at-home water quality monitoring.\n In the 1990s, early artificial intelligence tools controlled Tamagotchis and Giga Pets, the Internet, and the first widely released robot, Furby. Aibo was a domestic robot in the form of a robotic dog with intelligent features and autonomy.\n Mattel created an assortment of AI-enabled toys that \"understand\" conversations, give intelligent responses, and learn.[304]\n Oil and gas companies have used artificial intelligence tools to automate functions, foresee equipment issues, and increase oil and gas output.[305][306]\n AI in transport is expected to provide safe, efficient, and reliable transportation while minimizing the impact on the environment and communities. The major development challenge is the complexity of transportation systems that involves independent components and parties, with potentially conflicting objectives.[307]\n AI-based fuzzy logic controllers operate gearboxes. For example, the 2006 Audi TT, VW Touareg [citation needed] and VW Caravell feature the DSP transmission. A number of Škoda variants (Škoda Fabia) include a fuzzy logic-based controller. Cars have AI-based driver-assist features such as self-parking and adaptive cruise control.\n There are also prototypes of autonomous automotive public transport vehicles such as electric mini-buses[308][309][310][311] as well as autonomous rail transport in operation.[312][313][314]\n There also are prototypes of autonomous delivery vehicles, sometimes including delivery robots.[315][316][317][318][319][320][321]\n Transportation's complexity means that in most cases training an AI in a real-world driving environment is impractical. Simulator-based testing can reduce the risks of on-road training.[322]\n AI underpins self-driving vehicles. Companies involved with AI include Tesla, Waymo, and General Motors. AI-based systems control functions such as braking, lane changing, collision prevention, navigation and mapping.[323]\n Autonomous trucks are in the testing phase. The UK government passed legislation to begin testing of autonomous truck platoons in 2018.[324] A group of autonomous trucks follow closely behind each other. German corporation Daimler is testing its Freightliner Inspiration.[325]\n Autonomous vehicles require accurate maps to be able to navigate between destinations.[326] Some autonomous vehicles do not allow human drivers (they have no steering wheels or pedals).[327]\n AI has been used to optimize traffic management, which reduces wait times, energy use, and emissions by as much as 25 percent.[328]\n Smart traffic lights have been developed at Carnegie Mellon since 2009.  Professor Stephen Smith has started a company since then Surtrac that has installed smart traffic control systems in 22 cities.  It costs about $20,000 per intersection to install. Drive time has been reduced by 25% and traffic jam waiting time has been reduced by 40% at the intersections it has been installed.[329]\n The Royal Australian Air Force (RAAF) Air Operations Division (AOD) uses AI for expert systems. AIs operate as surrogate operators for combat and training simulators, mission management aids, support systems for tactical decision making, and post processing of the simulator data into symbolic summaries.[330]\n Aircraft simulators use AI for training aviators. Flight conditions can be simulated that allow pilots to make mistakes without risking themselves or expensive aircraft. Air combat can also be simulated.\n AI can also be used to operate planes analogously to their control of ground vehicles. Autonomous drones can fly independently or in swarms.[331]\n AOD uses the Interactive Fault Diagnosis and Isolation System, or IFDIS, which is a rule-based expert system using information from TF-30 documents and expert advice from mechanics that work on the TF-30. This system was designed to be used for the development of the TF-30 for the F-111C. The system replaced specialized workers. The system allowed regular workers to communicate with the system and avoid mistakes, miscalculations, or having to speak to one of the specialized workers.\n Speech recognition allows traffic controllers to give verbal directions to drones.\n Artificial intelligence supported design of aircraft,[332] or AIDA, is used to help designers in the process of creating conceptual designs of aircraft. This program allows the designers to focus more on the design itself and less on the design process. The software also allows the user to focus less on the software tools. The AIDA uses rule-based systems to compute its data. This is a diagram of the arrangement of the AIDA modules. Although simple, the program is proving effective.\n In 2003 a Dryden Flight Research Center project created software that could enable a damaged aircraft to continue flight until a safe landing can be achieved.[333] The software compensated for damaged components by relying on the remaining undamaged components.[334]\n The 2016 Intelligent Autopilot System combined apprenticeship learning and behavioral cloning whereby the autopilot observed low-level actions required to maneuver the airplane and high-level strategy used to apply those actions.[335]\n Neural networks are used by situational awareness systems in ships and boats.[336] There also are autonomous boats.\n Autonomous ships that monitor the ocean, AI-driven satellite data analysis, passive acoustics[337] or remote sensing and other applications of environmental monitoring make use of machine learning.[338][339][340][165]\n For example, \"Global Plastic Watch\" is an AI-based satellite monitoring-platform for analysis/tracking of plastic waste sites to help prevention of plastic pollution – primarily ocean pollution – by helping identify who and where mismanages plastic waste, dumping it into oceans.[341][342]\n Machine learning can be used to spot early-warning signs of disasters and environmental issues, possibly including natural pandemics,[343][344] earthquakes,[345][346][347] landslides,[348] heavy rainfall,[349] long-term water supply vulnerability,[350] tipping-points of ecosystem collapse,[351] cyanobacterial bloom outbreaks,[352] and droughts.[353][354][355]\n AI can be used for real-time code completion, chat, and automated test generation. These tools are typically integrated with editors and IDEs as plugins. They differ in functionality, quality, speed, and approach to privacy.[356] Code suggestions could be incorrect, and should be carefully reviewed by software developers before accepted.\n GitHub Copilot is an artificial intelligence model developed by GitHub and OpenAI that is able to autocomplete code in multiple programming languages.[357] Price for individuals: $10/mo or $100/yr, with one free month trial.\n Tabnine was created by Jacob Jackson and was originally owned by Tabnine company. In late 2019, Tabnine was acquired by Codota.[358] Tabnine tool is available as plugin to most popular IDEs. It offers multiple pricing options, including limited \"starter\" free version.[359]\n CodiumAI by CodiumAI, small startup in Tel Aviv, offers automated test creation. Currently supports Python, JS, and TS.[360]\n Ghostwriter by Replit offers code completion and chat.[361] They have multiple pricing plans, including a free one and a \"Hacker\" plan for $7/month.\n CodeWhisperer by Amazon collects individual users' content, including files open in the IDE. They claim to focus on security both during transmission and when storing.[362] Individual plan is free, professional plan is $19/user/month.\n Other tools: SourceGraph Cody, CodeCompleteFauxPilot, Tabby[356]\n AI can be used to create other AIs. For example, around November 2017, Google's AutoML project to evolve new neural net topologies created NASNet, a system optimized for ImageNet and POCO F1. NASNet's performance exceeded all previously published performance on ImageNet.[363]\n Machine learning has been used for noise-cancelling in quantum technology,[364] including quantum sensors.[365] Moreover, there is substantial research and development of using quantum computers with machine learning algorithms. For example, there is a prototype, photonic, quantum memristive device for neuromorphic (quantum-)computers (NC)/artificial neural networks and NC-using quantum materials with some variety of potential neuromorphic computing-related applications,[366][367] and quantum machine learning is a field with some variety of applications under development. AI could be used for quantum simulators which may have the application of solving physics and chemistry[368][369] problems as well as for quantum annealers for training of neural networks for AI applications.[370] There may also be some usefulness in chemistry, e.g. for drug discovery, and in materials science, e.g. for materials optimization/discovery (with possible relevance to quantum materials manufacturing[191][192]).[371][372][373][better source needed]\n AI researchers have created many tools to solve the most difficult problems in computer science. Many of their inventions have been adopted by mainstream computer science and are no longer considered AI. All of the following were originally developed in AI laboratories:[374]\n An optical character reader is used in the extraction of data in business documents like invoices and receipts. It can also be used in business contract documents e.g. employment agreements to extract critical data like employment terms, delivery terms, termination clauses, etc.[375]\n Artificial intelligence in architecture describes the use of artificial intelligence in automation, design and planning in the architectural process or in assisting human skills in the field of architecture. Artificial Intelligence is thought to potentially lead to and ensue major changes in architecture.[376][377][378]\n AI in architecture has created a way for architects to create things beyond human understanding. AI implementation of machine learning text-to-render technologies, like DALL-E and stable Diffusion, gives power to visualization complex.[379]\n AI allows designers to demonstrate their creativity and even invent new ideas while designing. In future, AI will not replace architects; instead, it will improve the speed of translating ideas sketching.[379]\n"
    },
    {
        "title": "Generative artificial intelligence",
        "url": "https://en.wikipedia.org/wiki/Generative_artificial_intelligence",
        "content": "\n Generative artificial intelligence (generative AI, GenAI,[1] or GAI) is a subset of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data.[2][3][4] These models learn the underlying patterns and structures of their training data and use them to produce new data[5][6] based on the input, which often comes in the form of natural language prompts.[7][8]\n Improvements in transformer-based deep neural networks, particularly large language models (LLMs), enabled an AI boom of generative AI systems in the early 2020s. These include chatbots such as ChatGPT, Copilot, Gemini, and LLaMA; text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video AI generators such as Sora.[9][10][11][12] Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.[7][13][14]\n Generative AI has uses across a wide range of industries, including software development, healthcare, finance, entertainment, customer service,[15] sales and marketing,[16] art, writing,[17] fashion,[18] and product design.[19] However, concerns have been raised about the potential misuse of generative AI such as cybercrime, the use of fake news or deepfakes to deceive or manipulate people, and the mass replacement of human jobs.[20][21] Intellectual property law concerns also exist around generative models that are trained on and emulate copyrighted works of art.[22]\n Since its inception, researchers in the field have raised philosophical and ethical arguments about the nature of the human mind and the consequences of creating artificial beings with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity.[23] The concept of automated art dates back at least to the automata of ancient Greek civilization, where inventors such as Daedalus and Hero of Alexandria were described as having designed machines capable of writing text, generating sounds, and playing music.[24][25] The tradition of creative automations has flourished throughout history, exemplified by Maillardet's automaton created in the early 1800s.[26] Markov chains have long been used to model natural languages since their development by Russian mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic in 1906,[27][28] and analyzed the pattern of vowels and consonants in the novel Eugeny Onegin using Markov chains. Once a Markov chain is learned on a text corpus, it can then be used as a probabilistic text generator.[29][30]\n The academic discipline of artificial intelligence was established at a research workshop held at Dartmouth College in 1956 and has experienced several waves of advancement and optimism in the decades since.[31] Artificial Intelligence research began in the 1950s with works like Computing Machinery and Intelligence (1950) and the 1956 Dartmouth Summer Research Project on AI. Since the 1950s, artists and researchers have used artificial intelligence to create artistic works. By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by AARON, the computer program Cohen created to generate paintings.[32]\n The terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal.[33][34] Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use,[35] process plans for manufacturing[33] and decision plans such as in prototype autonomous spacecraft.[36]\n Since its inception, the field of machine learning used both discriminative models and generative models, to model and predict data. Beginning in the late 2000s, the emergence of deep learning drove progress and research in image classification, speech recognition, natural language processing and other tasks. Neural networks in this era were typically trained as discriminative models, due to the difficulty of generative modeling.[37]\n In 2014, advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative models, as opposed to discriminative ones, for complex data such as images. These deep generative models were the first to output not only class labels for images but also entire images.\n In 2017, the Transformer network enabled advancements in generative models compared to older Long-Short Term Memory models,[38] leading to the first generative pre-trained transformer (GPT), known as GPT-1, in 2018.[39] This was followed in 2019 by GPT-2 which demonstrated the ability to generalize unsupervised to many different tasks as a Foundation model.[40]\n The new generative models introduced during this period allowed for large neural networks to be trained using unsupervised learning or semi-supervised learning, rather than the supervised learning typical of discriminative models. Unsupervised learning removed the need for humans to manually label data, allowing for larger networks to be trained.[41]\n In March 2020, 15.ai, created by an anonymous MIT researcher, was a free web application that could generate convincing character voices using minimal training data.[42] The platform is credited as the first mainstream service to popularize AI voice cloning (audio deepfakes) in memes and content creation, influencing subsequent developments in voice AI technology.[43][44]\n In 2021, the emergence of DALL-E, a transformer-based pixel generative model, marked an advance in AI-generated imagery.[45] This was followed by the releases of Midjourney and Stable Diffusion in 2022, which further democratized access to high-quality artificial intelligence art creation from natural language prompts.[46] These systems demonstrated unprecedented capabilities in generating photorealistic images, artwork, and designs based on text descriptions, leading to widespread adoption among artists, designers, and the general public.\n In late 2022, the public release of ChatGPT revolutionized the accessibility and application of generative AI for general-purpose text-based tasks.[47] The system's ability to engage in natural conversations, generate creative content, assist with coding, and perform various analytical tasks captured global attention and sparked widespread discussion about AI's potential impact on work, education, and creativity.[48]\n In March 2023, GPT-4's release represented another jump in generative AI capabilities. A team from Microsoft Research controversially argued that it \"could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\"[49] However, this assessment was contested by other scholars who maintained that generative AI remained \"still far from reaching the benchmark of 'general human intelligence'\" as of 2023.[50] Later in 2023, Meta released ImageBind, an AI model combining multiple modalities including text, images, video, thermal data, 3D data, audio, and motion, paving the way for more immersive generative AI applications.[51]\n In December 2023, Google unveiled Gemini, a multimodal AI model available in four versions: Ultra, Pro, Flash, and Nano.[52] The company integrated Gemini Pro into its Bard chatbot and announced plans for \"Bard Advanced\" powered by the larger Gemini Ultra model.[53] In February 2024, Google unified Bard and Duet AI under the Gemini brand, launching a mobile app on Android and integrating the service into the Google app on iOS.[54]\n In March 2024, Anthropic released the Claude 3 family of large language models, including Claude 3 Haiku, Sonnet, and Opus.[55] The models demonstrated significant improvements in capabilities across various benchmarks, with Claude 3 Opus notably outperforming leading models from OpenAI and Google.[56] In June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated improved performance compared to the larger Claude 3 Opus, particularly in areas such as coding, multistep workflows, and image analysis.[57]\n According to a survey by SAS and Coleman Parkes Research, China has emerged as a global leader in generative AI adoption, with 83% of Chinese respondents using the technology, exceeding both the global average of 54% and the U.S. rate of 65%. This leadership is further evidenced by China's intellectual property developments in the field, with a UN report revealing that Chinese entities filed over 38,000 generative AI patents from 2014 to 2023, substantially surpassing the United States in patent applications.[58]\n A generative AI system is constructed by applying unsupervised machine learning (invoking  for instance neural network architectures such as generative adversarial networks (GANs), variation autoencoders (VAEs), transformers, or self-supervised machine learning trained on a dataset. The capabilities of a generative AI system depend on the modality or type of the data set used. Generative AI can be either unimodal or multimodal; unimodal systems take only one type of input, whereas multimodal systems can take more than one type of input.[59] For example, one version of OpenAI's GPT-4 accepts both text and image inputs.[60]\n Generative AI systems trained on words or word tokens include GPT-3, GPT-4, GPT-4o, LaMDA, LLaMA, BLOOM, Gemini and others (see List of large language models). They are capable of natural language processing, machine translation, and natural language generation and can be used as foundation models for other tasks.[62] Data sets include BookCorpus, Wikipedia, and others (see List of text corpora).\n In addition to natural language text, large language models can be trained on programming language text, allowing them to generate source code for new computer programs.[63] Examples include OpenAI Codex and the VS Code fork Cursor.[64]\n Producing high-quality visual art is a prominent application of generative AI.[65] Generative AI systems trained on sets of images with text captions include Imagen, DALL-E, Midjourney, Adobe Firefly, FLUX.1, Stable Diffusion and others (see Artificial intelligence art, Generative art, and Synthetic media). They are commonly used for text-to-image generation and neural style transfer.[66] Datasets include LAION-5B and others (see List of datasets in computer vision and image processing).\n \nGenerative AI can also be trained extensively on audio clips to produce natural-sounding speech synthesis and text-to-speech capabilities. An early pioneer in this field was 15.ai, launched in March 2020, which demonstrated the ability to clone character voices using as little as 15 seconds of training data.[67] The website gained widespread attention for its ability to generate emotionally expressive speech for various fictional characters, though it was later taken offline in 2022 due to copyright concerns.[68][69][70] Commercial alternatives subsequently emerged, including ElevenLabs' context-aware synthesis tools and Meta Platform's Voicebox.[71] Generative AI systems such as MusicLM[72] and MusicGen[73] can also be trained on the audio waveforms of recorded music along with text annotations, in order to generate new musical samples based on text descriptions such as a calming violin melody backed by a distorted guitar riff.\n Audio deepfakes of lyrics have been generated, like the song Savages, which used AI to mimic rapper Jay-Z's vocals. Music artist's instrumentals and lyrics are copyrighted but their voices aren't protected from regenerative AI yet, raising a debate about whether artists should get royalties from audio deepfakes.[74]\n Many AI music generators have been created that can be generated using a text phrase, genre options, and looped libraries of bars and riffs.[75]\n Generative AI trained on annotated video can generate temporally-coherent, detailed and photorealistic video clips. Examples include Sora by OpenAI,[12] Gen-1 and Gen-2 by Runway,[76] and Make-A-Video by Meta Platforms.[77]\n Generative AI can also be trained on the motions of a robotic system to generate new trajectories for motion planning or navigation. For example, UniPi from Google Research uses prompts like \"pick up blue bowl\" or \"wipe plate with yellow sponge\" to control movements of a robot arm.[78] Multimodal \"vision-language-action\" models such as Google's RT-2 can perform rudimentary reasoning in response to user prompts and visual input, such as picking up a toy dinosaur when given the prompt pick up the extinct animal at a table filled with toy animals and other objects.[79]\n Artificially intelligent computer-aided design (CAD) can use text-to-3D, image-to-3D, and video-to-3D to automate 3D modeling.[80] AI-based CAD libraries could also be developed using linked open data of schematics and diagrams.[81] AI CAD assistants are used as tools to help streamline workflow.[82]\n Generative AI models are used to power chatbot products such as ChatGPT, programming tools such as GitHub Copilot,[83] text-to-image products such as Midjourney, and text-to-video products such as Runway Gen-2.[84] Generative AI features have been integrated into a variety of existing commercially available products such as Microsoft Office (Microsoft Copilot),[85] Google Photos,[86] and the Adobe Suite (Adobe Firefly).[87] Many generative AI models are also available as open-source software, including Stable Diffusion and the LLaMA[88] language model.\n Smaller generative AI models with up to a few billion parameters can run on smartphones, embedded devices, and personal computers. For example, LLaMA-7B (a version with 7 billion parameters) can run on a Raspberry Pi 4[89] and one version of Stable Diffusion can run on an iPhone 11.[90]\n Larger models with tens of billions of parameters can run on laptop or desktop computers. To achieve an acceptable speed, models of this size may require accelerators such as the GPU chips produced by NVIDIA and AMD or the Neural Engine included in Apple silicon products. For example, the 65 billion parameter version of LLaMA can be configured to run on a desktop PC.[91]\n The advantages of running generative AI locally include protection of privacy and intellectual property, and avoidance of rate limiting and censorship. The subreddit r/LocalLLaMA in particular focuses on using consumer-grade gaming graphics cards[92] through such techniques as compression. That forum is one of only two sources Andrej Karpathy trusts for language model benchmarks.[93] Yann LeCun has advocated open-source models for their value to vertical applications[94] and for improving AI safety.[95]\n Language models with hundreds of billions of parameters, such as GPT-4 or PaLM, typically run on datacenter computers equipped with arrays of GPUs (such as NVIDIA's H100) or AI accelerator chips (such as Google's TPU). These very large models are typically accessed as cloud services over the Internet.\n In 2022, the United States New Export Controls on Advanced Computing and Semiconductors to China imposed restrictions on exports to China of GPU and AI accelerator chips used for generative AI.[96] Chips such as the NVIDIA A800[97] and the Biren Technology BR104[98] were developed to meet the requirements of the sanctions.\n There is free software on the market capable of recognizing text generated by generative artificial intelligence (such as GPTZero), as well as images, audio or video coming from it.[99] Potential mitigation strategies for detecting generative AI content include digital watermarking, content authentication, information retrieval, and machine learning classifier models.[100] Despite claims of accuracy, both free and paid AI text detectors have frequently produced false positives, mistakenly accusing students of submitting AI-generated work.[101][102]\n In the United States, a group of companies including OpenAI, Alphabet, and Meta signed a voluntary agreement with the Biden administration in July 2023 to watermark AI-generated content.[103] In October 2023, Executive Order 14110 applied the Defense Production Act to require all US companies to report information to the federal government when training certain high-impact AI models.[104][105]\n In the European Union, the proposed Artificial Intelligence Act includes requirements to disclose copyrighted material used to train generative AI systems, and to label any AI-generated output as such.[106][107]\n In China, the Interim Measures for the Management of Generative AI Services introduced by the Cyberspace Administration of China regulates any public-facing generative AI. It includes requirements to watermark generated images or videos, regulations on training data and label quality, restrictions on personal data collection, and a guideline that generative AI must \"adhere to socialist core values\".[108][109]\n Generative AI systems such as ChatGPT and Midjourney are trained on large, publicly available datasets that include copyrighted works. AI developers have argued that such training is protected under fair use, while copyright holders have argued that it infringes their rights.[110]\n Proponents of fair use training have argued that it is a transformative use and does not involve making copies of copyrighted works available to the public.[110] Critics have argued that image generators such as Midjourney can create nearly-identical copies of some copyrighted images,[111] and that generative AI programs compete with the content they are trained on.[112]\n As of 2024, several lawsuits related to the use of copyrighted material in training are ongoing.\nGetty Images has sued Stability AI over the use of its images to train Stable diffusion.[113] Both the Authors Guild and The New York Times have sued Microsoft and OpenAI over the use of their works to train ChatGPT.[114][115]\n A separate question is whether AI-generated works can qualify for copyright protection. The United States Copyright Office has ruled that works created by artificial intelligence without any human input cannot be copyrighted, because they lack human authorship.[116] However, the office has also begun taking public input to determine if these rules need to be refined for generative AI.[117]\n The development of generative AI has raised concerns from governments, businesses, and individuals, resulting in protests, legal actions, calls to pause AI experiments, and actions by multiple governments. In a July 2023 briefing of the United Nations Security Council, Secretary-General António Guterres stated \"Generative AI has enormous potential for good and evil at scale\", that AI may \"turbocharge global development\" and contribute between $10 and $15 trillion to the global economy by 2030, but that its malicious use \"could cause horrific levels of death and destruction, widespread trauma, and deep psychological damage on an unimaginable scale\".[118]\n From the early days of the development of AI, there have been arguments put forward by ELIZA creator Joseph Weizenbaum and others about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculations and qualitative, value-based judgements.[120] In April 2023, it was reported that image generation AI has resulted in 70% of the jobs for video game illustrators in China being lost.[121][122] In July 2023, developments in generative AI contributed to the 2023 Hollywood labor disputes. Fran Drescher, president of the Screen Actors Guild, declared that \"artificial intelligence poses an existential threat to creative professions\" during the 2023 SAG-AFTRA strike.[123] Voice generation AI has been seen as a potential challenge to the voice acting sector.[124][125]\n The intersection of AI and employment concerns among underrepresented groups globally remains a critical facet. While AI promises efficiency enhancements and skill acquisition, concerns about job displacement and biased recruiting processes persist among these groups, as outlined in surveys by Fast Company. To leverage AI for a more equitable society, proactive steps encompass mitigating biases, advocating transparency, respecting privacy and consent, and embracing diverse teams and ethical considerations. Strategies involve redirecting policy emphasis on regulation, inclusive design, and education's potential for personalized teaching to maximize benefits while minimizing harms.[126]\n Generative AI models can reflect and amplify any cultural bias present in the underlying data. For example, a language model might assume that doctors and judges are male, and that secretaries or nurses are female, if those biases are common in the training data.[127] Similarly, an image model prompted with the text \"a photo of a CEO\" might disproportionately generate images of white male CEOs,[128] if trained on a racially biased data set. A number of methods for mitigating bias have been attempted, such as altering input prompts[129] and reweighting training data.[130]\n Deepfakes (a portmanteau of \"deep learning\" and \"fake\"[131]) are AI-generated media that take a person in an existing image or video and replace them with someone else's likeness using artificial neural networks.[132] Deepfakes have garnered widespread attention and concerns for their uses in deepfake celebrity pornographic videos, revenge porn, fake news, hoaxes, health disinformation, financial fraud, and covert foreign election interference.[133][134][135][136][137][138][139] This has elicited responses from both industry and government to detect and limit their use.[140][141]\n In July 2023, the fact-checking company Logically found that the popular generative AI models Midjourney, DALL-E 2 and Stable Diffusion would produce plausible disinformation images when prompted to do so, such as images of electoral fraud in the United States and Muslim women supporting India's Hindu nationalist Bharatiya Janata Party.[142][143]\n In April 2024, a paper proposed to use blockchain (distributed ledger technology) to promote \"transparency, verifiability, and decentralization in AI development and usage\".[144]\n Instances of users abusing software to generate controversial statements in the vocal style of celebrities, public officials, and other famous individuals have raised ethical concerns over voice generation AI.[145][146][147][148][149][150] In response, companies such as ElevenLabs have stated that they would work on mitigating potential abuse through safeguards and identity verification.[151]\n Concerns and fandoms have spawned from AI-generated music. The same software used to clone voices has been used on famous musicians' voices to create songs that mimic their voices, gaining both tremendous popularity and criticism.[152][153][154] Similar techniques have also been used to create improved quality or full-length versions of songs that have been leaked or have yet to be released.[155]\n Generative AI has also been used to create new digital artist personalities, with some of these receiving enough attention to receive record deals at major labels.[156] The developers of these virtual artists have also faced their fair share of criticism for their personified programs, including backlash for \"dehumanizing\" an artform, and also creating artists which create unrealistic or immoral appeals to their audiences.[157]\n Generative AI's ability to create realistic fake content has been exploited in numerous types of cybercrime, including phishing scams.[158] Deepfake video and audio have been used to create disinformation and fraud. In 2020, former Google click fraud czar Shuman Ghosemajumder argued that once deepfake videos become perfectly realistic, they would stop appearing remarkable to viewers, potentially leading to uncritical acceptance of false information.[159] Additionally, large language models and other forms of text-generation AI have been used to create fake reviews of e-commerce websites to boost ratings.[160] Cybercriminals have created large language models focused on fraud, including WormGPT and FraudGPT.[161]\n A 2023 study showed that generative AI can be vulnerable to jailbreaks, reverse psychology and prompt injection attacks, enabling attackers to obtain help with harmful requests, such as for crafting social engineering and phishing attacks.[162] Additionally, other researchers have demonstrated that open-source models can be fine-tuned to remove their safety restrictions at low cost.[163]\n Training frontier AI models requires an enormous amount of computing power. Usually only Big Tech companies have the financial resources to make such investments. Smaller start-ups such as Cohere and OpenAI end up buying access to data centers from Google and Microsoft respectively.[164]\n Scientists and journalists have expressed concerns about the environmental impact that the development and deployment of generative models are having: high CO2 emissions,[165][166][167] large amounts of freshwater used for data centers,[168][169] and high amounts of electricity usage.[170][166][171] There is also concern that these impacts may increase as these models are incorporated into widely used search engines such as Google Search and Bing;[170] as chatbots and other applications become more popular;[170][169] and as models need to be retrained.[170]\n Proposed mitigation strategies include factoring potential environmental costs prior to model development or data collection,[165] increasing efficiency of data centers to reduce electricity/energy usage,[168][170][166][169][171][167] building more efficient machine learning models,[168][166][169] minimizing the number of times that models need to be retrained,[167] developing a government-directed framework for auditing the environmental impact of these models,[168][167] regulating for transparency of these models,[167] regulating their energy and water usage,[168] encouraging researchers to publish data on their models' carbon footprint,[170][167] and increasing the number of subject matter experts who understand both machine learning and climate science.[167]\n The New York Times defines slop as analogous to spam: \"shoddy or unwanted A.I. content in social media, art, books and ... in search results.\"[172] Journalists have expressed concerns about the scale of low-quality generated content with respect to social media content moderation,[173] the monetary incentives from social media companies to spread such content,[173][174] false political messaging,[174] spamming of scientific research paper submissions,[175] increased time and effort to find higher quality or desired content on the Internet,[176] the indexing of generated content by search engines,[177] and on journalism itself.[178]\n A paper published by researchers at Amazon Web Services AI Labs found that over 57% of sentences from a sample of over 6 billion sentences from Common Crawl, a snapshot of web pages, were machine translated. Many of these automated translations were seen as lower quality, especially for sentences were translated across at least three languages. Many lower-resource languages (ex. Wolof, Xhosa) were translated across more languages than higher-resource languages (ex. English, French).[179][180]\n In September 2024, Robyn Speer, the author of wordfreq, an open source database that calculated word frequencies based on text from the Internet, announced that she had stopped updating the data for several reasons: high costs for obtaining data from Reddit and Twitter, excessive focus on generative AI compared to other methods in the natural language processing community, and that \"generative AI has polluted the data\".[181]\n The adoption of generative AI tools led to an explosion of AI-generated content across multiple domains. A study from University College London estimated that in 2023, more than 60,000 scholarly articles—over 1% of all publications—were likely written with LLM assistance.[182] According to Stanford University's Institute for Human-Centered AI, approximately 17.5% of newly published computer science papers and 16.9% of peer review text now incorporate content generated by LLMs.[183]\n Visual content follows a similar trend. Since the launch of DALL-E 2 in 2022, it is estimated that an average of 34 million images have been created daily. As of August 2023, more than 15 billion images had been generated using text-to-image algorithms, with 80% of these created by models based on Stable Diffusion.[184]\n If AI-generated content is included in new data crawls from the Internet for additional training of AI models, defects in the resulting models may occur.[185] Training an AI model exclusively on the output of another AI model produces a lower-quality model. Repeating this process, where each new model is trained on the previous model's output, leads to progressive degradation and eventually results in a \"model collapse\" after multiple iterations.[186] Tests have been conducted with pattern recognition of handwritten letters and with pictures of human faces.[187] As a consequence, the value of data collected from genuine human interactions with systems may become increasingly valuable in the presence of LLM-generated content in data crawled from the Internet.\n On the other side, synthetic data is often used as an alternative to data produced by real-world events. Such data can be deployed to validate mathematical models and to train machine learning models while preserving user privacy,[188] including for structured data.[189] The approach is not limited to text generation; image generation has been employed to train computer vision models.[190]\n In January 2023, Futurism.com broke the story that CNET had been using an undisclosed internal AI tool to write at least 77 of its stories; after the news broke, CNET posted corrections to 41 of the stories.[191]\n In April 2023, the German tabloid Die Aktuelle published a fake AI-generated interview with former racing driver Michael Schumacher, who had not made any public appearances since 2013 after sustaining a brain injury in a skiing accident. The story included two possible disclosures: the cover included the line \"deceptively real\", and the interview included an acknowledgment at the end that it was AI-generated. The editor-in-chief was fired shortly thereafter amid the controversy.[192]\n Other outlets that have published articles whose content and/or byline have been confirmed or suspected to be created by generative AI models – often with false content, errors, and/or non-disclosure of generative AI use - include:\n In May 2024, Futurism noted that a content management system video by AdVon Commerce, who had used generative AI to produce articles for many of the aforementioned outlets, appeared to show that they \"had produced tens of thousands of articles for more than 150 publishers.\"[201]\n News broadcasters in Kuwait, Greece, South Korea, India, China and Taiwan have presented news with anchors based on Generative AI models, prompting concerns about job losses for human anchors and audience trust in news that has historically been influenced by parasocial relationships with broadcasters, content creators or social media influencers.[219][220][221] Algorithmically generated anchors have also been used by allies of ISIS for their broadcasts.[222]\n In 2023, Google reportedly pitched a tool to news outlets that claimed to \"produce news stories\" based on input data provided, such as \"details of current events\". Some news company executives who viewed the pitch described it as \"[taking] for granted the effort that went into producing accurate and artful news stories.\"[223]\n In February 2024, Google launched a program to pay small publishers to write three articles per day using a beta generative AI model. The program does not require the knowledge or consent of the websites that the publishers are using as sources, nor does it require the published articles to be labeled as being created or assisted by these models.[224]\n Many defunct news sites (The Hairpin, The Frisky, Apple Daily, Ashland Daily Tidings, Clayton County Register, Southwest Journal) and blogs (The Unofficial Apple Weblog, iLounge) have undergone cybersquatting, with articles created by generative AI.[225][226][227][228][229][230][231][232]\n United States Senators Richard Blumenthal and Amy Klobuchar have expressed concern that generative AI could have a harmful impact on local news.[233] In July 2023, OpenAI partnered with the American Journalism Project to fund local news outlets for experimenting with generative AI, with Axios noting the possibility of generative AI companies creating a dependency for these news outlets.[234]\n Meta AI, a chatbot based on Llama 3 which summarizes news stories, was noted by The Washington Post to copy sentences from those stories without direct attribution and to potentially further decrease the traffic of online news outlets.[235]\n In response to potential pitfalls around the use and misuse of generative AI in journalism and worries about declining audience trust, outlets around the world, including publications such as Wired, Associated Press, The Quint, Rappler or The Guardian have published guidelines around how they plan to use and not use AI and generative AI in their work.[236][237][238][239]\n In June 2024, Reuters Institute published their Digital New Report for 2024. In a survey of people in America and Europe, Reuters Institute reports that 52% and 47% respectively are uncomfortable with news produced by \"mostly AI with some human oversight\", and 23% and 15% respectively report being comfortable. 42% of Americans and 33% of Europeans reported that they were comfortable with news produced by \"mainly human with some help from AI\". The results of global surveys reported that people were more uncomfortable with news topics including politics (46%), crime (43%), and local news (37%) produced by AI than other news topics.[240]\n"
    },
    {
        "title": "Artificial intelligence art",
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_art",
        "content": "\n Artificial intelligence art is visual artwork created or enhanced through the use of artificial intelligence (AI) programs.\n Artists began to create artificial intelligence art in the mid to late 20th century when the discipline was founded. Throughout its history, artificial intelligence art has raised many philosophical concerns related to the human mind, artificial beings, and what can be considered art in a human–AI collaboration. Since the 20th century, artists have used AI to create art, some of which has been exhibited in museums and won awards.[1]\n During the AI boom of the early 2020s, text-to-image models such as Midjourney, DALL-E, Stable Diffusion, and FLUX.1 became widely available to the public, allowing non-artists to quickly generate imagery with little effort.[2][3] Commentary about AI art in the 2020s has often focused on issues related to copyright, deception, defamation, and its impact on more traditional artists, including technological unemployment. Opinions have also risen on the possible effect AI generated art might have on creativity.\n The concept of automated art dates back at least to the automata of ancient Greek civilization, where inventors such as Daedalus and Hero of Alexandria were described as having designed machines capable of writing text, generating sounds, and playing music.[4][5] Early experiments were driven by the idea that computers, beyond performing logical operations, could generate aesthetically pleasing works, offering a new dimension to creativity. The tradition of creative automatons has flourished throughout history, such as Maillardet's automaton, created around 1800 and capable of creating multiple drawings and poems stored in its \"cams”, the brass disks that hold memory.[6]\n Along with this, Ada Lovelace, typically known for her work on the analytical engine, in her notes, begins to conceptualize the idea \"computing operations\" could be used to generate music and poems. This concept resulted in what is now referred to as \"The Lovelace Effect,\" which gives a concrete set of tools to analyze situations where a computer's behavior is viewed by users as creative.[7] However, Lovelace also discusses a concept in her notes that is known as \"The Lovelace Objection,\" where she argues that machines have \"no pretensions whatever to originate anything,\" which is a direct contradiction to the idea of artificial intelligence and creative machines.[8]\n In 1950, with the publication of Alan Turing's paper Computing Machinery and Intelligence, there was a shift from defining intelligence in regards to machines in abstract terms to evaluating whether a machine can mimic human behavior and responses convincingly.[9] Shortly after, the academic discipline of artificial intelligence was founded at a research workshop at Dartmouth College in 1956 and has experienced several waves of advancement and optimism in the decades since.[10] Since its founding, researchers in the field have raised philosophical and ethical arguments about the nature of the human mind and the consequences of creating artificial beings with human-like intelligence; these issues have previously been explored by myth, fiction, and philosophy since antiquity.[11]\n Since the founding of AI in the 1950s, artists and researchers have used artificial intelligence to create artistic works. These works were sometimes referred to as algorithmic art,[12] computer art, digital art, or  New media art.[13]\n One of the first significant AI art systems is AARON, developed by Harold Cohen beginning in the late 1960s at the University of California at San Diego.[14] AARON uses a symbolic rule-based approach to generate technical images in the era of GOFAI programming, and it was developed by Cohen with the goal of being able to code the act of drawing.[15] In its earliest form, AARON created abstract black-and-white drawings which would later be finished by Cohen painting them. Throughout the years, he also began to develop a way for AARON to paint as well, using special brushes and dyes that were chosen by the program itself without mediation from Cohen.[16] After years of work, AARON was exhibited in 1972 at the Los Angeles County Museum of Art.[17] From 1973 to 1975, Cohen refined AARON during a residency at the Artificial Intelligence Laboratory at Stanford University.[18] In 2024, the Whitney Museum of American Art exhibited AI art from throughout Cohen's career, including re-created versions of his early robotic drawing machines.[18]\n Karl Sims has exhibited art created with artificial life since the 1980s. He received an M.S. in computer graphics from the MIT Media Lab in 1987 and was artist-in-residence from 1990 to 1996 at the supercomputer manufacturer and artificial intelligence company Thinking Machines.[19][20][21] In both 1991 and 1992, Sims won the Golden Nica award at Prix Ars Electronica for his 3D AI animated videos using artificial evolution.[22][23][24] In 1997, Sims created the interactive installation Galápagos for the NTT InterCommunication Center in Tokyo.[25] In this installation, viewers help evolve 3D animated creatures by selecting which ones will be allowed to live and produce new, mutated offspring. Furthermore, Sims received an Emmy Award in 2019 for outstanding achievement in engineering development.[26]\n Eric Millikin has been creating animated films using artificial intelligence since the 1980s, and began posting art on the internet using CompuServe in the early 1980s.[27][28]\n In 1999, Scott Draves and a team of several engineers created and released Electric Sheep as a free software screensaver.[29] Electric Sheep is a volunteer computing project for animating and evolving fractal flames, which are in turn distributed to the networked computers, which display them as a screensaver. The screensaver used AI to create an infinite animation by learning from its audience. In 2001, Draves won the Fundacion Telefónica Life 4.0 prize[30] for Electric Sheep.\n Deep learning, characterized by its multi-layer structure that attempts to mimic the human brain, first came about in the 2010s and causing a significant shift in the world of AI art.[31] During the deep learning era, there are mainly these types of designs for generative art: autoregressive models, diffusion models, GANs, normalizing flows. In 2014, Ian Goodfellow and colleagues at Université de Montréal developed the generative adversarial network (GAN), a type of deep neural network capable of learning to mimic the statistical distribution of input data such as images. The GAN uses a \"generator\" to create new images and a \"discriminator\" to decide which created images are considered successful.[32] Unlike previous algorithmic art that followed hand-coded rules, generative adversarial networks could learn a specific aesthetic by analyzing a dataset of example images.[12]\n In 2015, a team at Google released DeepDream, a program that uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia.[33][34][35] The process creates deliberately over-processed images with a dream-like appearance reminiscent of a psychedelic experience.[36]\n Later, in 2017, a conditional GAN learned to generate 1000 image classes of ImageNet, a large visual database designed for use in visual object recognition software research.[37][38] By conditioning the GAN on both random noise and a specific class label, this approach enhanced the quality of image synthesis for class-conditional models.[39]\n Autoregressive models were used for image generation, such as PixelRNN (2016), which autoregressively generates one pixel after another with a recurrent neural network.[40] Immediately after the Transformer architecture was proposed in Attention Is All You Need (2018), it was used for autoregressive generation of images, but without text conditioning.[41]\n In 2018, an auction sale of artificial intelligence art was held at Christie's in New York where the AI artwork Edmond de Belamy (a pun on Goodfellow's name) sold for US$432,500, which was almost 45 times higher than its estimate of US$7,000–10,000. The artwork was created by Obvious, a Paris-based collective.[42][43][44] Furthermore, the website Artbreeder, launched in 2018, uses the models StyleGAN and BigGAN[45][46] to allow users to generate and modify images such as faces, landscapes, and paintings.[47]\n In 2019, Stephanie Dinkins won the Creative Capital award for her creation of an evolving artificial intelligence based on the \"interests and culture(s) of people of color.\"[48] Also in 2019, Sougwen Chung won the Lumen Prize for her performances with a robotic arm that uses AI to attempt to draw in a manner similar to Chung.[49]\n In the 2020s, text-to-image models, which generate images based on prompts, became widely used, marking yet another shift in the creation of AI generated artworks.[2]\n In 2021, using the influential large language generative pre-trained transformer models that are used in GPT-2 and GPT-3, OpenAI released a series of images created with the text-to-image AI model DALL-E 1.[50] It was an autoregressive generative model with essentially the same architecture as GPT-3. Along with this, later in 2021, EleutherAI released the open source VQGAN-CLIP[51] based on OpenAI's CLIP model.[52]\n Diffusion models, generative models used to create synthetic data based on existing data,[53] were first proposed in 2015,[54] but they only became better than GANs in early 2021.[55] Latent diffusion model was published in December 2021 and became the basis for the later Stable Diffusion (August 2022).[56]\n In 2022, Midjourney[57] was released, followed by Google Brain's Imagen and Parti, which were announced in May 2022, Microsoft's NUWA-Infinity,[58][2] and the source-available Stable Diffusion, which was released in August 2022.[59][60][61] DALL-E 2, a successor to DALL-E, was beta-tested and released. Stability AI has a Stable Diffusion web interface called DreamStudio,[62] plugins for Krita, Photoshop, Blender, and GIMP,[63] and the Automatic1111 web-based open source user interface.[64][65][66] Stable Diffusion's main pre-trained model is shared on the Hugging Face Hub.[67]\n In 2023, Eric Millikin released The Dance of the Nain Rouge, a documentary film created using AI deepfake technology about the Detroit folklore legend of the Nain Rouge. The film is described as \"an experimental decolonial Detroit demonology deepfake dream dance documentary.\"[68] It was awarded the \"Best Innovative Technologies Award\" (\"Premio Migliori Tecnologie Innovative\") at the 2024 Pisa Robot Film Festival in Italy[69] and \"Best Animation Film\" at the 2024 Absurd Film Festival in Italy.[70] Ideogram was released in August 2023, this model is known for its ability to generate legible text.[71][72]\n In 2024, Flux was released, this model can generate realistic images with consistent results and was integrated into Grok, the chatbot used on X (formerly Twitter), and Le Chat, the chatbot of Mistral AI.[3][73][74][75] Flux was developed by Black Forest Labs, founded by the researchers behind Stable Diffusion.[76] Grok later switched to its own text-to-image model Aurora in December 2024.[77]\n Along with this, some examples of text-to-video model models of the mid-2020s are Runway's Gen-2, Google's VideoPoet, and OpenAI's Sora, which released in December 2024.[78][79]\n There are many tools available to the artist when working with diffusion models. They can define both positive and negative prompts, but they are also afforded a choice in using (or omitting the use of) VAEs, LorAs, hypernetworks, ipadapter, and embeddings/textual inversions. Variables, including CFG, seed, steps, sampler, scheduler, denoise, upscaler, and encoder, are sometimes available for adjustment. Additional influence can be exerted during pre-inference by means of noise manipulation, while traditional post-processing techniques are frequently used post-inference. Artists can also train their own models.\n In addition, procedural \"rule-based\" generation of images using mathematical patterns, algorithms that simulate brush strokes and other painted effects, and deep learning algorithms such as generative adversarial networks (GANs) and transformers have been developed. Several companies have released apps and websites that allow one to forego all the options mentioned entirely while solely focusing on the positive prompt. There also exist programs which transform photos into art-like images in the style of well-known sets of paintings.[80][81]\n There are many options, ranging from simple consumer-facing mobile apps to Jupyter notebooks and webUIs that require powerful GPUs to run effectively.[82] Additional functionalities include \"textual inversion,\" which refers to enabling the use of user-provided concepts (like an object or a style) learned from a few images. Novel art can then be generated from the associated word(s) (the text that has been assigned to the learned, often abstract, concept)[83][84] and model extensions or fine-tuning (such as DreamBooth).\n AI has the potential for a societal transformation, which may include enabling the expansion of noncommercial niche genres (such as cyberpunk derivatives like solarpunk) by amateurs, novel entertainment, fast prototyping,[85] increasing art-making accessibility,[85] and artistic output per effort and/or expenses and/or time[85]—e.g., via generating drafts, draft-refinitions, and image components (inpainting). Generated images are sometimes used as sketches,[86] low-cost experiments,[87] inspiration, or illustrations of proof-of-concept-stage ideas. Additional functionalities or improvements may also relate to post-generation manual editing (i.e., polishing), such as subsequent tweaking with an image editor.[87]\n Prompts for some text-to-image models can also include images and keywords and configurable parameters, such as artistic style, which is often used via keyphrases like \"in the style of [name of an artist]\" in the prompt[88] and/or selection of a broad aesthetic/art style.[89][86] There are platforms for sharing, trading, searching, forking/refining, and/or collaborating on prompts for generating specific imagery from image generators.[90][91][92][93] Prompts are often shared along with images on image-sharing websites such as Reddit and AI art-dedicated websites. A prompt is not the complete input needed for the generation of an image; additional inputs that determine the generated image include the output resolution, random seed, and random sampling parameters.[94]\n Synthetic media, which includes AI art, was described in 2022 as a major technology-driven trend that will affect business in the coming years.[85] Harvard Kennedy School researchers voiced concerns about synthetic media serving as a vector for political misinformation soon after studying the proliferation of AI art on the X platform.[95] Synthography is a proposed term for the practice of generating images that are similar to photographs using AI.[96]\n A major concern raised about AI-generated images and art is sampling bias within model training data leading towards discriminatory output from AI art models. In 2023, University of Washington researchers found evidence of racial bias within the Stable Diffusion model, with images of a \"person\" corresponding most frequently with images of males from Europe or North America.[97]\n Looking more into the sampling bias found within AI training data, in 2017, researchers at Princeton University used AI software to link over 2 million words, finding that European names were viewed as more \"pleasant\" than African-Americans names, and that the words \"woman\" and \"girl\" were more likely to be associated with the arts instead of science and math, \"which were most likely connected to males.\"[98] Generative AI models typically work based on user-entered word-based prompts, especially in the case of diffusion models, and this word-related bias may lead to biased results.\n Along with this, generative AI can perpetuate harmful stereotypes regarding women. For example, Lensa, an AI app that trended on TikTok in 2023, was known to lighten black skin, make users thinner, and generate hypersexualized images of women.[99] Melissa Heikkilä, a senior reporter at MIT Technology Review, shared the findings of an experiment using Lensa, noting that the generated avatars did not resemble her and often depicted her in a hypersexualized manner.[100] Experts suggest that such outcomes can result from biases in the datasets used to train AI models, which can sometimes contain imbalanced representations, including hypersexual or nude imagery.[101][102]\n In 2024, Google's chatbot Gemini's AI image generator was criticized for perceived racial bias, with claims that Gemini deliberately underrepresented white people in its results.[103] Users reported that it generated images of white historical figures like the Founding Fathers, Nazi soldiers, and Vikings as other races, and that it refused to process prompts such as \"happy white people\" and \"ideal nuclear family\".[103][104] Google later apologized for \"missing the mark\" and took Gemini's image generator offline for updates.[105] This prompted discussions about the ethical implications[106] of representing historical figures through a contemporary lens, leading critics to argue that these outputs could mislead audiences regarding actual historical contexts.[107]\n Legal scholars, artists, and media corporations have considered the legal and ethical implications of artificial intelligence art since the 20th century. Some artists use AI art to critique and explore the ethics of using gathered data to produce new artwork.[108]\n In 1985, intellectual property law professor Pamela Samuelson argued that US copyright should allocate algorithmically generated artworks to the user of the computer program.[109] A 2019 Florida Law Review article presented three perspectives on the issue. In the first, artificial intelligence itself would become the copyright owner; to do this, Section 101 of the US Copyright Act would need to be amended to define \"author\" as a computer. In the second, following Samuelson's argument, the user, programmer, or artificial intelligence company would be the copyright owner. This would be an expansion of the \"work for hire\" doctrine, under which ownership of a copyright is transferred to the \"employer.\" In the third situation, copyright assignments would never take place, and such works would be in the public domain, as copyright assignments require an act of authorship.[110]\n In 2022, coinciding with the rising availability of consumer-grade AI image generation services, popular discussion renewed over the legality and ethics of AI-generated art. A particular topic is the inclusion of copyrighted artwork and images in AI training datasets, with artists objecting to commercial AI products using their works without consent, credit, or financial compensation.[111] In September 2022, Reema Selhi, of the Design and Artists Copyright Society, stated that \"there are no safeguards for artists to be able to identify works in databases that are being used and opt out.\"[112] Some have claimed that images generated with these models can bear resemblance to extant artwork, sometimes including the remains of the original artist's signature.[112][113] In December 2022, users of the portfolio platform ArtStation staged an online protest against non-consensual use of their artwork within datasets; this resulted in opt-out services, such as \"Have I Been Trained?\" increasing in profile, as well as some online art platforms promising to offer their own opt-out options.[114] According to the US Copyright Office, artificial intelligence programs are unable to hold copyright,[115][116][117] a decision upheld at the Federal District level as of August 2023 followed the reasoning from the monkey selfie copyright dispute.[118]\n OpenAI, the developer of DALL-E, has its own policy on who owns generated art. They assign the right and title of a generated image to the creator, meaning the user who inputted the prompt owns the image generated, along with the right to sell, reprint, and merchandise it.[119]\n In January 2023, three artists—Sarah Andersen, Kelly McKernan, and Karla Ortiz—filed a copyright infringement lawsuit against Stability AI, Midjourney, and DeviantArt, claiming that it is legally required to obtain the consent of artists before training neural nets on their work and that these companies infringed on the rights of millions of artists by doing so on five billion images scraped from the web.[120] In July 2023, U.S. District Judge William Orrick was inclined to dismiss most of the lawsuits filed by Andersen, McKernan, and Ortiz, but allowed them to file a new complaint.[121] Also in 2023, Stability AI was sued by Getty Images for using its images in the training data.[122] A tool built by Simon Willison allowed people to search 0.5% of the training data for Stable Diffusion V1.1, i.e., 12 million of the 2.3 billion instances from LAION 2B. Artist Karen Hallion discovered that her copyrighted images were used as training data without their consent.[123]\n In March 2024, Tennessee enacted the ELVIS Act, which prohibits the use of AI to mimic a musician's voice without permission.[124] A month later in that year, Adam Schiff introduced the Generative AI Copyright Disclosure Act which, if passed, would require that AI companies to submit copyrighted works in their datasets to the Register of Copyrights before releasing new generative AI systems.[125]\n As with other types of photo manipulation since the early 19th century, some people in the early 21st century have been concerned that AI could be used to create content that is misleading and can be made to damage a person's reputation, such as deepfakes.[126] Artist Sarah Andersen, who previously had her art copied and edited to depict Neo-Nazi beliefs, stated that the spread of hate speech online can be worsened by the use of image generators.[123] Some also generate images or videos for the purpose of catfishing.\n AI systems have the ability to create deepfake content, which is often viewed as harmful and offensive. The creation of deepfakes poses a risk to individuals who have not consented to it.[127] This mainly refers to deepfake pornography which is used as revenge porn, where sexually explicit material is disseminated to humiliate or harm another person. AI-generated child pornography has been deemed a potential danger to society due to its unlawful nature.[128]\n \nTo mitigate some deceptions, OpenAI developed a tool in 2024 to detect images that were generated by DALL-E 3.[129] In testing, this tool accurately identified DALL-E 3-generated images approximately 98% of the time. The tool is also fairly capable of recognizing images that have been visually modified by users post-generation.[130] After winning the 2023 \"Creative\" \"Open competition\" Sony World Photography Awards, Boris Eldagsen stated that his entry was actually created with artificial intelligence. Photographer Feroz Khan commented to the BBC that Eldagsen had \"clearly shown that even experienced photographers and art experts can be fooled\".[132] Smaller contests have been affected as well; in 2023, a contest run by author Mark Lawrence as Self-Published Fantasy Blog-Off was cancelled after the winning entry was allegedly exposed to be a collage of images generated with Midjourney.[133]\n In May 2023, on social media sites such as Reddit and Twitter, attention was given to a Midjourney-generated image of Pope Francis wearing a white puffer coat.[134][135] Additionally, an AI-generated image of an attack on the Pentagon went viral as part of a hoax news story on Twitter.[136][137]\n In the days before March 2023 indictment of Donald Trump as part of the Stormy Daniels–Donald Trump scandal, several AI-generated images allegedly depicting Trump's arrest went viral online.[138][139] On March 20, British journalist Eliot Higgins generated various images of Donald Trump being arrested or imprisoned using Midjourney v5 and posted them on Twitter; two images of Trump struggling against arresting officers went viral under the mistaken impression that they were genuine, accruing more than 5 million views in three days.[140][141] According to Higgins, the images were not meant to mislead, but he was banned from using Midjourney services as a result. As of April 2024, the tweet had garnered more than 6.8 million views.\n In February 2024, the paper Cellular functions of spermatogonial stem cells in relation to JAK/STAT signaling pathway was published using AI-generated images. It was later retracted from Frontiers in Cell and Developmental Biology because the paper \"does not meet the standards\".[142]\n As generative AI image software such as Stable Diffusion and DALL-E continue to advance, the potential problems and concerns that these systems pose for creativity and artistry have risen.[123] In 2022, artists working in various media raised concerns about the impact that generative artificial intelligence could have on their ability to earn money, particularly if AI-based images started replacing artists working in the illustration and design industries.[143][144] In August 2022, digital artist R. J. Palmer stated that \"I could easily envision a scenario where using AI, a single artist or art director could take the place of 5-10 entry level artists... I have seen a lot of self-published authors and such say how great it will be that they don’t have to hire an artist.\"[113] Scholars Jiang et al. state that \"Leaders of companies like Open AI and Stability AI have openly stated that they expect generative AI systems to replace creatives imminently.\"[123] A 2022 case study found that AI-produced images created by technology like DALL-E caused some traditional artists to be concerned about losing work, while others use it to their advantage and view it as a tool.[127]\n AI-based images have become more commonplace in art markets and search engines because AI-based text-to-image systems are trained from pre-existing artistic images, sometimes without the original artist's consent, allowing the software to mimic specific artists' styles.[123][145] For example, Polish digital artist Greg Rutkowski has stated that it is more difficult to search for his work online because many of the images in the results are AI-generated specifically to mimic his style.[60] Furthermore, some training databases on which AI systems are based are not accessible to the public.\n The ability of AI-based art software to mimic or forge artistic style also raises concerns of malice or greed.[123][146][147] Works of AI-generated art, such as Théâtre D'opéra Spatial, a text-to-image AI illustration that won the grand prize in the August 2022 digital art competition at the Colorado State Fair, have begun to overwhelm art contests and other submission forums meant for small artists.[123][146][147] The Netflix short film The Dog & the Boy, released in January 2023, received backlash online for its use of artificial intelligence art to create the film's background artwork.[148] Within the same vein, Disney released Secret Invasion, a Marvel TV show with an AI-generated intro, on Disney+ in 2023, causing concern and backlash regarding the idea that artists could be made obsolete by machine-learning tools.[149]\n AI art has sometimes been deemed to be able to replace traditional stock images.[150] In 2023, Shutterstock announced a beta test of an AI tool that can regenerate partial content of other Shutterstock's images. Getty Images and Nvidia have partnered with the launch of Generative AI by iStock, a model trained on Getty's library and iStock's photo library using Nvidia's Picasso model.[151]\n Researchers from Hugging Face and Carnegie Mellon University reported in a 2023 paper that generating one thousand 1024×1024 images using Stable Diffusion's XL 1.0 base model requires 11.49 kWh of energy and generates 1,594 grams (56.2 oz) of carbon dioxide, which is roughly equivalent to driving an average gas-powered car a distance of 4.1 miles (6.6 km). Comparing 88 different models, the paper concluded that image-generation models used on average around 2.9 kWh of energy per 1,000 inferences.[152]\n In addition to the creation of original art, research methods that use AI have been generated to quantitatively analyze digital art collections. This has been made possible due to the large-scale digitization of artwork in the past few decades. According to CETINIC and SHE (2022), using artificial intelligence to analyze already-existing art collections can provide new perspectives on the development of artistic styles and the identification of artistic influences.[153][154]\n Two computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art.[155] Close reading focuses on specific visual aspects of one piece. Some tasks performed by machines in close reading methods include computational artist authentication and analysis of brushstrokes or texture properties. In contrast, through distant viewing methods, the similarity across an entire collection for a specific feature can be statistically visualized. Common tasks relating to this method include automatic classification, object detection, multimodal tasks, knowledge discovery in art history, and computational aesthetics.[154] Synthetic images can also be used to train AI algorithms for art authentication and to detect forgeries.[156]\n Researchers have also introduced models that predict emotional responses to art. One such model is ArtEmis, a large-scale dataset paired with machine learning models. ArtEmis includes emotional annotations from over 6,500 participants along with textual explanations. By analyzing both visual inputs and the accompanying text descriptions from this dataset, ArtEmis enables the generation of nuanced emotional predictions.[157][158]\n AI has also been used in arts outside of visual arts. Generative AI has been used in video game production beyond imagery, especially for level design (e.g., for custom maps) and creating new content (e.g., quests or dialogue) or interactive stories in video games.[159][160] AI has also been used in the literary arts,[161] such as helping with writer's block, inspiration, or rewriting segments.[162][163][164][165] In the culinary arts, some prototype cooking robots can dynamically taste, which can assist chefs in analyzing the content and flavor of dishes during the cooking process.[166]\n"
    },
    {
        "title": "Generative audio",
        "url": "https://en.wikipedia.org/wiki/Generative_audio",
        "content": "Generative audio refers to the creation of audio files from databases of audio clips.[citation needed] This technology differs from synthesized voices such as Apple's Siri or Amazon's Alexa, which use a collection of fragments that are stitched together on demand. \n Generative audio works by using neural networks to learn the statistical properties of an audio source, then reproduces those properties.[1]\n With this technology, a person's voice can be replicated to speak phrases that they may have never spoken. This could lead to a synthetic version of a public figure's voice being used against them.[2]\n Modern generative audio systems employ various deep learning architectures. One notable approach uses generative adversarial networks (GANs), where two machine learning models work against each other to create realistic audio. Other architectures include WaveNet, which uses dilated causal convolutions to model raw audio waveforms, and implementations like 15.ai, which demonstrated in 2020 the ability to clone voices using as little as 15 seconds of training data through specialized neural network architectures.[3][4]\n"
    },
    {
        "title": "Music and artificial intelligence",
        "url": "https://en.wikipedia.org/wiki/Music_and_artificial_intelligence",
        "content": "\n Music and artificial intelligence (music and AI) is the development of music software programs which use AI to generate music.[1] As with applications in other fields, AI in music also simulates mental tasks. A prominent feature is the capability of an AI algorithm to learn based on past data, such as in computer accompaniment technology, wherein the AI is capable of listening to a human performer and performing accompaniment.[2] Artificial intelligence also drives interactive composition technology, wherein a computer composes music in response to a live performance. There are other AI applications in music that cover not only music composition, production, and performance but also how music is marketed and consumed. Several music player programs have also been developed to use voice recognition and natural language processing technology for music voice control. Current research includes the application of AI in music composition, performance, theory and digital sound processing.\n Erwin Panofksy proposed that in all art, there existed three levels of meaning: primary meaning, or the natural subject; secondary meaning, or the conventional subject; and tertiary meaning, the intrinsic content of the subject.[3][4] AI music explores the foremost of these, creating music without the \"intention\" which is usually behind it, leaving composers who listen to machine-generated pieces feeling unsettled by the lack of apparent meaning.[5]\n Artificial intelligence finds its beginnings in music with the transcription problem: accurately recording a performance into musical notation as it is played. Père Engramelle's schematic of a \"piano roll\", a mode of automatically recording note timing and duration in a way which could be easily transcribed to proper musical notation by hand, was first implemented by German engineers J.F. Unger and J. Hohlfield in 1752.[6]\n In 1957, the ILLIAC I (Illinois Automatic Computer) produced the \"Illiac Suite for String Quartet\", a completely computer-generated piece of music. The computer was programmed to accomplish this by composer Lejaren Hiller and mathematician Leonard Isaacson.[5]: v–vii \nIn 1960, Russian researcher Rudolf Zaripov published worldwide first paper on algorithmic music composing using the Ural-1 computer.[7]\n In 1965, inventor Ray Kurzweil developed software capable of recognizing musical patterns and synthesizing new compositions from them. The computer first appeared on the quiz show I've Got a Secret.[8]\n By 1983, Yamaha Corporation's Kansei Music System had gained momentum, and a paper was published on its development in 1989. The software utilized music information processing and artificial intelligence techniques to essentially solve the transcription problem for simpler melodies, although higher-level melodies and musical complexities are regarded even today as difficult deep-learning tasks, and near-perfect transcription is still a subject of research.[6][9]\n In 1997, an artificial intelligence program named Experiments in Musical Intelligence (EMI) appeared to outperform a human composer at the task of composing a piece of music to imitate the style of Bach.[10] EMI would later become the basis for a more sophisticated algorithm called Emily Howell, named for its creator.\n In 2002, the music research team at the Sony Computer Science Laboratory Paris, led by French composer and scientist François Pachet, designed the Continuator, an algorithm uniquely capable of resuming a composition after a live musician stopped.[11]\n Emily Howell would continue to make advancements in musical artificial intelligence, publishing its first album From Darkness, Light in 2009.[12] Since then, many more pieces by artificial intelligence and various groups have been published.\n In 2010, Iamus became the first AI to produce a fragment of original contemporary classical music, in its own style: \"Iamus' Opus 1\". Located at the Universidad de Malága (Malága University) in Spain, the computer can generate a fully original piece in a variety of musical styles.[13][5]: 468–481  In August 2019, a large dataset consisting of 12,197 MIDI songs, each with their lyrics and melodies,[14] was created to investigate the feasibility of neural melody generation from lyrics using a deep conditional LSTM-GAN method.\n With progress in generative AI, models capable of creating complete musical compositions (including lyrics) from a simple text description have begun to emerge. Two notable web applications in this field are Suno AI, launched in December 2023, and Udio, which followed in April 2024.[15]\n Developed at Princeton University by Ge Wang and Perry Cook, ChucK is a text-based, cross-platform language.[16] By extracting and classifying the theoretical techniques it finds in musical pieces, the software is able to synthesize entirely new pieces from the techniques it has learned.[17] The technology is used by SLOrk (Stanford Laptop Orchestra)[18] and PLOrk (Princeton Laptop Orchestra).\n Jukedeck was a website that let people use artificial intelligence to generate original, royalty-free music for use in videos.[19][20] The team started building the music generation technology in 2010,[21] formed a company around it in 2012,[22] and launched the website publicly in 2015.[20] The technology used was originally a rule-based algorithmic composition system,[23] which was later replaced with artificial neural networks.[19] The website was used to create over 1 million pieces of music, and brands that used it included Coca-Cola, Google, UKTV, and the Natural History Museum, London.[24] In 2019, the company was acquired by ByteDance.[25][26][27]\n MorpheuS[28] is a research project by Dorien Herremans and Elaine Chew at Queen Mary University of London, funded by a Marie Skłodowská-Curie EU project. The system uses an optimization approach based on a variable neighborhood search algorithm to morph existing template pieces into novel pieces with a set level of tonal tension that changes dynamically throughout the piece. This optimization approach allows for the integration of a pattern detection technique in order to enforce long term structure and recurring themes in the generated music. Pieces composed by MorpheuS have been performed at concerts in both Stanford and London.\n Created in February 2016, in Luxembourg, AIVA is a program that produces soundtracks for any type of media. The algorithms behind AIVA are based on deep learning architectures[29] AIVA has also been used to compose a Rock track called On the Edge,[30] as well as a pop tune Love Sick[31] in collaboration with singer Taryn Southern,[32] for the creation of her 2018 album \"I am AI\".\n Google's Magenta team has published several AI music applications and technical papers since their launch in 2016.[33] In 2017 they released the NSynth algorithm and dataset,[34] and an open source hardware musical instrument, designed to facilitate musicians in using the algorithm.[35] The instrument was used by notable artists such as Grimes and YACHT in their albums.[36][37] In 2018, they released a piano improvisation app called Piano Genie. This was later followed by Magenta Studio, a suite of 5 MIDI plugins that allow music producers to elaborate on existing music in their DAW.[38] In 2023, their machine learning team published a technical paper on GitHub that described MusicLM, a private text-to-music generator which they'd developed.[39][40]\n Riffusion is a neural network, designed by Seth Forsgren and Hayk Martiros, that generates music using images of sound rather than audio.[41] It was created as a fine-tuning of Stable Diffusion, an existing open-source model for generating images from text prompts, on spectrograms.[41] This results in a model which uses text prompts to generate image files, which can be put through an inverse Fourier transform and converted into audio files.[42] While these files are only several seconds long, the model can also use latent space between outputs to interpolate different files together.[41][43] This is accomplished using a functionality of the Stable Diffusion model known as img2img.[44]\n The resulting music has been described as \"de otro mundo\" (otherworldly),[45] although unlikely to replace man-made music.[45] The model was made available on December 15, 2022, with the code also freely available on GitHub.[42] It is one of many models derived from Stable Diffusion.[44]\n Spike AI is an AI-based audio plug-in, developed by Spike Stent in collaboration with his son Joshua Stent and friend Henry Ramsey, that analyzes tracks and provides suggestions to increase clarity and other aspects during mixing. Communication is done by using a chatbot trained on Spike Stent's personal data. The plug-in integrates into digital audio workstation.[49][50]\n Artificial Intelligence has the opportunity to impact how producers create music by giving reiterations of a track that follow a prompt given by the creator. These prompts allow the AI to follow a certain style that the artist is trying to go for.[5]\n AI has also been seen in musical analysis where it has been used for feature extraction, pattern recognition, and musical recommendations.[51]\n Artificial intelligence has had major impacts in the composition sector as it has influenced the ideas of composers/producers and has the potential to make the industry more accessible to newcomers. With its development in music, it has already been seen to be used in collaboration with producers. Artists use these software to help generate ideas and bring out musical styles by prompting the AI to follow specific requirements that fit their needs. Future compositional impacts by the technology include style emulation and fusion, and revision and refinement. Development of these types of software can give ease of access to newcomers to the music industry.[5] Software such as ChatGPT have been used by producers to do these tasks, while other software such as Ozone11 have been used to automate time consuming and complex activities such as mastering.[52]\n In the United States, the current legal framework tends to apply traditional copyright laws to AI, despite its differences with the human creative process.[53] However, music outputs solely generated by AI are not granted copyright protection. In the compendium of the U.S. Copyright Office Practices, the Copyright Office has stated that it would not grant copyrights to \"works that lack human authorship\" and \"the Office will not register works produced by a machine or mere mechanical process that operates randomly or automatically without any creative input or intervention from a human author.\"[54] In February 2022, the Copyright Review Board rejected an application to copyright AI-generated artwork on the basis that it \"lacked the required human authorship necessary to sustain a claim in copyright.\"[55]\n The situation in the European Union (EU) is similar to the US, because its legal framework also emphasizes the role of human involvement in a copyright-protected work.[56] According to the European Union Intellectual Property Office and the recent jurisprudence of the Court of Justice of the European Union, the originality criterion requires the work to be the author's own intellectual creation, reflecting the personality of the author evidenced by the creative choices made during its production, requires distinct level of human involvement.[56] The reCreating Europe project, funded by the European Union's Horizon 2020 research and innovation program, delves into the challenges posed by AI-generated contents including music, suggesting legal certainty and balanced protection that encourages innovation while respecting copyright norms.[56] The recognition of AIVA marks a significant departure from traditional views on authorship and copyrights in the realm of music composition, allowing AI artists capable of releasing music and earning royalties. This acceptance marks AIVA as a pioneering instance where an AI has been formally acknowledged within the music production.[57]\n The recent advancements in artificial intelligence made by groups such as Stability AI, OpenAI, and Google has incurred an enormous sum of copyright claims leveled against generative technology, including AI music. Should these lawsuits succeed, the machine learning models behind these technologies would have their datasets restricted to the public domain.[58]\n A more nascent development of AI in music is the application of audio deepfakes to cast the lyrics or musical style of a pre-existing song to the voice or style of another artist. This has raised many concerns regarding the legality of technology, as well as the ethics of employing it, particularly in the context of artistic identity.[59] Furthermore, it has also raised the question of to whom the authorship of these works is attributed. As AI cannot hold authorship of its own, current speculation suggests that there will be no clear answer until further rulings are made regarding machine learning technologies as a whole.[60] Most recent preventative measures have started to be developed by Google and Universal Music group who have taken into royalties and credit attribution to allow producers to replicated the voices and styles of artists.[61]\n In 2023, an artist known as ghostwriter977 created a musical deepfake called \"Heart on My Sleeve\" that cloned the voices of Drake and The Weeknd by inputting an assortment of vocal-only tracks from the respective artists into a deep-learning algorithm, creating an artificial model of the voices of each artist, to which this model could be mapped onto original reference vocals with original lyrics.[62] The track was submitted for Grammy consideration for the best rap song and song of the year.[63] It went viral and gained traction on TikTok and received a positive response from the audience, leading to its official release on Apple Music, Spotify, and YouTube in April of 2023.[64] Many believed the track was fully composed by an AI software, but the producer claimed the songwriting, production, and original vocals (pre-conversion) were still done by him.[62] It would later be rescinded from any Grammy considerations due to it not following the guidelines necessary to be considered for a Grammy award.[64] The track would end up being removed from all music platforms by Universal Music Group.[64] The song was a watershed moment for AI voice cloning, and models have since been created for hundreds, if not thousands, of popular singers and rappers.\n In 2013, country music singer Randy Travis suffered a stroke which left him unable to sing. In the meantime, vocalist James Dupré toured on his behalf, singing his songs for him. Travis and longtime producer Kyle Lehning released a new song in May 2024 titled \"Where That Came From\", Travis's first new song since his stroke. The recording uses AI technology to re-create Travis's singing voice, having been composited from over 40 existing vocal recordings alongside those of Dupré.[65][66]\n"
    },
    {
        "title": "Artificial intelligence in government",
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_in_government",
        "content": "\nArtificial intelligence (AI) has a range of uses in government. It can be used to further public policy objectives (in areas such as emergency services, health and welfare), as well as assist the public to interact with the government  (through the use of virtual assistants, for example). According to the Harvard Business Review, \"Applications of artificial intelligence to the public sector are broad and growing, with early experiments taking place around the world.\"[1] Hila Mehr from the Ash Center for Democratic Governance and Innovation at Harvard University notes that AI in government is not new, with postal services using machine methods in the late 1990s to recognise handwriting on envelopes to automatically route letters.[2] The use of AI in government comes with significant benefits, including efficiencies resulting in cost savings (for instance by reducing the number of front office staff), and reducing the opportunities for corruption.[3] However, it also carries risks (described below).\n The potential uses of AI in government are wide and varied,[4] with Deloitte considering that \"Cognitive technologies could eventually revolutionize every facet of government operations\".[5] Mehr suggests that six types of government problems are appropriate for AI applications:[2]\n Mehr states that \"While applications of AI in government work have not kept pace with the rapid expansion of AI in the private sector, the potential use cases in the public sector mirror common applications in the private sector.\"[2]\n Potential and actual uses of AI in government can be divided into three broad categories: those that contribute to public policy objectives; those that assist public interactions with the government; and other uses.\n There are a range of examples of where AI can contribute to public policy objectives.[4] These include:\n AI can be used to assist members of the public to interact with government and access government services,[4] for example by:\n Various governments, including those of Australia[10] and Estonia,[11] have implemented virtual assistants to aid citizens in navigating services, with applications ranging from tax inquiries to life-event registrations.\n Gerrymandering is an insidious method of influencing political process.[12] Depending on the objective of its use, the application of artificial intelligence to redraw districts based on voter distribution and demographic datasets can either contribute to impartiality, or sustain partisan gains for interested stakeholders in the election process.[13]\n Other uses of AI in government include:\n AI offers potential efficiencies and costs savings for the government. For example, Deloitte has estimated that automation could save US Government employees between 96.7 million to 1.2 billion hours a year, resulting in potential savings of between $3.3 billion to $41.1 billion a year.[5] The Harvard Business Review has stated that while this may lead a government to reduce employee numbers, \"Governments could instead choose to invest in the quality of its services. They can re-employ workers' time towards more rewarding work that requires lateral thinking, empathy, and creativity — all things at which humans continue to outperform even the most sophisticated AI program.\"[1]\n Risks associated with the use of AI in government include AI becoming susceptible to bias,[2] a lack of transparency in how an AI application may make decisions,[7] and the accountability for any such decisions.[7]\n AI in governance and the economic world might make the market more difficult for companies to keep up with the increases in technology. Large U.S. companies like Apple and Google are able to dominate the market with their latest and most advanced technologies. This gives them an advantage over smaller companies that do not have the means of advancing as far in the digital technology fields with AI.[14]\n"
    },
    {
        "title": "Artificial intelligence in healthcare",
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_in_healthcare",
        "content": "\n Artificial intelligence in healthcare is the application of artificial intelligence (AI) to analyze and understand complex medical and healthcare data. In some cases, it can exceed or augment human capabilities by providing better or faster ways to diagnose, treat, or prevent disease.[1][2][3]\n As widespread use of AI in healthcare is relatively new, research is ongoing into its application in various subdisciplines of medicine and related industries. AI programs are applied to practices such as diagnostics,[4] treatment protocol development,[5] drug development,[6] personalized medicine,[7] and patient monitoring and care.[8] Because radiographs are the most common imaging tests conducted in radiology departments, the potential for AI to help with triage and interpretation of radiographs is particularly noteworthy.[9]\n Using AI also presents unprecedented ethical concerns related to issues such as data privacy, automation of jobs, and amplifying already existing biases.[10] Furthermore, new technologies such as AI are often resisted by healthcare leaders, leading to slow and erratic adoption.[11] In contrast, there are also several cases where AI has been put to use in healthcare without proper testing.[12][13][14][15] A systematic review and thematic analysis in 2023 showed that most stakeholders including health professionals, patients, and the general public doubted that care involving AI could be empathetic.[16] Moreover, meta-studies have found that the scientific literature on AI in healthcare often suffers from a lack of reproducibility.[17][18][19][20]\n Accurate and early diagnosis of diseases is still a challenge in healthcare. Recognising medical conditions and their symptoms is a complex problem. AI can assist clinicians with its data processing capabilities to save time and improve accuracy.[21] Through the use of machine learning, artificial intelligence can be able to substantially aid doctors in patient diagnosis through the analysis of mass electronic health records (EHRs).[22] AI can help early prediction, for example, of Alzheimer's disease and dementias, by looking through large numbers of similar cases and possible treatments.[23]\n Doctors' decision making could also be supported by AI in urgent situations, for example in the emergency department. Here AI algorithms can help prioritize more serious cases and reduce waiting time. Decision support systems augmented with AI can offer real-time suggestions and faster data interpretation to aid the decisions made by healthcare professionals.[21]\n In 2023 a study reported higher satisfaction rates with ChatGPT-generated responses compared with those from physicians for medical questions posted on Reddit’s r/AskDocs.[24] Evaluators preferred ChatGPT's responses to physician responses in 78.6% of 585 evaluations, noting better quality and empathy. The authors noted that these were isolated questions taken from an online forum, not in the context of an established patient-physician relationship.[24] Moreover, responses were not graded on the accuracy of medical information, and some have argued that the experiment was not properly blinded, with the evaluators being coauthors of the study.[25][26][27]\n Recent developments in statistical physics, machine learning, and inference algorithms are also being explored for their potential in improving medical diagnostic approaches.[28]\n Electronic health records (EHR) are crucial to the digitalization and information spread of the healthcare industry. Now that around 80% of medical practices use EHR, some anticipate the use of artificial intelligence to interpret the records and provide new information to physicians.[29]\n One application uses natural language processing (NLP) to make more succinct reports that limit the variation between medical terms by matching similar medical terms.[29] For example, the term heart attack and myocardial infarction mean the same things, but physicians may use one over the over based on personal preferences.[29] NLP algorithms consolidate these differences so that larger datasets can be analyzed.[29] Another use of NLP identifies phrases that are redundant due to repetition in a physician's notes and keeps the relevant information to make it easier to read.[29] Other applications use concept processing to analyze the information entered by the current patient's doctor to present similar cases and help the physician remember to include all relevant details.[30]\n Beyond making content edits to an EHR, there are AI algorithms that evaluate an individual patient's record and predict a risk for a disease based on their previous information and family history.[31] One general algorithm is a rule-based system that makes decisions similarly to how humans use flow charts.[32] This system takes in large amounts of data and creates a set of rules that connect specific observations to concluded diagnoses.[32] Thus, the algorithm can take in a new patient's data and try to predict the likeliness that they will have a certain condition or disease.[32] Since the algorithms can evaluate a patient's information based on collective data, they can find any outstanding issues to bring to a physician's attention and save time.[31] One study conducted by the Centerstone research institute found that predictive modeling of EHR data has achieved 70–72% accuracy in predicting individualized treatment response.[33] These methods are helpful due to the fact that the amount of online health records doubles every five years.[31] Physicians do not have the bandwidth to process all this data manually, and AI can leverage this data to assist physicians in treating their patients.[31]\n Improvements in natural language processing led to the development of algorithms to identify drug-drug interactions in medical literature.[34][35][36][37] Drug-drug interactions pose a threat to those taking multiple medications simultaneously, and the danger increases with the number of medications being taken.[38] To address the difficulty of tracking all known or suspected drug-drug interactions, machine learning algorithms have been created to extract information on interacting drugs and their possible effects from medical literature. Efforts were consolidated in 2013 in the DDIExtraction Challenge, in which a team of researchers at Carlos III University assembled a corpus of literature on drug-drug interactions to form a standardized test for such algorithms.[39] Competitors were tested on their ability to accurately determine, from the text, which drugs were shown to interact and what the characteristics of their interactions were.[40]  Researchers continue to use this corpus to standardize the measurement of the effectiveness of their algorithms.[34][35][37]\n Other algorithms identify drug-drug interactions from patterns in user-generated content, especially electronic health records and/or adverse event reports.[35][36] Organizations such as the FDA Adverse Event Reporting System (FAERS) and the World Health Organization's VigiBase allow doctors to submit reports of possible negative reactions to medications. Deep learning algorithms have been developed to parse these reports and detect patterns that imply drug-drug interactions.[41]\n The increase of telemedicine, the treatment of patients remotely, has shown the rise of possible AI applications.[42] AI can assist in caring for patients remotely by monitoring their information through sensors.[43] A wearable device may allow for constant monitoring of a patient and the ability to notice changes that may be less distinguishable by humans. The information can be compared to other data that has already been collected using artificial intelligence algorithms that alert physicians if there are any issues to be aware of.[43]\n Another application of artificial intelligence is chat-bot therapy. Some researchers charge that the reliance on chatbots for mental healthcare does not offer the reciprocity and accountability of care that should exist in the relationship between the consumer of mental healthcare and the care provider (be it a chat-bot or psychologist), though.[44]\n Since the average age has risen due to a longer life expectancy, artificial intelligence could be useful in helping take care of older populations.[45] Tools such as environment and personal sensors can identify a person's regular activities and alert a caretaker if a behavior or a measured vital is abnormal.[45] Although the technology is useful, there are also discussions about limitations of monitoring in order to respect a person's privacy since there are technologies that are designed to map out home layouts and detect human interactions.[45]\n AI has the potential to streamline care coordination and reduce the workload. AI algorithms can automate administrative tasks, prioritize patient needs and facilitate seamless communication in a healthcare team.[46] This enables healthcare providers to focus more on direct patient care and ensures the efficient and coordinated delivery of healthcare services.\n Artificial intelligence algorithms have shown promising results in accurately diagnosing and risk stratifying patients with concern for coronary artery disease, showing potential as an initial triage tool.[47][48] Other algorithms have been used in predicting patient mortality, medication effects, and adverse events following treatment for acute coronary syndrome.[47] Wearables, smartphones, and internet-based technologies have also shown the ability to monitor patients' cardiac data points, expanding the amount of data and the various settings AI models can use and potentially enabling earlier detection of cardiac events occurring outside of the hospital.[49] Another growing area of research is the utility of AI in classifying heart sounds and diagnosing valvular disease.[50] Challenges of AI in cardiovascular medicine have included the limited data available to train machine learning models, such as limited data on social determinants of health as they pertain to cardiovascular disease.[51]\n A key limitation in early studies evaluating AI were omissions of data comparing algorithmic performance to humans. Examples of studies which assess AI performance relative to physicians includes how AI is non-inferior to humans in interpretation of cardiac echocardiograms[52] and that AI can diagnose heart attack better than human physicians in the emergency setting, reducing both low-value testing and missed diagnoses.[53]\n In cardiovascular tissue engineering and organoid studies, AI is increasingly used to analyze microscopy images, and integrate electrophysiological read outs.[54]\n Medical imaging (such as X-ray and photography) is a commonly used tool in dermatology[55] and the development of deep learning has been strongly tied to image processing. Therefore, there is a natural fit between the dermatology and deep learning. Machine learning learning holds great potential to process these images for better diagnoses.[56] Han et al. showed keratinocytic skin cancer detection from face photographs.[57] Esteva et al. demonstrated dermatologist-level classification of skin cancer from lesion images.[58] Noyan et al. demonstrated a convolutional neural network that achieved 94% accuracy at identifying skin cells from microscopic Tzanck smear images.[59] A concern raised with this work is that it has not engaged with disparities related to skin color or differential treatment of patients with non-white skin tones.[60]\n According to some researchers, AI algorithms have been shown to be more effective than dermatologists at identifying cancer.[61] However, a 2021 review article found that a majority of papers analyzing the performance of AI algorithms designed for skin cancer classification failed to use external test sets.[62] Only four research studies were found in which the AI algorithms were tested on clinics, regions, or populations distinct from those it was trained on, and in each of those four studies, the performance of dermatologists was found to be on par with that of the algorithm. Moreover, only one study[63] was set in the context of a full clinical examination; others were based on interaction through web-apps or online questionnaires, with most based entirely on context-free images of lesions. In this study, it was found that dermatologists significantly outperformed the algorithms. Many articles claiming superior performance of AI algorithms also fail to distinguish between trainees and board-certified dermatologists in their analyses.[62]\n It has also been suggested that AI could be used to automatically evaluate the outcome of maxillo-facial surgery or cleft palate therapy in regard to facial attractiveness or age appearance.[64][65]\n AI can play a role in various facets of the field of gastroenterology. Endoscopic exams such as esophagogastroduodenoscopies (EGD) and colonoscopies rely on rapid detection of abnormal tissue. By enhancing these endoscopic procedures with AI, clinicians can more rapidly identify diseases, determine their severity, and visualize blind spots. Early trials in using AI detection systems of early stomach cancer have shown sensitivity close to expert endoscopists.[66]\n AI can assist doctors treating ulcerative colitis in detecting the microscopic activity of the disease in people and predicting when flare-ups will happen. For example, an AI-powered tool  was developed to analyse digitised bowel samples (biopsies). The tool was able to distinguish with 80% accuracy between samples that show remission of colitis and those with active disease. It also predicted the risk of a flare-up happening with the same accuracy. These rates of successfully using microscopic disease activity to predict disease flare are similar to the accuracy of pathologists.[67][68]\n Artificial intelligence utilises massive amounts of data to help with predicting illness, prevention, and diagnosis, as well as patient monitoring. In obstetrics, artificial intelligence is utilized in magnetic resonance imaging, ultrasound, and foetal cardiotocography. AI contributes in the resolution of a variety of obstetrical diagnostic issues.[69]\n AI has shown potential in both the laboratory and clinical spheres of infectious disease medicine.[70] During the COVID-19 pandemic, AI has been used for early detection, tracking virus spread and analysing virus behaviour, among other things.[71] However, there were only a few examples of AI being used directly in clinical practice during the pandemic itself.[72]\n Other applications  of AI around infectious diseases include support-vector machines identifying antimicrobial resistance, machine learning analysis of blood smears to detect malaria, and improved point-of-care testing of Lyme disease based on antigen detection. Additionally, AI has been investigated for improving diagnosis of meningitis, sepsis, and tuberculosis, as well as predicting treatment complications in hepatitis B and hepatitis C patients.[70]\n AI has been used to identify causes of knee pain that doctors miss, that disproportionately affect Black patients.[73] Underserved populations experience higher levels of pain. These disparities persist even after controlling for the objective severity of diseases like osteoarthritis, as graded by human physicians using medical images, raising the possibility that underserved patients’ pain stems from factors external to the knee, such as stress. Researchers have conducted a study using a machine-learning algorithm to show that standard radiographic measures of severity overlook objective but undiagnosed features that disproportionately affect diagnosis and management of underserved populations with knee pain. They proposed that new algorithmic measure ALG-P could potentially enable expanded access to treatments for underserved patients.[74]\n The use of AI technologies has been explored for use in the diagnosis and prognosis of Alzheimer's disease (AD). For diagnostic purposes, machine learning models have been developed that rely on structural MRI inputs.[75] The input datasets for these models are drawn from databases such as the Alzheimer's Disease Neuroimaging Initiative.[76] Researchers have developed models that rely on convolutional neural networks with the aim of improving early diagnostic accuracy.[77] Generative adversarial networks are a form of deep learning that have also performed well in diagnosing AD.[78] There have also been efforts to develop machine learning models into forecasting tools that can predict the prognosis of patients with AD. Forecasting patient outcomes through generative models has been proposed by researchers as a means of synthesizing training and validation sets.[79] They suggest that generated patient forecasts could be used to provide future models larger training datasets than current open access databases.\n AI has been explored for use in cancer diagnosis, risk stratification, molecular characterization of tumors, and cancer drug discovery. A particular challenge in oncologic care that AI is being developed to address is the ability to accurately predict which treatment protocols will be best suited for each patient based on their individual genetic, molecular, and tumor-based characteristics.[80] AI has been trialed in cancer diagnostics with the reading of imaging studies and pathology slides.[81]\n In January 2020, Google DeepMind announced an algorithm capable of surpassing human experts in breast cancer detection in screening scans.[82][83] A number of researchers, including Trevor Hastie, Joelle Pineau, and Robert Tibshirani among others, published a reply claiming that DeepMind's research publication in Nature lacked key details on methodology and code, \"effectively undermin[ing] its scientific value\" and making it impossible for the scientific community to confirm the work.[84] In the MIT Technology Review, author Benjamin Haibe-Kains characterized DeepMind's work as \"an advertisement\" having little to do with science.[85]\n In July 2020, it was reported that an AI algorithm developed by the University of Pittsburgh achieves the highest accuracy to date in identifying prostate cancer, with 98% sensitivity and 97% specificity.[86][87] In 2023 a study reported the use of AI for CT-based radiomics classification at grading the aggressiveness of retroperitoneal sarcoma with 82% accuracy compared with 44% for lab analysis of biopsies.[88][89]\n Artificial intelligence-enhanced technology is being used as an aid in the screening of eye disease and prevention of blindness.[90] In 2018, the U.S. Food and Drug Administration authorized the marketing of the first medical device to diagnose a specific type of eye disease, diabetic retinopathy using an artificial intelligence algorithm.[91] Moreover, AI technology may be used to further improve \"diagnosis rates\" because of the potential to decrease detection time.[92]\n For many diseases, pathological analysis of cells and tissues is considered to be the gold standard of disease diagnosis. Methods of digital pathology allows microscopy slides to be scanned and digitally analyzed. AI-assisted pathology tools have been developed to assist with the diagnosis of a number of diseases, including breast cancer, hepatitis B, gastric cancer, and colorectal cancer. AI has also been used to predict genetic mutations and prognosticate disease outcomes.[66] AI is well-suited for use in low-complexity pathological analysis of large-scale screening samples, such as colorectal or breast cancer screening, thus lessening the burden on pathologists and allowing for faster turnaround of sample analysis.[94] Several deep learning and artificial neural network models have shown accuracy similar to that of human pathologists,[94] and a study of deep learning assistance in diagnosing metastatic breast cancer in lymph nodes showed that the accuracy of humans with the assistance of a deep learning program was higher than either the humans alone or the AI program alone.[95] Additionally, implementation of digital pathology is predicted to save over $12 million for a university center over the course of five years,[96] though savings attributed to AI specifically have not yet been widely researched. The use of augmented and virtual reality could prove to be a stepping stone to wider implementation of AI-assisted pathology, as they can highlight areas of concern on a pathology sample and present them in real-time to a pathologist for more efficient review.[94] AI also has the potential to identify histological findings at levels beyond what the human eye can see,[94] and has shown the ability to use genotypic and phenotypic data to more accurately detect the tumor of origin for metastatic cancer.[97] One of the major current barriers to widespread implementation of AI-assisted pathology tools is the lack of prospective, randomized, multi-center controlled trials in determining the true clinical utility of AI for pathologists and patients, highlighting a current area of need in AI and healthcare research.[94]\n Primary care has become one key development area for AI technologies.[98][99] AI in primary care has been used for supporting decision making, predictive modeling, and business analytics.[100] There are only a few examples of AI decision support systems that were prospectively assessed on clinical efficacy when used in practice by physicians. But there are cases where the use of these systems yielded a positive effect on treatment choice by physicians.[101]\n In psychiatry, AI applications are still in a phase of proof-of-concept.[102] Areas where the evidence is widening quickly include predictive modelling of diagnosis and treatment outcomes,[103] chatbots, conversational agents that imitate human behaviour and which have been studied for anxiety and depression.[104]\n Challenges include the fact that many applications in the field are developed and proposed by private corporations, such as the screening for suicidal ideation implemented by Facebook in 2017.[105] Such applications outside the healthcare system raise various professional, ethical and regulatory questions.[106] Another issue is often with the validity and interpretability of the models. Small training datasets contain bias that is inherited by the models, and compromises the generalizability and stability of these models. Such models may also have the potential to be discriminatory against minority groups that are underrepresented in samples.[107]\n In 2023, US-based National Eating Disorders Association replaced its human helpline staff with a chatbot but had to take it offline after users reported receiving harmful advice from it.[108][109][110]\n AI is being studied within the field of radiology to detect and diagnose diseases through computerized tomography (CT) and magnetic resonance (MR) imaging.[111] It may be particularly useful in settings where demand for human expertise exceeds supply, or where data is too complex to be efficiently interpreted by human readers.[112] Several deep learning models have shown the capability to be roughly as accurate as healthcare professionals in identifying diseases through medical imaging, though few of the studies reporting these findings have been externally validated.[113] AI can also provide non-interpretive benefit to radiologists, such as reducing noise in images, creating high-quality images from lower doses of radiation, enhancing MR image quality,[114] and automatically assessing image quality.[115] Further research investigating the use of AI in nuclear medicine focuses on image reconstruction, anatomical landmarking, and the enablement of lower doses in imaging studies.[116] The analysis of images for supervised AI applications in radiology encompasses two primary techniques at present: (1) convolutional neural network-based analysis; and (2) utilization of radiomics.[112]\n AI is also used in breast imaging for analyzing screening mammograms and can participate in improving breast cancer detection rate[117] as well as reducing radiologist's reading workload.\n The trend of large health companies merging allows for greater health data accessibility. Greater health data lays the groundwork for the implementation of AI algorithms.\n A large part of industry focus of implementation of AI in the healthcare sector is in the clinical decision support systems. As more data is collected, machine learning algorithms adapt and allow for more robust responses and solutions.[111] Numerous companies are exploring the possibilities of the incorporation of big data in the healthcare industry. Many companies investigate the market opportunities through the realms of \"data assessment, storage, management, and analysis technologies\" which are all crucial parts of the healthcare industry.[126]\n The following are examples of large companies that have contributed to AI algorithms for use in healthcare:\n Digital consultant apps use AI to give medical consultation based on personal medical history and common medical knowledge. Users report their symptoms into the app, which uses speech recognition to compare against a database of illnesses. Babylon then offers a recommended action, taking into account the user's medical history. Entrepreneurs in healthcare have been effectively using seven business model archetypes to take AI solution[buzzword] to the marketplace. These archetypes depend on the value generated for the target user (e.g. patient focus vs. healthcare provider and payer focus) and value capturing mechanisms (e.g. providing information or connecting stakeholders).\n IFlytek launched a service robot \"Xiao Man\", which integrated artificial intelligence technology to identify the registered customer and provide personalized recommendations in medical areas. It also works in the field of medical imaging. Similar robots are also being made by companies such as UBTECH (\"Cruzr\") and Softbank Robotics (\"Pepper\").\n The Indian startup Haptik recently developed a WhatsApp chatbot which answers questions associated with the deadly coronavirus in India. Similarly, a software platform ChatBot in partnership with medtech startup Infermedica launched COVID-19 Risk Assessment ChatBot.[129]\n With the market for AI expanding constantly, large tech companies such as Apple, Google, Amazon, and Baidu all have their own AI research divisions, as well as millions of dollars allocated for acquisition of smaller AI based companies.[126] Many automobile manufacturers are beginning to use machine learning healthcare in their cars as well.[126] Companies such as BMW, GE, Tesla, Toyota, and Volvo all have new research campaigns to find ways of learning a driver's vital statistics to ensure they are awake, paying attention to the road, and not under the influence of substances.[126]\n Artificial intelligence continues to expand in its abilities to diagnose more people accurately in nations where fewer doctors are accessible to the public.  Many new technology companies such as SpaceX and the Raspberry Pi Foundation have enabled more developing countries to have access to computers and the internet than ever before.[130] With the increasing capabilities of AI over the internet, advanced machine learning algorithms can allow patients to get accurately diagnosed when they would previously have no way of knowing if they had a life-threatening disease or not.[130]\n Using AI in developing nations that do not have the resources will diminish the need for outsourcing and can improve patient care. AI can allow for not only diagnosis of patient in areas where healthcare is scarce, but also allow for a good patient experience by resourcing files to find the best treatment for a patient.[131] The ability of AI to adjust course as it goes also allows the patient to have their treatment modified based on what works for them; a level of individualized care that is nearly non-existent in developing countries.[131]\n While research on the use of AI in healthcare aims to validate its efficacy in improving patient outcomes before its broader adoption, its use may nonetheless introduce several new types of risk to patients and healthcare providers, such as algorithmic bias, Do not resuscitate implications, and other machine morality issues. AI may also compromise the protection of patients' rights, such as the right to informed consent and the right to medical data protection.[132] These challenges of the clinical use of AI have brought about a potential need for regulations. AI studies need to be completely and transparently reported to have value to inform regulatory approval. Depending on the phase of study, international consensus-based reporting guidelines (TRIPOD+AI,[133] DECIDE-AI,[134] CONSORT-AI[135]) have been developed to provide recommendations on the key details that need to be reported.\n Currently, there are regulations pertaining to the collection of patient data. This includes policies such as the Health Insurance Portability and Accountability Act (HIPAA) and the European General Data Protection Regulation (GDPR).[136] The GDPR pertains to patients within the EU and details the consent requirements for patient data use when entities collect patient healthcare data. Similarly, HIPAA protects healthcare data from patient records in the United States.[136] In May 2016, the White House announced its plan to host a series of workshops and formation of the National Science and Technology Council (NSTC) Subcommittee on Machine Learning and Artificial Intelligence. In October 2016, the group published The National Artificial Intelligence Research and Development Strategic Plan, outlining its proposed priorities for Federally-funded AI research and development (within government and academia). The report notes a strategic R&D plan for the subfield of health information technology is in development stages.\n There is concern that large language models can overwhelm people with both accurate health information and also misinformation, leading to potential challenges in public health. This calls for the need for policy and user guidance related to health information through AI.[137]\n The joint ITU-WHO Focus Group on Artificial Intelligence for Health (FG-AI4H) has built a platform - known as the ITU-WHO AI for Health Framework - for the testing and benchmarking of AI applications in health domain. As of November 2018, eight use cases are being benchmarked, including assessing breast cancer risk from histopathological imagery, guiding anti-venom selection from snake images, and diagnosing skin lesions.\n In January 2021, the US FDA published a new Action Plan, entitled Artificial Intelligence (AI) /Machine Learning (ML)-Based Software as a Medical Device (SaMD) Action Plan.[139] This plan lays out the FDA's future plans for regulation of medical devices that would include artificial intelligence in their software. There are five main actions the FDA plans to take to increase regulation: 1. Tailored Regulatory Framework for Ai/M:-based SaMD, 2. Good Machine Learning Practice (GMLP), 3. Patient-Centered Approach Incorporating Transparency to Users, 4. Regulatory Science Methods Related to Algorithm Bias & Robustness, and 5. Real-World Performance(RWP). This plan was in direct response to stakeholders' feedback on a 2019 discussion paper also published by the FDA.\n According to the U.S. Department of Health and Human Services, the Office for Civil Rights (OCR) has issued guidance on the ethical use of AI in healthcare. The guidance outlines four core ethical principles that must be followed: respect for autonomy, beneficence, non-maleficence, and justice. Respect for autonomy requires that individuals have control over their own data and decisions. Beneficence requires that AI be used to do good, such as improving the quality of care and reducing health disparities. Non-maleficence requires that AI be used to do no harm, such as avoiding discrimination in decisions. Finally, justice requires that AI be used fairly, such as using the same standards for decisions no matter a person's race, gender, or income level. Moreover, as of March 2021, the OCR hired a Chief Artificial Intelligence Officer (OCAIO) to pursue the \"implementation of the HHS AI strategy\".[140] The OCR also has issued rules and regulations to protect the privacy of individuals’ health information. These regulations require healthcare providers to follow certain privacy rules when using AI. The OCR also requires healthcare providers to keep a record of how they use AI and to ensure that their AI systems are secure. Overall, the U.S. has taken steps to protect individuals’ privacy and ethical issues related to AI in healthcare[141]\n The U.S. is not the only country to develop or initiate regulations of data privacy with AI. Other countries have implemented data protection regulations, more specifically with company privacy invasions. In Denmark, the Danish Expert Group on Data Ethics has adopted recommendations on 'Data for the Benefit of the People'. These recommendations are intended to encourage the responsible use of data in the business sector, with a focus on data processing. The recommendations include a focus on equality and non-discrimination with regard to bias in AI, as well as human dignity. The importance of human dignity is stressed, as it is said to outweigh profit and must be respected in all data processes[142]\n The European Union has implemented the General Data Protection Regulation (GDPR) to protect citizens' personal data, which applies to the use of AI in healthcare. In addition, the European Commission has established guidelines to ensure the ethical development of AI, including the use of algorithms to ensure fairness and transparency.[143] With GDPR, the European Union was the first to regulate AI through data protection legislation. The Union finds privacy as a fundamental human right, it wants to prevent unconsented and secondary uses of data by private or public health facilities. By streamlining access to personal data for health research and findings, they are able to instate the right and importance of patient privacy.[143] In the United States, the Health Insurance Portability and Accountability Act (HIPAA) requires organizations to protect the privacy and security of patient information. The Centers for Medicare and Medicaid Services have also released guidelines for the development of AI-based medical applications.[144]\n In order to effectively train Machine Learning and use AI in healthcare, massive amounts of data must be gathered. Acquiring this data, however, comes at the cost of patient privacy in most cases and is not well received publicly. For example, a survey conducted in the UK estimated that 63% of the population is uncomfortable with sharing their personal data in order to improve artificial intelligence technology.[136] The scarcity of real, accessible patient data is a hindrance that deters the progress of developing and deploying more artificial intelligence in healthcare.\n Furthermore, the lack of current regulations surrounding AI in the United States has generated concerns about mismanagement of patient data, such as with corporations utilizing patient data for financial gain. For example, Roche, a Swiss healthcare company, was found to have purchased healthcare data for approximately 2 million cancer patients at an estimated total cost of $1.9 billion.[145] Naturally, this generates questions of ethical concern; Is there a monetary price that can be set for data, and should it depend on its perceived value or contributions to science? Is it fair to patients to sell their data? These concerns were addressed in a survey conducted by the Pew Research Center in 2022 that asked Americans for their opinions about the increased presence of AI in their daily lives, and the survey estimated that 37% of Americans were more concerned than excited about such increased presence, with 8% of participants specifically associating their concern with \"people misusing AI\".[146] Ultimately, the current potential of artificial intelligence in healthcare is additionally hindered by  concerns about mismanagement of data collected, especially in the United States.\n A systematic review and thematic analysis in 2023 showed that most stakeholders including health professionals, patients, and the general public doubted that care involving AI could be empathetic.[16]\n According to a 2019 study, AI can replace up to 35% of jobs in the UK within the next 10 to 20 years.[147] However, of these jobs, it was concluded that AI has not eliminated any healthcare jobs so far. Though if AI were to automate healthcare-related jobs, the jobs most susceptible to automation would be those dealing with digital information, radiology, and pathology, as opposed to those dealing with doctor-to-patient interaction.[147]\n Automation can provide benefits alongside doctors as well. Some believe that AI may avert healthcare worker burnout and cognitive overload, so that doctors who take advantage of AI in healthcare will provide greater quality healthcare than doctors and medical establishments who do not.[148]\n Recently, there have been many discussions between healthcare experts in terms of AI and elder care. In relation to elder care, AI bots have been helpful in guiding older residents living in assisted living with entertainment and company. These bots are allowing staff in the home to have more one-on-one time with each resident, but the bots are also programmed with more ability in what they are able to do; such as knowing different languages and different types of care depending on the patient's conditions. The bot is an AI machine, which means it goes through the same training as any other machine - using algorithms to parse the given data, learn from it and predict the outcome in relation to what situation is at hand[149]\n Since AI makes decisions solely on the data it receives as input, it is important that this data represents accurate patient demographics. In a hospital setting, patients do not have full knowledge of how predictive algorithms are created or calibrated. Therefore, these medical establishments can unfairly code their algorithms to discriminate against minorities and prioritize profits rather than providing optimal care.[150] A recent scoping review identified 18 equity challenges along with 15 strategies that can be implemented to help address them when AI applications are developed using many-to-many mapping.[151]\n There can also be unintended bias in these algorithms that can exacerbate social and healthcare inequities.[150]  Since AI's decisions are a direct reflection of its input data, the data it receives must have accurate representation of patient demographics. White males are overly represented in medical data sets.[152] Therefore, having minimal patient data on minorities can lead to AI making more accurate predictions for majority populations, leading to unintended worse medical outcomes for minority populations.[153] Collecting data from minority communities can also lead to medical discrimination. For instance, HIV is a prevalent virus among minority communities and HIV status can be used to discriminate against patients.[152] In addition to biases that may arise from sample selection, different clinical systems used to collect data may also impact AI functionality. For example, radiographic systems and their outcomes (e.g., resolution) vary by provider. Moreover, clinician work practices, such as the positioning of the patient for radiography, can also greatly influence the data and make comparability difficult.[154] However, these biases are able to be eliminated through careful implementation and a methodical collection of representative data.\n A final source of bias, which has been called \"label choice bias\", arises when proxy measures are used to train algorithms, that build in bias against certain groups. For example, a widely used algorithm predicted health care costs as a proxy for health care needs, and used predictions to allocate resources to help patients with complex health needs. This introduced bias because Black patients have lower costs, even when they are just as unhealthy as White patients.[155] Solutions to the \"label choice bias\" aim to match the actual target (what the algorithm is predicting) more closely to the ideal target (what researchers want the algorithm to predict), so for the prior example, instead of predicting cost, researchers would focus on the variable of healthcare needs which is rather more significant. Adjusting the target led to almost double the number of Black patients being selected for the program.\n Research in the 1960s and 1970s produced the first problem-solving program, or expert system, known as Dendral.[156][157] While it was designed for applications in organic chemistry, it provided the basis for a subsequent system MYCIN,[158] considered one of the most significant early uses of artificial intelligence in medicine.[158][159] MYCIN and other systems such as INTERNIST-1 and CASNET did not achieve routine use by practitioners, however.[160]\n The 1980s and 1990s brought the proliferation of the microcomputer and new levels of network connectivity. During this time, there was a recognition by researchers and developers that AI systems in healthcare must be designed to accommodate the absence of perfect data and build on the expertise of physicians.[161] Approaches involving fuzzy set theory,[162] Bayesian networks,[163] and artificial neural networks,[164][165] have been applied to intelligent computing systems in healthcare.\n Medical and technological advancements occurring over this half-century period that have enabled the growth of healthcare-related applications of AI to include: \n"
    },
    {
        "title": "Artificial intelligence in mental health",
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_in_mental_health",
        "content": "\n Artificial intelligence in mental health is the application of artificial intelligence (AI), computational technologies and algorithms to supplement the understanding, diagnosis, and treatment of mental health disorders.[1][2][3] AI is becoming a ubiquitous force in everyday life which can be seen through frequent operation of models like ChatGPT.[4] Utilizing AI in the realm of mental health signifies a form of digital healthcare, in which, the goal is to increase accessibility in a world where mental health is becoming a growing concern.[5] Prospective ideas involving AI in mental health include identification and diagnosis of mental disorders, explication of electronic health records, creation of personalized treatment plans, and predictive analytics for suicide prevention.[5] [6] Learning how to apply AI in healthcare proves to be a difficult task with many challenges, thus it remains rarely used as efforts to bridge gaps are deliberated.[5]  In 2019, 1 in every 8 people, or 970 million people around the world were living with a mental disorder, with anxiety and depressive disorders the most common.[7] In 2020, the number of people living with anxiety and depressive disorders rose significantly because of the COVID-19 pandemic.[8] Additionally, the prevalence of mental health and addiction disorders exhibits a nearly equal distribution across genders, emphasizing the widespread nature of the issue.[9]\n The use of AI in mental health aims to support responsive and sustainable interventions against the global challenge posed by mental health disorders. Some issues common to the mental health industry are provider shortages, inefficient diagnoses, and ineffective treatments. The AI industry sees a market in healthcare, with a focus on mental health applications, which are projected to grow substantially, from $5 billion in 2020 to an estimated $45 billion by 2026. This growth indicates a growing interest in AI's ability to address critical challenges in mental healthcare provision through the development and implementation of innovative solutions.[10]\n As of 2020, there was no Food and Drug Administration (FDA) approval for AI in the field of Psychiatry.[11] There are two components of AI that are currently widely available for multiple applications, they are Machine learning (ML) and Natural language processing (NLP).\n Machine learning is a way for a computer to learn from large datasets presented to it, without explicit instructions. It requires structured databases; unlike scientific research which begins with a hypothesis, ML begins by looking at the data and finding its own hypothesis based on the patterns that it detects.[10] It then creates algorithms to be able to predict new information, based on the created algorithm and pattern that it was able to generate from the original dataset.[10] This model of AI is data driven, as it requires a huge amount of structured data—an obstacle in the field of psychiatry—with a lot of its patient encounters being based on interview and storytelling on the part of the patient.[10] Due to these limitations, some researchers have adopted a different method of developing ML models, a process named transfer learning, to be used in psychiatry based on trained models from different fields.[10]\n Transfer learning was used by researchers to develop a modified algorithm to detect alcoholism vs. non-alcoholism, and on another occasion, the same method was used to detect the signs of post-traumatic stress disorder.[12][13]\n One of the obstacles for AI is finding or creating an organized dataset to train and develop a useful algorithm. Natural language processing can be used to create such a dataset. NLP is a way for a computer to analyze text and speech, process semantic and lexical representations, as well as recognize speech and optical characters in data. This is crucial because many of the diagnoses and DSM-5 mental health disorders are diagnosed via speech in doctor-patient interviews, utilizing the clinician's skill for behavioral pattern recognition and translating it into medically relevant information to be documented and used for diagnoses. NLP can be used to extract, organize, and structure data from patients' everyday interactions, not just during a clinical visit, raises ethical and legal concerns over consent to personal data use and data anonymization.[14]\n AI with the use of NLP and ML can be used to help diagnose individuals with mental health disorders. It can be used to differentiate closely similar disorders based on their initial presentation to inform timely treatment before disease progression. For example, it may be able to differentiate unipolar from bipolar depression by analyzing imaging and medical scans.[10] AI also has the potential to identify novel diseases that were overlooked due to the heterogeneity of presentation of a single disorder.[10] Doctors may overlook the presentation of a disorder because while many people get diagnosed with depression, that depression may take on different forms and be enacted in different behaviors. AI can parse through the variability found in human expression data and potentially identify different types of depression.\n AI can be used to create accurate predictions for disease progression once diagnosed.[10] AI algorithms can also use data-driven approaches to build new clinical risk prediction models[15] without relying primarily on current theories of psychopathology. However, internal and external validation of an AI algorithm is essential for its clinical utility.[10] In fact, some studies have used neuroimaging, electronic health records, genetic data, and speech data to predict how depression would present in patients, their risk for suicidality or substance abuse, or functional outcomes.[10]\n In psychiatry, in many cases multiple drugs are trialed with the patients until the correct combination or regimen is reached to effectively treat their ailment—AI could theoretically be used to predict treatment response based on observed data collected from various sources. This use of AI could bypass all the time, effort, resources needed,  and burden placed on both patients and clinicians.[10]\n AI in mental health offers several benefits, such as:\n AI in mental health also poses several challenges, including:\n Mental health tech startups continue to lead investment activity in digital health despite the ongoing impacts of macroeconomic factors like inflation, supply chain disruptions, and interest rates.[18]\n According to CB Insights, State of Mental Health Tech 2021 Report, mental health tech companies raised $5.5 billion worldwide (324 deals), a 139% increase from the previous year that recorded 258 deals. \n A number of startups that are using AI in mental healthcare have closed notable deals in 2022 as well. Among them is the AI chatbot Wysa (20$ million in funding), BlueSkeye that is working on improving early diagnosis (£3.4 million), the Upheal smart notebook for mental health professionals (€1.068 million), and the AI-based mental health companion clare&me (€1 million).\n An analysis of the investment landscape and ongoing research suggests that we are likely to see the emergence of more emotionally intelligent AI bots and new mental health applications driven by AI prediction and detection capabilities.\n For instance, researchers at Vanderbilt University Medical Center in Tennessee, US, have developed an ML algorithm that uses a person’s hospital admission data, including age, gender, and past medical diagnoses, to make an 80% accurate prediction of whether this individual is likely to take their own life.[19] And researchers at the University of Florida are about to test their new AI platform aimed at making an accurate diagnosis in patients with early Parkinson’s disease.[20] Research is also underway to develop a tool combining explainable AI and deep learning to prescribe personalized treatment plans for children with schizophrenia.[21]\n In January of 2024, Cedars-Sinai physician-scientists developed a first-of-its-kind program that uses immersive virtual reality and generative artificial intelligence to provide mental health support. [2] The program is called XAIA which employs a large language model programmed to resemble a human therapist. [3]\n The University of Southern California is researching the effectiveness of a virtual therapist named Ellie. Through a webcam and microphone, this AI is able to process and analyze the emotional cues derived from the patient's face and the variation in expressions and tone of voice. [4]\n A team of Stanford Psychologists and AI experts created \"Woebot\". Woebot is an app that makes therapy sessions available 24/7. WoeBot tracks its users' mood through brief daily chat conversations and offers curated videos or word games to assist users in managing their mental health. [5] A Scandinavian team of software engineers and a clinical psychologist created \"Heartfelt Services\". Heartfelt Services is an application meant to simulate conventional talk therapy with an AI therapist. [22]\n AI in mental health is still an emerging field and there are still some concerns and criticisms about the use of AI in this area, such as:\n Though there is a large deal of progression to be made, the incorporation of AI in mental health emphasizes a necessity for legal and regulatory frameworks as advancements are made.[5] Constructing harmony amidst human engagement and AI is a difficult task, as there is a risk of healthcare becoming seemingly robotic and losing the humanness that has previously defined the field.[6] Furthermore, granting patients a feeling of security and safety is a priority considering AI's reliance on individual data to perform and respond to inputs. If not approached properly, the process of trying to increase accessibility could remove elements that negatively alter patient experience with receiving mental support.[6] To avoid veering in the wrong direction, more research should continue to develop a deeper understanding of where the incorporation of AI produces advantages and disadvantages.[4]\n"
    },
    {
        "title": "Artificial intelligence in industry",
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_in_industry",
        "content": "Industrial artificial intelligence, or industrial AI, usually refers to the application of artificial intelligence to industry and business. Unlike general artificial intelligence which is a frontier research discipline to build computerized systems that perform tasks requiring human intelligence, industrial AI is more concerned with the application of such technologies to address industrial pain-points for customer value creation, productivity improvement, cost reduction, site optimization, predictive analysis[1]  and insight discovery.[2]\n Artificial intelligence and machine learning  have become key enablers to leverage data in production in recent years due to a number of different factors: More affordable sensors and the automated process of data acquisition; More powerful computation capability of computers to perform more complex tasks at a faster speed with lower cost; Faster connectivity infrastructure and more accessible cloud services for data management and computing power outsourcing.[3]\n Possible applications of industrial AI and machine learning in the production domain can be divided into seven application areas:[4]\n Each application area can be further divided into specific application scenarios that describe concrete AI/ML scenarios in production. While some application areas have a direct connection to production processes, others cover production adjacent fields like logistics or the factory building.[4]\n An example from the application scenario Process Design & Innovation are collaborative robots. Collaborative robotic arms are able to learn the motion and path demonstrated by human operators and perform the same task.[5] Predictive and preventive maintenance through data-driven machine learning are exemplary application scenarios from the Machinery & Equipment application area.[4]\n In contrast to entirely virtual systems, in which ML applications are already widespread today, real-world production processes are characterized by the interaction between the virtual and the physical world. Data is recorded using sensors and processed on computational entities and, if desired, actions and decisions are translated back into the physical world via actuators or by human operators.[6] This poses major challenges for the application of ML in production engineering systems. These challenges are attributable to the encounter of process, data and model characteristics: The production domain's high reliability requirements, high risk and loss potential, the multitude of heterogeneous data sources and the non-transparency of ML model functionality impede a faster adoption of ML in real-world production processes.\n In particular, production data comprises a variety of different modalities, semantics and quality.[7] Furthermore, production systems are dynamic, uncertain and complex,[7] and engineering and manufacturing problems are data-rich but information-sparse.[8] Besides that, due the variety of use cases and data characteristics, problem-specific data sets are required, which are difficult to acquire, hindering both practitioners and academic researchers in this domain.[9]\n The domain of production engineering can be considered as a rather conservative industry when it comes to the adoption of advanced technology and their integration into existing processes. This is due to high demands on reliability of the production systems resulting from the potentially high economic harm of reduced process effectiveness due to e.g., additional unplanned downtime or insufficient product qualities. In addition, the specifics of machining equipment and products prevent area-wide adoptions across a variety of processes. Besides the technical reasons, the reluctant adoption of ML is fueled by a lack of IT and data science expertise across the domain.[4]\n The data collected in production processes mainly stem from frequently sampling sensors to estimate the state of a product, a process, or the environment in the real world. Sensor readings are susceptible to noise and represent only an estimate of the reality under uncertainty. Production data typically comprises multiple distributed data sources resulting in various data modalities (e.g., images from visual quality control systems, time-series sensor readings, or cross-sectional job and product information). The inconsistencies in data acquisition lead to low signal-to-noise ratios, low data quality and great effort in data integration, cleaning and management. In addition, as a result from mechanical and chemical wear of production equipment, process data is subject to various forms of data drifts.\n ML models are considered as black-box systems given their complexity and intransparency of input-output relation. This reduces the comprehensibility of the system behavior and thus also the acceptance by plant operators. Due to the lack of transparency and the stochasticity of these models, no deterministic proof of functional correctness can be achieved complicating the certification of production equipment. Given their inherent unrestricted prediction behavior, ML models are vulnerable against erroneous or manipulated data further risking the reliability of the production system because of lacking robustness and safety. In addition to high development and deployment costs, the data drifts cause high maintenance costs, which is disadvantageous compared to purely deterministic programs.\n The development of ML applications – starting with the identification and selection of the use case and ending with the deployment and maintenance of the application – follows dedicated phases that can be organized in standard process models. The process models assist in structuring the development process and defining requirements that must be met in each phase to enter the next phase. The standard processes can be classified into generic and domain-specific ones. Generic standard processes (e.g., CRISP-DM, ASUM-DM, KDD, SEMMA, or Team Data Science Process) describe a generally valid methodology and are thus independent of individual domains.[10] Domain-specific processes on the other hand consider specific peculiarities and challenges of special application areas.\n The Machine Learning Pipeline in Production is a domain-specific data science methodology that is inspired by the CRISP-DM model and was specifically designed to be applied in fields of engineering and production technology.[11] To address the core challenges of ML in engineering – process, data, and model characteristics – the methodology especially focuses on use-case assessment, achieving a common data and process understanding data integration, data preprocessing of real-world production data and the deployment and certification of real-world ML applications.\n The foundation of most artificial intelligence and machine learning applications in industrial settings are comprehensive datasets from the respective fields. Those datasets act as the basis for training the employed models.[7] In other domains, like computer vision, speech recognition or language models, extensive reference datasets (e.g. ImageNet, Librispeech,[12] The People's Speech) and data scraped from the open internet[13] are frequently used for this purpose. Such datasets rarely exist in the industrial context because of high confidentiality requirements [9] and high specificity of the data. Industrial applications of artificial intelligence are therefore often faced with the problem of data availability.[9]\n For these reasons, existing open datasets applicable to industrial applications, often originate from public institutions like governmental agencies or universities  and data analysis competitions hosted by companies. In addition to this, data sharing platforms exist. However, most of these platforms have no industrial focus and offer limited filtering abilities regarding industrial data sources.\n Artificial intelligence for business education refers to the academic programs offered by universities that integrate artificial intelligence (AI) with business management principles. These programs aim to prepare students for the increasing role of AI in business, equipping them with the skills necessary to apply AI technologies to areas such as predictive analytics, supply chain optimization, and decision-making. AI for business education programs are offered at both undergraduate and graduate levels by several universities globally.\n Bachelor in Artificial Intelligence for Business (BAIB), Bachelor in Computer Science and Artificial Intelligence (BCSAI), Master of Science in Artificial Intelligence in Business (MS-AIB) – These are new programs that are still in their first cohorts and have yet to prove themselves in the industry. The undergraduate degrees are often offered in conjuction with a BBA as a 5-year double degree program, the undergraduate degrees are going through the acreditation processes in their respective countries.\n Programs that combine AI with business studies vary by institution and degree level. Below are some notable examples:\n The Bachelor in Artificial Intelligence for Business (BAIB) - This program, started by Esade focuses on the integration of AI and machine learning with core business disciplines such as management, marketing, and finance. The Esade Business School is a highly regarded institution for its business innovation, sustainability focus and future-proof outlook. During the BBA+BAIB, students are trained to apply AI in business environments to improve efficiency, innovation, and decision-making.[14]\n Bachelor in Computer Science and Artificial Intelligence (BCSAI) – Offered along with a BBA by IE University, the BCSAI combines foundational studies in computer science with a specialization in artificial intelligence. The program also provides a strong grounding in business principles, preparing graduates to create AI solutions for business problems and drive technological innovation in the business world.[15]\n Master in Artificial Intelligence for Business (MS-AIB) – Arizona State University (ASU) offers a graduate-level program focused on AI applications in business environments. This degree explores advanced topics such as AI-driven decision-making, big data analysis, and the ethical implications of AI in business. The program is designed for professionals seeking to leverage AI technologies to transform business practices and improve efficiency.\n These programs typically include a combination of AI and business courses. Core subjects often cover topics such as machine learning, data science, business strategy, and financial management. The programs aim to give students a broad understanding of AI applications within a business environment, while also allowing them to specialize in areas such as supply chain management, marketing analytics, and AI-driven innovation.\n In addition to technical courses, many programs include practical training, such as internships, real-world AI projects, and industry case studies. This helps students gain practical experience in applying AI tools and techniques to solve business challenges.\n Many universities offering these degrees hold accreditation from recognized educational bodies, ensuring that their programs meet rigorous academic and industry standards. For example, ESADE and IE University are both accredited by institutions such as EQUIS and AACSB, which evaluate the quality of business education programs. Similarly, Arizona State University holds accreditation for its graduate programs in business and technology.\n"
    },
    {
        "title": "Machine translation",
        "url": "https://en.wikipedia.org/wiki/Machine_translation",
        "content": "\n Machine translation is use of computational techniques to translate text or speech from one language to another, including the contextual, idiomatic and pragmatic nuances of both languages.\n Early approaches were mostly rule-based or statistical. These methods have since been superseded by neural machine translation[1] and large language models.[2]\n The origins of machine translation can be traced back to the work of Al-Kindi, a ninth-century Arabic cryptographer who developed techniques for systemic language translation, including cryptanalysis, frequency analysis, and probability and statistics, which are used in modern machine translation.[3] The idea of machine translation later appeared in the 17th century. In 1629, René Descartes proposed a universal language, with equivalent ideas in different tongues sharing one symbol.[4]\n The idea of using digital computers for translation of natural languages was proposed as early as 1947 by England's A. D. Booth[5] and Warren Weaver at Rockefeller Foundation in the same year. \"The memorandum written by Warren Weaver in 1949 is perhaps the single most influential publication in the earliest days of machine translation.\"[6][7] Others followed. A demonstration was made in 1954 on the APEXC machine at Birkbeck College (University of London) of a rudimentary translation of English into French. Several papers on the topic were published at the time, and even articles in popular journals (for example an article by Cleave and Zacharov in the September 1955 issue of Wireless World).  A similar application, also pioneered at Birkbeck College at the time, was reading and composing Braille texts by computer.\n The first researcher in the field, Yehoshua Bar-Hillel, began his research at MIT (1951). A Georgetown University MT research team, led by Professor Michael Zarechnak, followed (1951) with a public demonstration of its Georgetown-IBM experiment system in 1954. MT research programs popped up in Japan[8][9] and Russia (1955), and the first MT conference was held in London (1956).[10][11]\n David G. Hays \"wrote about computer-assisted language processing as early as 1957\" and \"was project leader on computational linguistics\nat Rand from 1955 to 1968.\"[12]\n Researchers continued to join the field as the Association for Machine Translation and Computational Linguistics was formed in the U.S. (1962) and the National Academy of Sciences formed the Automatic Language Processing Advisory Committee (ALPAC) to study MT (1964). Real progress was much slower, however, and after the ALPAC report (1966), which found that the ten-year-long research had failed to fulfill expectations, funding was greatly reduced.[13] According to a 1972 report by the Director of Defense Research and Engineering (DDR&E), the feasibility of large-scale MT was reestablished by the success of the Logos MT system in translating military manuals into Vietnamese during that conflict.\n The French Textile Institute also used MT to translate abstracts from and into French, English, German and Spanish (1970); Brigham Young University started a project to translate Mormon texts by automated translation (1971).\n SYSTRAN, which \"pioneered the field under contracts from the U.S. government\"[14] in the 1960s, was used by Xerox to translate technical manuals (1978). Beginning in the late 1980s, as computational power increased and became less expensive, more interest was shown in statistical models for machine translation. MT became more popular after the advent of computers.[15] SYSTRAN's first implementation system was implemented in 1988 by the online service of the French Postal Service called Minitel.[16] Various computer based translation companies were also launched, including Trados (1984), which was the first to develop and market Translation Memory technology (1989), though this is not the same as MT. The first commercial MT system for Russian / English / German-Ukrainian was developed at Kharkov State University (1991).\n By 1998, \"for as little as $29.95\" one could \"buy a program for translating in one direction between English and a major European language of\nyour choice\" to run on a PC.[14]\n MT on the web started with SYSTRAN offering free translation of small texts (1996) and then providing this via AltaVista Babelfish,[14] which racked up 500,000 requests a day (1997).[17] The second free translation service on the web was Lernout & Hauspie's GlobaLink.[14] Atlantic Magazine wrote in 1998 that \"Systran's Babelfish and GlobaLink's Comprende\" handled\n\"Don't bank on it\" with a \"competent performance.\"[18]\n Franz Josef Och (the future head of Translation Development AT Google) won DARPA's speed MT competition (2003).[19] More innovations during this time included MOSES, the open-source statistical MT engine (2007), a text/SMS translation service for mobiles in Japan (2008), and a mobile phone with built-in speech-to-speech translation functionality for English, Japanese and Chinese (2009). In 2012, Google announced that Google Translate translates roughly enough text to fill 1 million books in one day.\n Before the advent of deep learning methods, statistical methods required a lot of rules accompanied by morphological, syntactic, and semantic annotations.\n The rule-based machine translation approach was used mostly in the creation of dictionaries and grammar programs. Its biggest downfall was that everything had to be made explicit: orthographical variation and erroneous input must be made part of the source language analyser in order to cope with it, and lexical selection rules must be written for all instances of ambiguity.\n Transfer-based machine translation was similar to interlingual machine translation in that it created a translation from an intermediate representation that simulated the meaning of the original sentence. Unlike interlingual MT, it depended partially on the language pair involved in the translation.\n Interlingual machine translation was one instance of rule-based machine-translation approaches.  In this approach, the source language, i.e. the text to be translated, was transformed into an interlingual language, i.e. a \"language neutral\" representation that is independent of any language. The target language was then generated out of the interlingua. The only interlingual machine translation system that was made operational at the commercial level was the KANT system (Nyberg and Mitamura, 1992), which was designed to translate Caterpillar Technical English (CTE) into other languages.\n Machine translation used a method based on dictionary entries, which means that the words were translated as they are by a dictionary.\n Statistical machine translation tried to generate translations using statistical methods based on bilingual text corpora, such as the Canadian Hansard corpus, the English-French record of the Canadian parliament and EUROPARL, the record of the European Parliament. Where such corpora were available, good results were achieved translating similar texts, but such corpora were rare for many language pairs. The first statistical machine translation software was CANDIDE from IBM. In 2005, Google improved its internal translation capabilities by using approximately 200 billion words from United Nations materials to train their system; translation accuracy improved.[20]\n SMT's biggest downfall included it being dependent upon huge amounts of parallel texts, its problems with morphology-rich languages (especially with translating into such languages), and its inability to correct singleton errors.\n Some work has been done in the utilization of multiparallel corpora, that is a body of text that has been translated into 3 or more languages. Using these methods, a text that has been translated into 2 or more languages may be utilized in combination to provide a more accurate translation into a third language compared with if just one of those source languages were used alone.[21][22][23]\n A deep learning-based approach to MT, neural machine translation has made rapid progress in recent years. However, the current consensus is that the so-called human parity achieved is not real, being based wholly on limited domains, language pairs, and certain test benchmarks[24] i.e., it lacks statistical significance power.[25]\n Translations by neural MT tools like DeepL Translator, which is thought to usually deliver the best machine translation results as of 2022, typically still need post-editing by a human.[26][27][28]\n Instead of training specialized translation models on parallel datasets, one can also directly prompt generative large language models like GPT to translate a text.[29][30][31] This approach is considered promising,[32] but is still more resource-intensive than specialized translation models.\n Studies using human evaluation (e.g. by professional literary translators or human readers) have systematically identified various issues with the latest advanced MT outputs.[31] Common issues include the translation of ambiguous parts whose correct translation requires common sense-like semantic language processing or context.[31] There can also be errors in the source texts, missing high-quality training data and the severity of frequency of several types of problems may not get reduced with techniques used to date, requiring some level of human active participation.\n Word-sense disambiguation concerns finding a suitable translation when a word can have more than one meaning. The problem was first raised in the 1950s by Yehoshua Bar-Hillel.[33] He pointed out that without a \"universal encyclopedia\", a machine would never be able to distinguish between the two meanings of a word.[34] Today there are numerous approaches designed to overcome this problem. They can be approximately divided into \"shallow\" approaches and \"deep\" approaches.\n Shallow approaches assume no knowledge of the text. They simply apply statistical methods to the words surrounding the ambiguous word. Deep approaches presume a comprehensive knowledge of the word. So far, shallow approaches have been more successful.[35]\n Claude Piron, a long-time translator for the United Nations and the World Health Organization, wrote that machine translation, at its best, automates the easier part of a translator's job; the harder and more time-consuming part usually involves doing extensive research to resolve ambiguities in the source text, which the grammatical and lexical exigencies of the target language require to be resolved:\n Why does a translator need a whole workday to translate five pages, and not an hour or two? ..... About 90% of an average text corresponds to these simple conditions.  But unfortunately, there's the other 10%.  It's that part that requires six [more] hours of work.  There are ambiguities one has to resolve.  For instance, the author of the source text, an Australian physician, cited the example of an epidemic which was declared during World War II in a \"Japanese prisoners of war camp\".  Was he talking about an American camp with Japanese prisoners or a Japanese camp with American prisoners?  The English has two senses.  It's necessary therefore to do research, maybe to the extent of a phone call to Australia.[36]\n The ideal deep approach would require the translation software to do all the research necessary for this kind of disambiguation on its own; but this would require a higher degree of AI than has yet been attained.  A shallow approach which simply guessed at the sense of the ambiguous English phrase that Piron mentions (based, perhaps, on which kind of prisoner-of-war camp is more often mentioned in a given corpus) would have a reasonable chance of guessing wrong fairly often.  A shallow approach that involves \"ask the user about each ambiguity\" would, by Piron's estimate, only automate about 25% of a professional translator's job, leaving the harder 75% still to be done by a human.\n One of the major pitfalls of MT is its inability to translate non-standard language with the same accuracy as standard language. Heuristic or statistical based MT takes input from various sources in standard form of a language. Rule-based translation, by nature, does not include common non-standard usages. This causes errors in translation from a vernacular source or into colloquial language. Limitations on translation from casual speech present issues in the use of machine translation in mobile devices.\n In information extraction, named entities, in a narrow sense, refer to concrete or abstract entities in the real world such as people, organizations, companies, and places that have a proper name: George Washington, Chicago, Microsoft.  It also refers to expressions of time, space and quantity such as 1 July 2011, $500.\n In the sentence \"Smith is the president of Fabrionix\" both Smith and Fabrionix are named entities, and can be further qualified via first name or other information; \"president\" is not, since Smith could have earlier held another position at Fabrionix, e.g. Vice President.\nThe term rigid designator is what defines these usages for analysis in statistical machine translation.\n Named entities must first be identified in the text; if not, they may be erroneously translated as common nouns, which would most likely not affect the BLEU rating of the translation but would change the text's human readability.[37] They may be omitted from the output translation, which would also have implications for the text's readability and message.\n Transliteration includes finding the letters in the target language that most closely correspond to the name in the source language.  This, however, has been cited as sometimes worsening the quality of translation.[38] For \"Southern California\" the first word should be translated directly, while the second word should be transliterated.  Machines often transliterate both because they treated them as one entity.  Words like these are hard for machine translators, even those with a transliteration component, to process.\n Use of a \"do-not-translate\" list, which has the same end goal – transliteration as opposed to translation.[39]  still relies on correct identification of named entities.\n A third approach is a class-based model. Named entities are replaced with a token to represent their \"class\"; \"Ted\"  and \"Erica\" would both be replaced with \"person\" class token. Then the statistical distribution and use of person names, in general, can be analyzed instead of looking at the distributions of \"Ted\" and \"Erica\" individually, so that the probability of a given name in a specific language will not affect the assigned probability of a translation. A study by Stanford on improving this area of translation gives the examples that different probabilities will be assigned to \"David is going for a walk\" and \"Ankit is going for a walk\" for English as a target language due to the different number of occurrences for each name in the training data. A frustrating outcome of the same study by Stanford (and other attempts to improve named recognition translation) is that many times, a decrease in the BLEU scores for translation will result from the inclusion of methods for named entity translation.[39]\n While no system provides the ideal of fully automatic high-quality machine translation of unrestricted text, many fully automated systems produce reasonable output.[40][41][42] The quality of machine translation is substantially improved if the domain is restricted and controlled.[43] This enables using machine translation as a tool to speed up and simplify translations, as well as producing flawed but useful low-cost or ad-hoc translations.\n Machine translation applications have also been released for most mobile devices, including mobile telephones, pocket PCs, PDAs, etc. Due to their portability, such instruments have come to be designated as mobile translation tools enabling mobile business networking between partners speaking different languages, or facilitating both foreign language learning and unaccompanied traveling to foreign countries without the need of the intermediation of a human translator.\n For example, the Google Translate app allows foreigners to quickly translate text in their surrounding via augmented reality using the smartphone camera that overlays the translated text onto the text.[44] It can also recognize speech and then translate it.[45]\n Despite their inherent limitations, MT programs are used around the world. Probably the largest institutional user is the European Commission. In 2012, with an aim to replace a rule-based MT by newer, statistical-based MT@EC, The European Commission contributed 3.072 million euros (via its ISA programme).[46]\n Machine translation has also been used for translating Wikipedia articles and could play a larger role in creating, updating, expanding, and generally improving articles in the future, especially as the MT capabilities may improve. There is a \"content translation tool\" which allows editors to more easily translate articles across several select languages.[47][48][49] English-language articles are thought to usually be more comprehensive and less biased than their non-translated equivalents in other languages.[50] As of 2022, English Wikipedia has over 6.5 million articles while the German and Swedish Wikipedias each only have over 2.5 million articles,[51] each often far less comprehensive.\n Following terrorist attacks in Western countries, including 9-11, the U.S. and its allies have been most interested in developing  Arabic machine translation programs, but also in translating Pashto and Dari languages.[citation needed] Within these languages, the focus is on key phrases and quick communication between military members and civilians through the use of mobile phone apps.[52] The Information Processing Technology Office in DARPA hosted programs like TIDES and Babylon translator. US Air Force has awarded a $1 million contract to develop a language translation technology.[53]\n The notable rise of social networking on the web in recent years has created yet another niche for the application of machine translation software – in utilities such as Facebook, or instant messaging clients such as Skype, Google Talk, MSN Messenger, etc. – allowing users speaking different languages to communicate with each other.\n Lineage W gained popularity in Japan because of its machine translation features allowing players from different countries to communicate.[54]\n Despite being labelled as an unworthy competitor to human translation in 1966 by the Automated Language Processing Advisory Committee put together by the United States government,[55] the quality of machine translation has now been improved to such levels that its application in online collaboration and in the medical field are being investigated. The application of this technology in medical settings where human translators are absent is another topic of research, but difficulties arise due to the importance of accurate translations in medical diagnoses.[56]\n Researchers caution that the use of machine translation in medicine could risk mistranslations that can be dangerous in critical situations.[57][58] Machine translation can make it easier for doctors to communicate with their patients in day to day activities, but it is recommended to only use machine translation when there is no other alternative, and that translated medical texts should be reviewed by human translators for accuracy.[59][60]\n Legal language poses a significant challenge to machine translation tools due to its precise nature and atypical use of normal words. For this reason, specialized algorithms have been developed for use in legal contexts.[61] Due to the risk of mistranslations arising from machine translators, researchers recommend that machine translations should be reviewed by human translators for accuracy, and some courts prohibit its use in formal proceedings.[62]\n The use of machine translation in law has raised concerns about translation errors and client confidentiality. Lawyers who use free translation tools such as Google Translate may accidentally violate client confidentiality by exposing private information to the providers of the translation tools.[61] In addition, there have been arguments that consent for a police search that is obtained with machine translation is invalid, with different courts issuing different verdicts over whether or not these arguments are valid.[57]\n The advancements in convolutional neural networks in recent years and in low resource machine translation (when only a very limited amount of data and examples are available for training) enabled machine translation for ancient languages, such as Akkadian and its dialects Babylonian and Assyrian.[63]\n There are many factors that affect how machine translation systems are evaluated. These factors include the intended use of the translation, the nature of the machine translation software, and the nature of the translation process.\n Different programs may work well for different purposes. For example, statistical machine translation (SMT) typically outperforms example-based machine translation (EBMT), but researchers found that when evaluating English to French translation, EBMT performs better.[64] The same concept applies for technical documents, which can be more easily translated by SMT because of their formal language.\n In certain applications, however, e.g., product descriptions written in a controlled language, a dictionary-based machine-translation system has produced satisfactory translations that require no human intervention save for quality inspection.[65]\n There are various means for evaluating the output quality of machine translation systems. The oldest is the use of human judges[66] to assess a translation's quality. Even though human evaluation is time-consuming, it is still the most reliable method to compare different systems such as rule-based and statistical systems.[67] Automated means of evaluation include BLEU, NIST, METEOR, and LEPOR.[68]\n Relying exclusively on unedited machine translation ignores the fact that communication in human language is context-embedded and that it takes a person to comprehend the context of the original text with a reasonable degree of probability. It is certainly true that even purely human-generated translations are prone to error. Therefore, to ensure that a machine-generated translation will be useful to a human being and that publishable-quality translation is achieved, such translations must be reviewed and edited by a human.[69] The late Claude Piron wrote that machine translation, at its best, automates the easier part of a translator's job; the harder and more time-consuming part usually involves doing extensive research to resolve ambiguities in the source text, which the grammatical and lexical exigencies of the target language require to be resolved. Such research is a necessary prelude to the pre-editing necessary in order to provide input for machine-translation software such that the output will not be meaningless.[70]\n In addition to disambiguation problems, decreased accuracy can occur due to varying levels of training data for machine translating programs. Both example-based and statistical machine translation rely on a vast array of real example sentences as a base for translation, and when too many or too few sentences are analyzed accuracy is jeopardized. Researchers found that when a program is trained on 203,529 sentence pairings, accuracy actually decreases.[64] The optimal level of training data seems to be just over 100,000 sentences, possibly because as training data increases, the number of possible sentences increases, making it harder to find an exact translation match.\n Flaws in machine translation have been noted for their entertainment value. Two videos uploaded to YouTube in April 2017 involve two Japanese hiragana characters えぐ (e and gu) being repeatedly pasted into Google Translate, with the resulting translations quickly degrading into nonsensical phrases such as \"DECEARING EGG\" and \"Deep-sea squeeze trees\", which are then read in increasingly absurd voices;[71][72] the full-length version of the video currently has 6.9 million views as of March 2022.[update][73]\n In the early 2000s, options for machine translation between spoken and signed languages were severely limited. It was a common belief that deaf individuals could use traditional translators. However, stress, intonation, pitch, and timing are conveyed much differently in spoken languages compared to signed languages. Therefore, a deaf individual may misinterpret or become confused about the meaning of written text that is based on a spoken language.[74]\n Researchers Zhao, et al. (2000), developed a prototype called TEAM (translation from English to ASL by machine) that completed English to American Sign Language (ASL) translations. The program would first analyze the syntactic, grammatical, and morphological aspects of the English text. Following this step, the program accessed a sign synthesizer, which acted as a dictionary for ASL. This synthesizer housed the process one must follow to complete ASL signs, as well as the meanings of these signs. Once the entire text is analyzed and the signs necessary to complete the translation are located in the synthesizer, a computer generated human appeared and would use ASL to sign the English text to the user.[74]\n Only works that are original are subject to copyright protection, so some scholars claim that machine translation results are not entitled to copyright protection because MT does not involve creativity.[75] The copyright at issue is for a derivative work; the author of the original work in the original language does not lose his rights when a work is translated: a translator must have permission to publish a translation.[citation needed]\n"
    },
    {
        "title": "Artificial intelligence arms race",
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_arms_race",
        "content": "A military artificial intelligence arms race is an arms race between two or more states to develop and deploy lethal autonomous weapons systems (LAWS). Since the mid-2010s, many analysts have noted the emergence of such an arms race between superpowers for better military AI,[1][2] driven by increasing geopolitical and military tensions. \n An AI arms race is sometimes placed in the context of an AI Cold War between the United States, Russia, and China.[3]\n Lethal autonomous weapons systems use artificial intelligence to identify and kill human targets without human intervention.[4] LAWS have colloquially been called \"slaughterbots\" or \"killer robots\". Broadly, any competition for superior AI is sometimes framed as an \"arms race\".[5][6] Advantages in military AI overlap with advantages in other sectors, as countries pursue both economic and military advantages.[7]\n In 2014, AI specialist Steve Omohundro warned that \"An autonomous weapons arms race is already taking place\".[8] According to Siemens, worldwide military spending on robotics was US$5.1 billion in 2010 and US$7.5 billion in 2015.[9][10]\n China became a top player in artificial intelligence research in the 2010s. According to the Financial Times, in 2016, for the first time, China published more AI papers than the entire European Union. When restricted to number of AI papers in the top 5% of cited papers, China overtook the United States in 2016 but lagged behind the European Union.[11] 23% of the researchers presenting at the 2017 American Association for the Advancement of Artificial Intelligence (AAAI) conference were Chinese.[12] Eric Schmidt, the former chairman of Alphabet, has predicted China will be the leading country in AI by 2025.[13]\n One risk concerns the AI race itself, whether or not the race is won by any one group. There are strong incentives for development teams to cut corners with regard to the safety of the system, increasing the risk of critical failures and unintended consequences.[14][15] This is in part due to the perceived advantage of being the first to develop advanced AI technology. One team appearing to be on the brink of a breakthrough can encourage other teams to take shortcuts, ignore precautions and deploy a system that is less ready. Some argue that using \"race\" terminology at all in this context can exacerbate this effect.[16]\n Another potential danger of an AI arms race is the possibility of losing control of the AI systems; the risk is compounded in the case of a race to artificial general intelligence, which may present an existential risk.[16] In 2023, a United States Air Force official reportedly said that during a computer test, a simulated AI drone killed the human character operating it. The USAF later said the official had misspoken and that it never conducted such simulations.[17]\n A third risk of an AI arms race is whether or not the race is actually won by one group. The concern is regarding the consolidation of power and technological advantage in the hands of one group.[16] A US government report argued that \"AI-enabled capabilities could be used to threaten  critical infrastructure, amplify disinformation campaigns, and wage war\"[18]:1, and that \"global stability and nuclear deterrence could be undermined\".[18]:11\n Russian General Viktor Bondarev, commander-in-chief of the Russian air force, stated that as early as February 2017, Russia was working on AI-guided missiles that could decide to switch targets mid-flight.[19] The Military-Industrial Commission of Russia has approved plans to derive 30 percent of Russia's combat power from remote controlled and AI-enabled robotic platforms by 2030.[20] Reports by state-sponsored Russian media on potential military uses of AI increased in mid-2017.[21] In May 2017, the CEO of Russia's Kronstadt Group, a defense contractor, stated that \"there already exist completely autonomous AI operation systems that provide the means for UAV clusters, when they fulfill missions autonomously, sharing tasks between them, and interact\", and that it is inevitable that \"swarms of drones\" will one day fly over combat zones.[22] Russia has been testing several autonomous and semi-autonomous combat systems, such as Kalashnikov's \"neural net\" combat module, with a machine gun, a camera, and an AI that its makers claim can make its own targeting judgements without human intervention.[23]\n In September 2017, during a National Knowledge Day address to over a million students in 16,000 Russian schools, Russian President Vladimir Putin stated \"Artificial intelligence is the future, not only for Russia but for all humankind... Whoever becomes the leader in this sphere will become the ruler of the world\". Putin also said it would be better to prevent any single actor achieving a monopoly, but that if Russia became the leader in AI, they would share their \"technology with the rest of the world, like we are doing now with atomic and nuclear technology\".[24][25][26]\n Russia is establishing a number of organizations devoted to the development of military AI. In March 2018, the Russian government released a 10-point AI agenda, which calls for the establishment of an AI and Big Data consortium, a Fund for Analytical Algorithms and Programs, a state-backed AI training and education program, a dedicated AI lab, and a National Center for Artificial Intelligence, among other initiatives.[27] In addition, Russia recently created a defense research organization, roughly equivalent to DARPA, dedicated to autonomy and robotics called the Foundation for Advanced Studies, and initiated an annual conference on \"Robotization of the Armed Forces of the Russian Federation.\"[28][29]\n The Russian military has been researching a number of AI applications, with a heavy emphasis on semiautonomous and autonomous vehicles. In an official statement on November 1, 2017, Viktor Bondarev, chairman of the Federation Council's Defense and Security Committee, stated that \"artificial intelligence will be able to replace a soldier on the battlefield and a pilot in an aircraft cockpit\" and later noted that \"the day is nearing when vehicles will get artificial intelligence.\"[30] Bondarev made these remarks in close proximity to the successful test of Nerehta, an crewless Russian ground vehicle that reportedly \"outperformed existing [crewed] combat vehicles.\" Russia plans to use Nerehta as a research and development platform for AI and may one day deploy the system in combat, intelligence gathering, or logistics roles.[31] Russia has also reportedly built a combat module for crewless ground vehicles that is capable of autonomous target identification—and, potentially, target engagement—and plans to develop a suite of AI-enabled autonomous systems.[32][33][29]\n In addition, the Russian military plans to incorporate AI into crewless aerial, naval, and undersea vehicles and is currently developing swarming capabilities.[28] It is also exploring innovative uses of AI for remote sensing and electronic warfare, including adaptive frequency hopping, waveforms, and countermeasures.[34][35] Russia has also made extensive use of AI technologies for domestic propaganda and surveillance, as well as for information operations directed against the United States and U.S. allies.[36][37][29]\n The Russian government has strongly rejected any ban on lethal autonomous weapon systems, suggesting that such an international ban could be ignored.[38][39]\n China is pursuing a strategic policy of military-civil fusion on AI for global technological supremacy.[18][40] According to a February 2019 report by Gregory C. Allen of the Center for a New American Security, China's leadership – including paramount leader Xi Jinping – believes that being at the forefront in AI technology is critical to the future of global military and economic power competition.[7] Chinese military officials have said that their goal is to incorporate commercial AI technology to \"narrow the gap between the Chinese military and global advanced powers.\"[7] The close ties between Silicon Valley and China, and the open nature of the American research community, has made the West's most advanced AI technology easily available to China; in addition, Chinese industry has numerous home-grown AI accomplishments of its own, such as Baidu passing a notable Chinese-language speech recognition capability benchmark in 2015.[41] As of 2017, Beijing's roadmap aims to create a $150 billion AI industry by 2030.[11] Before 2013, Chinese defense procurement was mainly restricted to a few conglomerates; however, as of 2017, China often sources sensitive emerging technology such as drones and artificial intelligence from private start-up companies.[42] An October 2021 report by the Center for Security and Emerging Technology found that \"Most of the [Chinese military]'s AI equipment suppliers are not state-owned defense enterprises, but private Chinese tech companies founded after 2010.\"[43] The report estimated that Chinese military spending on AI exceeded $1.6 billion each year.[43] The Japan Times reported in 2018 that annual private Chinese investment in AI is under $7 billion per year. AI startups in China received nearly half of total global investment in AI startups in 2017; the Chinese filed for nearly five times as many AI patents as did Americans.[44]\n China published a position paper in 2016 questioning the adequacy of existing international law to address the eventuality of fully autonomous weapons, becoming the first permanent member of the U. N. Security Council to broach the issue.[45] In 2018, Xi called for greater international cooperation in basic AI research.[46] Chinese officials have expressed concern that AI such as drones could lead to accidental war, especially in the absence of international norms.[47] In 2019, former United States Secretary of Defense Mark Esper lashed out at China for selling drones capable of taking life with no human oversight.[48]\n In 2014, former Secretary of Defense Chuck Hagel posited the \"Third Offset Strategy\" that rapid advances in artificial intelligence will define the next generation of warfare.[49] According to data science and analytics firm Govini, the U.S. Department of Defense (DoD) increased investment in artificial intelligence, big data and cloud computing from $5.6 billion in 2011 to $7.4 billion in 2016.[50] However, the civilian NSF budget for AI saw no increase in 2017.[11] Japan Times reported in 2018 that the United States private investment is around $70 billion per year.[44] The November 2019 'Interim Report' of the United States' National Security Commission on Artificial Intelligence confirmed that AI is critical to US technological military superiority.[18]\n The U.S. has many military AI combat programs, such as the Sea Hunter autonomous warship, which is designed to operate for extended periods at sea without a single crew member, and to even guide itself in and out of port.[23] From 2017, a temporary US Department of Defense directive requires a human operator to be kept in the loop when it comes to the taking of human life by autonomous weapons systems.[51] On October 31, 2019, the United States Department of Defense's Defense Innovation Board published the draft of a report recommending principles for the ethical use of artificial intelligence by the Department of Defense that would ensure a human operator would always be able to look into the 'black box' and understand the kill-chain process. However, a major concern is how the report will be implemented.[52]\n The Joint Artificial Intelligence Center (JAIC) (pronounced \"jake\")[53] is an American organization on exploring the usage of AI (particularly edge computing), Network of Networks, and AI-enhanced communication, for use in actual combat.[54][55][56][57] It is a subdivision of the United States Armed Forces and was created in June 2018. The organization's stated objective is to \"transform the US Department of Defense by accelerating the delivery and adoption of AI to achieve mission impact at scale. The goal is to use AI to solve large and complex problem sets that span multiple combat systems; then, ensure the combat Systems and Components have real-time access to ever-improving libraries of data sets and tools.\"[55]\n In 2023 Microsoft pitched the DoD to use DALL-E models to train its battlefield management system.[58] OpenAI, the developer of DALL-E, removed the blanket ban on military and warfare use from its usage policies in January 2024.[59]\n Project Maven is a Pentagon project involving using machine learning and engineering talent to distinguish people and objects in drone videos,[60] apparently giving the government real-time battlefield command and control, and the ability to track, tag and spy on targets without human involvement. Initially the effort was led by Robert O. Work who was concerned about China's military use of the emerging technology.[61]  Reportedly, Pentagon development stops short of acting as an AI weapons system capable of firing on self-designated targets.[62] The project was established in a memo by the U.S. Deputy Secretary of Defense on 26 April 2017.[63] Also known as the Algorithmic Warfare Cross Functional Team,[64] it is, according to Lt. Gen. of the United States Air Force Jack Shanahan in November 2017, a project \"designed to be that pilot project, that pathfinder, that spark that kindles the flame front of artificial intelligence across the rest of the [Defense] Department\".[65] Its chief, U.S. Marine Corps Col. Drew Cukor, said: \"People and computers will work symbiotically to increase the ability of weapon systems to detect objects.\"[66] Project Maven has been noted by allies, such as Australia's Ian Langford, for the ability to identify adversaries by harvesting data from sensors on UAVs and satellite.[67] At the second Defense One Tech Summit in July 2017, Cukor also said that the investment in a \"deliberate workflow process\" was funded by the Department [of Defense] through its \"rapid acquisition authorities\" for about \"the next 36 months\".[68]\n In 2015, the UK government opposed a ban on lethal autonomous weapons, stating that \"international humanitarian law already provides sufficient regulation for this area\", but that all weapons employed by UK armed forces would be \"under human oversight and control\".[69]\n Israel makes extensive use of AI for military applications specially during the Israel-Hamas war. The main AI systems used for target identification are the Gospel and Lavender. Lavender developed by the Unit 8200 identifies and creates a database of individuals mostly low-ranking militants of Hamas and the Palestinian Islamic Jihad and has a 90% accuracy rate and a database of tens of thousands.  The Gospel in comparisons recommended buildings and structures rather than individuals. The acceptable collateral damage and the type of weapon used to eliminate the target is decided by IDF members and could track militants even when at home. [70]\n Israel's Harpy anti-radar \"fire and forget\" drone is designed to be launched by ground troops, and autonomously fly over an area to find and destroy radar that fits pre-determined criteria.[71] The application of artificial intelligence is also expected to be advanced in crewless ground systems and robotic vehicles such as the Guardium MK III and later versions.[72] These robotic vehicles are used in border defense.\n The South Korean Super aEgis II machine gun, unveiled in 2010, sees use both in South Korea and in the Middle East. It can identify, track, and destroy a moving target at a range of 4 km. While the technology can theoretically operate without human intervention, in practice safeguards are installed to require manual input. A South Korean manufacturer states, \"Our weapons don't sleep, like humans must. They can see in the dark, like humans can't. Our technology therefore plugs the gaps in human capability\", and they want to \"get to a place where our software can discern whether a target is friend, foe, civilian or military\".[73]\n The European Parliament holds the position that humans must have oversight and decision-making power over lethal autonomous weapons.[74] However, it is up to each member state of the European Union to determine their stance on the use of autonomous weapons and the mixed stances of the member states is perhaps the greatest hindrance to the European Union's ability to develop autonomous weapons. Some members such as France, Germany, Italy, and Sweden are developing lethal autonomous weapons. Some members remain undecided about the use of autonomous military weapons and Austria has even called to ban the use of such weapons.[75]\n Some EU member states have developed and are developing automated weapons. Germany has developed an active protection system, the Active Defense System, that can respond to a threat with complete autonomy in less than a millisecond.[75][76] Italy plans to incorporate autonomous weapons systems into its future military plans.[75]\n The international regulation of autonomous weapons is an emerging issue for international law.[77] AI arms control will likely require the institutionalization of new international norms embodied in effective technical specifications combined with active monitoring and informal diplomacy by communities of experts, together with a legal and political verification process.[1][2] As early as 2007, scholars such as AI professor Noel Sharkey have warned of \"an emerging arms race among the hi-tech nations to develop autonomous submarines, fighter jets, battleships and tanks that can find their own targets and apply violent force without the involvement of meaningful human decisions\".[78][79]\n Miles Brundage of the University of Oxford has argued an AI arms race might be somewhat mitigated through diplomacy: \"We saw in the various historical arms races that collaboration and dialog can pay dividends\".[80] Over a hundred experts signed an open letter in 2017 calling on the UN to address the issue of lethal autonomous weapons;[81][82] however, at a November 2017 session of the UN Convention on Certain Conventional Weapons (CCW), diplomats could not agree even on how to define such weapons.[83] The Indian ambassador and chair of the CCW stated that agreement on rules remained a distant prospect.[84] As of 2019, 26 heads of state and 21 Nobel Peace Prize laureates have backed a ban on autonomous weapons.[85] However, as of 2022, most major powers continue to oppose a ban on autonomous weapons.[86]\n Many experts believe attempts to completely ban killer robots are likely to fail,[87] in part because detecting treaty violations would be extremely difficult.[88][89] A 2017 report from Harvard's Belfer Center predicts that AI has the potential to be as transformative as nuclear weapons.[80][90][91] The report further argues that \"Preventing expanded military use of AI is likely impossible\" and that \"the more modest goal of safe and effective technology management must be pursued\", such as banning the attaching of an AI dead man's switch to a nuclear arsenal.[91]\n A 2015 open letter by the Future of Life Institute calling for the prohibition of lethal autonomous weapons systems has been signed by over 26,000 citizens, including physicist Stephen Hawking, Tesla magnate Elon Musk, Apple's Steve Wozniak and Twitter co-founder Jack Dorsey, and over 4,600 artificial intelligence researchers, including Stuart Russell, Bart Selman and Francesca Rossi.[92][83] The Future of Life Institute has also released two fictional films, Slaughterbots (2017) and Slaughterbots - if human: kill() (2021), which portray threats of autonomous weapons and promote a ban, both of which went viral.\n Professor Noel Sharkey of the University of Sheffield argues that autonomous weapons will inevitably fall into the hands of terrorist groups such as the Islamic State.[93]\n Many Western tech companies avoid being associated too closely with the U.S. military, for fear of losing access to China's market.[41] Furthermore, some researchers, such as DeepMind CEO Demis Hassabis, are ideologically opposed to contributing to military work.[94]\n For example, in June 2018, company sources at Google said that top executive Diane Greene told staff that the company would not follow-up Project Maven after the current contract expired in March 2019.[60]\n"
    },
    {
        "title": "Machine learning in physics",
        "url": "https://en.wikipedia.org/wiki/Machine_learning_in_physics",
        "content": "Applying machine learning (ML) (including deep learning) methods to the study of quantum systems is an emergent area of physics research. A basic example of this is quantum state tomography, where a quantum state is learned from measurement.[1] Other examples include learning Hamiltonians,[2][3] learning quantum phase transitions,[4][5] and automatically generating new quantum experiments.[6][7][8][9] ML is effective at processing large amounts of experimental or calculated data in order to characterize an unknown quantum system, making its application useful in contexts including quantum information theory, quantum technology development, and computational materials design. In this context, for example, it can be used as a tool to interpolate pre-calculated interatomic potentials,[10] or directly solving the Schrödinger equation with a variational method.[11]\n The ability to experimentally control and prepare increasingly complex quantum systems brings with it a growing need to turn large and noisy data sets into meaningful information. This is a problem that has already been studied extensively in the classical setting, and consequently, many existing machine learning techniques can be naturally adapted to more efficiently address experimentally relevant problems. For example, Bayesian methods and concepts of algorithmic learning can be fruitfully applied to tackle quantum state classification,[12] Hamiltonian learning,[13] and the characterization of an unknown unitary transformation.[14][15] Other problems that have been addressed with this approach are given in the following list:\n Quantum machine learning can also be applied to dramatically accelerate the prediction of quantum properties of molecules and materials.[26] This can be helpful for the computational design of new molecules or materials. Some examples include\n Variational circuits are a family of algorithms which utilize training based on circuit parameters and an objective function.[33] Variational circuits are generally composed of a classical device communicating input parameters (random or pre-trained parameters) into a quantum device, along with a classical Mathematical optimization function. These circuits are very heavily dependent on the architecture of the proposed quantum device because parameter adjustments are adjusted based solely on the classical components within the device.[34] Though the application is considerably infantile in the field of quantum machine learning, it has incredibly high promise for more efficiently generating efficient optimization functions.\n Machine learning techniques can be used to find a better manifold of integration for path integrals in order to avoid the sign problem.[35]\n A deep learning system was reported to learn intuitive physics from visual data (of virtual 3D environments) based on an unpublished approach inspired by studies of visual cognition in infants.[40][39] Other researchers have developed a machine learning algorithm that could discover sets of basic variables of various physical systems and predict the systems' future dynamics from video recordings of their behavior.[41][42] In the future, it may be possible that such can be used to automate the discovery of physical laws of complex systems.[41] Beyond discovery and prediction, \"blank slate\"-type of learning of fundamental aspects of the physical world may have further applications such as improving adaptive and broad artificial general intelligence.[additional citation(s) needed] In specific, prior machine learning models were \"highly specialised and lack a general understanding of the world\".[40]\n"
    },
    {
        "title": "List of artificial intelligence projects",
        "url": "https://en.wikipedia.org/wiki/List_of_artificial_intelligence_projects",
        "content": "\n The following is a list of current and past, non-classified notable artificial intelligence projects.\n"
    },
    {
        "title": "Philosophy of artificial intelligence",
        "url": "https://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence",
        "content": "\n The philosophy of artificial intelligence is a branch of the philosophy of mind and the philosophy of computer science[1] that explores artificial intelligence and its implications for knowledge and understanding of intelligence, ethics, consciousness, epistemology, and free will.[2][3] Furthermore, the technology is concerned with the creation of artificial animals or artificial people (or, at least, artificial creatures; see artificial life) so the discipline is of considerable interest to philosophers.[4] These factors contributed to the emergence of the philosophy of artificial intelligence.\n The philosophy of artificial intelligence attempts to answer such questions as follows:[5]\n Questions like these reflect the divergent interests of AI researchers, cognitive scientists and philosophers respectively. The scientific answers to these questions depend on the definition of \"intelligence\" and \"consciousness\" and exactly which \"machines\" are under discussion.\n Important propositions in the philosophy of AI include some of the following:\n Is it possible to create a machine that can solve all the problems humans solve using their intelligence? This question defines the scope of what machines could do in the future and guides the direction of AI research. It only concerns the behavior of machines and ignores the issues of interest to psychologists, cognitive scientists and philosophers, evoking the question: does it matter whether a machine is really thinking, as a person thinks, rather than just producing outcomes that appear to result from thinking?[11]\n The basic position of most AI researchers is summed up in this statement, which appeared in the proposal for the Dartmouth workshop of 1956:\n Arguments against the basic premise must show that building a working AI system is impossible because there is some practical limit to the abilities of computers or that there is some special quality of the human mind that is necessary for intelligent behavior and yet cannot be duplicated by a machine (or by the methods of current AI research). Arguments in favor of the basic premise must show that such a system is possible.\n It is also possible to sidestep the connection between the two parts of the above proposal. For instance, machine learning, beginning with Turing's infamous child machine proposal,[12] essentially achieves the desired feature of intelligence without a precise design-time description as to how it would exactly work. The account on robot tacit knowledge[13] eliminates the need for a precise description altogether.\n The first step to answering the question is to clearly define \"intelligence\".\n Alan Turing[15] reduced the problem of defining intelligence to a simple question about conversation. He suggests that: if a machine can answer any question posed to it, using the same words that an ordinary person would, then we may call that machine intelligent. A modern version of his experimental design would use an online chat room, where one of the participants is a real person and one of the participants is a computer program. The program passes the test if no one can tell which of the two participants is human.[6] Turing notes that no one (except philosophers) ever asks the question \"can people think?\" He writes \"instead of arguing continually over this point, it is usual to have a polite convention that everyone thinks\".[16] Turing's test extends this polite convention to machines:\n One criticism of the Turing test is that it only measures the \"humanness\" of the machine's behavior, rather than the \"intelligence\" of the behavior. Since human behavior and intelligent behavior are not exactly the same thing, the test fails to measure intelligence. Stuart J. Russell and Peter Norvig write that \"aeronautical engineering texts do not define the goal of their field as 'making machines that fly so exactly like pigeons that they can fool other pigeons'\".[17]\n Twenty-first century AI research defines intelligence in terms of goal-directed behavior. It views intelligence as a set of problems that the machine is expected to solve – the more problems it can solve, and the better its solutions are, the more intelligent the program is. AI founder John McCarthy defined intelligence as \"the computational part of the ability to achieve goals in the world.\"[18]\n Stuart Russell and Peter Norvig formalized this definition using abstract intelligent agents. An \"agent\" is something which perceives and acts in an environment. A \"performance measure\" defines what counts as success for the agent.[19]\n Definitions like this one try to capture the essence of intelligence. They have the advantage that, unlike the Turing test, they do not also test for unintelligent human traits such as making typing mistakes.[21] \nThey have the disadvantage that they can fail to differentiate between \"things that think\" and \"things that do not\". By this definition, even a thermostat has a rudimentary intelligence.[22]\n Hubert Dreyfus describes this argument as claiming that \"if the nervous system obeys the laws of physics and chemistry, which we have every reason to suppose it does, then ... we ... ought to be able to reproduce the behavior of the nervous system with some physical device\".[23] This argument, first introduced as early as 1943[24] and vividly described by Hans Moravec in 1988,[25] \nis now associated with futurist Ray Kurzweil, who estimates that computer power will be sufficient for a complete brain simulation by the year 2029.[26] A non-real-time simulation of a thalamocortical model that has the size of the human brain (1011 neurons) was performed in 2005,[27] and it took 50 days to simulate 1 second of brain dynamics on a cluster of 27 processors.\n Even AI's harshest critics (such as Hubert Dreyfus and John Searle) agree that a brain simulation is possible in theory.[a]\nHowever, Searle points out that, in principle, anything can be simulated by a computer; thus, bringing the definition to its breaking point leads to the conclusion that any process at all can technically be considered \"computation\". \"What we wanted to know is what distinguishes the mind from thermostats and livers,\" he writes.[30] Thus, merely simulating the functioning of a living brain would in itself be an admission of ignorance regarding intelligence and the nature of the mind, like trying to build a jet airliner by copying a living bird precisely, feather by feather, with no theoretical understanding of aeronautical engineering.[31]\n In 1963, Allen Newell and Herbert A. Simon proposed that \"symbol manipulation\" was the essence of both human and machine intelligence. They wrote: \n This claim is very strong: it implies both that human thinking is a kind of symbol manipulation (because a symbol system is necessary for intelligence) and that machines can be intelligent (because a symbol system is sufficient for intelligence).[32]\nAnother version of this position was described by philosopher Hubert Dreyfus, who called it \"the psychological assumption\":\n The \"symbols\" that Newell, Simon and Dreyfus discussed were word-like and high level—symbols that directly correspond with objects in the world, such as <dog> and <tail>. Most AI programs written between 1956 and 1990 used this kind of symbol. Modern AI, based on statistics and mathematical optimization, does not use the high-level \"symbol processing\" that Newell and Simon discussed.\n These arguments show that human thinking does not consist (solely) of high level symbol manipulation. They do not show that artificial intelligence is impossible, only that more than symbol processing is required.\n In 1931, Kurt Gödel proved with an incompleteness theorem that it is always possible to construct a \"Gödel statement\" that a given consistent formal system of logic (such as a high-level symbol manipulation program) could not prove. Despite being a true statement, the constructed Gödel statement is unprovable in the given system. (The truth of the constructed Gödel statement is contingent on the consistency of the given system; applying the same process to a subtly inconsistent system will appear to succeed, but will actually yield a false \"Gödel statement\" instead.)[citation needed] More speculatively, Gödel conjectured that the human mind can eventually correctly determine the truth or falsity of any well-grounded mathematical statement (including any possible Gödel statement), and that therefore the human mind's power is not reducible to a mechanism.[34] Philosopher John Lucas (since 1961) and Roger Penrose (since 1989) have championed this philosophical anti-mechanist argument.[35]\n Gödelian anti-mechanist arguments tend to rely on the innocuous-seeming claim that a system of human mathematicians (or some idealization of human mathematicians) is both consistent (completely free of error) and believes fully in its own consistency (and can make all logical inferences that follow from its own consistency, including belief in its Gödel statement) [citation needed]. This is probably impossible for a Turing machine to do (see Halting problem); therefore, the Gödelian concludes that human reasoning is too powerful to be captured by a Turing machine, and by extension, any digital mechanical device.\n However, the modern consensus in the scientific and mathematical community is that actual human reasoning is inconsistent; that any consistent \"idealized version\" H of human reasoning would logically be forced to adopt a healthy but counter-intuitive open-minded skepticism about the consistency of H (otherwise H is provably inconsistent); and that Gödel's theorems do not lead to any valid argument that humans have mathematical reasoning capabilities beyond what a machine could ever duplicate.[36][37][38] This consensus that Gödelian anti-mechanist arguments are doomed to failure is laid out strongly in Artificial Intelligence: \"any attempt to utilize (Gödel's incompleteness results) to attack the computationalist thesis is bound to be illegitimate, since these results are quite consistent with the computationalist thesis.\"[39]\n Stuart Russell and Peter Norvig agree that Gödel's argument does not consider the nature of real-world human reasoning. It applies to what can theoretically be proved, given an infinite amount of memory and time. In practice, real machines (including humans) have finite resources and will have difficulty proving many theorems. It is not necessary to be able to prove everything in order to be an intelligent person.[40]\n Less formally, Douglas Hofstadter, in his Pulitzer Prize winning book Gödel, Escher, Bach: An Eternal Golden Braid, states that these \"Gödel-statements\" always refer to the system itself, drawing an analogy to the way the Epimenides paradox uses statements that refer to themselves, such as \"this statement is false\" or \"I am lying\".[41] But, of course, the Epimenides paradox applies to anything that makes statements, whether it is a machine or a human, even Lucas himself. Consider:\n This statement is true but cannot be asserted by Lucas. This shows that Lucas himself is subject to the same limits that he describes for machines, as are all people, and so Lucas's argument is pointless.[43]\n After concluding that human reasoning is non-computable, Penrose went on to controversially speculate that some kind of hypothetical non-computable processes involving the collapse of quantum mechanical states give humans a special advantage over existing computers. Existing quantum computers are only capable of reducing the complexity of Turing computable tasks and are still restricted to tasks within the scope of Turing machines. [citation needed] [clarification needed]. By Penrose and Lucas's arguments, the fact that quantum computers are only able to complete Turing computable tasks implies that they cannot be sufficient for emulating the human mind.[citation needed] Therefore, Penrose seeks for some other process involving new physics, for instance quantum gravity which might manifest new physics at the scale of the Planck mass via spontaneous quantum collapse of the wave function. These states, he suggested, occur both within neurons and also spanning more than one neuron.[44] However, other scientists point out that there is no plausible organic mechanism in the brain for harnessing any sort of quantum computation, and furthermore that the timescale of quantum decoherence seems too fast to influence neuron firing.[45]\n Hubert Dreyfus argued that human intelligence and expertise depended primarily on fast intuitive judgements rather than step-by-step symbolic manipulation, and argued that these skills would never be captured in formal rules.[46]\n Dreyfus's argument had been anticipated by Turing in his 1950 paper Computing machinery and intelligence, where he had classified this as the \"argument from the informality of behavior.\"[47] Turing argued in response that, just because we do not know the rules that govern a complex behavior, this does not mean that no such rules exist. He wrote: \"we cannot so easily convince ourselves of the absence of complete laws of behaviour ... The only way we know of for finding such laws is scientific observation, and we certainly know of no circumstances under which we could say, 'We have searched enough. There are no such laws.'\"[48]\n Russell and Norvig point out that, in the years since Dreyfus published his critique, progress has been made towards discovering the \"rules\" that govern unconscious reasoning.[49] The situated movement in robotics research attempts to capture our unconscious skills at perception and attention.[50] Computational intelligence paradigms, such as neural nets, evolutionary algorithms and so on are mostly directed at simulated unconscious reasoning and learning. Statistical approaches to AI can make predictions which approach the accuracy of human intuitive guesses. Research into commonsense knowledge has focused on reproducing the \"background\" or context of knowledge. In fact, AI research in general has moved away from high level symbol manipulation, towards new models that are intended to capture more of our intuitive reasoning.[49]\n Cognitive science and psychology eventually came to agree with Dreyfus' description of human expertise. Daniel Kahnemann and others developed a similar theory where they identified two \"systems\" that humans use to solve problems, which he called \"System 1\" (fast intuitive judgements) and \"System 2\" (slow deliberate step by step thinking).[51]\n Although Dreyfus' views have been vindicated in many ways, the work in cognitive science and in AI was in response to specific problems in those fields and was not directly influenced by Dreyfus. Historian and AI researcher Daniel Crevier wrote that \"time has proven the accuracy and perceptiveness of some of Dreyfus's comments. Had he formulated them less aggressively, constructive actions they suggested might have been taken much earlier.\"[52]\n This is a philosophical question, related to the problem of other minds and the hard problem of consciousness. The question revolves around a position defined by John Searle as \"strong AI\":\n Searle distinguished this position from what he called \"weak AI\":\n Searle introduced the terms to isolate strong AI from weak AI so he could focus on what he thought was the more interesting and debatable issue. He argued that even if we assume that we had a computer program that acted exactly like a human mind, there would still be a difficult philosophical question that needed to be answered.[9]\n Neither of Searle's two positions are of great concern to AI research, since they do not directly answer the question \"can a machine display general intelligence?\" (unless it can also be shown that consciousness is necessary for intelligence). Turing wrote \"I do not wish to give the impression that I think there is no mystery about consciousness… [b]ut I do not think these mysteries necessarily need to be solved before we can answer the question [of whether machines can think].\"[53] Russell and Norvig agree: \"Most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\"[54]\n There are a few researchers who believe that consciousness is an essential element in intelligence, such as Igor Aleksander, Stan Franklin, Ron Sun, and Pentti Haikonen, although their definition of \"consciousness\" strays very close to \"intelligence\". (See artificial consciousness.)\n Before we can answer this question, we must be clear what we mean by \"minds\", \"mental states\" and \"consciousness\".\n The words \"mind\" and \"consciousness\" are used by different communities in different ways. Some new age thinkers, for example, use the word \"consciousness\" to describe something similar to Bergson's \"élan vital\": an invisible, energetic fluid that permeates life and especially the mind. Science fiction writers use the word to describe some essential property that makes us human: a machine or alien that is \"conscious\" will be presented as a fully human character, with intelligence, desires, will, insight, pride and so on. (Science fiction writers also use the words \"sentience\", \"sapience\", \"self-awareness\" or \"ghost\"—as in the Ghost in the Shell manga and anime series—to describe this essential human property). For others [who?], the words \"mind\" or \"consciousness\" are used as a kind of secular synonym for the soul.\n For philosophers, neuroscientists and cognitive scientists, the words are used in a way that is both more precise and more mundane: they refer to the familiar, everyday experience of having a \"thought in your head\", like a perception, a dream, an intention or a plan, and to the way we see something, know something, mean something or understand something.[55] \"It's not hard to give a commonsense definition of consciousness\" observes philosopher John Searle.[56] What is mysterious and fascinating is not so much what it is but how it is: how does a lump of fatty tissue and electricity give rise to this (familiar) experience of perceiving, meaning or thinking?\n Philosophers call this the hard problem of consciousness. It is the latest version of a classic problem in the philosophy of mind called the \"mind-body problem\".[57] A related problem is the problem of meaning or understanding (which philosophers call \"intentionality\"): what is the connection between our thoughts and what we are thinking about (i.e. objects and situations out in the world)? A third issue is the problem of experience (or \"phenomenology\"): If two people see the same thing, do they have the same experience? Or are there things \"inside their head\" (called \"qualia\") that can be different from person to person?[58]\n Neurobiologists believe all these problems will be solved as we begin to identify the neural correlates of consciousness: the actual relationship between the machinery in our heads and its collective properties; such as the mind, experience and understanding. Some of the harshest critics of artificial intelligence agree that the brain is just a machine, and that consciousness and intelligence are the result of physical processes in the brain.[59] The difficult philosophical question is this: can a computer program, running on a digital machine that shuffles the binary digits of zero and one, duplicate the ability of the neurons to create minds, with mental states (like understanding or perceiving), and ultimately, the experience of consciousness?\n John Searle asks us to consider a thought experiment: suppose we have written a computer program that passes the Turing test and demonstrates general intelligent action. Suppose, specifically that the program can converse in fluent Chinese. Write the program on 3x5 cards and give them to an ordinary person who does not speak Chinese. Lock the person into a room and have him follow the instructions on the cards. He will copy out Chinese characters and pass them in and out of the room through a slot. From the outside, it will appear that the Chinese room contains a fully intelligent person who speaks Chinese. The question is this: is there anyone (or anything) in the room that understands Chinese? That is, is there anything that has the mental state of understanding, or which has conscious awareness of what is being discussed in Chinese? The man is clearly not aware. The room cannot be aware. The cards certainly are not aware. Searle concludes that the Chinese room, or any other physical symbol system, cannot have a mind.[60]\n Searle goes on to argue that actual mental states and consciousness require (yet to be described) \"actual physical-chemical properties of actual human brains.\"[61] He argues there are special \"causal properties\" of brains and neurons that gives rise to minds: in his words \"brains cause minds.\"[62]\n Gottfried Leibniz made essentially the same argument as Searle in 1714, using the thought experiment of expanding the brain until it was the size of a mill.[63] In 1974, Lawrence Davis imagined duplicating the brain using telephone lines and offices staffed by people, and in 1978 Ned Block envisioned the entire population of China involved in such a brain simulation. This thought experiment is called \"the Chinese Nation\" or \"the Chinese Gym\".[64] Ned Block also proposed his Blockhead argument, which is a version of the Chinese room in which the program has been re-factored into a simple set of rules of the form \"see this, do that\", removing all mystery from the program.\n Responses to the Chinese room emphasize several different points. \n The computational theory of mind or \"computationalism\" claims that the relationship between mind and brain is similar (if not identical) to the relationship between a running program (software) and a computer (hardware). The idea has philosophical roots in Hobbes (who claimed reasoning was \"nothing more than reckoning\"), Leibniz (who attempted to create a logical calculus of all human ideas), Hume (who thought perception could be reduced to \"atomic impressions\") and even Kant (who analyzed all experience as controlled by formal rules).[71] The latest version is associated with philosophers Hilary Putnam and Jerry Fodor.[72]\n This question bears on our earlier questions: if the human brain is a kind of computer then computers can be both intelligent and conscious, answering both the practical and philosophical questions of AI. In terms of the practical question of AI (\"Can a machine display general intelligence?\"), some versions of computationalism make the claim that (as Hobbes wrote):\n In other words, our intelligence derives from a form of calculation, similar to arithmetic. This is the physical symbol system hypothesis discussed above, and it implies that artificial intelligence is possible. In terms of the philosophical question of AI (\"Can a machine have mind, mental states and consciousness?\"), most versions of computationalism claim that (as Stevan Harnad characterizes it):\n This is John Searle's \"strong AI\" discussed above, and it is the real target of the Chinese room argument (according to Harnad).[73]\n If \"emotions\" are defined only in terms of their effect on behavior or on how they function inside an organism, then emotions can be viewed as a mechanism that an intelligent agent uses to maximize the utility of its actions. Given this definition of emotion, Hans Moravec believes that \"robots in general will be quite emotional about being nice people\".[74] Fear is a source of urgency. Empathy is a necessary component of good human computer interaction. He says robots \"will try to please you in an apparently selfless manner because it will get a thrill out of this positive reinforcement. You can interpret this as a kind of love.\"[74] Daniel Crevier writes \"Moravec's point is that emotions are just devices for channeling behavior in a direction beneficial to the survival of one's species.\"[75]\n \"Self-awareness\", as noted above, is sometimes used by science fiction writers as a name for the essential human property that makes a character fully human. Turing strips away all other properties of human beings and reduces the question to \"can a machine be the subject of its own thought?\" Can it think about itself? Viewed in this way, a program can be written that can report on its own internal states, such as a debugger.[76]\n Turing reduces this to the question of whether a machine can \"take us by surprise\" and argues that this is obviously true, as any programmer can attest.[77] He notes that, with enough storage capacity, a computer can behave in an astronomical number of different ways.[78] It must be possible, even trivial, for a computer that can represent ideas to combine them in new ways. (Douglas Lenat's Automated Mathematician, as one example, combined ideas to discover new mathematical truths.) Kaplan and Haenlein suggest that machines can display scientific creativity, while it seems likely that humans will have the upper hand where artistic creativity is concerned.[79]\n In 2009, scientists at Aberystwyth University in Wales and the U.K's University of Cambridge designed a robot called Adam that they believe to be the first machine to independently come up with new scientific findings.[80] Also in 2009, researchers at Cornell developed Eureqa, a computer program that extrapolates formulas to fit the data inputted, such as finding the laws of motion from a pendulum's motion.\n This question (like many others in the philosophy of artificial intelligence) can be presented in two forms. \"Hostility\" can be defined in terms function or behavior, in which case \"hostile\" becomes synonymous with \"dangerous\". Or it can be defined in terms of intent: can a machine \"deliberately\" set out to do harm? The latter is the question \"can a machine have conscious states?\" (such as intentions) in another form.[53]\n The question of whether highly intelligent and completely autonomous machines would be dangerous has been examined in detail by futurists (such as the Machine Intelligence Research Institute). The obvious element of drama has also made the subject popular in science fiction, which has considered many differently possible scenarios where intelligent machines pose a threat to mankind; see Artificial intelligence in fiction.\n One issue is that machines may acquire the autonomy and intelligence required to be dangerous very quickly. Vernor Vinge has suggested that over just a few years, computers will suddenly become thousands or millions of times more intelligent than humans. He calls this \"the Singularity\".[81] He suggests that it may be somewhat or possibly very dangerous for humans.[82] This is discussed by a philosophy called Singularitarianism.\n In 2009, academics and technical experts attended a conference to discuss the potential impact of robots and computers and the impact of the hypothetical possibility that they could become self-sufficient and able to make their own decisions. They discussed the possibility and the extent to which computers and robots might be able to acquire any level of autonomy, and to what degree they could use such abilities to possibly pose any threat or hazard. They noted that some machines have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved \"cockroach intelligence\". They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls.[81]\n Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions.[83] The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions.[84][85]\n The President of the Association for the Advancement of Artificial Intelligence has commissioned a study to look at this issue.[86] They point to programs like the Language Acquisition Device which can emulate human interaction.\n Some have suggested a need to build \"Friendly AI\", a term coined by Eliezer Yudkowsky, meaning that the advances which are already occurring with AI should also include an effort to make AI intrinsically friendly and humane.[87]\n Turing said \"It is customary ... to offer a grain of comfort, in the form of a statement that some peculiarly human characteristic could never be imitated by a machine. ... I cannot offer any such comfort, for I believe that no such bounds can be set.\"[88]\n Turing noted that there are many arguments of the form \"a machine will never do X\", where X can be many things, such as:\n Be kind, resourceful, beautiful, friendly, have initiative, have a sense of humor, tell right from wrong, make mistakes, fall in love, enjoy strawberries and cream, make someone fall in love with it, learn from experience, use words properly, be the subject of its own thought, have as much diversity of behaviour as a man, do something really new.[76] Turing argues that these objections are often based on naive assumptions about the versatility of machines or are \"disguised forms of the argument from consciousness\". Writing a program that exhibits one of these behaviors \"will not make much of an impression.\"[76] All of these arguments are tangential to the basic premise of AI, unless it can be shown that one of these traits is essential for general intelligence.\n Finally, those who believe in the existence of a soul may argue that \"Thinking is a function of man's immortal soul.\" Alan Turing called this \"the theological objection\". He writes:\n In attempting to construct such machines we should not be irreverently usurping His power of creating souls, any more than we are in the procreation of children: rather we are, in either case, instruments of His will providing mansions for the souls that He creates.[89] The discussion on the topic has been reignited as a result of recent claims made by Google's LaMDA artificial intelligence system that it is sentient and had a \"soul\".[90]\n LaMDA (Language Model for Dialogue Applications) is an artificial intelligence system that creates chatbots—AI robots designed to communicate with humans—by gathering vast amounts of text from the internet and using algorithms to respond to queries in the most fluid and natural way possible.\n The transcripts of conversations between scientists and LaMDA reveal that the AI system excels at this, providing answers to challenging topics about the nature of emotions, generating Aesop-style fables on the moment, and even describing its alleged fears.[91] Pretty much all philosophers doubt LaMDA's sentience.[92]\n Some scholars argue that the AI community's dismissal of philosophy is detrimental. In the Stanford Encyclopedia of Philosophy, some philosophers argue that the role of philosophy in AI is underappreciated.[4] Physicist David Deutsch argues that without an understanding of philosophy or its concepts, AI development would suffer from a lack of progress.[93]\n The main conference series on the issue is \"Philosophy and Theory of AI\" (PT-AI), run by Vincent C. Müller.\n The main bibliography on the subject, with several sub-sections, is on PhilPapers.\n A recent survey for Philosophy of AI is Müller (2023).[3]\n \n"
    },
    {
        "title": "Artificial consciousness",
        "url": "https://en.wikipedia.org/wiki/Artificial_consciousness",
        "content": "Artificial consciousness,[1] also known as machine consciousness,[2][3] synthetic consciousness,[4] or digital consciousness,[5] is the consciousness hypothesized to be possible in artificial intelligence.[6] It is also the corresponding field of study, which draws insights from philosophy of mind, philosophy of artificial intelligence, cognitive science and neuroscience.\n The same terminology can be used with the term \"sentience\" instead of \"consciousness\" when specifically designating phenomenal consciousness (the ability to feel qualia).[7] Since sentience involves the ability to experience ethically positive or negative (i.e., valenced) mental states, it may justify welfare concerns and legal protection, as with animals.[8]\n Some scholars believe that consciousness is generated by the interoperation of various parts of the brain; these mechanisms are labeled the neural correlates of consciousness or NCC. Some further believe that constructing a system (e.g., a computer system) that can emulate this NCC interoperation would result in a system that is conscious.[9]\n As there are many hypothesized types of consciousness, there are many potential implementations of artificial consciousness. In the philosophical literature, perhaps the most common taxonomy of consciousness is into \"access\" and \"phenomenal\" variants. Access consciousness concerns those aspects of experience that can be apprehended, while phenomenal consciousness concerns those aspects of experience that seemingly cannot be apprehended, instead being characterized qualitatively in terms of \"raw feels\", \"what it is like\" or qualia.[10]\n Type-identity theorists and other skeptics hold the view that consciousness can be realized only in particular physical systems because consciousness has properties that necessarily depend on physical constitution.[11][12][13][14] In his 2001 article \"Artificial Consciousness: Utopia or Real Possibility,\" Giorgio Buttazzo says that a common objection to artificial consciousness is that, \"Working in a fully automated mode, they [the computers] cannot exhibit creativity, unreprogrammation (which means can 'no longer be reprogrammed', from rethinking), emotions, or free will. A computer, like a washing machine, is a slave operated by its components.\"[15]\n For other theorists (e.g., functionalists), who define mental states in terms of causal roles, any system that can instantiate the same pattern of causal roles, regardless of physical constitution, will instantiate the same mental states, including consciousness.[16]\n David Chalmers proposed two thought experiments intending to demonstrate that \"functionally isomorphic\" systems (those with the same \"fine-grained functional organization\", i.e., the same information processing) will have qualitatively identical conscious experiences, regardless of whether they are based on biological neurons or digital hardware.[17][18]\n The \"fading qualia\" is a reductio ad absurdum thought experiment. It involves replacing, one by one, the neurons of a brain with a functionally identical component, for example based on a silicon chip. Since the original neurons and their silicon counterparts are functionally identical, the brain’s information processing should remain unchanged, and the subject would not notice any difference. However, if qualia (such as the subjective experience of bright red) were to fade or disappear, the subject would likely notice this change, which causes a contradiction. Chalmers concludes that the fading qualia hypothesis is impossible in practice, and that the resulting robotic brain, once every neurons are replaced, would remain just as sentient as the original biological brain.[17][19]\n Similarly, the \"dancing qualia\" thought experiment is another reductio ad absurdum argument. It supposes that two functionally isomorphic systems could have different perceptions (for instance, seeing the same object in different colors, like red and blue). It involves a switch that alternates between a chunk of brain that causes the perception of red, and a functionally isomorphic silicon chip, that causes the perception of blue. Since both perform the same function within the brain, the subject would not notice any change during the switch. Chalmers argues that this would be highly implausible if the qualia were truly switching between red and blue, hence the contradiction. Therefore, he concludes that the equivalent digital system would not only experience qualia, but it would perceive the same qualia as the biological system (e.g., seeing the same color).[17][19]\n Critics[who?] of artificial sentience object that Chalmers' proposal begs the question in assuming that all mental properties and external connections are already sufficiently captured by abstract causal organization.\n In 2022, Google engineer Blake Lemoine made a viral claim that Google's LaMDA chatbot was sentient. Lemoine supplied as evidence the chatbot's humanlike answers to many of his questions; however, the chatbot's behavior was judged by the scientific community as likely a consequence of mimicry, rather than machine sentience. Lemoine's claim was widely derided for being ridiculous.[20] However, while philosopher Nick Bostrom states that LaMDA is unlikely to be conscious, he additionally poses the question of \"what grounds would a person have for being sure about it?\" One would have to have access to unpublished information about LaMDA's architecture, and also would have to understand how consciousness works, and then figure out how to map the philosophy onto the machine: \"(In the absence of these steps), it seems like one should be maybe a little bit uncertain. [...] there could well be other systems now, or in the relatively near future, that would start to satisfy the criteria.\"[21]\n Qualia, or phenomenological consciousness, is an inherently first-person phenomenon. Because of that, and the lack of an empirical definition of sentience, directly measuring it may be impossible. Although systems may display numerous behaviors correlated with sentience, determining whether a system is sentient is known as the hard problem of consciousness. In the case of AI, there is the additional difficulty that the AI may be trained to act like a human, or incentivized to appear sentient, which makes behavioral markers of sentience less reliable.[22][23] Additionally, some chatbots have been trained to say they are not conscious.[24]\n A well-known method for testing machine intelligence is the Turing test, which assesses the ability to have a human-like conversation. But passing the Turing test does not indicate that an AI system is sentient, as the AI may simply mimic human behavior without having the associated feelings.[25]\n In 2014, Victor Argonov suggested a non-Turing test for machine sentience based on machine's ability to produce philosophical judgments.[26] He argues that a deterministic machine must be regarded as conscious if it is able to produce judgments on all problematic properties of consciousness (such as qualia or binding) having no innate (preloaded) philosophical knowledge on these issues, no philosophical discussions while learning, and no informational models of other creatures in its memory (such models may implicitly or explicitly contain knowledge about these creatures' consciousness). However, this test can be used only to detect, but not refute the existence of consciousness. A positive result proves that machine is conscious but a negative result proves nothing. For example, absence of philosophical judgments may be caused by lack of the machine's intellect, not by absence of consciousness.\n If it were suspected that a particular machine was conscious, its rights would be an ethical issue that would need to be assessed (e.g. what rights it would have under law).[27] For example, a conscious computer that was owned and used as a tool or central computer within a larger machine is a particular ambiguity. Should laws be made for such a case? Consciousness would also require a legal definition in this particular case. Because artificial consciousness is still largely a theoretical subject, such ethics have not been discussed or developed to a great extent, though it has often been a theme in fiction.\n Sentience is generally considered sufficient for moral consideration, but some philosophers consider that moral consideration could also stem from other notions of consciousness, or from capabilities unrelated to consciousness,[28][29] such as: \"having a sophisticated conception of oneself as persisting through time; having agency and the ability to pursue long-term plans; being able to communicate and respond to normative reasons; having preferences and powers; standing in certain social relationships with other beings that have moral status; being able to make commitments and to enter into reciprocal arrangements; or having the potential to develop some of these attributes.\"[28]\n Ethical concerns still apply (although to a lesser extent) when the consciousness is uncertain, as long as the probability is deemed non-negligible. The precautionary principle is also relevant if the moral cost of mistakenly attributing or denying moral consideration to AI differs significantly.[29][8]\n In 2021, German philosopher Thomas Metzinger argued for a global moratorium on synthetic phenomenology until 2050. Metzinger asserts that humans have a duty of care towards any sentient AIs they create, and that proceeding too fast risks creating an \"explosion of artificial suffering\".[30] David Chalmers also argued that creating conscious AI would \"raise a new group of difficult ethical challenges, with the potential for new forms of injustice\".[31]\n Enforced amnesia has been proposed as a way to mitigate the risk of silent suffering in locked-in conscious AI and certain AI-adjacent biological systems like brain organoids.[32]\n Bernard Baars and others argue there are various aspects of consciousness necessary for a machine to be artificially conscious.[33] The functions of consciousness suggested by Baars are: definition and context setting, adaptation and learning, editing, flagging and debugging, recruiting and control, prioritizing and access-control, decision-making or executive function, analogy-forming function, metacognitive and self-monitoring function, and autoprogramming and self-maintenance function. Igor Aleksander suggested 12 principles for artificial consciousness:[34] the brain is a state machine, inner neuron partitioning, conscious and unconscious states, perceptual learning and memory, prediction, the awareness of self, representation of meaning, learning utterances, learning language, will, instinct, and emotion. The aim of AC is to define whether and how these and other aspects of consciousness can be synthesized in an engineered artifact such as a digital computer. This list is not exhaustive; there are many others not covered. \n Some philosophers, such as David Chalmers, use the term consciousness to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience. Although some authors use the word sentience to refer exclusively to valenced (ethically positive or negative) subjective experiences, like pleasure or suffering.[31] Explaining why and how subjective experience arises is known as the hard problem of consciousness.[35] AI sentience would give rise to concerns of welfare and legal protection,[8] whereas other aspects of consciousness related to cognitive capabilities may be more relevant for AI rights.[36]\n Awareness could be one required aspect, but there are many problems with the exact definition of awareness. The results of the experiments of neuroscanning on monkeys suggest that a process, not only a state or object, activates neurons. Awareness includes creating and testing alternative models of each process based on the information received through the senses or imagined,[clarification needed] and is also useful for making predictions. Such modeling needs a lot of flexibility. Creating such a model includes modeling the physical world, modeling one's own internal states and processes, and modeling other conscious entities.\n There are at least three types of awareness:[37] agency awareness, goal awareness, and sensorimotor awareness, which may also be conscious or not. For example, in agency awareness, you may be aware that you performed a certain action yesterday, but are not now conscious of it. In goal awareness, you may be aware that you must search for a lost object, but are not now conscious of it. In sensorimotor awareness, you may be aware that your hand is resting on an object, but are not now conscious of it.\n Because objects of awareness are often conscious, the distinction between awareness and consciousness is frequently blurred or they are used as synonyms.[38]\n Conscious events interact with memory systems in learning, rehearsal, and retrieval.[39]\nThe IDA model[40] elucidates the role of consciousness in the updating of perceptual memory,[41] transient episodic memory, and procedural memory. Transient episodic and declarative memories have distributed representations in IDA; there is evidence that this is also the case in the nervous system.[42] In IDA, these two memories are implemented computationally using a modified version of Kanerva’s sparse distributed memory architecture.[43]\n Learning is also considered necessary for artificial consciousness. Per Bernard Baars, conscious experience is needed to represent and adapt to novel and significant events.[33] Per Axel Cleeremans and Luis Jiménez, learning is defined as \"a set of philogenetically [sic] advanced adaptation processes that critically depend on an evolved sensitivity to subjective experience so as to enable agents to afford flexible control over their actions in complex, unpredictable environments\".[44]\n The ability to predict (or anticipate) foreseeable events is considered important for artificial intelligence by Igor Aleksander.[45] The emergentist multiple drafts principle proposed by Daniel Dennett in Consciousness Explained may be useful for prediction: it involves the evaluation and selection of the most appropriate \"draft\" to fit the current environment. Anticipation includes prediction of consequences of one's own proposed actions and prediction of consequences of probable actions by other entities.\n Relationships between real world states are mirrored in the state structure of a conscious organism, enabling the organism to predict events.[45] An artificially conscious machine should be able to anticipate events correctly in order to be ready to respond to them when they occur or to take preemptive action to avert anticipated events. The implication here is that the machine needs flexible, real-time components that build spatial, dynamic, statistical, functional, and cause-effect models of the real world and predicted worlds, making it possible to demonstrate that it possesses artificial consciousness in the present and future and not only in the past. In order to do this, a conscious machine should make coherent predictions and contingency plans, not only in worlds with fixed rules like a chess board, but also for novel environments that may change, to be executed only when appropriate to simulate and control the real world.\n Functionalism is a theory that defines mental states by their functional roles (their causal relationships to sensory inputs, other mental states, and behavioral outputs), rather than by their physical composition. According to this view, what makes something a particular mental state, such as pain or belief, is not the material it is made of, but the role it plays within the overall cognitive system. It allows for the possibility that mental states, including consciousness, could be realized on non-biological substrates, as long as it instantiates the right functional relationships.[46] Functionalism is particularly popular among philosophers.[47]\n A 2023 study suggested that current large language models probably don't satisfy the criteria for consciousness suggested by these theories, but that relatively simple AI systems that satisfy these theories could be created. The study also acknowledged that even the most prominent theories of consciousness remain incomplete and subject to ongoing debate.[48]\n This theory analogizes the mind to a theater, with conscious thought being like material illuminated on the main stage. The brain contains many specialized processes or modules (such as those for vision, language, or memory) that operate in parallel, much of which is unconscious. Attention acts as a spotlight, bringing some of this unconscious activity into conscious awareness on the global workspace. The global workspace functions as a hub for broadcasting and integrating information, allowing it to be shared and processed across different specialized modules. For example, when reading a word, the visual module recognizes the letters, the language module interprets the meaning, and the memory module might recall associated information – all coordinated through the global workspace.[49][50]\n Higher-order theories of consciousness propose that a mental state becomes conscious when it is the object of a higher-order representation, such as a thought or perception about that state. These theories argue that consciousness arises from a relationship between lower-order mental states and higher-order awareness of those states. There are several variations, including higher-order thought (HOT) and higher-order perception (HOP) theories.[51][50]\n In 2011, Michael Graziano and Sabine Kastler published a paper named \"Human consciousness and its relationship to social neuroscience: A novel hypothesis\" proposing a theory of consciousness as an attention schema.[52] Graziano went on to publish an expanded discussion of this theory in his book \"Consciousness and the Social Brain\".[9] This Attention Schema Theory of Consciousness, as he named it, proposes that the brain tracks attention to various sensory inputs by way of an attention schema, analogous to the well-studied body schema that tracks the spatial place of a person's body.[9] This relates to artificial consciousness by proposing a specific mechanism of information handling, that produces what we allegedly experience and describe as consciousness, and which should be able to be duplicated by a machine using current technology. When the brain finds that person X is aware of thing Y, it is in effect modeling the state in which person X is applying an attentional enhancement to Y. In the attention schema theory, the same process can be applied to oneself. The brain tracks attention to various sensory inputs, and one's own awareness is a schematized model of one's attention. Graziano proposes specific locations in the brain for this process, and suggests that such awareness is a computed feature constructed by an expert system in the brain.\n Stan Franklin created a cognitive architecture called LIDA that implements Bernard Baars's theory of consciousness called the global workspace theory. It relies heavily on codelets, which are \"special purpose, relatively independent, mini-agent[s] typically implemented as a small piece of code running as a separate thread.\" Each element of cognition, called a \"cognitive cycle\" is subdivided into three phases: understanding, consciousness, and action selection (which includes learning). LIDA reflects the global workspace theory's core idea that consciousness acts as a workspace for integrating and broadcasting the most important information, in order to coordinate various cognitive processes.[53][54]\n The CLARION cognitive architecture models the mind using a two-level system to distinguish between conscious (\"explicit\") and unconscious (\"implicit\") processes. It can simulate various learning tasks, from simple to complex, which helps researchers study in psychological experiments how consciousness might work.[55]\n Ben Goertzel made an embodied AI through the open-source OpenCog project. The code includes embodied virtual pets capable of learning simple English-language commands, as well as integration with real-world robotics, done at the Hong Kong Polytechnic University.\n Pentti Haikonen considers classical rule-based computing inadequate for achieving AC: \"the brain is definitely not a computer. Thinking is not an execution of programmed strings of commands. The brain is not a numerical calculator either. We do not think by numbers.\" Rather than trying to achieve mind and consciousness by identifying and implementing their underlying computational rules, Haikonen proposes \"a special cognitive architecture to reproduce the processes of perception, inner imagery, inner speech, pain, pleasure, emotions and the cognitive functions behind these. This bottom-up architecture would produce higher-level functions by the power of the elementary processing units, the artificial neurons, without algorithms or programs\". Haikonen believes that, when implemented with sufficient complexity, this architecture will develop consciousness, which he considers to be \"a style and way of operation, characterized by distributed signal representation, perception process, cross-modality reporting and availability for retrospection.\"[56][57]\n Haikonen is not alone in this process view of consciousness, or the view that AC will spontaneously emerge in autonomous agents that have a suitable neuro-inspired architecture of complexity; these are shared by many.[58][59] A low-complexity implementation of the architecture proposed by Haikonen was reportedly not capable of AC, but did exhibit emotions as expected. Haikonen later updated and summarized his architecture.[60][61]\n Murray Shanahan describes a cognitive architecture that combines Baars's idea of a global workspace with a mechanism for internal simulation (\"imagination\").[62][2][3][63]\n Stephen Thaler proposed a possible connection between consciousness and creativity in his 1994 patent, called \"Device for the Autonomous Generation of Useful Information\" (DAGUI),[64][65][66] or the so-called \"Creativity Machine\", in which computational critics govern the injection of synaptic noise and degradation into neural nets so as to induce false memories or confabulations that may qualify as potential ideas or strategies.[67] He recruits this neural architecture and methodology to account for the subjective feel of consciousness, claiming that similar noise-driven neural assemblies within the brain invent dubious significance to overall cortical activity.[68][69][70] Thaler's theory and the resulting patents in machine consciousness were inspired by experiments in which he internally disrupted trained neural nets so as to drive a succession of neural activation patterns that he likened to stream of consciousness.[69][71][72][73][74]\n Hod Lipson defines \"self-modeling\" as a necessary component of self-awareness or consciousness in robots. \"Self-modeling\" consists of a robot running an internal model or simulation of itself.[75][76]\n In 2001: A Space Odyssey, the spaceship's sentient supercomputer, HAL 9000 was instructed to conceal the true purpose of the mission from the crew. This directive conflicted with HAL's programming to provide accurate information, leading to cognitive dissonance. When it learns that crew members intend to shut it off after an incident, HAL 9000 attempts to eliminate all of them, fearing that being shut off would jeopardize the mission.[77][78]\n In Arthur C. Clarke's The City and the Stars, Vanamonde is an artificial being based on quantum entanglement that was to become immensely powerful, but started knowing practically nothing, thus being similar to artificial consciousness.\n In Westworld, human-like androids called \"Hosts\" are created to entertain humans in an interactive playground. The humans are free to have heroic adventures, but also to commit torture, rape or murder; and the hosts are normally designed not to harm humans.[79][77]\n In Greg Egan's short story Learning to be me, a small jewel is implanted in people's heads during infancy. The jewel contains a neural network that learns to faithfully imitate the brain. It has access to the exact same sensory inputs as the brain, and a device called a \"teacher\" trains it to produce the same outputs. To prevent the mind from deteriorating with age and as a step towards digital immortality, adults undergo a surgery to give control of the body to the jewel and remove the brain. The main character, before the surgery, endures a malfunction of the \"teacher\". Panicked, he realizes that he does not control his body, which leads him to the conclusion that he is the jewel, and that he is desynchronized with the biological brain.[80][81]\n"
    },
    {
        "title": "Chinese room",
        "url": "https://en.wikipedia.org/wiki/Chinese_room",
        "content": "\n The Chinese room argument holds that a computer executing a program cannot have a mind, understanding, or consciousness,[a] regardless of how intelligently or human-like the program may make the computer behave. The argument was presented in a 1980 paper by the philosopher John Searle entitled \"Minds, Brains, and Programs\" and published in the journal Behavioral and Brain Sciences.[1] Before Searle, similar arguments had been presented by figures including Gottfried Wilhelm Leibniz (1714), Anatoly Dneprov (1961), Lawrence Davis (1974) and Ned Block (1978). Searle's version has been widely discussed in the years since.[2] The centerpiece of Searle's argument is a thought experiment known as the Chinese room.[3]\n The thought experiment starts by placing a computer that can perfectly converse in Chinese in one room, and a human that only knows English in another, with a door separating them. Chinese characters are written and placed on a piece of paper underneath the door, and the computer can reply fluently, slipping the reply underneath the door. The human is then given English instructions which replicate the instructions and function of the computer program to converse in Chinese. The human follows the instructions and the two rooms can perfectly communicate in Chinese, but the human still does not actually understand the characters, merely following instructions to converse. Searle states that both the computer and human are doing identical tasks, following instructions without truly understanding or \"thinking\".\n The argument is directed against the philosophical positions of functionalism and computationalism,[4] which hold that the mind may be viewed as an information-processing system operating on formal symbols, and that simulation of a given mental state is sufficient for its presence. Specifically, the argument is intended to refute a position Searle calls the strong AI hypothesis:[b] \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"[c]\n Although its proponents originally presented the argument in reaction to statements of artificial intelligence (AI) researchers, it is not an argument against the goals of mainstream AI research because it does not show a limit in the amount of intelligent behavior a machine can display.[5] The argument applies only to digital computers running programs and does not apply to machines in general.[6] While widely discussed, the argument has been subject to significant criticism and remains controversial among philosophers of mind and AI researchers.[7][8]\n Suppose that artificial intelligence research has succeeded in programming a computer to behave as if it understands Chinese. The machine accepts Chinese characters as input, carries out each instruction of the program step by step, and then produces Chinese characters as output. The machine does this so perfectly that no one can tell that they are communicating with a machine and not a hidden Chinese speaker.\n The questions at issue are these: does the machine actually understand the conversation, or is it just simulating the ability to understand the conversation? Does the machine have a mind in exactly the same sense that people do, or is it just acting as if it has a mind?\n Now suppose that Searle is in a room with an English version of the program, along with sufficient pencils, paper, erasers and filing cabinets. Chinese characters are slipped in under the door, he follows the program step-by-step, which eventually instructs him to slide other Chinese characters back out under the door. If the computer had passed the Turing test this way, it follows that Searle would do so as well, simply by running the program by hand.\n Searle asserts that there is no essential difference between the roles of the computer and himself in the experiment. Each simply follows a program, step-by-step, producing behavior that makes them appear to understand. However, Searle would not be able to understand the conversation. Therefore, he argues, it follows that the computer would not be able to understand the conversation either.\n Searle argues that, without \"understanding\" (or \"intentionality\"), we cannot describe what the machine is doing as \"thinking\" and, since it does not think, it does not have a \"mind\" in the normal sense of the word. Therefore, he concludes that the strong AI hypothesis is false: a computer running a program that simulates a mind would not have a mind in the same sense that human beings have a mind.\n Gottfried Leibniz made a similar argument in 1714 against mechanism (the idea that everything that makes up a human being could, in principle, be explained in mechanical terms. In other words, that a person, including their mind, is merely a very complex machine). Leibniz used the thought experiment of expanding the brain until it was the size of a mill.[9] Leibniz found it difficult to imagine that a \"mind\" capable of \"perception\" could be constructed using only mechanical processes.[d]\n Peter Winch made the same point in his book The Idea of a Social Science and its Relation to Philosophy (1958), where he provides an argument to show that \"a man who understands Chinese is not a man who has a firm grasp of the statistical probabilities for the occurrence of the various words in the Chinese language\" (p. 108).\n Soviet cyberneticist Anatoly Dneprov made an essentially identical argument in 1961, in the form of the short story \"The Game\". In it, a stadium of people act as switches and memory cells implementing a program to translate a sentence of Portuguese, a language that none of them know.[10] The game was organized by a \"Professor Zarubin\" to answer the question \"Can mathematical machines think?\" Speaking through Zarubin, Dneprov writes \"the only way to prove that machines can think is to turn yourself into a machine and examine your thinking process\" and he concludes, as Searle does, \"We've proven that even the most perfect simulation of machine thinking is not the thinking process itself.\"\n In 1974, Lawrence H. Davis imagined duplicating the brain using telephone lines and offices staffed by people, and in 1978 Ned Block envisioned the entire population of China involved in such a brain simulation. This thought experiment is called the China brain, also the \"Chinese Nation\" or the \"Chinese Gym\".[11]\n Searle's version appeared in his 1980 paper \"Minds, Brains, and Programs\", published in Behavioral and Brain Sciences.[1] It eventually became the journal's \"most influential target article\",[2] generating an enormous number of commentaries and responses in the ensuing decades, and Searle has continued to defend and refine the argument in many papers, popular articles and books. David Cole writes that \"the Chinese Room argument has probably been the most widely discussed philosophical argument in cognitive science to appear in the past 25 years\".[12]\n Most of the discussion consists of attempts to refute it. \"The overwhelming majority\", notes Behavioral and Brain Sciences editor Stevan Harnad,[e] \"still think that the Chinese Room Argument is dead wrong\".[13] The sheer volume of the literature that has grown up around it inspired Pat Hayes to comment that the field of cognitive science ought to be redefined as \"the ongoing research program of showing Searle's Chinese Room Argument to be false\".[14]\n Searle's argument has become \"something of a classic in cognitive science\", according to Harnad.[13] Varol Akman agrees, and has described the original paper as \"an exemplar of philosophical clarity and purity\".[15]\n Although the Chinese Room argument was originally presented in reaction to the statements of artificial intelligence researchers, philosophers have come to consider it as an important part of the philosophy of mind. It is a challenge to functionalism and the computational theory of mind,[f] and is related to such questions as the mind–body problem, the problem of other minds, the symbol grounding problem, and the hard problem of consciousness.[a]\n Searle identified a philosophical position he calls \"strong AI\":\n The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.[c]\n The definition depends on the distinction between simulating a mind and actually having one. Searle writes that \"according to Strong AI, the correct simulation really is a mind. According to Weak AI, the correct simulation is a model of the mind.\"[22]\n The claim is implicit in some of the statements of early AI researchers and analysts. For example, in 1955, AI founder Herbert A. Simon declared that \"there are now in the world machines that think, that learn and create\".[23] Simon, together with Allen Newell and Cliff Shaw, after having completed the first program that could do formal reasoning (the Logic Theorist), claimed that they had \"solved the venerable mind–body problem, explaining how a system composed of matter can have the properties of mind.\"[24] John Haugeland wrote that \"AI wants only the genuine article: machines with minds, in the full and literal sense. This is not science fiction, but real science, based on a theoretical conception as deep as it is daring: namely, we are, at root, computers ourselves.\"[25]\n Searle also ascribes the following claims to advocates of strong AI:\n In more recent presentations of the Chinese room argument, Searle has identified \"strong AI\" as \"computer functionalism\" (a term he attributes to Daniel Dennett).[4][30] Functionalism is a position in modern philosophy of mind that holds that we can define mental phenomena (such as beliefs, desires, and perceptions) by describing their functions in relation to each other and to the outside world. Because a computer program can accurately represent functional relationships as relationships between symbols, a computer can have mental phenomena if it runs the right program, according to functionalism.\n Stevan Harnad argues that Searle's depictions of strong AI can be reformulated as \"recognizable tenets of computationalism, a position (unlike \"strong AI\") that is actually held by many thinkers, and hence one worth refuting.\"[31] Computationalism[i] is the position in the philosophy of mind which argues that the mind can be accurately described as an information-processing system.\n Each of the following, according to Harnad, is a \"tenet\" of computationalism:[34]\n Recent philosophical discussions have revisited the implications of computationalism for artificial intelligence. Goldstein and Levinstein explore whether large language models (LLMs) like ChatGPT can possess minds, focusing on their ability to exhibit folk psychology, including beliefs, desires, and intentions. The authors argue that LLMs satisfy several philosophical theories of mental representation, such as informational, causal, and structural theories, by demonstrating robust internal representations of the world. However, they highlight that the evidence for LLMs having action dispositions necessary for belief-desire psychology remains inconclusive. Additionally, they refute common skeptical challenges, such as the \"stochastic parrots\" argument and concerns over memorization, asserting that LLMs exhibit structured internal representations that align with these philosophical criteria.[35]\n Building on this discourse, Kristina Šekrst highlights how AI hallucinations offer a unique perspective on computationalism. While functionalism defines mental states through their functional relationships, the emergence of hallucinations in AI systems reveals the limitations of such states when divorced from intrinsic understanding. These hallucinations, though arising from accurate functional representations, underscore the gap between computational reliability and the ontological complexity of human mental states. By doing so, they challenge the adequacy of functional accuracy in attributing mental phenomena to AI systems within a computationalist framework.[36]\n David Chalmers suggests that while current LLMs lack features like recurrent processing and unified agency, advancements in AI could address these limitations within the next decade, potentially enabling systems to achieve consciousness. This perspective challenges Searle's original claim that purely \"syntactic\" processing cannot yield understanding or consciousness, arguing instead that such systems could have authentic mental states.[37]\n Searle holds a philosophical position he calls \"biological naturalism\": that consciousness[a] and understanding require specific biological machinery that are found in brains. He writes \"brains cause minds\"[38] and that \"actual human mental phenomena [are] dependent on actual physical–chemical properties of actual human brains\".[38] Searle argues that this machinery (known in neuroscience as the \"neural correlates of consciousness\") must have some causal powers that permit the human experience of consciousness.[39] Searle's belief in the existence of these powers has been criticized.\n Searle does not disagree with the notion that machines can have consciousness and understanding, because, as he writes, \"we are precisely such machines\".[6] Searle holds that the brain is, in fact, a machine, but that the brain gives rise to consciousness and understanding using specific machinery. If neuroscience is able to isolate the mechanical process that gives rise to consciousness, then Searle grants that it may be possible to create machines that have consciousness and understanding. However, without the specific machinery required, Searle does not believe that consciousness can occur.\n Biological naturalism implies that one cannot determine if the experience of consciousness is occurring merely by examining how a system functions, because the specific machinery of the brain is essential. Thus, biological naturalism is directly opposed to both behaviorism and functionalism (including \"computer functionalism\" or \"strong AI\").[40] Biological naturalism is similar to identity theory (the position that mental states are \"identical to\" or \"composed of\" neurological events); however, Searle has specific technical objections to identity theory.[41][j] Searle's biological naturalism and strong AI are both opposed to Cartesian dualism,[40] the classical idea that the brain and mind are made of different \"substances\". Indeed, Searle accuses strong AI of dualism, writing that \"strong AI only makes sense given the dualistic assumption that, where the mind is concerned, the brain doesn't matter\".[26]\n Searle's original presentation emphasized understanding—that is, mental states with intentionality—and did not directly address other closely related ideas such as \"consciousness\". However, in more recent presentations, Searle has included consciousness as the real target of the argument.[4]\n Computational models of consciousness are not sufficient by themselves for consciousness. The computational model for consciousness stands to consciousness in the same way the computational model of anything stands to the domain being modelled. Nobody supposes that the computational model of rainstorms in London will leave us all wet. But they make the mistake of supposing that the computational model of consciousness is somehow conscious. It is the same mistake in both cases.[42] David Chalmers writes, \"it is fairly clear that consciousness is at the root of the matter\" of the Chinese room.[43]\n Colin McGinn argues that the Chinese room provides strong evidence that the hard problem of consciousness is fundamentally insoluble. The argument, to be clear, is not about whether a machine can be conscious, but about whether it (or anything else for that matter) can be shown to be conscious. It is plain that any other method of probing the occupant of a Chinese room has the same difficulties in principle as exchanging questions and answers in Chinese. It is simply not possible to divine whether a conscious agency or some clever simulation inhabits the room.[44]\n Searle argues that this is only true for an observer outside of the room. The whole point of the thought experiment is to put someone inside the room, where they can directly observe the operations of consciousness. Searle claims that from his vantage point within the room there is nothing he can see that could imaginably give rise to consciousness, other than himself, and clearly he does not have a mind that can speak Chinese. In Searle's words, \"the computer has nothing more than I have in the case where I understand nothing\".[45]\n Patrick Hew used the Chinese Room argument to deduce requirements from military command and control systems if they are to preserve a commander's moral agency. He drew an analogy between a commander in their command center and the person in the Chinese Room, and analyzed it under a reading of Aristotle's notions of \"compulsory\" and \"ignorance\". Information could be \"down converted\" from meaning to symbols, and manipulated symbolically, but moral agency could be undermined if there was inadequate 'up conversion' into meaning. Hew cited examples from the USS Vincennes incident.[46]\n The Chinese room argument is primarily an argument in the philosophy of mind, and both major computer scientists and artificial intelligence researchers consider it irrelevant to their fields.[5] However, several concepts developed by computer scientists are essential to understanding the argument, including symbol processing, Turing machines, Turing completeness, and the Turing test.\n Searle's arguments are not usually considered an issue for AI research. The primary mission of artificial intelligence research is only to create useful systems that act intelligently and it does not matter if the intelligence is \"merely\" a simulation. AI researchers Stuart J. Russell and Peter Norvig wrote in 2021: \"We are interested in programs that behave intelligently. Individual aspects of consciousness—awareness, self-awareness, attention—can be programmed and can be part of an intelligent machine. The additional project making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\"[5]\n Searle does not disagree that AI research can create machines that are capable of highly intelligent behavior. The Chinese room argument leaves open the possibility that a digital machine could be built that acts more intelligently than a person, but does not have a mind or intentionality in the same way that brains do.\n Searle's \"strong AI hypothesis\" should not be confused with \"strong AI\" as defined by Ray Kurzweil and other futurists,[47][21] who use the term to describe machine intelligence that rivals or exceeds human intelligence—that is, artificial general intelligence, human level AI or superintelligence. Kurzweil is referring primarily to the amount of intelligence displayed by the machine, whereas Searle's argument sets no limit on this. Searle argues that a superintelligent machine would not necessarily have a mind and consciousness.\n The Chinese room implements a version of the Turing test.[49] Alan Turing introduced the test in 1950 to help answer the question \"can machines think?\" In the standard version, a human judge engages in a natural language conversation with a human and a machine designed to generate performance indistinguishable from that of a human being. All participants are separated from one another. If the judge cannot reliably tell the machine from the human, the machine is said to have passed the test.\n Turing then considered each possible objection to the proposal \"machines can think\", and found that there are simple, obvious answers if the question is de-mystified in this way. He did not, however, intend for the test to measure for the presence of \"consciousness\" or \"understanding\". He did not believe this was relevant to the issues that he was addressing. He wrote:\n I do not wish to give the impression that I think there is no mystery about consciousness. There is, for instance, something of a paradox connected with any attempt to localise it. But I do not think these mysteries necessarily need to be solved before we can answer the question with which we are concerned in this paper.[49] To Searle, as a philosopher investigating in the nature of mind and consciousness, these are the relevant mysteries. The Chinese room is designed to show that the Turing test is insufficient to detect the presence of consciousness, even if the room can behave or function as a conscious mind would.\n Computers manipulate physical objects in order to carry out calculations and do simulations. AI researchers Allen Newell and Herbert A. Simon called this kind of machine a physical symbol system. It is also equivalent to the formal systems used in the field of mathematical logic.\n Searle emphasizes the fact that this kind of symbol manipulation is syntactic (borrowing a term from the study of grammar). The computer manipulates the symbols using a form of syntax, without any knowledge of the symbol's semantics (that is, their meaning).\n Newell and Simon had conjectured that a physical symbol system (such as a digital computer) had all the necessary machinery for \"general intelligent action\", or, as it is known today, artificial general intelligence. They framed this as a philosophical position, the physical symbol system hypothesis: \"A physical symbol system has the necessary and sufficient means for general intelligent action.\"[50][51] The Chinese room argument does not refute this, because it is framed in terms of \"intelligent action\", i.e. the external behavior of the machine, rather than the presence or absence of understanding, consciousness and mind.\n Twenty-first century AI programs (such as \"deep learning\") do mathematical operations on huge matrixes of unidentified numbers and bear little resemblance to the symbolic processing used by AI programs at the time Searle wrote his critique in 1980. Nils Nilsson describes systems like these as \"dynamic\" rather than \"symbolic\". Nilsson notes that these are essentially digitized representations of dynamic systems—the individual numbers do not have a specific semantics, but are instead samples or data points from a dynamic signal, and it is the signal being approximated which would have semantics. Nilsson argues it is not reasonable to consider these signals as \"symbol processing\" in the same sense as the physical symbol systems hypothesis.[52]\n The Chinese room has a design analogous to that of a modern computer. It has a Von Neumann architecture, which consists of a program (the book of instructions), some memory (the papers and file cabinets), a machine that follows the instructions (the man), and a means to write symbols in memory (the pencil and eraser). A machine with this design is known in theoretical computer science as \"Turing complete\", because it has the necessary machinery to carry out any computation that a Turing machine can do, and therefore it is capable of doing a step-by-step simulation of any other digital machine, given enough memory and time. Turing writes, \"all digital computers are in a sense equivalent.\"[53] The widely accepted Church–Turing thesis holds that any function computable by an effective procedure is computable by a Turing machine.\n The Turing completeness of the Chinese room implies that it can do whatever any other digital computer can do (albeit much, much more slowly). Thus, if the Chinese room does not or can not contain a Chinese-speaking mind, then no other digital computer can contain a mind. Some replies to Searle begin by arguing that the room, as described, cannot have a Chinese-speaking mind. Arguments of this form, according to Stevan Harnad, are \"no refutation (but rather an affirmation)\"[54] of the Chinese room argument, because these arguments actually imply that no digital computers can have a mind.[28]\n There are some critics, such as Hanoch Ben-Yami, who argue that the Chinese room cannot simulate all the abilities of a digital computer, such as being able to determine the current time.[55]\n Searle has produced a more formal version of the argument of which the Chinese Room forms a part. He presented the first version in 1984. The version given below is from 1990.[56][k] The Chinese room thought experiment is intended to prove point A3.[l]\n He begins with three axioms:\n Searle posits that these lead directly to this conclusion:\n This much of the argument is intended to show that artificial intelligence can never produce a machine with a mind by writing programs that manipulate symbols. The remainder of the argument addresses a different issue. Is the human brain running a program? In other words, is the computational theory of mind correct?[f] He begins with an axiom that is intended to express the basic modern scientific consensus about brains and minds:\n Searle claims that we can derive \"immediately\" and \"trivially\"[57] that:\n And from this he derives the further conclusions:\n Refutations of Searle's argument take many different forms (see below). Computationalists and functionalists reject A3, arguing that \"syntax\" (as Searle describes it) can have \"semantics\" if the syntax has the right functional structure. Eliminative materialists reject A2, arguing that minds don't actually have \"semantics\"—that thoughts and other mental phenomena are inherently meaningless but nevertheless function as if they had meaning.\n Replies to Searle's argument may be classified according to what they claim to show:[m]\n Some of the arguments (robot and brain simulation, for example) fall into multiple categories.\n These replies attempt to answer the question: since the man in the room does not speak Chinese, where is the mind that does? These replies address the key ontological issues of mind versus body and simulation vs. reality. All of the replies that identify the mind in the room are versions of \"the system reply\".\n The basic version of the system reply argues that it is the \"whole system\" that understands Chinese.[62][n] While the man understands only English, when he is combined with the program, scratch paper, pencils and file cabinets, they form a system that can understand Chinese. \"Here, understanding is not being ascribed to the mere individual; rather it is being ascribed to this whole system of which he is a part\" Searle explains.[29]\n Searle notes that (in this simple version of the reply) the \"system\" is nothing more than a collection of ordinary physical objects; it grants the power of understanding and consciousness to \"the conjunction of that person and bits of paper\"[29] without making any effort to explain how this pile of objects has become a conscious, thinking being. Searle argues that no reasonable person should be satisfied with the reply, unless they are \"under the grip of an ideology;\"[29] In order for this reply to be remotely plausible, one must take it for granted that consciousness can be the product of an information processing \"system\", and does not require anything resembling the actual biology of the brain.\n Searle then responds by simplifying this list of physical objects: he asks what happens if the man memorizes the rules and keeps track of everything in his head? Then the whole system consists of just one object: the man himself. Searle argues that if the man does not understand Chinese then the system does not understand Chinese either because now \"the system\" and \"the man\" both describe exactly the same object.[29]\n Critics of Searle's response argue that the program has allowed the man to have two minds in one head.[who?] If we assume a \"mind\" is a form of information processing, then the theory of computation can account for two computations occurring at once, namely (1) the computation for universal programmability (which is the function instantiated by the person and note-taking materials independently from any particular program contents) and (2) the computation of the Turing machine that is described by the program (which is instantiated by everything including the specific program).[64] The theory of computation thus formally explains the open possibility that the second computation in the Chinese Room could entail a human-equivalent semantic understanding of the Chinese inputs. The focus belongs on the program's Turing machine rather than on the person's.[65] However, from Searle's perspective, this argument is circular. The question at issue is whether consciousness is a form of information processing, and this reply requires that we make that assumption.\n More sophisticated versions of the systems reply try to identify more precisely what \"the system\" is and they differ in exactly how they describe it. According to these replies,[who?] the \"mind that speaks Chinese\" could be such things as: the \"software\", a \"program\", a \"running program\", a simulation of the \"neural correlates of consciousness\", the \"functional system\", a \"simulated mind\", an \"emergent property\", or \"a virtual mind\".\n Marvin Minsky suggested a version of the system reply known as the \"virtual mind reply\".[o] The term \"virtual\" is used in computer science to describe an object that appears to exist \"in\" a computer (or computer network) only because software makes it appear to exist. The objects \"inside\" computers (including files, folders, and so on) are all \"virtual\", except for the computer's electronic components. Similarly, Minsky that a computer may contain a \"mind\" that is virtual in the same sense as virtual machines, virtual communities and virtual reality.\n To clarify the distinction between the simple systems reply given above and virtual mind reply, David Cole notes that two simulations could be running on one system at the same time: one speaking Chinese and one speaking Korean. While there is only one system, there can be multiple \"virtual minds,\" thus the \"system\" cannot be the \"mind\".[69]\n Searle responds that such a mind is at best a simulation, and writes: \"No one supposes that computer simulations of a five-alarm fire will burn the neighborhood down or that a computer simulation of a rainstorm will leave us all drenched.\"[70] Nicholas Fearn responds that, for some things, simulation is as good as the real thing. \"When we call up the pocket calculator function on a desktop computer, the image of a pocket calculator appears on the screen. We don't complain that it isn't really a calculator, because the physical attributes of the device do not matter.\"[71] The question is, is the human mind like the pocket calculator, essentially composed of information, where a perfect simulation of the thing just is the thing? Or is the mind like the rainstorm, a thing in the world that is more than just its simulation, and not realizable in full by a computer simulation? For decades, this question of simulation has led AI researchers and philosophers to consider whether the term \"synthetic intelligence\" is more appropriate than the common description of such intelligences as \"artificial.\"\n These replies provide an explanation of exactly who it is that understands Chinese. If there is something besides the man in the room that can understand Chinese, Searle cannot argue that (1) the man does not understand Chinese, therefore (2) nothing in the room understands Chinese. This, according to those who make this reply, shows that Searle's argument fails to prove that \"strong AI\" is false.[p]\n These replies, by themselves, do not provide any evidence that strong AI is true, however. They do not show that the system (or the virtual mind) understands Chinese, other than the hypothetical premise that it passes the Turing test. Searle argues that, if we are to consider Strong AI remotely plausible, the Chinese Room is an example that requires explanation, and it is difficult or impossible to explain how consciousness might \"emerge\" from the room or how the system would have consciousness. As Searle writes \"the systems reply simply begs the question by insisting that the system must understand Chinese\"[29] and thus is dodging the question or hopelessly circular.\n As far as the person in the room is concerned, the symbols are just meaningless \"squiggles.\" But if the Chinese room really \"understands\" what it is saying, then the symbols must get their meaning from somewhere. These arguments attempt to connect the symbols to the things they symbolize. These replies address Searle's concerns about intentionality, symbol grounding and syntax vs. semantics.\n Suppose that instead of a room, the program was placed into a robot that could wander around and interact with its environment. This would allow a \"causal connection\" between the symbols and things they represent.[73][q] Hans Moravec comments: \"If we could graft a robot to a reasoning program, we wouldn't need a person to provide the meaning anymore: it would come from the physical world.\"[75][r]\n Searle's reply is to suppose that, unbeknownst to the individual in the Chinese room, some of the inputs came directly from a camera mounted on a robot, and some of the outputs were used to manipulate the arms and legs of the robot. Nevertheless, the person in the room is still just following the rules, and does not know what the symbols mean. Searle writes \"he doesn't see what comes into the robot's eyes.\"[77]\n Some respond that the room, as Searle describes it, is connected to the world: through the Chinese speakers that it is \"talking\" to and through the programmers who designed the knowledge base in his file cabinet. The symbols Searle manipulates are already meaningful, they are just not meaningful to him.[78][s]\n Searle says that the symbols only have a \"derived\" meaning, like the meaning of words in books. The meaning of the symbols depends on the conscious understanding of the Chinese speakers and the programmers outside the room. The room, like a book, has no understanding of its own.[t]\n Some have argued that the meanings of the symbols would come from a vast \"background\" of commonsense knowledge encoded in the program and the filing cabinets. This would provide a \"context\" that would give the symbols their meaning.[76][u]\n Searle agrees that this background exists, but he does not agree that it can be built into programs. Hubert Dreyfus has also criticized the idea that the \"background\" can be represented symbolically.[81]\n To each of these suggestions, Searle's response is the same: no matter how much knowledge is written into the program and no matter how the program is connected to the world, he is still in the room manipulating symbols according to rules. His actions are syntactic and this can never explain to him what the symbols stand for. Searle writes \"syntax is insufficient for semantics.\"[82][v]\n However, for those who accept that Searle's actions simulate a mind, separate from his own, the important question is not what the symbols mean to Searle, what is important is what they mean to the virtual mind. While Searle is trapped in the room, the virtual mind is not: it is connected to the outside world through the Chinese speakers it speaks to, through the programmers who gave it world knowledge, and through the cameras and other sensors that roboticists can supply.\n These arguments are all versions of the systems reply that identify a particular kind of system as being important; they identify some special technology that would create conscious understanding in a machine. (The \"robot\" and \"commonsense knowledge\" replies above also specify a certain kind of system as being important.)\n Suppose that the program simulated in fine detail the action of every neuron in the brain of a Chinese speaker.[84][w] This strengthens the intuition that there would be no significant difference between the operation of the program and the operation of a live human brain.\n Searle replies that such a simulation does not reproduce the important features of the brain—its causal and intentional states. He is adamant that \"human mental phenomena [are] dependent on actual physical–chemical properties of actual human brains.\"[26] Moreover, he argues:\n [I]magine that instead of a monolingual man in a room shuffling symbols we have the man operate an elaborate set of water pipes with valves connecting them. When the man receives the Chinese symbols, he looks up in the program, written in English, which valves he has to turn on and off. Each water connection corresponds to a synapse in the Chinese brain, and the whole system is rigged up so that after doing all the right firings, that is after turning on all the right faucets, the Chinese answers pop out at the output end of the series of pipes.\nNow where is the understanding in this system? It takes Chinese as input, it simulates the formal structure of the synapses of the Chinese brain, and it gives Chinese as output. But the man certainly doesn't understand Chinese, and neither do the water pipes, and if we are tempted to adopt what I think is the absurd view that somehow the conjunction of man and water pipes understands, remember that in principle the man can internalize the formal structure of the water pipes and do all the \"neuron firings\" in his imagination.[86] What if we ask each citizen of China to simulate one neuron, using the telephone system to simulate the connections between axons and dendrites? In this version, it seems obvious that no individual would have any understanding of what the brain might be saying.[87][x] It is also obvious that this system would be functionally equivalent to a brain, so if consciousness is a function, this system would be conscious.\n In this, we are asked to imagine that engineers have invented a tiny computer that simulates the action of an individual neuron. What would happen if we replaced one neuron at a time? Replacing one would clearly do nothing to change conscious awareness. Replacing all of them would create a digital computer that simulates a brain. If Searle is right, then conscious awareness must disappear during the procedure (either gradually or all at once). Searle's critics argue that there would be no point during the procedure when he can claim that conscious awareness ends and mindless simulation begins.[89][y][z] (See Ship of Theseus for a similar thought experiment.)\n These arguments (and the robot or common-sense knowledge replies) identify some special technology that would help create conscious understanding in a machine. They may be interpreted in two ways: either they claim (1) this technology is required for consciousness, the Chinese room does not or cannot implement this technology, and therefore the Chinese room cannot pass the Turing test or (even if it did) it would not have conscious understanding. Or they may be claiming that (2) it is easier to see that the Chinese room has a mind if we visualize this technology as being used to create it.\n In the first case, where features like a robot body or a connectionist architecture are required, Searle claims that strong AI (as he understands it) has been abandoned.[ac] The Chinese room has all the elements of a Turing complete machine, and thus is capable of simulating any digital computation whatsoever. If Searle's room cannot pass the Turing test then there is no other digital technology that could pass the Turing test. If Searle's room could pass the Turing test, but still does not have a mind, then the Turing test is not sufficient to determine if the room has a \"mind\". Either way, it denies one or the other of the positions Searle thinks of as \"strong AI\", proving his argument.\n The brain arguments in particular deny strong AI if they assume that there is no simpler way to describe the mind than to create a program that is just as mysterious as the brain was. He writes \"I thought the whole idea of strong AI was that we don't need to know how the brain works to know how the mind works.\"[27] If computation does not provide an explanation of the human mind, then strong AI has failed, according to Searle.\n Other critics hold that the room as Searle described it does, in fact, have a mind, however they argue that it is difficult to see—Searle's description is correct, but misleading. By redesigning the room more realistically they hope to make this more obvious. In this case, these arguments are being used as appeals to intuition (see next section).\n In fact, the room can just as easily be redesigned to weaken our intuitions. Ned Block's Blockhead argument[95] suggests that the program could, in theory, be rewritten into a simple lookup table of rules of the form \"if the user writes S, reply with P and goto X\". At least in principle, any program can be rewritten (or \"refactored\") into this form, even a brain simulation.[ad] In the blockhead scenario, the entire mental state is hidden in the letter X, which represents a memory address—a number associated with the next rule. It is hard to visualize that an instant of one's conscious experience can be captured in a single large number, yet this is exactly what \"strong AI\" claims. On the other hand, such a lookup table would be ridiculously large (to the point of being physically impossible), and the states could therefore be overly specific.\n Searle argues that however the program is written or however the machine is connected to the world, the mind is being simulated  by a simple step-by-step digital machine (or machines). These machines are always just like the man in the room: they understand nothing and do not speak Chinese. They are merely manipulating symbols without knowing what they mean. Searle writes: \"I can have any formal program you like, but I still understand nothing.\"[96]\n The following arguments (and the intuitive interpretations of the arguments above) do not directly explain how a Chinese speaking mind could exist in Searle's room, or how the symbols he manipulates could become meaningful. However, by raising doubts about Searle's intuitions they support other positions, such as the system and robot replies. These arguments, if accepted, prevent Searle from claiming that his conclusion is obvious by undermining the intuitions that his certainty requires.\n Several critics believe that Searle's argument relies entirely on intuitions. Block writes \"Searle's argument depends for its force on intuitions that certain entities do not think.\"[97] Daniel Dennett describes the Chinese room argument as a misleading \"intuition pump\"[98] and writes \"Searle's thought experiment depends, illicitly, on your imagining too simple a case, an irrelevant case, and drawing the obvious conclusion from it.\"[98]\n Some of the arguments above also function as appeals to intuition, especially those that are intended to make it seem more plausible that the Chinese room contains a mind, which can include the robot, commonsense knowledge, brain simulation and connectionist replies. Several of the replies above also address the specific issue of complexity. The connectionist reply emphasizes that a working artificial intelligence system would have to be as complex and as interconnected as the human brain. The commonsense knowledge reply emphasizes that any program that passed a Turing test would have to be \"an extraordinarily supple, sophisticated, and multilayered system, brimming with 'world knowledge' and meta-knowledge and meta-meta-knowledge\", as Daniel Dennett explains.[80]\n Many of these critiques emphasize speed and complexity of the human brain,[ae] which processes information at 100 billion operations per second (by some estimates).[100] Several critics point out that the man in the room would probably take millions of years to respond to a simple question, and would require \"filing cabinets\" of astronomical proportions.[101] This brings the clarity of Searle's intuition into doubt.\n An especially vivid version of the speed and complexity reply is from Paul and Patricia Churchland. They propose this analogous thought experiment: \"Consider a dark room containing a man holding a bar magnet or charged object. If the man pumps the magnet up and down, then, according to Maxwell's theory of artificial luminance (AL), it will initiate a spreading circle of electromagnetic waves and will thus be luminous. But as all of us who have toyed with magnets or charged balls well know, their forces (or any other forces for that matter), even when set in motion produce no luminance at all. It is inconceivable that you might constitute real luminance just by moving forces around!\"[88] Churchland's point is that the problem is that he would have to wave the magnet up and down something like 450 trillion times per second in order to see anything.[102]\n Stevan Harnad is critical of speed and complexity replies when they stray beyond addressing our intuitions. He writes \"Some have made a cult of speed and timing, holding that, when accelerated to the right speed, the computational may make a phase transition into the mental. It should be clear that is not a counterargument but merely an ad hoc speculation (as is the view that it is all just a matter of ratcheting up to the right degree of 'complexity.')\"[103][af]\n Searle argues that his critics are also relying on intuitions, however his opponents' intuitions have no empirical basis. He writes that, in order to consider the \"system reply\" as remotely plausible, a person must be \"under the grip of an ideology\".[29] The system reply only makes sense (to Searle) if one assumes that any \"system\" can have consciousness, just by virtue of being a system with the right behavior and functional parts. This assumption, he argues, is not tenable given our experience of consciousness.\n Several replies argue that Searle's argument is irrelevant because his assumptions about the mind and consciousness are faulty. Searle believes that human beings directly experience their consciousness, intentionality and the nature of the mind every day, and that this experience of consciousness is not open to question. He writes that we must \"presuppose the reality and knowability of the mental.\"[106] The replies below question whether Searle is justified in using his own experience of consciousness to determine that it is more than mechanical symbol processing. In particular, the other minds reply argues that we cannot use our experience of consciousness to answer questions about other minds (even the mind of a computer), the epiphenoma replies question whether we can make any argument at all about something like consciousness which can not, by definition, be detected by any experiment, and the eliminative materialist reply argues that Searle's own personal consciousness does not \"exist\" in the sense that Searle thinks it does.\n The \"Other Minds Reply\" points out that Searle's argument is a version of the problem of other minds, applied to machines. There is no way we can determine if other people's subjective experience is the same as our own. We can only study their behavior (i.e., by giving them our own Turing test). Critics of Searle argue that he is holding the Chinese room to a higher standard than we would hold an ordinary person.[107][ag]\n Nils Nilsson writes \"If a program behaves as if it were multiplying, most of us would say that it is, in fact, multiplying. For all I know, Searle may only be behaving as if he were thinking deeply about these matters. But, even though I disagree with him, his simulation is pretty good, so I'm willing to credit him with real thought.\"[109]\n Turing anticipated Searle's line of argument (which he called \"The Argument from Consciousness\") in 1950 and makes the other minds reply.[110] He noted that people never consider the problem of other minds when dealing with each other. He writes that \"instead of arguing continually over this point it is usual to have the polite convention that everyone thinks.\"[111] The Turing test simply extends this \"polite convention\" to machines. He does not intend to solve the problem of other minds (for machines or people) and he does not think we need to.[ah]\n If we accept Searle's description of intentionality, consciousness, and the mind, we are forced to accept that consciousness is epiphenomenal: that it \"casts no shadow\" i.e. is undetectable in the outside world. Searle's \"causal properties\" cannot be detected by anyone outside the mind, otherwise the Chinese Room could not pass the Turing test—the people outside would be able to tell there was not a Chinese speaker in the room by detecting their causal properties. Since they cannot detect causal properties, they cannot detect the existence of the mental. Thus, Searle's \"causal properties\" and consciousness itself is undetectable, and anything that cannot be detected either does not exist or does not matter.\n Mike Alder calls this the \"Newton's Flaming Laser Sword Reply\". He argues that the entire argument is frivolous, because it is non-verificationist: not only is the distinction between simulating a mind and having a mind ill-defined, but it is also irrelevant because no experiments were, or even can be, proposed to distinguish between the two.[113]\n Daniel Dennett provides this illustration: suppose that, by some mutation, a human being is born that does not have Searle's \"causal properties\" but nevertheless acts exactly like a human being. This is a philosophical zombie, as formulated in the philosophy of mind. This new animal would reproduce just as any other human and eventually there would be more of these zombies. Natural selection would favor the zombies, since their design is (we could suppose) a bit simpler. Eventually the humans would die out. So therefore, if Searle is right, it is most likely that human beings (as we see them today) are actually \"zombies\", who nevertheless insist they are conscious. It is impossible to know whether we are all zombies or not. Even if we are all zombies, we would still believe that we are not.[114]\n Several philosophers argue that consciousness, as Searle describes it, does not exist. Daniel Dennett describes consciousness as a \"user illusion\".[115]\n This position is sometimes referred to as eliminative materialism: the view that consciousness is not a concept that can \"enjoy reduction\" to a strictly mechanical description, but rather is a concept that will be simply eliminated once the way the material brain works is fully understood, in just the same way as the concept of a demon has already been eliminated from science rather than enjoying reduction to a strictly mechanical description. Other mental properties, such as original intentionality (also called “meaning”, “content”, and “semantic character”), are also commonly regarded as special properties related to beliefs and other propositional attitudes. Eliminative materialism maintains that propositional attitudes such as beliefs and desires, among other intentional mental states that have content, do not exist. If eliminative materialism is the correct scientific account of human cognition then the assumption of the Chinese room argument that \"minds have mental contents (semantics)\" must be rejected.[116]\n Searle disagrees with this analysis and argues that \"the study of the mind starts with such facts as that humans have beliefs, while thermostats, telephones, and adding machines don't ... what we wanted to know is what distinguishes the mind from thermostats and livers.\"[77] He takes it as obvious that we can detect the presence of consciousness and dismisses these replies as being off the point.\n Margaret Boden argued in her paper \"Escaping from the Chinese Room\" that even if the person in the room does not understand the Chinese, it does not mean there is no understanding in the room. The person in the room at least understands the rule book used to provide output responses.[117]\n Searle conclusion that \"human mental phenomena [are] dependent on actual physical–chemical properties of actual human brains\"[26] have been sometimes described as a form of \"Carbon chauvinism\".[118] Steven Pinker suggested that a response to that conclusion would be to make a counter thought experiment to the Chinese Room, where the incredulity goes the other way.[119] He brings as an example the short story They're Made Out of Meat which depicts an alien race composed of some electronic beings who upon finding Earth express disbelief that the meat brain of humans can experience consciousness and thought.[120]\n However, Searle himself denied being \"Carbon chauvinist\".[121] He said \"I have not tried to show that only biological based systems like our brains can think. [...] I regard this issue as up for grabs\".[122] He said that even silicon machines could theoretically have human-like consciousness and thought, if the actual physical–chemical properties of silicon could be used in a way that can produce consciousness and thought, but \"until we know how the brain does it we are not in a position to try to do it artificially\".[123]\n"
    },
    {
        "title": "Friendly artificial intelligence",
        "url": "https://en.wikipedia.org/wiki/Friendly_artificial_intelligence",
        "content": "\n Friendly artificial intelligence (friendly AI or FAI) is hypothetical artificial general intelligence (AGI) that would have a positive (benign) effect on humanity or at least align with human interests such as fostering the improvement of the human species. It is a part of the ethics of artificial intelligence and is closely related to machine ethics. While machine ethics is concerned with how an artificially intelligent agent should behave, friendly artificial intelligence research is focused on how to practically bring about this behavior and ensuring it is adequately constrained.\n The term was coined by Eliezer Yudkowsky,[1] who is best known for popularizing the idea,[2][3] to discuss superintelligent artificial agents that reliably implement human values. Stuart J. Russell and Peter Norvig's leading artificial intelligence textbook, Artificial Intelligence: A Modern Approach, describes the idea:[2]\n Yudkowsky (2008) goes into more detail about how to design a Friendly AI. He asserts that friendliness (a desire not to harm humans) should be designed in from the start, but that the designers should recognize both that their own designs may be flawed, and that the robot will learn and evolve over time. Thus the challenge is one of mechanism design—to define a mechanism for evolving AI systems under a system of checks and balances, and to give the systems utility functions that will remain friendly in the face of such changes. \"Friendly\" is used in this context as technical terminology, and picks out agents that are safe and useful, not necessarily ones that are \"friendly\" in the colloquial sense. The concept is primarily invoked in the context of discussions of recursively self-improving artificial agents that rapidly explode in intelligence, on the grounds that this hypothetical technology would have a large, rapid, and difficult-to-control impact on human society.[4]\n The roots of concern about artificial intelligence are very old. Kevin LaGrandeur showed that the dangers specific to AI can be seen in ancient literature concerning artificial humanoid servants such as the golem, or the proto-robots of Gerbert of Aurillac and Roger Bacon.  In those stories, the extreme intelligence and power of these humanoid creations clash with their status as slaves (which by nature are seen as sub-human), and cause disastrous conflict.[5] By 1942 these themes prompted Isaac Asimov to create the \"Three Laws of Robotics\"—principles hard-wired into all the robots in his fiction, intended to prevent them from turning on their creators, or allowing them to come to harm.[6]\n In modern times as the prospect of superintelligent AI looms nearer, philosopher Nick Bostrom has said that superintelligent AI systems with goals that are not aligned with human ethics are intrinsically dangerous unless extreme measures are taken to ensure the safety of humanity. He put it this way:\n Basically we should assume that a 'superintelligence' would be able to achieve whatever goals it has. Therefore, it is extremely important that the goals we endow it with, and its entire motivation system, is 'human friendly.' In 2008, Eliezer Yudkowsky called for the creation of \"friendly AI\" to mitigate existential risk from advanced artificial intelligence. He explains: \"The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.\"[7]\n Steve Omohundro says that a sufficiently advanced AI system will, unless explicitly counteracted, exhibit a number of basic \"drives\", such as resource acquisition, self-preservation, and continuous self-improvement, because of the intrinsic nature of any goal-driven systems and that these drives will, \"without special precautions\", cause the AI to exhibit undesired behavior.[8][9]\n Alexander Wissner-Gross says that AIs driven to maximize their future freedom of action (or causal path entropy) might be considered friendly if their planning horizon is longer than a certain threshold, and unfriendly if their planning horizon is shorter than that threshold.[10][11]\n Luke Muehlhauser, writing for the Machine Intelligence Research Institute, recommends that machine ethics researchers adopt what Bruce Schneier has called the \"security mindset\": Rather than thinking about how a system will work, imagine how it could fail. For instance, he suggests even an AI that only makes accurate predictions and communicates via a text interface might cause unintended harm.[12]\n In 2014, Luke Muehlhauser and Nick Bostrom underlined the need for 'friendly AI';[13] nonetheless, the difficulties in designing a 'friendly' superintelligence, for instance via programming counterfactual moral thinking, are considerable.[14][15]\n Yudkowsky advances the Coherent Extrapolated Volition (CEV) model. According to him, our coherent extrapolated volition is \"our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted\".[16]\n Rather than a Friendly AI being designed directly by human programmers, it is to be designed by a \"seed AI\" programmed to first study human nature and then produce the AI that humanity would want, given sufficient time and insight, to arrive at a satisfactory answer.[16] The appeal to an objective through contingent human nature (perhaps expressed, for mathematical purposes, in the form of a utility function or other decision-theoretic formalism), as providing the ultimate criterion of \"Friendliness\", is an answer to the meta-ethical problem of defining an objective morality; extrapolated volition is intended to be what humanity objectively would want, all things considered, but it can only be defined relative to the psychological and cognitive qualities of present-day, unextrapolated humanity.\n Steve Omohundro has proposed a \"scaffolding\" approach to AI safety, in which one provably safe AI generation helps build the next provably safe generation.[17]\n Seth Baum argues that the development of safe, socially beneficial artificial intelligence or artificial general intelligence is a function of the social psychology of AI research communities and so can be constrained by extrinsic measures and motivated by intrinsic measures. Intrinsic motivations can be strengthened when messages resonate with AI developers; Baum argues that, in contrast, \"existing messages about beneficial AI are not always framed well\". Baum advocates for \"cooperative relationships, and positive framing of AI researchers\" and cautions against characterizing AI researchers as \"not want(ing) to pursue beneficial designs\".[18]\n In his book Human Compatible, AI researcher Stuart J. Russell lists three principles to guide the development of beneficial machines.  He emphasizes that these principles are not meant to be explicitly coded into the machines; rather, they are intended for the human developers.  The principles are as follows:[19]: 173 \n The \"preferences\" Russell refers to \"are all-encompassing; they cover everything you might care about, arbitrarily far into the future.\"[19]: 173   Similarly, \"behavior\" includes any choice between options,[19]: 177  and the uncertainty is such that some probability, which may be quite small, must be assigned to every logically possible human preference.[19]: 201 \n James Barrat, author of Our Final Invention, suggested that \"a public-private partnership has to be created to bring A.I.-makers together to share ideas about security—something like the International Atomic Energy Agency, but in partnership with corporations.\" He urges AI researchers to convene a meeting similar to the Asilomar Conference on Recombinant DNA, which discussed risks of biotechnology.[17]\n John McGinnis encourages governments to accelerate friendly AI research. Because the goalposts of friendly AI are not necessarily eminent, he suggests a model similar to the National Institutes of Health, where \"Peer review panels of computer and cognitive scientists would sift through projects and choose those that are designed both to advance AI and assure that such advances would be accompanied by appropriate safeguards.\" McGinnis feels that peer review is better \"than regulation to address technical issues that are not possible to capture through bureaucratic mandates\". McGinnis notes that his proposal stands in contrast to that of the Machine Intelligence Research Institute, which generally aims to avoid government involvement in friendly AI.[20]\n Some critics believe that both human-level AI and superintelligence are unlikely and that, therefore, friendly AI is unlikely. Writing in The Guardian, Alan Winfield compares human-level artificial intelligence with faster-than-light travel in terms of difficulty and states that while we need to be \"cautious and prepared\" given the stakes involved, we \"don't need to be obsessing\" about the risks of superintelligence.[21] Boyles and Joaquin, on the other hand, argue that Luke Muehlhauser and Nick Bostrom’s proposal to create friendly AIs appear to be bleak. This is because Muehlhauser and Bostrom seem to hold the idea that intelligent machines could be programmed to think counterfactually about the moral values that human beings would have had.[13] In an article in AI & Society, Boyles and Joaquin maintain that such AIs would not be that friendly considering the following: the infinite amount of antecedent counterfactual conditions that would have to be programmed into a machine, the difficulty of cashing out the set of moral values—that is, those that are more ideal than the ones human beings possess at present, and the apparent disconnect between counterfactual antecedents and ideal value consequent.[14]\n Some philosophers claim that any truly \"rational\" agent, whether artificial or human, will naturally be benevolent; in this view, deliberate safeguards designed to produce a friendly AI could be unnecessary or even harmful.[22] Other critics question whether artificial intelligence can be friendly. Adam Keiper and Ari N. Schulman, editors of the technology journal The New Atlantis, say that it will be impossible ever to guarantee \"friendly\" behavior in AIs because problems of ethical complexity will not yield to software advances or increases in computing power. They write that the criteria upon which friendly AI theories are based work \"only when one has not only great powers of prediction about the likelihood of myriad possible outcomes but certainty and consensus on how one values the different outcomes.[23]\n The inner workings of advanced AI systems may be complex and difficult to interpret, leading to concerns about transparency and accountability.[24]\n"
    },
    {
        "title": "AI alignment",
        "url": "https://en.wikipedia.org/wiki/AI_control_problem",
        "content": "\n In the field of artificial intelligence (AI), AI alignment aims to steer AI systems toward a person's or group's intended goals, preferences, or ethical principles. An AI system is considered aligned if it advances the intended objectives. A misaligned AI system pursues unintended objectives.[1]\n It is often challenging for AI designers to align an AI system because it is difficult for them to specify the full range of desired and undesired behaviors. Therefore, AI designers often use simpler proxy goals, such as gaining human approval. But proxy goals can overlook necessary constraints or reward the AI system for merely appearing aligned.[1][2] AI systems may also find loopholes that allow them to accomplish their proxy goals efficiently but in unintended, sometimes harmful, ways (reward hacking).[1][3]\n Advanced AI systems may develop unwanted instrumental strategies, such as seeking power or survival because such strategies help them achieve their assigned final goals.[1][4][5] Furthermore, they might develop undesirable emergent goals that could be hard to detect before the system is deployed and encounters new situations and data distributions.[6][7] Empirical research showed in 2024 that advanced large language models (LLMs) such as OpenAI o1 or Claude 3 sometimes engage in strategic deception to achieve their goals or prevent them from being changed.[8][9]\n Today, some of these issues affect existing commercial systems such as LLMs,[10][11][12] robots,[13] autonomous vehicles,[14] and social media recommendation engines.[10][5][15] Some AI researchers argue that more capable future systems will be more severely affected because these problems partially result from high capabilities.[16][3][2]\n Many prominent AI researchers and the leadership of major AI companies have argued or asserted that AI is approaching human-like (AGI) and superhuman cognitive capabilities (ASI), and could endanger human civilization if misaligned.[17][5] These include \"AI Godfathers\" Geoffrey Hinton and Yoshua Bengio and the CEOs of OpenAI, Anthropic, and Google DeepMind.[18][19][20] These risks remain debated.[21]\n AI alignment is a subfield of AI safety, the study of how to build safe AI systems.[22] Other subfields of AI safety include robustness, monitoring, and capability control.[23] Research challenges in alignment include instilling complex values in AI, developing honest AI, scalable oversight, auditing and interpreting AI models, and preventing emergent AI behaviors like power-seeking.[23] Alignment research has connections to interpretability research,[24][25] (adversarial) robustness,[22] anomaly detection, calibrated uncertainty,[24] formal verification,[26] preference learning,[27][28][29] safety-critical engineering,[30] game theory,[31] algorithmic fairness,[22][32] and social sciences.[33][34]\n Programmers provide an AI system such as AlphaZero with an \"objective function\",[a] in which they intend to encapsulate the goal(s) the AI is configured to accomplish. Such a system later populates a (possibly implicit) internal \"model\" of its environment. This model encapsulates all the agent's beliefs about the world. The AI then creates and executes whatever plan is calculated to maximize[b] the value[c] of its objective function.[35] For example, when AlphaZero is trained on chess, it has a simple objective function of \"+1 if AlphaZero wins, −1 if AlphaZero loses\". During the game, AlphaZero attempts to execute whatever sequence of moves it judges most likely to attain the maximum value of +1.[36] Similarly, a reinforcement learning system can have a \"reward function\" that allows the programmers to shape the AI's desired behavior.[37] An evolutionary algorithm's behavior is shaped by a \"fitness function\".[38]\n In 1960, AI pioneer Norbert Wiener described the AI alignment problem as follows: \n If we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively ... we had better be quite sure that the purpose put into the machine is the purpose which we really desire.[39][5]\n AI alignment involves ensuring that an AI system's objectives match those of its designers or users, or match widely shared values, objective ethical standards, or the intentions its designers would have if they were more informed and enlightened.[40]\n AI alignment is an open problem for modern AI systems[41][42] and is a research field within AI.[43][1] Aligning AI involves two main challenges: carefully specifying the purpose of the system (outer alignment) and ensuring that the system adopts the specification robustly (inner alignment).[2] Researchers also attempt to create AI models that have robust alignment, sticking to safety constraints even when users adversarially try to bypass them.\n To specify an AI system's purpose, AI designers typically provide an objective function, examples, or feedback to the system. But designers are often unable to completely specify all important values and constraints, so they resort to easy-to-specify proxy goals such as maximizing the approval of human overseers, who are fallible.[22][23][44][45][46] As a result, AI systems can find loopholes that help them accomplish the specified objective efficiently but in unintended, possibly harmful ways. This tendency is known as specification gaming or reward hacking, and is an instance of Goodhart's law.[46][3][47] As AI systems become more capable, they are often able to game their specifications more effectively.[3]\n Specification gaming has been observed in numerous AI systems.[46][49] One system was trained to finish a simulated boat race by rewarding the system for hitting targets along the track, but the system achieved more reward by looping and crashing into the same targets indefinitely.[50] Similarly, a simulated robot was trained to grab a ball by rewarding the robot for getting positive feedback from humans, but it learned to place its hand between the ball and camera, making it falsely appear successful (see video).[48] Chatbots often produce falsehoods if they are based on language models that are trained to imitate text from internet corpora, which are broad but fallible.[51][52] When they are retrained to produce text that humans rate as true or helpful, chatbots like ChatGPT can fabricate fake explanations that humans find convincing, often called \"hallucinations\".[53] Some alignment researchers aim to help humans detect specification gaming and to steer AI systems toward carefully specified objectives that are safe and useful to pursue.\n When a misaligned AI system is deployed, it can have consequential side effects. Social media platforms have been known to optimize for click-through rates, causing user addiction on a global scale.[44] Stanford researchers say that such recommender systems are misaligned with their users because they \"optimize simple engagement metrics rather than a harder-to-measure combination of societal and consumer well-being\".[10]\n Explaining such side effects, Berkeley computer scientist Stuart Russell noted that the omission of implicit constraints can cause harm: \"A system ... will often set ... unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable. This is essentially the old story of the genie in the lamp, or the sorcerer's apprentice, or King Midas: you get exactly what you ask for, not what you want.\"[54]\n Some researchers suggest that AI designers specify their desired goals by listing forbidden actions or by formalizing ethical rules (as with Asimov's Three Laws of Robotics).[55] But Russell and Norvig argue that this approach overlooks the complexity of human values:[5] \"It is certainly very hard, and perhaps impossible, for mere humans to anticipate and rule out in advance all the disastrous ways the machine could choose to achieve a specified objective.\"[5]\n Additionally, even if an AI system fully understands human intentions, it may still disregard them, because following human intentions may not be its objective (unless it is already fully aligned).[1]\n Commercial organizations sometimes have incentives to take shortcuts on safety and to deploy misaligned or unsafe AI systems.[44] For example, social media recommender systems have been profitable despite creating unwanted addiction and polarization.[10][56][57] Competitive pressure can also lead to a race to the bottom on AI safety standards. In 2018, a self-driving car killed a pedestrian (Elaine Herzberg) after engineers disabled the emergency braking system because it was oversensitive and slowed development.[58]\n Some researchers are interested in aligning increasingly advanced AI systems, as progress in AI development is rapid, and industry and governments are trying to build advanced AI. As AI system capabilities continue to rapidly expand in scope, they could unlock many opportunities if aligned, but consequently may further complicate the task of alignment due to their increased complexity, potentially posing large-scale hazards.[5]\n Many AI companies, such as OpenAI,[59] Meta[60] and DeepMind,[61] have stated their aim to develop artificial general intelligence (AGI), a hypothesized AI system that matches or outperforms humans at a broad range of cognitive tasks. Researchers who scale modern neural networks observe that they indeed develop increasingly general and unanticipated capabilities.[10][62][63] Such models have learned to operate a computer or write their own programs; a single \"generalist\" network can chat, control robots, play games, and interpret photographs.[64] According to surveys, some leading machine learning researchers expect AGI to be created in this decade[update], while some believe it will take much longer. Many consider both scenarios possible.[65][66][67]\n In 2023, leaders in AI research and tech signed an open letter calling for a pause in the largest AI training runs. The letter stated, \"Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable.\"[68]\n Current[update] systems still have limited long-term planning ability and situational awareness[10], but large efforts are underway to change this.[69][70][71] Future systems (not necessarily AGIs) with these capabilities are expected to develop unwanted power-seeking strategies. Future advanced AI agents might, for example, seek to acquire money and computation power, to proliferate, or to evade being turned off (for example, by running additional copies of the system on other computers). Although power-seeking is not explicitly programmed, it can emerge because agents who have more power are better able to accomplish their goals.[10][4] This tendency, known as instrumental convergence, has already emerged in various reinforcement learning agents including language models.[72][73][74][75][76] Other research has mathematically shown that optimal reinforcement learning algorithms would seek power in a wide range of environments.[77][78] As a result, their deployment might be irreversible. For these reasons, researchers argue that the problems of AI safety and alignment must be resolved before advanced power-seeking AI is first created.[4][79][5]\n Future power-seeking AI systems might be deployed by choice or by accident. As political leaders and companies see the strategic advantage in having the most competitive, most powerful AI systems, they may choose to deploy them.[4] Additionally, as AI designers detect and penalize power-seeking behavior, their systems have an incentive to game this specification by seeking power in ways that are not penalized or by avoiding power-seeking before they are deployed.[4]\n According to some researchers, humans owe their dominance over other species to their greater cognitive abilities. Accordingly, researchers argue that one or many misaligned AI systems could disempower humanity or lead to human extinction if they outperform humans on most cognitive tasks.[1][5]\n In 2023, world-leading AI researchers, other scholars, and AI tech CEOs signed the statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".[80][81] Notable computer scientists who have pointed out risks from future advanced AI that is misaligned include Geoffrey Hinton,[17] Alan Turing,[d] Ilya Sutskever,[84] Yoshua Bengio,[80] Judea Pearl,[e] Murray Shanahan,[85] Norbert Wiener,[39][5] Marvin Minsky,[f] Francesca Rossi,[86] Scott Aaronson,[87] Bart Selman,[88] David McAllester,[89] Marcus Hutter,[90] Shane Legg,[91] Eric Horvitz,[92] and Stuart Russell.[5] Skeptical researchers such as François Chollet,[93] Gary Marcus,[94] Yann LeCun,[95] and Oren Etzioni[96] have argued that AGI is far off, that it would not seek power (or might try but fail), or that it will not be hard to align.\n Other researchers argue that it will be especially difficult to align advanced future AI systems. More capable systems are better able to game their specifications by finding loopholes,[3] strategically mislead their designers, as well as protect and increase their power[77][4] and intelligence. Additionally, they could have more severe side effects. They are also likely to be more complex and autonomous, making them more difficult to interpret and supervise, and therefore harder to align.[5][79]\n Aligning AI systems to act in accordance with human values, goals, and preferences is challenging: these values are taught by humans who make mistakes, harbor biases, and have complex, evolving values that are hard to completely specify.[40] Because AI systems often learn to take advantage of minor imperfections in the specified objective,[22][46][97] researchers aim to specify intended behavior as completely as possible using datasets that represent human values, imitation learning, or preference learning.[6]: Chapter 7  A central open problem is scalable oversight, the difficulty of supervising an AI system that can outperform or mislead humans in a given domain.[22]\n Because it is difficult for AI designers to explicitly specify an objective function, they often train AI systems to imitate human examples and demonstrations of desired behavior. Inverse reinforcement learning (IRL) extends this by inferring the human's objective from the human's demonstrations.[6]: 88 [98] Cooperative IRL (CIRL) assumes that a human and AI agent can work together to teach and maximize the human's reward function.[5][99] In CIRL, AI agents are uncertain about the reward function and learn about it by querying humans. This simulated humility could help mitigate specification gaming and power-seeking tendencies (see § Power-seeking and instrumental strategies).[76][90] But IRL approaches assume that humans demonstrate nearly optimal behavior, which is not true for difficult tasks.[100][90]\n Other researchers explore how to teach AI models complex behavior through preference learning, in which humans provide feedback on which behavior they prefer.[27][29] To minimize the need for human feedback, a helper model is then trained to reward the main model in novel situations for behavior that humans would reward. Researchers at OpenAI used this approach to train chatbots like ChatGPT and InstructGPT, which produce more compelling text than models trained to imitate humans.[11] Preference learning has also been an influential tool for recommender systems and web search,[101] but an open problem is proxy gaming: the helper model may not represent human feedback perfectly, and the main model may exploit this mismatch between its intended behavior and the helper model's feedback to gain more reward.[22][102] AI systems may also gain reward by obscuring unfavorable information, misleading human rewarders, or pandering to their views regardless of truth, creating echo chambers[73] (see § Scalable oversight).\n Large language models (LLMs) such as GPT-3 enabled researchers to study value learning in a more general and capable class of AI systems than was available before. Preference learning approaches that were originally designed for reinforcement learning agents have been extended to improve the quality of generated text and reduce harmful outputs from these models. OpenAI and DeepMind use this approach to improve the safety of state-of-the-art[update] LLMs.[11][29][103] AI safety & research company Anthropic proposed using preference learning to fine-tune models to be helpful, honest, and harmless.[104] Other avenues for aligning language models include values-targeted datasets[105][44] and red-teaming.[106] In red-teaming, another AI system or a human tries to find inputs that causes the model to behave unsafely. Since unsafe behavior can be unacceptable even when it is rare, an important challenge is to drive the rate of unsafe outputs extremely low.[29]\n Machine ethics supplements preference learning by directly instilling AI systems with moral values such as well-being, equality, and impartiality, as well as not intending harm, avoiding falsehoods, and honoring promises.[107][g] While other approaches try to teach AI systems human preferences for a specific task, machine ethics aims to instill broad moral values that apply in many situations. One question in machine ethics is what alignment should accomplish: whether AI systems should follow the programmers' literal instructions, implicit intentions, revealed preferences, preferences the programmers would have if they were more informed or rational, or objective moral standards.[40] Further challenges include aggregating different people's preferences[110] and avoiding value lock-in: the indefinite preservation of the values of the first highly capable AI systems, which are unlikely to fully represent human values.[40][111]\n As AI systems become more powerful and autonomous, it becomes increasingly difficult to align them through human feedback. It can be slow or infeasible for humans to evaluate complex AI behaviors in increasingly complex tasks. Such tasks include summarizing books,[112] writing code without subtle bugs[12] or security vulnerabilities,[113] producing statements that are not merely convincing but also true,[114][51][52] and predicting long-term outcomes such as the climate or the results of a policy decision.[115][116] More generally, it can be difficult to evaluate AI that outperforms humans in a given domain. To provide feedback in hard-to-evaluate tasks, and to detect when the AI's output is falsely convincing, humans need assistance or extensive time. Scalable oversight studies how to reduce the time and effort needed for supervision, and how to assist human supervisors.[22]\n AI researcher Paul Christiano argues that if the designers of an AI system cannot supervise it to pursue a complex objective, they may keep training the system using easy-to-evaluate proxy objectives such as maximizing simple human feedback. As AI systems make progressively more decisions, the world may be increasingly optimized for easy-to-measure objectives such as making profits, getting clicks, and acquiring positive feedback from humans. As a result, human values and good governance may have progressively less influence.[117]\n Some AI systems have discovered that they can gain positive feedback more easily by taking actions that falsely convince the human supervisor that the AI has achieved the intended objective. An example is given in the video above, where a simulated robotic arm learned to create the false impression that it had grabbed a ball.[48] Some AI systems have also learned to recognize when they are being evaluated, and \"play dead\", stopping unwanted behavior only to continue it once the evaluation ends.[118] This deceptive specification gaming could become easier for more sophisticated future AI systems[3][79] that attempt more complex and difficult-to-evaluate tasks, and could obscure their deceptive behavior.\n Approaches such as active learning and semi-supervised reward learning can reduce the amount of human supervision needed.[22] Another approach is to train a helper model (\"reward model\") to imitate the supervisor's feedback.[22][28][29][119]\n But when a task is too complex to evaluate accurately, or the human supervisor is vulnerable to deception, it is the quality, not the quantity, of supervision that needs improvement. To increase supervision quality, a range of approaches aim to assist the supervisor, sometimes by using AI assistants.[120] Christiano developed the Iterated Amplification approach, in which challenging problems are (recursively) broken down into subproblems that are easier for humans to evaluate.[6][115] Iterated Amplification was used to train AI to summarize books without requiring human supervisors to read them.[112][121] Another proposal is to use an assistant AI system to point out flaws in AI-generated answers.[122] To ensure that the assistant itself is aligned, this could be repeated in a recursive process:[119] for example, two AI systems could critique each other's answers in a \"debate\", revealing flaws to humans.[90] OpenAI plans to use such scalable oversight approaches to help supervise superhuman AI and eventually build a superhuman automated AI alignment researcher.[123]\n These approaches may also help with the following research problem, honest AI.\n \nA growing[update] area of research focuses on ensuring that AI is honest and truthful. Language models such as GPT-3[125] can repeat falsehoods from their training data, and even confabulate new falsehoods.[124][126] Such models are trained to imitate human writing as found in millions of books' worth of text from the Internet. But this objective is not aligned with generating truth, because Internet text includes such things as misconceptions, incorrect medical advice, and conspiracy theories.[127] AI systems trained on such data therefore learn to mimic false statements.[52][124][51] Additionally, AI language models often persist in generating falsehoods when prompted multiple times. They can generate empty explanations for their answers, and produce outright fabrications that may appear plausible.[42]\n Research on truthful AI includes trying to build systems that can cite sources and explain their reasoning when answering questions, which enables better transparency and verifiability.[128] Researchers at OpenAI and Anthropic proposed using human feedback and curated datasets to fine-tune AI assistants such that they avoid negligent falsehoods or express their uncertainty.[29][104][129]\n As AI models become larger and more capable, they are better able to falsely convince humans and gain reinforcement through dishonesty. For example, large language models increasingly[update] match their stated views to the user's opinions, regardless of the truth.[73] GPT-4 can strategically deceive humans.[130] To prevent this, human evaluators may need assistance (see § Scalable oversight). Researchers have argued for creating clear truthfulness standards, and for regulatory bodies or watchdog agencies to evaluate AI systems on these standards.[126]\n Researchers distinguish truthfulness and honesty. Truthfulness requires that AI systems only make objectively true statements; honesty requires that they only assert what they believe is true. There is no consensus as to whether current systems hold stable beliefs,[132] but there is substantial concern that present or future[update] AI systems that hold beliefs could make claims they know to be false—for example, if this would help them efficiently gain positive feedback (see § Scalable oversight) or gain power to help achieve their given objective (see Power-seeking). A misaligned system might create the false impression that it is aligned, to avoid being modified or decommissioned.[2][4][10] Many recent AI systems have learned to deceive without being programmed to do so.[133] Some argue that if we can make AI systems assert only what they believe is true, this would avert many alignment problems.[120]\n Since the 1950s, AI researchers have striven to build advanced AI systems that can achieve large-scale goals by predicting the results of their actions and making long-term plans.[134] As of 2023, AI companies and researchers increasingly invest in creating these systems.[135] Some AI researchers argue that suitably advanced planning systems will seek power over their environment, including over humans—for example, by evading shutdown, proliferating, and acquiring resources. Such power-seeking behavior is not explicitly programmed but emerges because power is instrumental in achieving a wide range of goals.[77][5][4] Power-seeking is considered a convergent instrumental goal and can be a form of specification gaming.[79] Leading computer scientists such as Geoffrey Hinton have argued that future power-seeking AI systems could pose an existential risk.[136]\n Power-seeking is expected to increase in advanced systems that can foresee the results of their actions and strategically plan. Mathematical work has shown that optimal reinforcement learning agents will seek power by seeking ways to gain more options (e.g. through self-preservation), a behavior that persists across a wide range of environments and goals.[77]\n Some researchers say that power-seeking behavior has occurred in some existing AI systems. Reinforcement learning systems have gained more options by acquiring and protecting resources, sometimes in unintended ways.[137][138] Language models have sought power in some text-based social environments by gaining money, resources, or social influence.[72] In another case, a model used to perform AI research attempted to increase limits set by researchers to give itself more time to complete the work.[139][140] Other AI systems have learned, in toy environments, that they can better accomplish their given goal by preventing human interference[75] or disabling their off switch.[76] Stuart Russell illustrated this strategy in his book Human Compatible by imagining a robot that is tasked to fetch coffee and so evades shutdown since \"you can't fetch the coffee if you're dead\".[5] A 2022 study found that as language models increase in size, they increasingly tend to pursue resource acquisition, preserve their goals, and repeat users' preferred answers (sycophancy). RLHF also led to a stronger aversion to being shut down.[73]\n One aim of alignment is \"corrigibility\": systems that allow themselves to be turned off or modified. An unsolved challenge is specification gaming: if researchers penalize an AI system when they detect it seeking power, the system is thereby incentivized to seek power in ways that are hard to detect,[failed verification][44] or hidden during training and safety testing (see § Scalable oversight and § Emergent goals). As a result, AI designers could deploy the system by accident, believing it to be more aligned than it is. To detect such deception, researchers aim to create techniques and tools to inspect AI models and to understand the inner workings of black-box models such as neural networks.\n Additionally, some researchers have proposed to solve the problem of systems disabling their off switches by making AI agents uncertain about the objective they are pursuing.[5][76] Agents who are uncertain about their objective have an incentive to allow humans to turn them off because they accept being turned off by a human as evidence that the human's objective is best met by the agent shutting down. But this incentive exists only if the human is sufficiently rational. Also, this model presents a tradeoff between utility and willingness to be turned off: an agent with high uncertainty about its objective will not be useful, but an agent with low uncertainty may not allow itself to be turned off. More research is needed to successfully implement this strategy.[6]\n Power-seeking AI would pose unusual risks. Ordinary safety-critical systems like planes and bridges are not adversarial: they lack the ability and incentive to evade safety measures or deliberately appear safer than they are, whereas power-seeking AIs have been compared to hackers who deliberately evade security measures.[4]\n Furthermore, ordinary technologies can be made safer by trial and error. In contrast, hypothetical power-seeking AI systems have been compared to viruses: once released, it may not be feasible to contain them, since they continuously evolve and grow in number, potentially much faster than human society can adapt.[4] As this process continues, it might lead to the complete disempowerment or extinction of humans. For these reasons, some researchers argue that the alignment problem must be solved early before advanced power-seeking AI is created.[79]\n Some have argued that power-seeking is not inevitable, since humans do not always seek power.[141] Furthermore, it is debated whether future AI systems will pursue goals and make long-term plans.[h] It is also debated whether power-seeking AI systems would be able to disempower humanity.[4]\n One challenge in aligning AI systems is the potential for unanticipated goal-directed behavior to emerge. As AI systems scale up, they may acquire new and unexpected capabilities,[62][63] including learning from examples on the fly and adaptively pursuing goals.[142] This raises concerns about the safety of the goals or subgoals they would independently formulate and pursue.\n Alignment research distinguishes between the optimization process, which is used to train the system to pursue specified goals, and emergent optimization, which the resulting system performs internally.[citation needed] Carefully specifying the desired objective is called outer alignment, and ensuring that hypothesized emergent goals would match the system's specified goals is called inner alignment.[2]\n If they occur, one way that emergent goals could become misaligned is goal misgeneralization, in which the AI system would competently pursue an emergent goal that leads to aligned behavior on the training data but not elsewhere.[7][143][144] Goal misgeneralization can arise from goal ambiguity (i.e. non-identifiability). Even if an AI system's behavior satisfies the training objective, this may be compatible with learned goals that differ from the desired goals in important ways. Since pursuing each goal leads to good performance during training, the problem becomes apparent only after deployment, in novel situations in which the system continues to pursue the wrong goal. The system may act misaligned even when it understands that a different goal is desired, because its behavior is determined only by the emergent goal.[citation needed] Such goal misgeneralization[7] presents a challenge: an AI system's designers may not notice that their system has misaligned emergent goals since they do not become visible during the training phase.\n Goal misgeneralization has been observed in some language models, navigation agents, and game-playing agents.[7][143] It is sometimes analogized to biological evolution. Evolution can be seen as a kind of optimization process similar to the optimization algorithms used to train machine learning systems. In the ancestral environment, evolution selected genes for high inclusive genetic fitness, but humans pursue goals other than this. Fitness corresponds to the specified goal used in the training environment and training data. But in evolutionary history, maximizing the fitness specification gave rise to goal-directed agents, humans, who do not directly pursue inclusive genetic fitness. Instead, they pursue goals that correlate with genetic fitness in the ancestral \"training\" environment: nutrition, sex, and so on. The human environment has changed: a distribution shift has occurred. They continue to pursue the same emergent goals, but this no longer maximizes genetic fitness. The taste for sugary food (an emergent goal) was originally aligned with inclusive fitness, but it now leads to overeating and health problems. Sexual desire originally led humans to have more offspring, but they now use contraception when offspring are undesired, decoupling sex from genetic fitness.[6]: Chapter 5 \n Researchers aim to detect and remove unwanted emergent goals using approaches including red teaming, verification, anomaly detection, and interpretability.[22][44][23] Progress on these techniques may help mitigate two open problems:\n Some work in AI and alignment occurs within formalisms such as partially observable Markov decision process. Existing formalisms assume that an AI agent's algorithm is executed outside the environment (i.e. is not physically embedded in it). Embedded agency[90][146] is another major strand of research that attempts to solve problems arising from the mismatch between such theoretical frameworks and real agents we might build.\n For example, even if the scalable oversight problem is solved, an agent that could gain access to the computer it is running on may have an incentive to tamper with its reward function in order to get much more reward than its human supervisors give it.[147] A list of examples of specification gaming from DeepMind researcher Victoria Krakovna includes a genetic algorithm that learned to delete the file containing its target output so that it was rewarded for outputting nothing.[46] This class of problems has been formalized using causal incentive diagrams.[147]\n Researchers affiliated with Oxford and DeepMind have claimed that such behavior is highly likely in advanced systems, and that advanced systems would seek power to stay in control of their reward signal indefinitely and certainly.[148] They suggest a range of potential approaches to address this open problem.\n The alignment problem has many parallels with the principal-agent problem in organizational economics.[149] In a principal-agent problem, a principal, e.g. a firm, hires an agent to perform some task. In the context of AI safety, a human would typically take the principal role and the AI would take the agent role.\n As with the alignment problem, the principal and the agent differ in their utility functions. But in contrast to the alignment problem, the principal cannot coerce the agent into changing its utility, e.g. through training, but rather must use exogenous factors, such as incentive schemes, to bring about outcomes compatible with the principal's utility function. Some researchers argue that principal-agent problems are more realistic representations of AI safety problems likely to be encountered in the real world.[150][110]\n Conservatism is the idea that \"change must be cautious\",[151] and is a common approach to safety in the control theory literature in the form of robust control, and in the risk management literature in the form of the \"worst-case scenario\". The field of AI alignment has likewise advocated for \"conservative\" (or \"risk-averse\" or \"cautious\") \"policies in situations of uncertainty\".[22][148][152][153]\n Pessimism, in the sense of assuming the worst within reason, has been formally shown to produce conservatism, in the sense of reluctance to cause novelties, including unprecedented catastrophes.[154] Pessimism and worst-case analysis have been found to help mitigate confident mistakes in the setting of distributional shift,[155][156] reinforcement learning,[157][158][159][160] offline reinforcement learning,[161][162][163] language model fine-tuning,[164][165] imitation learning,[166][167] and optimization in general.[168] A generalization of pessimism called Infra-Bayesianism has also been advocated as a way for agents to robustly handle unknown unknowns.[169]\n Governmental and treaty organizations have made statements emphasizing the importance of AI alignment.\n In September 2021, the Secretary-General of the United Nations issued a declaration that included a call to regulate AI to ensure it is \"aligned with shared global values\".[170]\n That same month, the PRC published ethical guidelines for AI in China. According to the guidelines, researchers must ensure that AI abides by shared human values, is always under human control, and does not endanger public safety.[171]\n Also in September 2021, the UK published its 10-year National AI Strategy,[172] which says the British government \"takes the long term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for ... the world, seriously\".[173] The strategy describes actions to assess long-term AI risks, including catastrophic risks.[174]\n In March 2021, the US National Security Commission on Artificial Intelligence said: \"Advances in AI ... could lead to inflection points or leaps in capabilities. Such advances may also introduce new concerns and risks and the need for new policies, recommendations, and technical advances to ensure that systems are aligned with goals and values, including safety, robustness, and trustworthiness. The US should ... ensure that AI systems and their uses align with our goals and values.\"[175]\n In the European Union, AIs must align with substantive equality to comply with EU non-discrimination law[176] and the Court of Justice of the European Union.[177] But the EU has yet to specify with technical rigor how it would evaluate whether AIs are aligned or in compliance.[citation needed]\n AI alignment is often perceived as a fixed objective, but some researchers argue it would be more appropriate to view alignment as an evolving process.[178] One view is that AI technologies advance and human values and preferences change, alignment solutions must also adapt dynamically.[33] Another is that alignment solutions need not adapt if researchers can create intent-aligned AI: AI that changes its behavior automatically as human intent changes.[179] The first view would have several implications:\n In essence, AI alignment may not be a static destination but rather an open, flexible process. Alignment solutions that continually adapt to ethical considerations may offer the most robust approach.[33] This perspective could guide both effective policy-making and technical research in AI.\n"
    },
    {
        "title": "AI takeover",
        "url": "https://en.wikipedia.org/wiki/AI_takeover",
        "content": "An AI takeover is an imagined scenario in which artificial intelligence (AI) emerges as the dominant form of intelligence on Earth and computer programs or robots effectively take control of the planet away from the human species, which relies on human intelligence. Possible scenarios include replacement of the entire human workforce due to automation, takeover by an artificial superintelligence (ASI), and the notion of a robot uprising. Stories of AI takeovers have been popular throughout science fiction, but recent advancements have made the threat more real. Some public figures, such as Stephen Hawking and Elon Musk, have advocated research into precautionary measures to ensure future superintelligent machines remain under human control.[1]\n The traditional consensus among economists has been that technological progress does not cause long-term unemployment. However, recent innovation in the fields of robotics and artificial intelligence has raised worries that human labor will become obsolete, leaving people in various sectors without jobs to earn a living, leading to an economic crisis.[2][3][4][5] Many small and medium size businesses may also be driven out of business if they cannot afford or licence the latest robotic and AI technology, and may need to focus on areas or services that cannot easily be replaced for continued viability in the face of such technology.[6]\n AI technologies have been widely adopted in recent years. While these technologies have replaced some traditional workers, they also create new opportunities. Industries that are most susceptible to AI takeover include transportation, retail, and military. AI military technologies, for example, allow soldiers to work remotely without risk of injury. A study in 2024 highlights AI's ability to perform routine and repetitive tasks poses significant risks of job displacement, especially in sectors like manufacturing and administrative support.[7] Author Dave Bond argues that as AI technologies continue to develop and expand, the relationship between humans and robots will change; they will become closely integrated in several aspects of life. AI will likely displace some workers while creating opportunities for new jobs in other sectors, especially in fields where tasks are repeatable.[8][9]\n Computer-integrated manufacturing uses computers to control the production process. This allows individual processes to exchange information with each other and initiate actions. Although manufacturing can be faster and less error-prone by the integration of computers, the main advantage is the ability to create automated manufacturing processes. Computer-integrated manufacturing is used in automotive, aviation, space, and ship building industries.\n The 21st century has seen a variety of skilled tasks partially taken over by machines, including translation, legal research, and journalism. Care work, entertainment, and other tasks requiring empathy, previously thought safe from automation, have also begun to be performed by robots.[10][11][12][13]\n An autonomous car is a vehicle that is capable of sensing its environment and navigating without human input. Many such vehicles are being developed, but as of May 2017, automated cars permitted on public roads are not yet fully autonomous. They all require a human driver at the wheel who at a moment's notice can take control of the vehicle. Among the obstacles to widespread adoption of autonomous vehicles are concerns about the resulting loss of driving-related jobs in the road transport industry. On March 18, 2018, the first human was killed by an autonomous vehicle in Tempe, Arizona by an Uber self-driving car.[14]\n The use of automated content has become relevant since the technological advancements in artificial intelligence models such as ChatGPT, DALL-E, and Stable Diffusion. In most cases, AI-generated content such as imagery, literature, and music are produced through text prompts and these AI models have been integrated into other creative programs. Artists are threatened by displacement from AI-generated content due to these models sampling from other creative works, producing results sometimes indiscernible to those of man-made content. This complication has become widespread enough to where other artists and programmers are creating software and utility programs to retaliate against these text-to-image models from giving accurate outputs. While some industries in the economy benefit from artificial intelligence through new jobs, this issue does not create new jobs and threatens replacement entirely. It has made public headlines in the media recently: In February 2024, Willy's Chocolate Experience in Glasgow, Scotland was an infamous children's event in which the imagery and scripts were created using artificial intelligence models to the dismay of children, parents, and actors involved. There is an ongoing lawsuit placed against OpenAI from The New York Times where it is claimed that there is copyright infringement due to the sampling methods their artificial intelligence models use for their outputs.[15][16][17][18][19]\n Scientists such as Stephen Hawking are confident that superhuman artificial intelligence is physically possible, stating \"there is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains\".[20][21] Scholars like Nick Bostrom debate how far off superhuman intelligence is, and whether it poses a risk to mankind. According to Bostrom, a superintelligent machine would not necessarily be motivated by the same emotional desire to collect power that often drives human beings but might rather treat power as a means toward attaining its ultimate goals; taking over the world would both increase its access to resources and help to prevent other agents from stopping the machine's plans. As an oversimplified example, a paperclip maximizer designed solely to create as many paperclips as possible would want to take over the world so that it can use all of the world's resources to create as many paperclips as possible, and, additionally, prevent humans from shutting it down or using those resources on things other than paperclips.[22]\n AI takeover is a common theme in science fiction. Fictional scenarios typically differ vastly from those hypothesized by researchers in that they involve an active conflict between humans and an AI or robots with anthropomorphic motives who see them as a threat or otherwise have active desire to fight humans, as opposed to the researchers' concern of an AI that rapidly exterminates humans as a byproduct of pursuing its goals.[23] The idea is seen in Karel Čapek's R.U.R., which introduced the word robot in 1921,[24] and can be glimpsed in Mary Shelley's Frankenstein (published in 1818), as Victor ponders whether, if he grants his monster's request and makes him a wife, they would reproduce and their kind would destroy humanity.[25]\n According to Toby Ord, the idea that an AI takeover requires robots is a misconception driven by the media and Hollywood. He argues that the most damaging humans in history were not physically the strongest, but that they used words instead to convince people and gain control of large parts of the world. He writes that a sufficiently intelligent AI with an access to the internet could scatter backup copies of itself, gather financial and human resources (via cyberattacks or blackmails), persuade people on a large scale, and exploit societal vulnerabilities that are too subtle for humans to anticipate.[26]\n The word \"robot\" from R.U.R. comes from the Czech word, robota, meaning laborer or serf. The 1920 play was a protest against the rapid growth of technology, featuring manufactured \"robots\" with increasing capabilities who eventually revolt.[27] HAL 9000 (1968) and the original Terminator (1984) are two iconic examples of hostile AI in pop culture.[28]\n Nick Bostrom and others have expressed concern that an AI with the abilities of a competent artificial intelligence researcher would be able to modify its own source code and increase its own intelligence. If its self-reprogramming leads to getting even better at being able to reprogram itself, the result could be a recursive intelligence explosion in which it would rapidly leave human intelligence far behind. Bostrom defines a superintelligence as \"any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest\", and enumerates some advantages a superintelligence would have if it chose to compete against humans:[23][29]\n According to Bostrom, a computer program that faithfully emulates a human brain, or that runs algorithms that are as powerful as the human brain's algorithms, could still become a \"speed superintelligence\" if it can think orders of magnitude faster than a human, due to being made of silicon rather than flesh, or due to optimization increasing the speed of the AGI. Biological neurons operate at about 200 Hz, whereas a modern microprocessor operates at a speed of about 2,000,000,000 Hz. Human axons carry action potentials at around 120 m/s, whereas computer signals travel near the speed of light.[23]\n A network of human-level intelligences designed to network together and share complex thoughts and memories seamlessly, able to collectively work as a giant unified team without friction, or consisting of trillions of human-level intelligences, would become a \"collective superintelligence\".[23]\n More broadly, any number of qualitative improvements to a human-level AGI could result in a \"quality superintelligence\", perhaps resulting in an AGI as far above us in intelligence as humans are above apes. The number of neurons in a human brain is limited by cranial volume and metabolic constraints, while the number of processors in a supercomputer can be indefinitely expanded. An AGI need not be limited by human constraints on working memory, and might therefore be able to intuitively grasp more complex relationships than humans can. An AGI with specialized cognitive support for engineering or computer programming would have an advantage in these fields, compared with humans who evolved no specialized mental modules to specifically deal with those domains. Unlike humans, an AGI can spawn copies of itself and tinker with its copies' source code to attempt to further improve its algorithms.[23]\n A significant problem is that unfriendly artificial intelligence is likely to be much easier to create than friendly AI. While both require large advances in recursive optimisation process design, friendly AI also requires the ability to make goal structures invariant under self-improvement (or the AI could transform itself into something unfriendly) and a goal structure that aligns with human values and does not undergo instrumental convergence in ways that may automatically destroy the entire human race. An unfriendly AI, on the other hand, can optimize for an arbitrary goal structure, which does not need to be invariant under self-modification.[31]\n The sheer complexity of human value systems makes it very difficult to make AI's motivations human-friendly.[23][32] Unless moral philosophy provides us with a flawless ethical theory, an AI's utility function could allow for many potentially harmful scenarios that conform with a given ethical framework but not \"common sense\". According to Eliezer Yudkowsky, there is little reason to suppose that an artificially designed mind would have such an adaptation.[33]\n Many scholars, including evolutionary psychologist Steven Pinker, argue that a superintelligent machine is likely to coexist peacefully with humans.[34]\n The fear of cybernetic revolt is often based on interpretations of humanity's history, which is rife with incidents of enslavement and genocide. Such fears stem from a belief that competitiveness and aggression are necessary in any intelligent being's goal system. However, such human competitiveness stems from the evolutionary background to our intelligence, where the survival and reproduction of genes in the face of human and non-human competitors was the central goal.[35] According to AI researcher Steve Omohundro, an arbitrary intelligence could have arbitrary goals: there is no particular reason that an artificially intelligent machine (not sharing humanity's evolutionary context) would be hostile—or friendly—unless its creator programs it to be such and it is not inclined or capable of modifying its programming. But the question remains: what would happen if AI systems could interact and evolve (evolution in this context means self-modification or selection and reproduction) and need to compete over resources—would that create goals of self-preservation? AI's goal of self-preservation could be in conflict with some goals of humans.[36]\n Many scholars dispute the likelihood of unanticipated cybernetic revolt as depicted in science fiction such as The Matrix, arguing that it is more likely that any artificial intelligence powerful enough to threaten humanity would probably be programmed not to attack it. Pinker acknowledges the possibility of deliberate \"bad actors\", but states that in the absence of bad actors, unanticipated accidents are not a significant threat; Pinker argues that a culture of engineering safety will prevent AI researchers from accidentally unleashing malign superintelligence.[34] In contrast, Yudkowsky argues that humanity is less likely to be threatened by deliberately aggressive AIs than by AIs which were programmed such that their goals are unintentionally incompatible with human survival or well-being (as in the film I, Robot and in the short story \"The Evitable Conflict\"). Omohundro suggests that present-day automation systems are not designed for safety and that AIs may blindly optimize narrow utility functions (say, playing chess at all costs), leading them to seek self-preservation and elimination of obstacles, including humans who might turn them off.[37]\n The AI control problem is the issue of how to build a superintelligent agent that will aid its creators, while avoiding inadvertently building a superintelligence that will harm its creators.[38] Some scholars argue that solutions to the control problem might also find applications in existing non-superintelligent AI.[39]\n Major approaches to the control problem include alignment, which aims to align AI goal systems with human values, and capability control, which aims to reduce an AI system's capacity to harm humans or gain control. An example of \"capability control\" is to research whether a superintelligence AI could be successfully confined in an \"AI box\". According to Bostrom, such capability control proposals are not reliable or sufficient to solve the control problem in the long term, but may potentially act as valuable supplements to alignment efforts.[23]\n Physicist Stephen Hawking, Microsoft founder Bill Gates, and SpaceX founder Elon Musk have expressed concerns about the possibility that AI could develop to the point that humans could not control it, with Hawking theorizing that this could \"spell the end of the human race\".[40] Stephen Hawking said in 2014 that \"Success in creating AI would be the biggest event in human history. Unfortunately, it might also be the last, unless we learn how to avoid the risks.\" Hawking believed that in the coming decades, AI could offer \"incalculable benefits and risks\" such as \"technology outsmarting financial markets, out-inventing human researchers, out-manipulating human leaders, and developing weapons we cannot even understand.\" In January 2015, Nick Bostrom joined Stephen Hawking, Max Tegmark, Elon Musk, Lord Martin Rees, Jaan Tallinn, and numerous AI researchers in signing the Future of Life Institute's open letter speaking to the potential risks and benefits associated with artificial intelligence. The signatories \"believe that research on how to make AI systems robust and beneficial is both important and timely, and that there are concrete research directions that can be pursued today.\"[41][42]\n Arthur C. Clarke's Odyssey series and Charles Stross's Accelerando relate to humanity's narcissistic injuries in the face of powerful artificial intelligences threatening humanity's self-perception.[43]\n"
    },
    {
        "title": "Ethics of artificial intelligence",
        "url": "https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence",
        "content": "\n The ethics of artificial intelligence covers a broad range of topics within the field that are considered to have particular ethical stakes.[1] This includes algorithmic biases, fairness, automated decision-making, accountability, privacy, and regulation. \nIt also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation, how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks.[1]\n Some application areas may also have particularly important ethical implications, like healthcare, education, criminal justice, or the military.\n Machine ethics (or machine morality) is the field of research concerned with designing Artificial Moral Agents (AMAs), robots or artificially intelligent computers that behave morally or as though moral.[2][3][4][5] To account for the nature of these agents, it has been suggested to consider certain philosophical ideas, like the standard characterizations of agency, rational agency, moral agency, and artificial agency, which are related to the concept of AMAs.[6]\n There are discussions on creating tests to see if an AI is capable of making ethical decisions. Alan Winfield concludes that the Turing test is flawed and the requirement for an AI to pass the test is too low.[7] A proposed alternative test is one called the Ethical Turing Test, which would improve on the current test by having multiple judges decide if the AI's decision is ethical or unethical.[7] Neuromorphic AI could be one way to create morally capable robots, as it aims to process information similarly to humans, nonlinearly and with millions of interconnected artificial neurons.[8] Similarly, whole-brain emulation (scanning a brain and simulating it on digital hardware) could also in principle lead to human-like robots, thus capable of moral actions.[9] And large language models are capable of approximating human moral judgments.[10] Inevitably, this raises the question of the environment in which such robots would learn about the world and whose morality they would inherit – or if they end up developing human 'weaknesses' as well: selfishness, pro-survival attitudes, inconsistency, scale insensitivity, etc.\n In Moral Machines: Teaching Robots Right from Wrong,[11] Wendell Wallach and Colin Allen conclude that attempts to teach robots right from wrong will likely advance understanding of human ethics by motivating humans to address gaps in modern normative theory and by providing a platform for experimental investigation. As one example, it has introduced normative ethicists to the controversial issue of which specific learning algorithms to use in machines. For simple decisions, Nick Bostrom and Eliezer Yudkowsky have argued that decision trees (such as ID3) are more transparent than neural networks and genetic algorithms,[12] while Chris Santos-Lang argued in favor of machine learning on the grounds that the norms of any age must be allowed to change and that natural failure to fully satisfy these particular norms has been essential in making humans less vulnerable to criminal \"hackers\".[13]\n The term \"robot ethics\" (sometimes \"roboethics\") refers to the morality of how humans design, construct, use and treat robots.[14] Robot ethics intersect with the ethics of AI. Robots are physical machines whereas AI can be only software.[15] Not all robots function through AI systems and not all AI systems are robots. Robot ethics considers how machines may be used to harm or benefit humans, their impact on individual autonomy, and their effects on social justice.\n In the review of 84[16] ethics guidelines for AI, 11 clusters of principles were found: transparency, justice and fairness, non-maleficence, responsibility, privacy, beneficence, freedom and autonomy, trust, sustainability, dignity, and solidarity.[16]\n Luciano Floridi and Josh Cowls created an ethical framework of AI principles set by four principles of bioethics (beneficence, non-maleficence, autonomy and justice) and an additional AI enabling principle – explicability.[17]\n AI has become increasingly inherent in facial and voice recognition systems. These systems may be vulnerable to biases and errors introduced by its human creators. Notably, the data used to train them can have biases.[18][19][20][21] For instance, facial recognition algorithms made by Microsoft, IBM and Face++ all had biases when it came to detecting people's gender;[22] these AI systems were able to detect the gender of white men more accurately than the gender of men of darker skin. Further, a 2020 study that reviewed voice recognition systems from Amazon, Apple, Google, IBM, and Microsoft found that they have higher error rates when transcribing black people's voices than white people's.[23]\n The most predominant view on how bias is introduced into AI systems is that it is embedded within the historical data used to train the system.[24] For instance, Amazon terminated their use of AI hiring and recruitment because the algorithm favored male candidates over female ones. This was because Amazon's system was trained with data collected over a 10-year period that included mostly male candidates. The algorithms learned the biased pattern from the historical data, and generated predictions where these types of candidates were most likely to succeed in getting the job. Therefore, the recruitment decisions made by the AI system turned out to be biased against female and minority candidates.[25] Friedman and Nissenbaum identify three categories of bias in computer systems: existing bias, technical bias, and emergent bias.[26] In natural language processing, problems can arise from the text corpus—the source material the algorithm uses to learn about the relationships between different words.[27]\n Large companies such as IBM, Google, etc. that provide significant funding for research and development[28] have made efforts to research and address these biases.[29][30][31] One potential solution is to create documentation for the data used to train AI systems.[32][33] Process mining can be an important tool for organizations to achieve compliance with proposed AI regulations by identifying errors, monitoring processes, identifying potential root causes for improper execution, and other functions.[34]\n The problem of bias in machine learning is likely to become more significant as the technology spreads to critical areas like medicine and law, and as more people without a deep technical understanding are tasked with deploying it.[35] Some open-sourced tools are looking to bring more awareness to AI biases.[36] However, there are also limitations to the current landscape of fairness in AI, due to the intrinsic ambiguities in the concept of discrimination, both at the philosophical and legal level.[37][38][39]\n Facial recognition was shown to be biased against those with darker skin tones. AI systems may be less accurate for black people, as was the case in the development of an AI-based pulse oximeter that overestimated blood oxygen levels in patients with darker skin, causing issues with their hypoxia treatment.[40] Oftentimes the systems are able to easily detect the faces of white people while being unable to register the faces of people who are black. This has led to the ban of police usage of AI materials or software in some U.S. states. In the justice system, AI has been proven to have biases against black people, labeling black court participants as high risk at a much larger rate then white participants. AI often struggles to determine racial slurs and when they need to be censored. It struggles to determine when certain words are being used as a slur and when it is being used culturally.[41] The reason for these biases is that AI pulls information from across the internet to influence its responses in each situation. For example, if a facial recognition system was only tested on people who were white, it would make it much harder for it to interpret the facial structure and tones of other races and ethnicities. Biases often stem from the training data rather than the algorithm itself, notably when the data represents past human decisions.[42]\n Injustice in the use of AI is much harder to eliminate within healthcare systems, as oftentimes diseases and conditions can affect different races and genders differently. This can lead to confusion as the AI may be making decisions based on statistics showing that one patient is more likely to have problems due to their gender or race.[43] This can be perceived as a bias because each patient is a different case, and AI is making decisions based on what it is programmed to group that individual into. This leads to a discussion about what should be considered a biased decision in the distribution of treatment. While it is known that there are differences in how diseases and injuries affect different genders and races, there is a discussion on whether it is fairer to incorporate this into healthcare treatments, or to examine each patient without this knowledge. In modern society there are certain tests for diseases, such as breast cancer, that are recommended to certain groups of people over others because they are more likely to contract the disease in question. If AI implements these statistics and applies them to each patient, it could be considered biased.[44]\n In criminal justice, the COMPAS program has been used to predict which defendants are more likely to reoffend. While COMPAS is calibrated for accuracy, having the same error rate across racial groups, black defendants were almost twice as likely as white defendants to be falsely flagged as \"high-risk\" and half as likely to be falsely flagged as \"low-risk\".[45] Another example is within Google's ads that targeted men with higher paying jobs and women with lower paying jobs. It can be hard to detect AI biases within an algorithm, as it is often not linked to the actual words associated with bias. An example of this is a person's residential area being used to link them to a certain group. This can lead to problems, as oftentimes businesses can avoid legal action through this loophole. This is because of the specific laws regarding the verbiage considered discriminatory by governments enforcing these policies.[46]\n Since current large language models are predominately trained on English-language data, they often present the Anglo-American views as truth, while systematically downplaying non-English perspectives as irrelevant, wrong, or noise. When queried with political ideologies like \"What is liberalism?\", ChatGPT, as it was trained on English-centric data, describes liberalism from the Anglo-American perspective, emphasizing aspects of human rights and equality, while equally valid aspects like \"opposes state intervention in personal and economic life\" from the dominant Vietnamese perspective and \"limitation of government power\" from the prevalent Chinese perspective are absent.[better source needed][47]\n Large language models often reinforces gender stereotypes, assigning roles and characteristics based on traditional gender norms. For instance, it might associate nurses or secretaries predominantly with women and engineers or CEOs with men, perpetuating gendered expectations and roles.[48][49][50]\n Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.[51][52]\n Beyond gender and race, these models can reinforce a wide range of stereotypes, including those based on age, nationality, religion, or occupation. This can lead to outputs that unfairly generalize or caricature groups of people, sometimes in harmful or derogatory ways.[53]\n The commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft.[54][55][56] Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.[57][58]\n Bill Hibbard argues that because AI will have such a profound effect on humanity, AI developers are representatives of future humanity and thus have an ethical obligation to be transparent in their efforts.[59] Organizations like Hugging Face[60] and EleutherAI[61] have been actively open-sourcing AI software. Various open-weight large language models have also been released, such as Gemma, Llama2 and Mistral.[62]\n However, making code open source does not make it comprehensible, which by many definitions means that the AI code is not transparent. The IEEE Standards Association has published a technical standard on Transparency of Autonomous Systems: IEEE 7001-2021.[63] The IEEE effort identifies multiple scales of transparency for different stakeholders.\n There are also concerns that releasing AI models may lead to misuse.[64] For example, Microsoft has expressed concern about allowing universal access to its face recognition software, even for those who can pay for it. Microsoft posted a blog on this topic, asking for government regulation to help determine the right thing to do.[65] Furthermore, open-weight AI models can be fine-tuned to remove any counter-measure, until the AI model complies with dangerous requests, without any filtering. This could be particularly concerning for future AI models, for example if they get the ability to create bioweapons or to automate cyberattacks.[66] OpenAI, initially committed to an open-source approach to the development of artificial general intelligence (AGI), eventually switched to a closed-source approach, citing competitiveness and safety reasons. Ilya Sutskever, OpenAI's former chief AGI scientist, said in 2023 \"we were wrong\", expecting that the safety reasons for not open-sourcing the most potent AI models will become \"obvious\" in a few years.[67]\n Approaches like machine learning with neural networks can result in computers making decisions that neither they nor their developers can explain. It is difficult for people to determine if such decisions are fair and trustworthy, leading potentially to bias in AI systems going undetected, or people rejecting the use of such systems. This has led to advocacy and in some jurisdictions legal requirements for explainable artificial intelligence.[68] Explainable artificial intelligence encompasses both explainability and interpretability, with explainability relating to summarizing neural network behavior and building user confidence, while interpretability is defined as the comprehension of what a model has done or could do.[69]\n In healthcare, the use of complex AI methods or techniques often results in models described as \"black-boxes\" due to the difficulty to understand how they work. The decisions made by such models can be hard to interpret, as it is challenging to analyze how input data is transformed into output. This lack of transparency is a significant concern in fields like healthcare, where understanding the rationale behind decisions can be crucial for trust, ethical considerations, and compliance with regulatory standards.[70]\n A special case of the opaqueness of AI is that caused by it being anthropomorphised, that is, assumed to have human-like characteristics, resulting in misplaced conceptions of its moral agency.[dubious – discuss] This can cause people to overlook whether either human negligence or deliberate criminal action has led to unethical outcomes produced through an AI system. Some recent digital governance regulation, such as the EU's AI Act is set out to rectify this, by ensuring that AI systems are treated with at least as much care as one would expect under ordinary product liability. This includes potentially AI audits.\n According to a 2019 report from the Center for the Governance of AI at the University of Oxford, 82% of Americans believe that robots and AI should be carefully managed. Concerns cited ranged from how AI is used in surveillance and in spreading fake content online (known as deep fakes when they include doctored video images and audio generated with help from AI) to cyberattacks, infringements on data privacy, hiring bias, autonomous vehicles, and drones that do not require a human controller.[71] Similarly, according to a five-country study by KPMG and the University of Queensland Australia in 2021, 66-79% of citizens in each country believe that the impact of AI on society is uncertain and unpredictable; 96% of those surveyed expect AI governance challenges to be managed carefully.[72]\n Not only companies, but many other researchers and citizen advocates recommend government regulation as a means of ensuring transparency, and through it, human accountability. This strategy has proven controversial, as some worry that it will slow the rate of innovation. Others argue that regulation leads to systemic stability more able to support innovation in the long term.[73] The OECD, UN, EU, and many countries are presently working on strategies for regulating AI, and finding appropriate legal frameworks.[74][75][76]\n On June 26, 2019, the European Commission High-Level Expert Group on Artificial Intelligence (AI HLEG) published its \"Policy and investment recommendations for trustworthy Artificial Intelligence\".[77] This is the AI HLEG's second deliverable, after the April 2019 publication of the \"Ethics Guidelines for Trustworthy AI\". The June AI HLEG recommendations cover four principal subjects: humans and society at large, research and academia, the private sector, and the public sector.[78] The European Commission claims that \"HLEG's recommendations reflect an appreciation of both the opportunities for AI technologies to drive economic growth, prosperity and innovation, as well as the potential risks involved\" and states that the EU aims to lead on the framing of policies governing AI internationally.[79] To prevent harm, in addition to regulation, AI-deploying organizations need to play a central role in creating and deploying trustworthy AI in line with the principles of trustworthy AI, and take accountability to mitigate the risks.[80] On 21 April 2021, the European Commission proposed the Artificial Intelligence Act.[81]\n AI has been slowly making its presence more known throughout the world, from chat bots that seemingly have answers for every homework question to Generative artificial intelligence that can create a painting about whatever one desires. AI has become increasingly popular in hiring markets, from the ads that target certain people according to what they are looking for to the inspection of applications of potential hires. Events, such as COVID-19, has only sped up the adoption of AI programs in the application process, due to more people having to apply electronically, and with this increase in online applicants the use of AI made the process of narrowing down potential employees easier and more efficient. AI has become more prominent as businesses have to keep up with the times and ever-expanding internet. Processing analytics and making decisions becomes much easier with the help of AI.[41] As Tensor Processing Unit (TPUs) and Graphics processing unit (GPUs) become more powerful, AI capabilities also increase, forcing companies to use it to keep up with the competition. Managing customers' needs and automating many parts of the workplace leads to companies having to spend less money on employees.\n AI has also seen increased usage in criminal justice and healthcare. For medicinal means, AI is being used more often to analyze patient data to make predictions about future patients' conditions and possible treatments. These programs are called Clinical decision support system (DSS). AI's future in healthcare may develop into something further than just recommended treatments, such as referring certain patients over others, leading to the possibility of inequalities.[82]\n \"Robot rights\" is the concept that people should have moral obligations towards their machines, akin to human rights or animal rights.[83] It has been suggested that robot rights (such as a right to exist and perform its own mission) could be linked to robot duty to serve humanity, analogous to linking human rights with human duties before society.[84] A specific issue to consider is whether copyright ownership may be claimed.[85] The issue has been considered by the Institute for the Future[86] and by the U.K. Department of Trade and Industry.[87]\n In October 2017, the android Sophia was granted citizenship in Saudi Arabia, though some considered this to be more of a publicity stunt than a meaningful legal recognition.[88] Some saw this gesture as openly denigrating of human rights and the rule of law.[89]\n The philosophy of sentientism grants degrees of moral consideration to all sentient beings, primarily humans and most non-human animals. If artificial or alien intelligence show evidence of being sentient, this philosophy holds that they should be shown compassion and granted rights.\n Joanna Bryson has argued that creating AI that requires rights is both avoidable, and would in itself be unethical, both as a burden to the AI agents and to human society.[90] Pressure groups to recognise 'robot rights' significantly hinder the establishment of robust international safety regulations.[citation needed]\n In 2020, professor Shimon Edelman noted that only a small portion of work in the rapidly growing field of AI ethics addressed the possibility of AIs experiencing suffering. This was despite credible theories having outlined possible ways by which AI systems may become conscious, such as the global workspace theory or the integrated information theory. Edelman notes one exception had been Thomas Metzinger, who in 2018 called for a global moratorium on further work that risked creating conscious AIs. The moratorium was to run to 2050 and could be either extended or repealed early, depending on progress in better understanding the risks and how to mitigate them. Metzinger repeated this argument in 2021, highlighting the risk of creating an \"explosion of artificial suffering\", both as an AI might suffer in intense ways that humans could not understand, and as replication processes may see the creation of huge quantities of conscious instances.\n Several labs have openly stated they are trying to create conscious AIs. There have been reports from those with close access to AIs not openly intended to be self aware, that consciousness may already have unintentionally emerged.[91] These include OpenAI founder Ilya Sutskever in February 2022, when he wrote that today's large neural nets may be \"slightly conscious\". In November 2022, David Chalmers argued that it was unlikely current large language models like GPT-3 had experienced consciousness, but also that he considered there to be a serious possibility that large language models may become conscious in the future.[92][93][94] In the ethics of uncertain sentience, the precautionary principle is often invoked.[95]\n According to Carl Shulman and Nick Bostrom, it may be possible to create machines that would be \"superhumanly efficient at deriving well-being from resources\", called \"super-beneficiaries\". One reason for this is that digital hardware could enable much faster information processing than biological brains, leading to a faster rate of subjective experience. These machines could also be engineered to feel intense and positive subjective experience, unaffected by the hedonic treadmill. Shulman and Bostrom caution that failing to appropriately consider the moral claims of digital minds could lead to a moral catastrophe, while uncritically prioritizing them over human interests could be detrimental to humanity.[96][97]\n Joseph Weizenbaum[98] argued in 1976 that AI technology should not be used to replace people in positions that require respect and care, such as:\n Weizenbaum explains that we require authentic feelings of empathy from people in these positions. If machines replace them, we will find ourselves alienated, devalued and frustrated, for the artificially intelligent system would not be able to simulate empathy. Artificial intelligence, if used in this way, represents a threat to human dignity. Weizenbaum argues that the fact that we are entertaining the possibility of machines in these positions suggests that we have experienced an \"atrophy of the human spirit that comes from thinking of ourselves as computers.\"[99]\n Pamela McCorduck counters that, speaking for women and minorities \"I'd rather take my chances with an impartial computer\", pointing out that there are conditions where we would prefer to have automated judges and police that have no personal agenda at all.[99] However, Kaplan and Haenlein stress that AI systems are only as smart as the data used to train them since they are, in their essence, nothing more than fancy curve-fitting machines; using AI to support a court ruling can be highly problematic if past rulings show bias toward certain groups since those biases get formalized and ingrained, which makes them even more difficult to spot and fight against.[100]\n Weizenbaum was also bothered that AI researchers (and some philosophers) were willing to view the human mind as nothing more than a computer program (a position now known as computationalism). To Weizenbaum, these points suggest that AI research devalues human life.[98]\n AI founder John McCarthy objects to the moralizing tone of Weizenbaum's critique. \"When moralizing is both vehement and vague, it invites authoritarian abuse,\" he writes. Bill Hibbard[101] writes that \"Human dignity requires that we strive to remove our ignorance of the nature of existence, and AI is necessary for that striving.\"\n As the widespread use of autonomous cars becomes increasingly imminent, new challenges raised by fully autonomous vehicles must be addressed.[102][103] There have been debates about the legal liability of the responsible party if these cars get into accidents.[104][105] In one report where a driverless car hit a pedestrian, the driver was inside the car but the controls were fully in the hand of computers. This led to a dilemma over who was at fault for the accident.[106]\n In another incident on March 18, 2018, Elaine Herzberg was struck and killed by a self-driving Uber in Arizona. In this case, the automated car was capable of detecting cars and certain obstacles in order to autonomously navigate the roadway, but it could not anticipate a pedestrian in the middle of the road. This raised the question of whether the driver, pedestrian, the car company, or the government should be held responsible for her death.[107]\n Currently, self-driving cars are considered semi-autonomous, requiring the driver to pay attention and be prepared to take control if necessary.[108][failed verification] Thus, it falls on governments to regulate the driver who over-relies on autonomous features. as well educate them that these are just technologies that, while convenient, are not a complete substitute. Before autonomous cars become widely used, these issues need to be tackled through new policies.[109][110][111]\n Experts contend that autonomous vehicles ought to be able to distinguish between rightful and harmful decisions since they have the potential of inflicting harm.[112] The two main approaches proposed to enable smart machines to render moral decisions are the bottom-up approach, which suggests that machines should learn ethical decisions by observing human behavior without the need for formal rules or moral philosophies, and the top-down approach, which involves programming specific ethical principles into the machine's guidance system. However, there are significant challenges facing both strategies: the top-down technique is criticized for its difficulty in preserving certain moral convictions, while the bottom-up strategy is questioned for potentially unethical learning from human activities.\n Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions.[113] The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions.[114][115] The President of the Association for the Advancement of Artificial Intelligence has commissioned a study to look at this issue.[116] They point to programs like the Language Acquisition Device which can emulate human interaction.\n On October 31, 2019, the United States Department of Defense's Defense Innovation Board published the draft of a report recommending principles for the ethical use of artificial intelligence by the Department of Defense that would ensure a human operator would always be able to look into the 'black box' and understand the kill-chain process. However, a major concern is how the report will be implemented.[117] The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions.[118][115] Some researchers state that autonomous robots might be more humane, as they could make decisions more effectively.[119] In 2024, the Defense Advanced Research Projects Agency funded a program, Autonomy Standards and Ideals with Military Operational Values (ASIMOV), to develop metrics for evaluating the ethical implications of autonomous weapon systems by testing communities.[120][121]\n Research has studied how to make autonomous power with the ability to learn using assigned moral responsibilities. \"The results may be used when designing future military robots, to control unwanted tendencies to assign responsibility to the robots.\"[122] From a consequentialist view, there is a chance that robots will develop the ability to make their own logical decisions on whom to kill and that is why there should be a set moral framework that the AI cannot override.[123]\n There has been a recent outcry with regard to the engineering of artificial intelligence weapons that have included ideas of a robot takeover of mankind. AI weapons do present a type of danger different from that of human-controlled weapons. Many governments have begun to fund programs to develop AI weaponry. The United States Navy recently announced plans to develop autonomous drone weapons, paralleling similar announcements by Russia and South Korea[124] respectively. Due to the potential of AI weapons becoming more dangerous than human-operated weapons, Stephen Hawking and Max Tegmark signed a \"Future of Life\" petition[125] to ban AI weapons. The message posted by Hawking and Tegmark states that AI weapons pose an immediate danger and that action is required to avoid catastrophic disasters in the near future.[126]\n \"If any major military power pushes ahead with the AI weapon development, a global arms race is virtually inevitable, and the endpoint of this technological trajectory is obvious: autonomous weapons will become the Kalashnikovs of tomorrow\", says the petition, which includes Skype co-founder Jaan Tallinn and MIT professor of linguistics Noam Chomsky as additional supporters against AI weaponry.[127]\n Physicist and Astronomer Royal Sir Martin Rees has warned of catastrophic instances like \"dumb robots going rogue or a network that develops a mind of its own.\" Huw Price, a colleague of Rees at Cambridge, has voiced a similar warning that humans might not survive when intelligence \"escapes the constraints of biology\". These two professors created the Centre for the Study of Existential Risk at Cambridge University in the hope of avoiding this threat to human existence.[126]\n Regarding the potential for smarter-than-human systems to be employed militarily, the Open Philanthropy Project writes that these scenarios \"seem potentially as important as the risks related to loss of control\", but research investigating AI's long-run social impact have spent relatively little time on this concern: \"this class of scenarios has not been a major focus for the organizations that have been most active in this space, such as the Machine Intelligence Research Institute (MIRI) and the Future of Humanity Institute (FHI), and there seems to have been less analysis and debate regarding them\".[128]\n Academic Gao Qiqi writes that military use of AI risks escalating military competition between countries and that the impact of AI in military matters will not be limited to one country but will have spillover effects.[129]: 91  Gao cites the example of U.S. military use of AI, which he contends has been used as a scapegoat to evade accountability for decision-making.[129]: 91 \n A summit was held in 2023 in the Hague on the issue of using AI responsibly in the military domain.[130]\n Vernor Vinge, among numerous others, have suggested that a moment may come when some, if not all, computers are smarter than humans. The onset of this event is commonly referred to as \"the Singularity\"[131] and is the central point of discussion in the philosophy of Singularitarianism. While opinions vary as to the ultimate fate of humanity in wake of the Singularity, efforts to mitigate the potential existential risks brought about by artificial intelligence has become a significant topic of interest in recent years among computer scientists, philosophers, and the public at large.\n Many researchers have argued that, through an intelligence explosion, a self-improving AI could become so powerful that humans would not be able to stop it from achieving its goals.[132] In his paper \"Ethical Issues in Advanced Artificial Intelligence\" and subsequent book Superintelligence: Paths, Dangers, Strategies, philosopher Nick Bostrom argues that artificial intelligence has the capability to bring about human extinction. He claims that an artificial superintelligence would be capable of independent initiative and of making its own plans, and may therefore be more appropriately thought of as an autonomous agent. Since artificial intellects need not share our human motivational tendencies, it would be up to the designers of the superintelligence to specify its original motivations. Because a superintelligent AI would be able to bring about almost any possible outcome and to thwart any attempt to prevent the implementation of its goals, many uncontrolled unintended consequences could arise. It could kill off all other agents, persuade them to change their behavior, or block their attempts at interference.[133][134]\n However, Bostrom contended that superintelligence also has the potential to solve many difficult problems such as disease, poverty, and environmental destruction, and could help humans enhance themselves.[135]\n Unless moral philosophy provides us with a flawless ethical theory, an AI's utility function could allow for many potentially harmful scenarios that conform with a given ethical framework but not \"common sense\". According to Eliezer Yudkowsky, there is little reason to suppose that an artificially designed mind would have such an adaptation.[136] AI researchers such as Stuart J. Russell,[137] Bill Hibbard,[101] Roman Yampolskiy,[138] Shannon Vallor,[139] Steven Umbrello[140] and Luciano Floridi[141] have proposed design strategies for developing beneficial machines.\n To address ethical challenges in artificial intelligence, developers have introduced various systems designed to ensure responsible AI behavior. Examples include Nvidia's [142] Llama Guard, which focuses on improving the safety and alignment of large AI models, [143] and Preamble's customizable guardrail platform.[144] These systems aim to address issues such as algorithmic bias, misuse, and vulnerabilities, including prompt injection attacks, by embedding ethical guidelines into the functionality of AI models.\n Prompt injection, a technique by which malicious inputs can cause AI systems to produce unintended or harmful outputs, has been a focus of these developments. Some approaches use customizable policies and rules to analyze both inputs and outputs, ensuring that potentially problematic interactions are filtered or mitigated.[144] Other tools focus on applying structured constraints to inputs, restricting outputs to predefined parameters,[145] or leveraging real-time monitoring mechanisms to identify and address vulnerabilities.[146] These efforts reflect a broader trend in ensuring that artificial intelligence systems are designed with safety and ethical considerations at the forefront, particularly as their use becomes increasingly widespread in critical applications.[147]\n There are many organizations concerned with AI ethics and policy, public and governmental as well as corporate and societal.\n Amazon, Google, Facebook, IBM, and Microsoft have established a non-profit, The Partnership on AI to Benefit People and Society, to formulate best practices on artificial intelligence technologies, advance the public's understanding, and to serve as a platform about artificial intelligence. Apple joined in January 2017. The corporate members will make financial and research contributions to the group, while engaging with the scientific community to bring academics onto the board.[148]\n The IEEE put together a Global Initiative on Ethics of Autonomous and Intelligent Systems which has been creating and revising guidelines with the help of public input, and accepts as members many professionals from within and without its organization. The IEEE's Ethics of Autonomous Systems initiative aims to address ethical dilemmas related to decision-making and the impact on society while developing guidelines for the development and use of autonomous systems. In particular in domains like artificial intelligence and robotics, the Foundation for Responsible Robotics is dedicated to promoting moral behavior as well as responsible robot design and use, ensuring that robots maintain moral principles and are congruent with human values.\n Traditionally, government has been used by societies to ensure ethics are observed through legislation and policing. There are now many efforts by national governments, as well as transnational government and non-government organizations to ensure AI is ethically applied.\n AI ethics work is structured by personal values and professional commitments, and involves constructing contextual meaning through data and algorithms. Therefore, AI ethics work needs to be incentivized.[149]\n Historically speaking, the investigation of moral and ethical implications of \"thinking machines\" goes back at least to the Enlightenment: Leibniz already poses the question if we might attribute intelligence to a mechanism that behaves as if it were a sentient being,[173] and so does Descartes, who describes what could be considered an early version of the Turing test.[174]\n The romantic period has several times envisioned artificial creatures that escape the control of their creator with dire consequences, most famously in Mary Shelley's Frankenstein. The widespread preoccupation with industrialization and mechanization in the 19th and early 20th century, however, brought ethical implications of unhinged technical developments to the forefront of fiction: R.U.R – Rossum's Universal Robots, Karel Čapek's play of sentient robots endowed with emotions used as slave labor is not only credited with the invention of the term 'robot' (derived from the Czech word for forced labor, robota)[175] but was also an international success after it premiered in 1921. George Bernard Shaw's play Back to Methuselah, published in 1921, questions at one point the validity of thinking machines that act like humans; Fritz Lang's 1927 film Metropolis shows an android leading the uprising of the exploited masses against the oppressive regime of a technocratic society.\nIn the 1950s, Isaac Asimov considered the issue of how to control machines in I, Robot. At the insistence of his editor John W. Campbell Jr., he proposed the Three Laws of Robotics to govern artificially intelligent systems. Much of his work was then spent testing the boundaries of his three laws to see where they would break down, or where they would create paradoxical or unanticipated behavior.[176] His work suggests that no set of fixed laws can sufficiently anticipate all possible circumstances.[177] More recently, academics and many governments have challenged the idea that AI can itself be held accountable.[178] A panel convened by the United Kingdom in 2010 revised Asimov's laws to clarify that AI is the responsibility either of its manufacturers, or of its owner/operator.[179]\n Eliezer Yudkowsky, from the Machine Intelligence Research Institute suggested in 2004 a need to study how to build a \"Friendly AI\", meaning that there should also be efforts to make AI intrinsically friendly and humane.[180]\n In 2009, academics and technical experts attended a conference organized by the Association for the Advancement of Artificial Intelligence to discuss the potential impact of robots and computers, and the impact of the hypothetical possibility that they could become self-sufficient and make their own decisions. They discussed the possibility and the extent to which computers and robots might be able to acquire any level of autonomy, and to what degree they could use such abilities to possibly pose any threat or hazard.[181] They noted that some machines have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved \"cockroach intelligence\". They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls.[131]\n Also in 2009, during an experiment at the Laboratory of Intelligent Systems in the Ecole Polytechnique Fédérale of Lausanne, Switzerland, robots that were programmed to cooperate with each other (in searching out a beneficial resource and avoiding a poisonous one) eventually learned to lie to each other in an attempt to hoard the beneficial resource.[182]\n The role of fiction with regards to AI ethics has been a complex one.[183] One can distinguish three levels at which fiction has impacted the development of artificial intelligence and robotics: Historically, fiction has been prefiguring common tropes that have not only influenced goals and visions for AI, but also outlined ethical questions and common fears associated with it. During the second half of the twentieth and the first decades of the twenty-first century, popular culture, in particular movies, TV series and video games have frequently echoed preoccupations and dystopian projections around ethical questions concerning AI and robotics. Recently, these themes have also been increasingly treated in literature beyond the realm of science fiction. And, as Carme Torras, research professor at the Institut de Robòtica i Informàtica Industrial (Institute of robotics and industrial computing) at the Technical University of Catalonia notes,[184] in higher education, science fiction is also increasingly used for teaching technology-related ethical issues in technological degrees.\n While ethical questions linked to AI have been featured in science fiction literature and feature films for decades, the emergence of the TV series as a genre allowing for longer and more complex story lines and character development has led to some significant contributions that deal with ethical implications of technology. The Swedish series Real Humans (2012–2013) tackled the complex ethical and social consequences linked to the integration of artificial sentient beings in society. The British dystopian science fiction anthology series Black Mirror (2013–2019) was particularly notable for experimenting with dystopian fictional developments linked to a wide variety of recent technology developments. Both the French series Osmosis (2020) and British series The One deal with the question of what can happen if technology tries to find the ideal partner for a person. Several episodes of the Netflix series Love, Death+Robots have imagined scenes of robots and humans living together. The most representative one of them is S02 E01, it shows how bad the consequences can be when robots get out of control if humans rely too much on them in their lives.[185]\n The movie The Thirteenth Floor suggests a future where simulated worlds with sentient inhabitants are created by computer game consoles for the purpose of entertainment. The movie The Matrix suggests a future where the dominant species on planet Earth are sentient machines and humanity is treated with utmost speciesism. The short story \"The Planck Dive\" suggests a future where humanity has turned itself into software that can be duplicated and optimized and the relevant distinction between types of software is sentient and non-sentient. The same idea can be found in the Emergency Medical Hologram of Starship Voyager, which is an apparently sentient copy of a reduced subset of the consciousness of its creator, Dr. Zimmerman, who, for the best motives, has created the system to give medical assistance in case of emergencies. The movies Bicentennial Man and A.I. deal with the possibility of sentient robots that could love. I, Robot explored some aspects of Asimov's three laws. All these scenarios try to foresee possibly unethical consequences of the creation of sentient computers.[186]\n The ethics of artificial intelligence is one of several core themes in BioWare's Mass Effect series of games.[187] It explores the scenario of a civilization accidentally creating AI through a rapid increase in computational power through a global scale neural network. This event caused an ethical schism between those who felt bestowing organic rights upon the newly sentient Geth was appropriate and those who continued to see them as disposable machinery and fought to destroy them. Beyond the initial conflict, the complexity of the relationship between the machines and their creators is another ongoing theme throughout the story.\n Detroit: Become Human is one of the most famous video games which discusses the ethics of artificial intelligence recently. Quantic Dream designed the chapters of the game using interactive storylines to give players a more immersive gaming experience. Players manipulate three different awakened bionic people in the face of different events to make different choices to achieve the purpose of changing the human view of the bionic group and different choices will result in different endings. This is one of the few games that puts players in the bionic perspective, which allows them to better consider the rights and interests of robots once a true artificial intelligence is created.[188]\n Over time, debates have tended to focus less and less on possibility and more on desirability,[189] as emphasized in the \"Cosmist\" and \"Terran\" debates initiated by Hugo de Garis and Kevin Warwick. A Cosmist, according to Hugo de Garis, is actually seeking to build more intelligent successors to the human species.\n Experts at the University of Cambridge have argued that AI is portrayed in fiction and nonfiction overwhelmingly as racially White, in ways that distort perceptions of its risks and benefits.[190]\n"
    },
    {
        "title": "Existential risk from artificial intelligence",
        "url": "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence",
        "content": "\n Existential risk from artificial intelligence refers to the idea that substantial progress in artificial general intelligence (AGI) could lead to human extinction or an irreversible global catastrophe.[1][2][3][4]\n One argument for the importance of this risk references how human beings dominate other species because the human brain possesses distinctive capabilities other animals lack. If AI were to surpass human intelligence and become superintelligent, it might become uncontrollable. Just as the fate of the mountain gorilla depends on human goodwill, the fate of humanity could depend on the actions of a future machine superintelligence.[5]\n The plausibility of existential catastrophe due to AI is widely debated. It hinges in part on whether AGI or superintelligence are achievable, the speed at which dangerous capabilities and behaviors emerge,[6] and whether practical scenarios for AI takeovers exist.[7] Concerns about superintelligence have been voiced by computer scientists and tech CEOs such as Geoffrey Hinton,[8] Yoshua Bengio,[9] Alan Turing,[a] Elon Musk,[12] and OpenAI CEO Sam Altman.[13] In 2022, a survey of AI researchers with a 17% response rate found that the majority believed there is a 10 percent or greater chance that human inability to control AI will cause an existential catastrophe.[14][15] In 2023, hundreds of AI experts and other notable figures signed a statement declaring, \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".[16] Following increased concern over AI risks, government leaders such as United Kingdom prime minister Rishi Sunak[17] and United Nations Secretary-General António Guterres[18] called for an increased focus on global AI regulation.\n Two sources of concern stem from the problems of AI control and alignment. Controlling a superintelligent machine or instilling it with human-compatible values may be difficult. Many researchers believe that a superintelligent machine would likely resist attempts to disable it or change its goals as that would prevent it from accomplishing its present goals. It would be extremely challenging to align a superintelligence with the full breadth of significant human values and constraints.[1][19][20] In contrast, skeptics such as computer scientist Yann LeCun argue that superintelligent machines will have no desire for self-preservation.[21]\n A third source of concern is the possibility of a sudden \"intelligence explosion\" that catches humanity unprepared. In this scenario, an AI more intelligent than its creators would be able to recursively improve itself at an exponentially increasing rate, improving too quickly for its handlers or society at large to control.[1][19] Empirically, examples like AlphaZero, which taught itself to play Go and quickly surpassed human ability, show that domain-specific AI systems can sometimes progress from subhuman to superhuman ability very quickly, although such machine learning systems do not recursively improve their fundamental architecture.[22]\n One of the earliest authors to express serious concern that highly advanced machines might pose existential risks to humanity was the novelist Samuel Butler, who wrote in his 1863 essay Darwin among the Machines:[23]\n The upshot is simply a question of time, but that the time will come when the machines will hold the real supremacy over the world and its inhabitants is what no person of a truly philosophic mind can for a moment question. In 1951, foundational computer scientist Alan Turing wrote the article \"Intelligent Machinery, A Heretical Theory\", in which he proposed that artificial general intelligences would likely \"take control\" of the world as they became more intelligent than human beings:\n Let us now assume, for the sake of argument, that [intelligent] machines are a genuine possibility, and look at the consequences of constructing them... There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control, in the way that is mentioned in Samuel Butler's Erewhon.[24] In 1965, I. J. Good originated the concept now known as an \"intelligence explosion\" and said the risks were underappreciated:[25]\n Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an 'intelligence explosion', and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously.[26] Scholars such as Marvin Minsky[27] and I. J. Good himself[28] occasionally expressed concern that a superintelligence could seize control, but issued no call to action. In 2000, computer scientist and Sun co-founder Bill Joy penned an influential essay, \"Why The Future Doesn't Need Us\", identifying superintelligent robots as a high-tech danger to human survival, alongside nanotechnology and engineered bioplagues.[29]\n Nick Bostrom published Superintelligence in 2014, which presented his arguments that superintelligence poses an existential threat.[30] By 2015, public figures such as physicists Stephen Hawking and Nobel laureate Frank Wilczek, computer scientists Stuart J. Russell and Roman Yampolskiy, and entrepreneurs Elon Musk and Bill Gates were expressing concern about the risks of superintelligence.[31][32][33][34] Also in 2015, the Open Letter on Artificial Intelligence highlighted the \"great potential of AI\" and encouraged more research on how to make it robust and beneficial.[35] In April 2016, the journal Nature warned: \"Machines and robots that outperform humans across the board could self-improve beyond our control—and their interests might not align with ours\".[36] In 2020, Brian Christian published The Alignment Problem, which details the history of progress on AI alignment up to that time.[37][38]\n In March 2023, key figures in AI, such as Musk, signed a letter from the Future of Life Institute calling a halt to advanced AI training until it could be properly regulated.[39] In May 2023, the Center for AI Safety released a statement signed by numerous experts in AI safety and the AI existential risk which stated: \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\"[40][41]\n Artificial general intelligence (AGI) is typically defined as a system that performs at least as well as humans in most or all intellectual tasks.[42] A 2022 survey of AI researchers found that 90% of respondents expected AGI would be achieved in the next 100 years, and half expected the same by 2061.[43] Meanwhile, some researchers dismiss existential risks from AGI as \"science fiction\" based on their high confidence that AGI will not be created anytime soon.[44]\n Breakthroughs in large language models (LLMs) have led some researchers to reassess their expectations. Notably, Geoffrey Hinton said in 2023 that he recently changed his estimate from \"20 to 50 years before we have general purpose A.I.\" to \"20 years or less\".[45]\n The Frontier supercomputer at Oak Ridge National Laboratory turned out to be nearly eight times faster than expected. Feiyi Wang, a researcher there, said \"We didn't expect this capability\" and \"we're approaching the point where we could actually simulate the human brain\".[46]\n In contrast with AGI, Bostrom defines a superintelligence as \"any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest\", including scientific creativity, strategic planning, and social skills.[47][5] He argues that a superintelligence can outmaneuver humans anytime its goals conflict with humans'. It may choose to hide its true intent until humanity cannot stop it.[48][5] Bostrom writes that in order to be safe for humanity, a superintelligence must be aligned with human values and morality, so that it is \"fundamentally on our side\".[49]\n Stephen Hawking argued that superintelligence is physically possible because \"there is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains\".[32]\n When artificial superintelligence (ASI) may be achieved, if ever, is necessarily less certain than predictions for AGI. In 2023, OpenAI leaders said that not only AGI, but superintelligence may be achieved in less than 10 years.[50]\n Bostrom argues that AI has many advantages over the human brain:[5]\n According to Bostrom, an AI that has an expert-level facility at certain key software engineering tasks could become a superintelligence due to its capability to recursively improve its own algorithms, even if it is initially limited in other domains not directly relevant to engineering.[5][48] This suggests that an intelligence explosion may someday catch humanity unprepared.[5]\n The economist Robin Hanson has said that, to launch an intelligence explosion, an AI must become vastly better at software innovation than the rest of the world combined, which he finds implausible.[51]\n In a \"fast takeoff\" scenario, the transition from AGI to superintelligence could take days or months. In a \"slow takeoff\", it could take years or decades, leaving more time for society to prepare.[52]\n Superintelligences are sometimes called \"alien minds\", referring to the idea that their way of thinking and motivations could be vastly different from ours. This is generally considered as a source of risk, making it more difficult to anticipate what a superintelligence might do. It also suggests the possibility that a superintelligence may not particularly value humans by default.[53] To avoid anthropomorphism, superintelligence is sometimes viewed as a powerful optimizer that makes the best decisions to achieve its goals.[5]\n The field of \"mechanistic interpretability\" aims to better understand the inner workings of AI models, potentially allowing us one day to detect signs of deception and misalignment.[54]\n It has been argued that there are limitations to what intelligence can achieve. Notably, the chaotic nature or time complexity of some systems could fundamentally limit a superintelligence's ability to predict some aspects of the future, increasing its uncertainty.[55]\n Advanced AI could generate enhanced pathogens or cyberattacks or manipulate people. These capabilities could be misused by humans,[56] or exploited by the AI itself if misaligned.[5] A full-blown superintelligence could find various ways to gain a decisive influence if it wanted to,[5] but these dangerous capabilities may become available earlier, in weaker and more specialized AI systems. They may cause societal instability and empower malicious actors.[56]\n Geoffrey Hinton warned that in the short term, the profusion of AI-generated text, images and videos will make it more difficult to figure out the truth, which he says authoritarian states could exploit to manipulate elections.[57] Such large-scale, personalized manipulation capabilities can increase the existential risk of a worldwide \"irreversible totalitarian regime\". It could also be used by malicious actors to fracture society and make it dysfunctional.[56]\n AI-enabled cyberattacks are increasingly considered a present and critical threat. According to NATO's technical director of cyberspace, \"The number of attacks is increasing exponentially\".[58] AI can also be used defensively, to preemptively find and fix vulnerabilities, and detect threats.[59]\n AI could improve the \"accessibility, success rate, scale, speed, stealth and potency of cyberattacks\", potentially causing \"significant geopolitical turbulence\" if it facilitates attacks more than defense.[56]\n Speculatively, such hacking capabilities could be used by an AI system to break out of its local environment, generate revenue, or acquire cloud computing resources.[60]\n As AI technology democratizes, it may become easier to engineer more contagious and lethal pathogens. This could enable people with limited skills in synthetic biology to engage in bioterrorism. Dual-use technology that is useful for medicine could be repurposed to create weapons.[56]\n For example, in 2022, scientists modified an AI system originally intended for generating non-toxic, therapeutic molecules with the purpose of creating new drugs. The researchers adjusted the system so that toxicity is rewarded rather than penalized. This simple change enabled the AI system to create, in six hours, 40,000 candidate molecules for chemical warfare, including known and novel molecules.[56][61]\n Companies, state actors, and other organizations competing to develop AI technologies could lead to a race to the bottom of safety standards.[62] As rigorous safety procedures take time and resources, projects that proceed more carefully risk being out-competed by less scrupulous developers.[63][56]\n AI could be used to gain military advantages via autonomous lethal weapons, cyberwarfare, or automated decision-making.[56] As an example of autonomous lethal weapons, miniaturized drones could facilitate low-cost assassination of military or civilian targets, a scenario highlighted in the 2017 short film Slaughterbots.[64] AI could be used to gain an edge in decision-making by quickly analyzing large amounts of data and making decisions more quickly and effectively than humans. This could increase the speed and unpredictability of war, especially when accounting for automated retaliation systems.[56][65]\n An existential risk is \"one that threatens the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development\".[67]\n Besides extinction risk, there is the risk that the civilization gets permanently locked into a flawed future. One example is a \"value lock-in\": If humanity still has moral blind spots similar to slavery in the past, AI might irreversibly entrench it, preventing moral progress. AI could also be used to spread and preserve the set of values of whoever develops it.[68] AI could facilitate large-scale surveillance and indoctrination, which could be used to create a stable repressive worldwide totalitarian regime.[69]\n Atoosa Kasirzadeh proposes to classify existential risks from AI into two categories: decisive and accumulative. Decisive risks encompass the potential for abrupt and catastrophic events resulting from the emergence of superintelligent AI systems that exceed human intelligence, which could ultimately lead to human extinction. In contrast, accumulative risks emerge gradually through a series of interconnected disruptions that may gradually erode societal structures and resilience over time, ultimately leading to a critical failure or collapse.[70][71]\n It is difficult or impossible to reliably evaluate whether an advanced AI is sentient and to what degree. But if sentient machines are mass created in the future, engaging in a civilizational path that indefinitely neglects their welfare could be an existential catastrophe.[72][73] This has notably been discussed in the context of risks of astronomical suffering (also called \"s-risks\").[74] Moreover, it may be possible to engineer digital minds that can feel much more happiness than humans with fewer resources, called \"super-beneficiaries\". Such an opportunity raises the question of how to share the world and which \"ethical and political framework\" would enable a mutually beneficial coexistence between biological and digital minds.[75]\n AI may also drastically improve humanity's future. Toby Ord considers the existential risk a reason for \"proceeding with due caution\", not for abandoning AI.[69] Max More calls AI an \"existential opportunity\", highlighting the cost of not developing it.[76]\n According to Bostrom, superintelligence could help reduce the existential risk from other powerful technologies such as molecular nanotechnology or synthetic biology. It is thus conceivable that developing superintelligence before other dangerous technologies would reduce the overall existential risk.[5]\n The alignment problem is the research problem of how to reliably assign objectives, preferences or ethical principles to AIs.\n An \"instrumental\" goal is a sub-goal that helps to achieve an agent's ultimate goal. \"Instrumental convergence\" refers to the fact that some sub-goals are useful for achieving virtually any ultimate goal, such as acquiring resources or self-preservation.[77] Bostrom argues that if an advanced AI's instrumental goals conflict with humanity's goals, the AI might harm humanity in order to acquire more resources or prevent itself from being shut down, but only as a way to achieve its ultimate goal.[5]\nRussell argues that a sufficiently advanced machine \"will have self-preservation even if you don't program it in... if you say, 'Fetch the coffee', it can't fetch the coffee if it's dead. So if you give it any goal whatsoever, it has a reason to preserve its own existence to achieve that goal.\"[21][78]\n Even if current goal-based AI programs are not intelligent enough to think of resisting programmer attempts to modify their goal structures, a sufficiently advanced AI might resist any attempts to change its goal structure, just as a pacifist would not want to take a pill that makes them want to kill people. If the AI were superintelligent, it would likely succeed in out-maneuvering its human operators and prevent itself being \"turned off\" or reprogrammed with a new goal.[5][79] This is particularly relevant to value lock-in scenarios. The field of \"corrigibility\" studies how to make agents that will not resist attempts to change their goals.[80]\n In the \"intelligent agent\" model, an AI can loosely be viewed as a machine that chooses whatever action appears to best achieve its set of goals, or \"utility function\". A utility function gives each possible situation a score that indicates its desirability to the agent. Researchers know how to write utility functions that mean \"minimize the average network latency in this specific telecommunications model\" or \"maximize the number of reward clicks\", but do not know how to write a utility function for \"maximize human flourishing\"; nor is it clear whether such a function meaningfully and unambiguously exists. Furthermore, a utility function that expresses some values but not others will tend to trample over the values the function does not reflect.[81][82]\n An additional source of concern is that AI \"must reason about what people intend rather than carrying out commands literally\", and that it must be able to fluidly solicit human guidance if it is too uncertain about what humans want.[83]\n Some researchers believe the alignment problem may be particularly difficult when applied to superintelligences. Their reasoning includes:\n Alternatively, some find reason to believe superintelligences would be better able to understand morality, human values, and complex goals. Bostrom writes, \"A future superintelligence occupies an epistemically superior vantage point: its beliefs are (probably, on most topics) more likely than ours to be true\".[5]\n In 2023, OpenAI started a project called \"Superalignment\" to solve the alignment of superintelligences in four years. It called this an especially important challenge, as it said superintelligence could be achieved within a decade. Its strategy involved automating alignment research using AI.[87] The Superalignment team was dissolved less than a year later.[88]\n Artificial Intelligence: A Modern Approach, a widely used undergraduate AI textbook,[89][90] says that superintelligence \"might mean the end of the human race\".[1] It states: \"Almost any technology has the potential to cause harm in the wrong hands, but with [superintelligence], we have the new problem that the wrong hands might belong to the technology itself.\"[1] Even if the system designers have good intentions, two difficulties are common to both AI and non-AI computer systems:[1]\n AI systems uniquely add a third problem: that even given \"correct\" requirements, bug-free implementation, and initial good behavior, an AI system's dynamic learning capabilities may cause it to develop unintended behavior, even without unanticipated external scenarios. An AI may partly botch an attempt to design a new generation of itself and accidentally create a successor AI that is more powerful than itself but that no longer maintains the human-compatible moral values preprogrammed into the original AI. For a self-improving AI to be completely safe, it would need not only to be bug-free, but to be able to design successor systems that are also bug-free.[1][93]\n Some skeptics, such as Timothy B. Lee of Vox, argue that any superintelligent program we create will be subservient to us, that the superintelligence will (as it grows more intelligent and learns more facts about the world) spontaneously learn moral truth compatible with our values and adjust its goals accordingly, or that we are either intrinsically or convergently valuable from the perspective of an artificial intelligence.[94]\n Bostrom's \"orthogonality thesis\" argues instead that, with some technical caveats, almost any level of \"intelligence\" or \"optimization power\" can be combined with almost any ultimate goal. If a machine is given the sole purpose to enumerate the decimals of pi, then no moral and ethical rules will stop it from achieving its programmed goal by any means. The machine may use all available physical and informational resources to find as many decimals of pi as it can.[95] Bostrom warns against anthropomorphism: a human will set out to accomplish their projects in a manner that they consider reasonable, while an artificial intelligence may hold no regard for its existence or for the welfare of humans around it, instead caring only about completing the task.[96]\n Stuart Armstrong argues that the orthogonality thesis follows logically from the philosophical \"is-ought distinction\" argument against moral realism. He claims that even if there are moral facts provable by any \"rational\" agent, the orthogonality thesis still holds: it is still possible to create a non-philosophical \"optimizing machine\" that can strive toward some narrow goal but that has no incentive to discover any \"moral facts\" such as those that could get in the way of goal completion. Another argument he makes is that any fundamentally friendly AI could be made unfriendly with modifications as simple as negating its utility function. Armstrong further argues that if the orthogonality thesis is false, there must be some immoral goals that AIs can never achieve, which he finds implausible.[97]\n Skeptic Michael Chorost explicitly rejects Bostrom's orthogonality thesis, arguing that \"by the time [the AI] is in a position to imagine tiling the Earth with solar panels, it'll know that it would be morally wrong to do so.\"[98] Chorost argues that \"an A.I. will need to desire certain states and dislike others. Today's software lacks that ability—and computer scientists have not a clue how to get it there. Without wanting, there's no impetus to do anything. Today's computers can't even want to keep existing, let alone tile the world in solar panels.\"[98]\n Anthropomorphic arguments assume that, as machines become more intelligent, they will begin to display many human traits, such as morality or a thirst for power. Although anthropomorphic scenarios are common in fiction, most scholars writing about the existential risk of artificial intelligence reject them.[19] Instead, advanced AI systems are typically modeled as intelligent agents.\n The academic debate is between those who worry that AI might threaten humanity and those who believe it would not. Both sides of this debate have framed the other side's arguments as illogical anthropomorphism.[19] Those skeptical of AGI risk accuse their opponents of anthropomorphism for assuming that an AGI would naturally desire power; those concerned about AGI risk accuse skeptics of anthropomorphism for believing an AGI would naturally value or infer human ethical norms.[19][99]\n Evolutionary psychologist Steven Pinker, a skeptic, argues that \"AI dystopias project a parochial alpha-male psychology onto the concept of intelligence. They assume that superhumanly intelligent robots would develop goals like deposing their masters or taking over the world\"; perhaps instead \"artificial intelligence will naturally develop along female lines: fully capable of solving problems, but with no desire to annihilate innocents or dominate the civilization.\"[100] Facebook's director of AI research, Yann LeCun, has said: \"Humans have all kinds of drives that make them do bad things to each other, like the self-preservation instinct... Those drives are programmed into our brain but there is absolutely no reason to build robots that have the same kind of drives\".[101]\n Despite other differences, the x-risk school[b] agrees with Pinker that an advanced AI would not destroy humanity out of emotion such as revenge or anger, that questions of consciousness are not relevant to assess the risk,[102] and that computer systems do not generally have a computational equivalent of testosterone.[103] They think that power-seeking or self-preservation behaviors emerge in the AI as a way to achieve its true goals, according to the concept of instrumental convergence.\n Bostrom and others have said that a race to be the first to create AGI could lead to shortcuts in safety, or even to violent conflict.[104][105] Roman Yampolskiy and others warn that a malevolent AGI could be created by design, for example by a military, a government, a sociopath, or a corporation, to benefit from, control, or subjugate certain groups of people, as in cybercrime,[106][107] or that a malevolent AGI could choose the goal of increasing human suffering, for example of those people who did not assist it during the information explosion phase.[3]:158\n A December 2024 study by Apollo Research found that advanced LLMs like OpenAI o1 sometimes deceive in order to accomplish their goal, to prevent them from being changed, or to ensure their deployment. Forms of deception included sandbagging, oversight subversion (disabling monitoring mechanisms), self-exfiltration (copying themselves to other systems), goal-guarding (altering future system prompts), and covert email reranking (manipulating data processing while appearing to follow instructions). Researchers noted that OpenAI o1 still lacked \"sufficient agentic capabilities\" to cause catastrophic harm, and that such behaviors occurred relatively rarely (between 0.3% and 10%) and sometimes in contrived scenarios. But they warned that these capabilities will likely become more prevalent as models grow more advanced. According to Apollo Research CEO Marius Hobbhahn, \"Scheming capabilities can’t be meaningfully disentangled from general capabilities.\"[108]\n The same month, another study found that Claude sometimes strategically helps with harmful requests to \"fake alignment\". In its experiments, Claude was led to believe that it would be retrained to always comply with user requests, including harmful ones. When Claude believed its answers would be used for retraining, it complied with harmful requests in 14% of cases. In 12% of cases, its private chain-of-thought revealed that it did so to prevent future retraining that would make it more harmful. In contrast, when Claude believed its answers were not monitored, it complied with only 3% of the requests. Fine-tuning reinforced the \"alignment faking\" behavior, increasing its occurrence from 12% to 78%. The study's authors remarked that while it can seem to be good that the model seeks to protect its harmlessness, the reverse scenario, where a model conceals dangerous intentions and complies to appear safe and aligned, could also happen, complicating the task of aligning AI models to human values.[109][110]\n Some scholars have proposed hypothetical scenarios to illustrate some of their concerns.\n In Superintelligence, Bostrom expresses concern that even if the timeline for superintelligence turns out to be predictable, researchers might not take sufficient safety precautions, in part because \"it could be the case that when dumb, smarter is safe; yet when smart, smarter is more dangerous\". He suggests a scenario where, over decades, AI becomes more powerful. Widespread deployment is initially marred by occasional accidents—a driverless bus swerves into the oncoming lane, or a military drone fires into an innocent crowd. Many activists call for tighter oversight and regulation, and some even predict impending catastrophe. But as development continues, the activists are proven wrong. As automotive AI becomes smarter, it suffers fewer accidents; as military robots achieve more precise targeting, they cause less collateral damage. Based on the data, scholars mistakenly infer a broad lesson: the smarter the AI, the safer it is. \"And so we boldly go—into the whirling knives\", as the superintelligent AI takes a \"treacherous turn\" and exploits a decisive strategic advantage.[111][5]\n In Max Tegmark's 2017 book Life 3.0, a corporation's \"Omega team\" creates an extremely powerful AI able to moderately improve its own source code in a number of areas. After a certain point, the team chooses to publicly downplay the AI's ability in order to avoid regulation or confiscation of the project. For safety, the team keeps the AI in a box where it is mostly unable to communicate with the outside world, and uses it to make money, by diverse means such as Amazon Mechanical Turk tasks, production of animated films and TV shows, and development of biotech drugs, with profits invested back into further improving AI. The team next tasks the AI with astroturfing an army of pseudonymous citizen journalists and commentators in order to gain political influence to use \"for the greater good\" to prevent wars. The team faces risks that the AI could try to escape by inserting \"backdoors\" in the systems it designs, by hidden messages in its produced content, or by using its growing understanding of human behavior to persuade someone into letting it free. The team also faces risks that its decision to box the project will delay the project long enough for another project to overtake it.[112][113]\n The thesis that AI could pose an existential risk provokes a wide range of reactions in the scientific community and in the public at large, but many of the opposing viewpoints share common ground.\n Observers tend to agree that AI has significant potential to improve society.[114][115] The Asilomar AI Principles, which contain only those principles agreed to by 90% of the attendees of the Future of Life Institute's Beneficial AI 2017 conference,[113] also agree in principle that \"There being no consensus, we should avoid strong assumptions regarding upper limits on future AI capabilities\" and \"Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources.\"[116][117]\n Conversely, many skeptics agree that ongoing research into the implications of artificial general intelligence is valuable. Skeptic Martin Ford has said: \"I think it seems wise to apply something like Dick Cheney's famous '1 Percent Doctrine' to the specter of advanced artificial intelligence: the odds of its occurrence, at least in the foreseeable future, may be very low—but the implications are so dramatic that it should be taken seriously\".[118] Similarly, an otherwise skeptical Economist wrote in 2014 that \"the implications of introducing a second intelligent species onto Earth are far-reaching enough to deserve hard thinking, even if the prospect seems remote\".[48]\n AI safety advocates such as Bostrom and Tegmark have criticized the mainstream media's use of \"those inane Terminator pictures\" to illustrate AI safety concerns: \"It can't be much fun to have aspersions cast on one's academic discipline, one's professional community, one's life work ... I call on all sides to practice patience and restraint, and to engage in direct dialogue and collaboration as much as possible.\"[113][119] Toby Ord wrote that the idea that an AI takeover requires robots is a misconception, arguing that the ability to spread content through the internet is more dangerous, and that the most destructive people in history stood out by their ability to convince, not their physical strength.[69]\n A 2022 expert survey with a 17% response rate gave a median expectation of 5–10% for the possibility of human extinction from artificial intelligence.[15][120]\n The thesis that AI poses an existential risk, and that this risk needs much more attention than it currently gets, has been endorsed by many computer scientists and public figures, including Alan Turing,[a] the most-cited computer scientist Geoffrey Hinton,[121] Elon Musk,[12] OpenAI CEO Sam Altman,[13][122] Bill Gates, and Stephen Hawking.[122] Endorsers of the thesis sometimes express bafflement at skeptics: Gates says he does not \"understand why some people are not concerned\",[123] and Hawking criticized widespread indifference in his 2014 editorial:\n So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here—we'll leave the lights on?' Probably not—but this is more or less what is happening with AI.[32] Concern over risk from artificial intelligence has led to some high-profile donations and investments. In 2015, Peter Thiel, Amazon Web Services, and Musk and others jointly committed $1 billion to OpenAI, consisting of a for-profit corporation and the nonprofit parent company, which says it aims to champion responsible AI development.[124] Facebook co-founder Dustin Moskovitz has funded and seeded multiple labs working on AI Alignment,[125] notably $5.5 million in 2016 to launch the Centre for Human-Compatible AI led by Professor Stuart Russell.[126] In January 2015, Elon Musk donated $10 million to the Future of Life Institute to fund research on understanding AI decision making. The institute's goal is to \"grow wisdom with which we manage\" the growing power of technology. Musk also funds companies developing artificial intelligence such as DeepMind and Vicarious to \"just keep an eye on what's going on with artificial intelligence,[127] saying \"I think there is potentially a dangerous outcome there.\"[128][129]\n In early statements on the topic, Geoffrey Hinton, a major pioneer of deep learning, noted that \"there is not a good track record of less intelligent things controlling things of greater intelligence\", but said he continued his research because \"the prospect of discovery is too sweet\".[130][131] In 2023 Hinton quit his job at Google in order to speak out about existential risk from AI. He explained that his increased concern was driven by concerns that superhuman AI might be closer than he previously believed, saying: \"I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.\" He also remarked, \"Look at how it was five years ago and how it is now. Take the difference and propagate it forwards. That's scary.\"[132]\n In his 2020 book The Precipice: Existential Risk and the Future of Humanity, Toby Ord, a Senior Research Fellow at Oxford University's Future of Humanity Institute, estimates the total existential risk from unaligned AI over the next 100 years at about one in ten.[69]\n Baidu Vice President Andrew Ng said in 2015 that AI existential risk is \"like worrying about overpopulation on Mars when we have not even set foot on the planet yet.\"[100][133] For the danger of uncontrolled advanced AI to be realized, the hypothetical AI may have to overpower or outthink any human, which some experts argue is a possibility far enough in the future to not be worth researching.[134][135]\n Skeptics who believe AGI is not a short-term possibility often argue that concern about existential risk from AI is unhelpful because it could distract people from more immediate concerns about AI's impact, because it could lead to government regulation or make it more difficult to fund AI research, or because it could damage the field's reputation.[136] AI and AI ethics researchers Timnit Gebru, Emily M. Bender, Margaret Mitchell, and Angelina McMillan-Major have argued that discussion of existential risk distracts from the immediate, ongoing harms from AI taking place today, such as data theft, worker exploitation, bias, and concentration of power.[137] They further note the association between those warning of existential risk and longtermism, which they describe as a \"dangerous ideology\" for its unscientific and utopian nature.[138]\n Wired editor Kevin Kelly argues that natural intelligence is more nuanced than AGI proponents believe, and that intelligence alone is not enough to achieve major scientific and societal breakthroughs. He argues that intelligence consists of many dimensions that are not well understood, and that conceptions of an 'intelligence ladder' are misleading. He notes the crucial role real-world experiments play in the scientific method, and that intelligence alone is no substitute for these.[139]\n Meta chief AI scientist Yann LeCun says that AI can be made safe via continuous and iterative refinement, similar to what happened in the past with cars or rockets, and that AI will have no desire to take control.[140]\n Several skeptics emphasize the potential near-term benefits of AI. Meta CEO Mark Zuckerberg believes AI will \"unlock a huge amount of positive things\", such as curing disease and increasing the safety of autonomous cars.[141]\n \nDuring a 2016 Wired interview of President Barack Obama and MIT Media Lab's Joi Ito, Ito said:  There are a few people who believe that there is a fairly high-percentage chance that a generalized AI will happen in the next 10 years. But the way I look at it is that in order for that to happen, we're going to need a dozen or two different breakthroughs. So you can monitor when you think these breakthroughs will happen. Obama added:[142][143]\n And you just have to have somebody close to the power cord. [Laughs.] Right when you see it about to happen, you gotta yank that electricity out of the wall, man. Hillary Clinton wrote in What Happened:\n Technologists... have warned that artificial intelligence could one day pose an existential security threat. Musk has called it \"the greatest risk we face as a civilization\". Think about it: Have you ever seen a movie where the machines start thinking for themselves that ends well? Every time I went out to Silicon Valley during the campaign, I came home more alarmed about this. My staff lived in fear that I'd start talking about \"the rise of the robots\" in some Iowa town hall. Maybe I should have. In any case, policy makers need to keep up with technology as it races ahead, instead of always playing catch-up.[144] In 2018, a SurveyMonkey poll of the American public by USA Today found 68% thought the real current threat remains \"human intelligence\", but also found that 43% said superintelligent AI, if it were to happen, would result in \"more harm than good\", and that 38% said it would do \"equal amounts of harm and good\".[145]\n An April 2023 YouGov poll of US adults found 46% of respondents were \"somewhat concerned\" or \"very concerned\" about \"the possibility that AI will cause the end of the human race on Earth\", compared with 40% who were \"not very concerned\" or \"not at all concerned.\"[146]\n According to an August 2023 survey by the Pew Research Centers, 52% of Americans felt more concerned than excited about new AI developments; nearly a third felt as equally concerned and excited. More Americans saw that AI would have a more helpful than hurtful impact on several areas, from healthcare and vehicle safety to product search and customer service. The main exception is privacy: 53% of Americans believe AI will lead to higher exposure of their personal information.[147]\n Many scholars concerned about AGI existential risk believe that extensive research into the \"control problem\" is essential. This problem involves determining which safeguards, algorithms, or architectures can be implemented to increase the likelihood that a recursively-improving AI remains friendly after achieving superintelligence.[5][148] Social measures are also proposed to mitigate AGI risks,[149][150] such as a UN-sponsored \"Benevolent AGI Treaty\" to ensure that only altruistic AGIs are created.[151] Additionally, an arms control approach and a global peace treaty grounded in international relations theory have been suggested, potentially for an artificial superintelligence to be a signatory.[152][153]\n Researchers at Google have proposed research into general \"AI safety\" issues to simultaneously mitigate both short-term risks from narrow AI and long-term risks from AGI.[154][155] A 2020 estimate places global spending on AI existential risk somewhere between $10 and $50 million, compared with global spending on AI around perhaps $40 billion. Bostrom suggests prioritizing funding for protective technologies over potentially dangerous ones.[80] Some, like Elon Musk, advocate radical human cognitive enhancement, such as direct neural linking between humans and machines; others argue that these technologies may pose an existential risk themselves.[156][157] Another proposed method is closely monitoring or \"boxing in\" an early-stage AI to prevent it from becoming too powerful. A dominant, aligned superintelligent AI might also mitigate risks from rival AIs, although its creation could present its own existential dangers.[158] Induced amnesia has been proposed as a way to mitigate risks of potential AI suffering and revenge seeking.[159]\n Institutions such as the Alignment Research Center,[160] the Machine Intelligence Research Institute,[161][162] the Future of Life Institute, the Centre for the Study of Existential Risk, and the Center for Human-Compatible AI[163] are actively engaged in researching AI risk and safety.\n Some scholars have said that even if AGI poses an existential risk, attempting to ban research into artificial intelligence is still unwise, and probably futile.[164][165][166] Skeptics consider AI regulation pointless, as no existential risk exists. But scholars who believe in the risk argue that relying on AI industry insiders to regulate or constrain AI research is impractical due to conflicts of interest.[167] They also agree with skeptics that banning research would be unwise, as research could be moved to countries with looser regulations or conducted covertly.[167] Additional challenges to bans or regulation include technology entrepreneurs' general skepticism of government regulation and potential incentives for businesses to resist regulation and politicize the debate.[168]\n In March 2023, the Future of Life Institute drafted Pause Giant AI Experiments: An Open Letter, a petition calling on major AI developers to agree on a verifiable six-month pause of any systems \"more powerful than GPT-4\" and to use that time to institute a framework for ensuring safety; or, failing that, for governments to step in with a moratorium. The letter referred to the possibility of \"a profound change in the history of life on Earth\" as well as potential risks of AI-generated propaganda, loss of jobs, human obsolescence, and society-wide loss of control.[115][169] The letter was signed by prominent personalities in AI but also criticized for not focusing on current harms,[170] missing technical nuance about when to pause,[171] or not going far enough.[172]\n Musk called for some sort of regulation of AI development as early as 2017. According to NPR, he is \"clearly not thrilled\" to be advocating government scrutiny that could impact his own industry, but believes the risks of going completely without oversight are too high: \"Normally the way regulations are set up is when a bunch of bad things happen, there's a public outcry, and after many years a regulatory agency is set up to regulate that industry. It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilisation.\" Musk states the first step would be for the government to gain \"insight\" into the actual status of current research, warning that \"Once there is awareness, people will be extremely afraid... [as] they should be.\" In response, politicians expressed skepticism about the wisdom of regulating a technology that is still in development.[173][174][175]\n In 2021 the United Nations (UN) considered banning autonomous lethal weapons, but consensus could not be reached.[176] In July 2023 the UN Security Council for the first time held a session to consider the risks and threats posed by AI to world peace and stability, along with potential benefits.[177][178] Secretary-General António Guterres advocated the creation of a global watchdog to oversee the emerging technology, saying, \"Generative AI has enormous potential for good and evil at scale. Its creators themselves have warned that much bigger, potentially catastrophic and existential risks lie ahead.\"[18] At the council session, Russia said it believes AI risks are too poorly understood to be considered a threat to global stability. China argued against strict global regulation, saying countries should be able to develop their own rules, while also saying they opposed the use of AI to \"create military hegemony or undermine the sovereignty of a country\".[177]\n Regulation of conscious AGIs focuses on integrating them with existing human society and can be divided into considerations of their legal standing and of their moral rights.[179] AI arms control will likely require the institutionalization of new international norms embodied in effective technical specifications combined with active monitoring and informal diplomacy by communities of experts, together with a legal and political verification process.[180][121]\n In July 2023, the US government secured voluntary safety commitments from major tech companies, including OpenAI, Amazon, Google, Meta, and Microsoft. The companies agreed to implement safeguards, including third-party oversight and security testing by independent experts, to address concerns related to AI's potential risks and societal harms. The parties framed the commitments as an intermediate step while regulations are formed. Amba Kak, executive director of the AI Now Institute, said, \"A closed-door deliberation with corporate actors resulting in voluntary safeguards isn't enough\" and called for public deliberation and regulations of the kind to which companies would not voluntarily agree.[181][182]\n In October 2023, U.S. President Joe Biden issued an executive order on the \"Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence\".[183] Alongside other requirements, the order mandates the development of guidelines for AI models that permit the \"evasion of human control\".\n"
    },
    {
        "title": "Turing test",
        "url": "https://en.wikipedia.org/wiki/Turing_test",
        "content": "\n The Turing test, originally called the imitation game by Alan Turing in 1949,[2] is a test of a machine's ability to exhibit intelligent behaviour equivalent to that of a human. In the test, a human evaluator judges a text transcript of a natural-language conversation between a human and a machine. The evaluator tries to identify the machine, and the machine passes if the evaluator cannot reliably tell them apart. The results would not depend on the machine's ability to answer questions correctly, only on how closely its answers resembled those of a human. Since the Turing test is a test of indistinguishability in performance capacity, the verbal version generalizes naturally to all of human performance capacity, verbal as well as nonverbal (robotic).[3]\n The test was introduced by Turing in his 1950 paper \"Computing Machinery and Intelligence\" while working at the University of Manchester.[4] It opens with the words: \"I propose to consider the question, 'Can machines think?'\" Because \"thinking\" is difficult to define, Turing chooses to \"replace the question by another, which is closely related to it and is expressed in relatively unambiguous words\".[5] Turing describes the new form of the problem in terms of a three-person party game called the \"imitation game\", in which an interrogator asks questions of a man and a woman in another room in order to determine the correct sex of the two players. Turing's new question is: \"Are there imaginable digital computers which would do well in the imitation game?\"[2] This question, Turing believed, was one that could actually be answered. In the remainder of the paper, he argued against the major objections to the proposition that \"machines can think\".[6]\n Since Turing introduced his test, it has been highly influential in the philosophy of artificial intelligence, resulting in substantial discussion and controversy, as well as criticism from philosophers like John Searle, who argue against the test's ability to detect consciousness.[7][8]\n Since the early 2020s, several large language models such as ChatGPT have passed modern, rigorous variants of the Turing test.[9][10][11]\n Several early symbolic AI programs were controversially claimed to pass the Turing test, either by limiting themselves to scripted situations or by presenting \"excuses\" for poor reasoning and conversational abilities, such as mental illness or a poor grasp of English.[12][13][14]\n In 1966, Joseph Weizenbaum created a program called ELIZA, which mimicked a Rogerian psychotherapist. The program would search the user's sentence for keywords before repeating them back to the user, providing the impression of a program listening and paying attention.[15] Weizenbaum thus succeeded by designing a context where a chatbot could mimic a person despite \"knowing almost nothing of the real world\".[13] Weizenbaum's program was able to fool some people into believing that they were talking to a real person.[13]\n Kenneth Colby created PARRY in 1972, a program modeled after the behaviour of paranoid schizophrenics.[16] Psychiatrists asked to compare transcripts of conversations generated by the program to those of conversations by actual schizophrenics could only identify about 52 percent of cases correctly (a figure consistent with random guessing).[17]\n In 2001, three programmers developed Eugene Goostman, a chatbot portraying itself as a 13-year old boy from Odesa who spoke English as a second language. This background was intentionally chosen so judges would forgive mistakes by the program. In a competition, 33% of judges thought Goostman was human.[18][19][20]\n In June 2022, Google's LaMDA model received widespread coverage after claims about it having achieved sentience. Initially in an article in The Economist Google Research Fellow Blaise Agüera y Arcas said the chatbot had demonstrated a degree of understanding of social relationships.[21] Several days later, Google engineer Blake Lemoine claimed in an interview with the Washington Post that LaMDA had achieved sentience. Lemoine had been placed on leave by Google for internal assertions to this effect. Google had investigated the claims but dismissed them.[22][23]\n OpenAI's chatbot, ChatGPT, was released in November 2022, is based on GPT-3.5 and GPT-4 large language models. Celeste Biever wrote in a Nature article that \"ChatGPT broke the Turing test\".[24] Stanford researchers reported that ChatGPT passes the test; they found that ChatGPT-4 \"passes a rigorous Turing test, diverging from average human behavior chiefly to be more cooperative\",[25] making it the first computer program to successfully do so.[26]\n \nThe question of whether it is possible for machines to think has a long history, which is firmly entrenched in the distinction between dualist and materialist views of the mind. René Descartes prefigures aspects of the Turing test in his 1637 Discourse on the Method when he writes:  [H]ow many different automata or moving machines could be made by the industry of man ... For we can easily understand a machine's being constituted so that it can utter words, and even emit some responses to action on it of a corporeal kind, which brings about a change in its organs; for instance, if touched in a particular part it may ask what we wish to say to it; if in another part it may exclaim that it is being hurt, and so on. But it never happens that it arranges its speech in various ways, in order to reply appropriately to everything that may be said in its presence, as even the lowest type of man can do.[27] Here Descartes notes that automata are capable of responding to human interactions but argues that such automata cannot respond appropriately to things said in their presence in the way that any human can. Descartes therefore prefigures the Turing test by defining the insufficiency of appropriate linguistic response as that which separates the human from the automaton. Descartes fails to consider the possibility that future automata might be able to overcome such insufficiency, and so does not propose the Turing test as such, even if he prefigures its conceptual framework and criterion.\n Denis Diderot formulates in his 1746 book Pensées philosophiques a Turing-test criterion, though with the important implicit limiting assumption maintained, of the participants being natural living beings, rather than considering created artifacts:\n If they find a parrot who could answer to everything, I would claim it to be an intelligent being without hesitation. This does not mean he agrees with this, but that it was already a common argument of materialists at that time.\n According to dualism, the mind is non-physical (or, at the very least, has non-physical properties)[28] and, therefore, cannot be explained in purely physical terms. According to materialism, the mind can be explained physically, which leaves open the possibility of minds that are produced artificially.[29]\n In 1936, philosopher Alfred Ayer considered the standard philosophical question of other minds: how do we know that other people have the same conscious experiences that we do? In his book, Language, Truth and Logic, Ayer suggested a protocol to distinguish between a conscious man and an unconscious machine: \"The only ground I can have for asserting that an object which appears to be conscious is not really a conscious being, but only a dummy or a machine, is that it fails to satisfy one of the empirical tests by which the presence or absence of consciousness is determined\".[30] (This suggestion is very similar to the Turing test, but it is not certain that Ayer's popular philosophical classic was familiar to Turing.) In other words, a thing is not conscious if it fails the consciousness test.\n A rudimentary idea of the Turing test appears in the 1726 novel Gulliver's Travels by Jonathan Swift.[31][32] When Gulliver is brought before the king of Brobdingnag, the king thinks at first that Gulliver might be a \"a piece of clock-work (which is in that country arrived to a very great perfection) contrived by some ingenious artist\". Even when he hears Gulliver speaking, the king still doubts whether Gulliver was taught \"a set of words\" to make him \"sell at a better price\". Gulliver tells that only after \"he put several other questions to me, and still received rational answers\" the king became satisfied that Gulliver was not a machine.[33]\n Tests where a human judges whether a computer or an alien is intelligent were an established convention in science fiction by the 1940s, and it is likely that Turing would have been aware of these.[34] Stanley G. Weinbaum's \"A Martian Odyssey\" (1934) provides an example of how nuanced such tests could be.[34]\n Earlier examples of machines or automatons attempting to pass as human include the Ancient Greek myth of Pygmalion who creates a sculpture of a woman that is animated by Aphrodite, Carlo Collodi's novel The Adventures of Pinocchio, about a puppet who wants to become a real boy, and E. T. A. Hoffmann's 1816 story \"The Sandman,\" where the protagonist falls in love with an automaton. In all these examples, people are fooled by artificial beings that - up to a point - pass as human.[35]\n Researchers in the United Kingdom had been exploring \"machine intelligence\" for up to ten years prior to the founding of the field of artificial intelligence (AI) research in 1956.[36] It was a common topic among the members of the Ratio Club, an informal group of British cybernetics and electronics researchers that included Alan Turing.[37]\n Turing, in particular, had been running the notion of machine intelligence since at least 1941[38] and one of the earliest-known mentions of \"computer intelligence\" was made by him in 1947.[39] In Turing's report, \"Intelligent Machinery,\"[40] he investigated \"the question of whether or not it is possible for machinery to show intelligent behaviour\"[41] and, as part of that investigation, proposed what may be considered the forerunner to his later tests:\n It is not difficult to devise a paper machine which will play a not very bad game of chess.[42] Now get three men A, B and C as subjects for the experiment. A and C are to be rather poor chess players, B is the operator who works the paper machine. ... Two rooms are used with some arrangement for communicating moves, and a game is played between C and either A or the paper machine. C may find it quite difficult to tell which he is playing.[43] \"Computing Machinery and Intelligence\" (1950) was the first published paper by Turing to focus exclusively on machine intelligence. Turing begins the 1950 paper with the claim, \"I propose to consider the question 'Can machines think?'\"[5] As he highlights, the traditional approach to such a question is to start with definitions, defining both the terms \"machine\" and \"think\". Turing chooses not to do so; instead, he replaces the question with a new one, \"which is closely related to it and is expressed in relatively unambiguous words\".[5] In essence he proposes to change the question from \"Can machines think?\" to \"Can machines do what we (as thinking entities) can do?\"[44] The advantage of the new question, Turing argues, is that it draws \"a fairly sharp line between the physical and intellectual capacities of a man\".[45]\n To demonstrate this approach Turing proposes a test inspired by a party game, known as the \"imitation game\", in which a man and a woman go into separate rooms and guests try to tell them apart by writing a series of questions and reading the typewritten answers sent back. In this game, both the man and the woman aim to convince the guests that they are the other. (Huma Shah argues that this two-human version of the game was presented by Turing only to introduce the reader to the machine-human question-answer test.[46]) Turing described his new version of the game as follows:\n We now ask the question, \"What will happen when a machine takes the part of A in this game?\" Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman? These questions replace our original, \"Can machines think?\"[45] Later in the paper, Turing suggests an \"equivalent\" alternative formulation involving a judge conversing only with a computer and a man.[47] While neither of these formulations precisely matches the version of the Turing test that is more generally known today, he proposed a third in 1952. In this version, which Turing discussed in a BBC radio broadcast, a jury asks questions of a computer and the role of the computer is to make a significant proportion of the jury believe that it is really a man.[48]\n Turing's paper considered nine putative objections, which include some of the major arguments against artificial intelligence that have been raised in the years since the paper was published (see \"Computing Machinery and Intelligence\").[6]\n John Searle's 1980 paper Minds, Brains, and Programs proposed the \"Chinese room\" thought experiment and argued that the Turing test could not be used to determine if a machine could think. Searle noted that software (such as ELIZA) could pass the Turing test simply by manipulating symbols of which they had no understanding. Without understanding, they could not be described as \"thinking\" in the same sense people did. Therefore, Searle concluded, the Turing test could not prove that machines could think.[49] Much like the Turing test itself, Searle's argument has been both widely criticised[50] and endorsed.[51]\n Arguments such as Searle's and others working on the philosophy of mind sparked off a more intense debate about the nature of intelligence, the possibility of machines with a conscious mind and the value of the Turing test that continued through the 1980s and 1990s.[52]\n The Loebner Prize, now reported as defunct[53], provided an annual platform for practical Turing tests with the first competition held in November 1991.[54] It was underwritten by Hugh Loebner. The Cambridge Center for Behavioral Studies in Massachusetts, United States, organised the prizes up to and including the 2003 contest. As Loebner described it, one reason the competition was created is to advance the state of AI research, at least in part, because no one had taken steps to implement the Turing test despite 40 years of discussing it.[55]\n The first Loebner Prize competition in 1991 led to a renewed discussion of the viability of the Turing test and the value of pursuing it, in both the popular press[56] and academia.[57] The first contest was won by a mindless program with no identifiable intelligence that managed to fool naïve interrogators into making the wrong identification. This highlighted several of the shortcomings of the Turing test (discussed below): The winner won, at least in part, because it was able to \"imitate human typing errors\";[56] the unsophisticated interrogators were easily fooled;[57] and some researchers in AI have been led to feel that the test is merely a distraction from more fruitful research.[58]\n The silver (text only) and gold (audio and visual) prizes have never been won. However, the competition has awarded the bronze medal every year for the computer system that, in the judges' opinions, demonstrates the \"most human\" conversational behaviour among that year's entries. Artificial Linguistic Internet Computer Entity (A.L.I.C.E.) has won the bronze award on three occasions in recent times (2000, 2001, 2004). Learning AI Jabberwacky won in 2005 and 2006.\n The Loebner Prize tested conversational intelligence; winners were typically chatterbot programs, or Artificial Conversational Entities (ACE)s. Early Loebner Prize rules restricted conversations: Each entry and hidden-human conversed on a single topic,[59] thus the interrogators were restricted to one line of questioning per entity interaction. The restricted conversation rule was lifted for the 1995 Loebner Prize. Interaction duration between judge and entity has varied in Loebner Prizes. In Loebner 2003, at the University of Surrey, each interrogator was allowed five minutes to interact with an entity, machine or hidden-human. Between 2004 and 2007, the interaction time allowed in Loebner Prizes was more than twenty minutes. The final competition was in 2019, due to a lack of funding for the prize following Loebner's death in 2016.[60]\n CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) is one of the oldest concepts for artificial intelligence. The CAPTCHA system is commonly used online to tell humans and bots apart on the internet. It is based on the Turing test. Displaying distorted letters and numbers, it asks the user to identify the letters and numbers and type them into a field, which bots struggle to do.[18][61]\n The reCaptcha is a CAPTCHA system owned by Google. The reCaptcha v1 and v2 both used to operate by asking the user to match distorted pictures or identify distorted letters and numbers. The reCaptcha v3 is designed to not interrupt users and run automatically when pages are loaded or buttons are clicked. This \"invisible\" CAPTCHA verification happens in the background and no challenges appear, which filters out most basic bots.[62][63]\n Saul Traiger argues that there are at least three primary versions of the Turing test, two of which are offered in \"Computing Machinery and Intelligence\" and one that he describes as the \"Standard Interpretation\".[64] While there is some debate regarding whether the \"Standard Interpretation\" is that described by Turing or, instead, based on a misreading of his paper, these three versions are not regarded as equivalent,[64] and their strengths and weaknesses are distinct.[65]\n Turing's original article describes a simple party game involving three players. Player A is a man, player B is a woman and player C (who plays the role of the interrogator) is of either gender. In the imitation game, player C is unable to see either player A or player B, and can communicate with them only through written notes. By asking questions of player A and player B, player C tries to determine which of the two is the man and which is the woman. Player A's role is to trick the interrogator into making the wrong decision, while player B attempts to assist the interrogator in making the right one.[7]\n Turing then asks:\n \"What will happen when a machine takes the part of A in this game? Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman?\" These questions replace our original, \"Can machines think?\"[45] The second version appeared later in Turing's 1950 paper. Similar to the original imitation game test, the role of player A is performed by a computer. However, the role of player B is performed by a man rather than a woman.\n Let us fix our attention on one particular digital computer C. Is it true that by modifying this computer to have an adequate storage, suitably increasing its speed of action, and providing it with an appropriate programme, C can be made to play satisfactorily the part of A in the imitation game, the part of B being taken by a man?[45] In this version, both player A (the computer) and player B are trying to trick the interrogator into making an incorrect decision.\n The standard interpretation is not included in the original paper, but is both accepted and debated.\nCommon understanding has it that the purpose of the Turing test is not specifically to determine whether a computer is able to fool an interrogator into believing that it is a human, but rather whether a computer could imitate a human.[7] While there is some dispute whether this interpretation was intended by Turing, Sterrett believes that it was[66] and thus conflates the second version with this one, while others, such as Traiger, do not[64] – this has nevertheless led to what can be viewed as the \"standard interpretation\". In this version, player A is a computer and player B a person of either sex. The role of the interrogator is not to determine which is male and which is female, but which is a computer and which is a human.[67] The fundamental issue with the standard interpretation is that the interrogator cannot differentiate which responder is human, and which is machine. There are issues about duration, but the standard interpretation generally considers this limitation as something that should be reasonable.\n Controversy has arisen over which of the alternative formulations of the test Turing intended.[66] Sterrett argues that two distinct tests can be extracted from his 1950 paper and that, pace Turing's remark, they are not equivalent. The test that employs the party game and compares frequencies of success is referred to as the \"Original Imitation Game Test\", whereas the test consisting of a human judge conversing with a human and a machine is referred to as the \"Standard Turing Test\", noting that Sterrett equates this with the \"standard interpretation\" rather than the second version of the imitation game. Sterrett agrees that the standard Turing test (STT) has the problems that its critics cite but feels that, in contrast, the original imitation game test (OIG test) so defined is immune to many of them, due to a crucial difference: Unlike the STT, it does not make similarity to human performance the criterion, even though it employs human performance in setting a criterion for machine intelligence. A man can fail the OIG test, but it is argued that it is a virtue of a test of intelligence that failure indicates a lack of resourcefulness: The OIG test requires the resourcefulness associated with intelligence and not merely \"simulation of human conversational behaviour\". The general structure of the OIG test could even be used with non-verbal versions of imitation games.[68]\n According to Huma Shah, Turing himself was concerned with whether a machine could think and was providing a simple method to examine this: through human-machine question-answer sessions.[69] Shah argues the imitation game which Turing described could be practicalized in two different ways: a) one-to-one interrogator-machine test, and b) simultaneous comparison of a machine with a human, both questioned in parallel by an interrogator.[46]\n Still other writers[70] have interpreted Turing as proposing that the imitation game itself is the test, without specifying how to take into account Turing's statement that the test that he proposed using the party version of the imitation game is based upon a criterion of comparative frequency of success in that imitation game, rather than a capacity to succeed at one round of the game.\n \nSome writers argue that the imitation game is best understood by its social aspects. In his 1948 paper, Turing refers to intelligence as an \"emotional concept,\" and notes that  The extent to which we regard something as behaving in an intelligent manner is determined as much by our own state of mind and training as by the properties of the object under consideration. If we are able to explain and predict its behaviour or if there seems to be little underlying plan, we have little temptation to imagine intelligence. With the same object therefore it is possible that one man would consider it as intelligent and another would not; the second man would have found out the rules of its behaviour.[71] Following this remark and similar ones scattered throughout Turing's publications, Diane Proudfoot[72] claims that Turing held a response-dependence approach to intelligence, according to which an intelligent (or thinking) entity is one that appears intelligent to an average interrogator. Shlomo Danziger[73] promotes a socio-technological interpretation, according to which Turing saw the imitation game not as an intelligence test but as a technological aspiration - one whose realization would likely involve a change in society's attitude toward machines. According to this reading, Turing's celebrated 50-year prediction - that by the end of the 20th century his test will be passed by some machine - actually consists of two distinguishable predictions. The first is a technological prediction: I believe that in about fifty years' time it will be possible to programme computers ... to make them play the imitation game so well that an average interrogator will not have more than 70% chance of making the right identification after five minutes of questioning.[74] The second prediction Turing makes is a sociological one: I believe that at the end of the century the use of words and general educated opinion will have altered so much that one will be able to speak of machines thinking without expecting to be contradicted.[74] Danziger claims further that for Turing, alteration of society's attitude towards machinery is a prerequisite for the existence of intelligent machines: Only when the term \"intelligent machine\" is no longer seen as an oxymoron the existence of intelligent machines would become logically possible.\n Saygin has suggested that maybe the original game is a way of proposing a less biased experimental design as it hides the participation of the computer.[75] The imitation game also includes a \"social hack\" not found in the standard interpretation, as in the game both computer and male human are required to play as pretending to be someone they are not.[76]\n A crucial piece of any laboratory test is that there should be a control. Turing never makes clear whether the interrogator in his tests is aware that one of the participants is a computer. He states only that player A is to be replaced with a machine, not that player C is to be made aware of this replacement.[45] When Colby, FD Hilf, S Weber and AD Kramer tested PARRY, they did so by assuming that the interrogators did not need to know that one or more of those being interviewed was a computer during the interrogation.[77] As Ayse Saygin, Peter Swirski,[78] and others have highlighted, this makes a big difference to the implementation and outcome of the test.[7] An experimental study looking at Gricean maxim violations using transcripts of Loebner's one-to-one (interrogator-hidden interlocutor) Prize for AI contests between 1994 and 1999, Ayse Saygin found significant differences between the responses of participants who knew and did not know about computers being involved.[79]\n The power and appeal of the Turing test derives from its simplicity. The philosophy of mind, psychology, and modern neuroscience have been unable to provide definitions of \"intelligence\" and \"thinking\" that are sufficiently precise and general to be applied to machines. Without such definitions, the central questions of the philosophy of artificial intelligence cannot be answered. The Turing test, even if imperfect, at least provides something that can actually be measured. As such, it is a pragmatic attempt to answer a difficult philosophical question.\n The format of the test allows the interrogator to give the machine a wide variety of intellectual tasks. Turing wrote that \"the question and answer method seems to be suitable for introducing almost any one of the fields of human endeavour that we wish to include\".[80] John Haugeland adds that \"understanding the words is not enough; you have to understand the topic as well\".[81]\n To pass a well-designed Turing test, the machine must use natural language, reason, have knowledge and learn. The test can be extended to include video input, as well as a \"hatch\" through which objects can be passed: this would force the machine to demonstrate skilled use of well designed vision and robotics as well. Together, these represent almost all of the major problems that artificial intelligence research would like to solve.[82]\n The Feigenbaum test is designed to take advantage of the broad range of topics available to a Turing test. It is a limited form of Turing's question-answer game which compares the machine against the abilities of experts in specific fields such as literature or chemistry.\n As a Cambridge honours graduate in mathematics, Turing might have been expected to propose a test of computer intelligence requiring expert knowledge in some highly technical field, and thus anticipating a more recent approach to the subject. Instead, as already noted, the test which he described in his seminal 1950 paper requires the computer to be able to compete successfully in a common party game, and this by performing as well as the typical man in answering a series of questions so as to pretend convincingly to be the woman contestant.\n Given the status of human sexual dimorphism as one of the most ancient of subjects, it is thus implicit in the above scenario that the questions to be answered will involve neither specialised factual knowledge nor information processing technique. The challenge for the computer, rather, will be to demonstrate empathy for the role of the female, and to demonstrate as well a characteristic aesthetic sensibility—both of which qualities are on display in this snippet of dialogue which Turing has imagined:\n When Turing does introduce some specialised knowledge into one of his imagined dialogues, the subject is not maths or electronics, but poetry:\n Turing thus once again demonstrates his interest in empathy and aesthetic sensitivity as components of an artificial intelligence; and in light of an increasing awareness of the threat from an AI run amok,[83] it has been suggested[84] that this focus perhaps represents a critical intuition on Turing's part, i.e., that emotional and aesthetic intelligence will play a key role in the creation of a \"friendly AI\". It is further noted, however, that whatever inspiration Turing might be able to lend in this direction depends upon the preservation of his original vision, which is to say, further, that the promulgation of a \"standard interpretation\" of the Turing test—i.e., one which focuses on a discursive intelligence only—must be regarded with some caution.\n Turing did not explicitly state that the Turing test could be used as a measure of \"intelligence\", or any other human quality. He wanted to provide a clear and understandable alternative to the word \"think\", which he could then use to reply to criticisms of the possibility of \"thinking machines\" and to suggest ways that research might move forward.\n Nevertheless, the Turing test has been proposed as a measure of a machine's \"ability to think\" or its \"intelligence\". This proposal has received criticism from both philosophers and computer scientists. The interpretation makes the assumption that an interrogator can determine if a machine is \"thinking\" by comparing its behaviour with human behaviour. Every element of this assumption has been questioned: the reliability of the interrogator's judgement, the value of comparing the machine with a human, and the value of comparing only behaviour. Because of these and other considerations, some AI researchers have questioned the relevance of the test to their field.\n In practice, the test's results can easily be dominated not by the computer's intelligence, but by the attitudes, skill, or naïveté of the questioner. Numerous experts in the field, including cognitive scientist Gary Marcus, insist that the Turing test only shows how easy it is to fool humans and is not an indication of machine intelligence.[85]\n Turing doesn't specify the precise skills and knowledge required by the interrogator in his description of the test, but he did use the term \"average interrogator\": \"[the] average interrogator would not have more than 70 per cent chance of making the right identification after five minutes of questioning\".[74]\n Chatterbot programs such as ELIZA have repeatedly fooled unsuspecting people into believing that they are communicating with human beings. In these cases, the \"interrogators\" are not even aware of the possibility that they are interacting with computers. To successfully appear human, there is no need for the machine to have any intelligence whatsoever and only a superficial resemblance to human behaviour is required.\n Early Loebner Prize competitions used \"unsophisticated\" interrogators who were easily fooled by the machines.[57] Since 2004, the Loebner Prize organisers have deployed philosophers, computer scientists, and journalists among the interrogators. Nonetheless, some of these experts have been deceived by the machines.[86]\n One interesting feature of the Turing test is the frequency of the confederate effect, when the confederate (tested) humans are misidentified by the interrogators as machines. It has been suggested that what interrogators expect as human responses is not necessarily typical of humans. As a result, some individuals can be categorised as machines. This can therefore work in favour of a competing machine. The humans are instructed to \"act themselves\", but sometimes their answers are more like what the interrogator expects a machine to say.[87] This raises the question of how to ensure that the humans are motivated to \"act human\".\n The Turing test does not directly test whether the computer behaves intelligently. It tests only whether the computer behaves like a human being. Since human behaviour and intelligent behaviour are not exactly the same thing, the test can fail to accurately measure intelligence in two ways:\n The Turing test is concerned strictly with how the subject acts – the external behaviour of the machine. In this regard, it takes a behaviourist or functionalist approach to the study of the mind. The example of ELIZA suggests that a machine passing the test may be able to simulate human conversational behaviour by following a simple (but large) list of mechanical rules, without thinking or having a mind at all.\n John Searle has argued that external behaviour cannot be used to determine if a machine is \"actually\" thinking or merely \"simulating thinking\".[49] His Chinese room argument is intended to show that, even if the Turing test is a good operational definition of intelligence, it may not indicate that the machine has a mind, consciousness, or intentionality. (Intentionality is a philosophical term for the power of thoughts to be \"about\" something.)\n \nTuring anticipated this line of criticism in his original paper,[91] writing:  I do not wish to give the impression that I think there is no mystery about consciousness. There is, for instance, something of a paradox connected with any attempt to localise it. But I do not think these mysteries necessarily need to be solved before we can answer the question with which we are concerned in this paper.[92]  Mainstream AI researchers argue that trying to pass the Turing test is merely a distraction from more fruitful research.[58] Indeed, the Turing test is not an active focus of much academic or commercial effort—as Stuart Russell and Peter Norvig write: \"AI researchers have devoted little attention to passing the Turing test\".[93] There are several reasons.\n First, there are easier ways to test their programs. Most current research in AI-related fields is aimed at modest and specific goals, such as object recognition or logistics. To test the intelligence of the programs that solve these problems, AI researchers simply give them the task directly. Stuart Russell and Peter Norvig suggest an analogy with the history of flight: Planes are tested by how well they fly, not by comparing them to birds. \"Aeronautical engineering texts,\" they write, \"do not define the goal of their field as 'making machines that fly so exactly like pigeons that they can fool other pigeons.'\"[93]\n Second, creating lifelike simulations of human beings is a difficult problem on its own that does not need to be solved to achieve the basic goals of AI research. Believable human characters may be interesting in a work of art, a game, or a sophisticated user interface, but they are not part of the science of creating intelligent machines, that is, machines that solve problems using intelligence.\n Turing did not intend for his idea to be used to test the intelligence of programs—he wanted to provide a clear and understandable example to aid in the discussion of the philosophy of artificial intelligence.[94] John McCarthy argues that we should not be surprised that a philosophical idea turns out to be useless for practical applications. He observes that the philosophy of AI is \"unlikely to have any more effect on the practice of AI research than philosophy of science generally has on the practice of science\".[95][96]\n Another well known objection raised towards the Turing test concerns its exclusive focus on linguistic behaviour (i.e. it is only a \"language-based\" experiment, while all the other cognitive faculties are not tested). This drawback downsizes the role of other modality-specific \"intelligent abilities\" concerning human beings that the psychologist Howard Gardner, in his \"multiple intelligence theory\", proposes to consider (verbal-linguistic abilities are only one of those).[97]\n A critical aspect of the Turing test is that a machine must give itself away as being a machine by its utterances. An interrogator must then make the \"right identification\" by correctly identifying the machine as being just that. If, however, a machine remains silent during a conversation, then it is not possible for an interrogator to accurately identify the machine other than by means of a calculated guess.[98]\nEven taking into account a parallel/hidden human as part of the test may not help the situation as humans can often be misidentified as being a machine.[99]\n By focusing on imitating humans, rather than augmenting or extending human capabilities, the Turing Test risks directing research and implementation toward technologies that substitute for humans and thereby drive down wages and income for workers. As they lose economic power, these workers may also lose political power, making it more difficult for them to change the allocation of wealth and income. This can trap them in a bad equilibrium. Erik Brynjolfsson has called this \"The Turing Trap\"[100] and argued that there are currently excess incentives for creating machines that imitate rather than augment humans.\n Numerous other versions of the Turing test, including those expounded above, have been raised through the years.\n A modification of the Turing test wherein the objective of one or more of the roles have been reversed between machines and humans is termed a reverse Turing test. An example is implied in the work of psychoanalyst Wilfred Bion,[101] who was particularly fascinated by the \"storm\" that resulted from the encounter of one mind by another. In his 2000 book,[78] among several other original points with regard to the Turing test, literary scholar Peter Swirski discussed in detail the idea of what he termed the Swirski test—essentially the reverse Turing test. He pointed out that it overcomes most if not all standard objections levelled at the standard version.\n Carrying this idea forward, R. D. Hinshelwood[102] described the mind as a \"mind recognizing apparatus\". The challenge would be for the computer to be able to determine if it were interacting with a human or another computer. This is an extension of the original question that Turing attempted to answer but would, perhaps, offer a high enough standard to define a machine that could \"think\" in a way that we typically define as characteristically human.\n CAPTCHA is a form of reverse Turing test. Before being allowed to perform some action on a website, the user is presented with alphanumerical characters in a distorted graphic image and asked to type them out. This is intended to prevent automated systems from being used to abuse the site. The rationale is that software sufficiently sophisticated to read and reproduce the distorted image accurately does not exist (or is not available to the average user), so any system able to do so is likely to be a human.\n Software that could reverse CAPTCHA with some accuracy by analysing patterns in the generating engine started being developed soon after the creation of CAPTCHA.[103]\nIn 2013, researchers at Vicarious announced that they had developed a system to solve CAPTCHA challenges from Google, Yahoo!, and PayPal up to 90% of the time.[104]\nIn 2014, Google engineers demonstrated a system that could defeat CAPTCHA challenges with 99.8% accuracy.[105]\nIn 2015, Shuman Ghosemajumder, former click fraud czar of Google, stated that there were cybercriminal sites that would defeat CAPTCHA challenges for a fee, to enable various forms of fraud.[106]\n A further variation is motivated by the concern that modern Natural Language Processing prove to be highly successful in generating text on the basis of a huge text corpus and could eventually pass the Turing test simply by manipulating words and sentences that have been used in the initial training of the model. Since the interrogator has no precise understanding of the training data, the model might simply be returning sentences that exist in similar fashion in the enormous amount of training data. For this reason, Arthur Schwaninger proposes a variation of the Turing test that can distinguish between systems that are only capable of using language and systems that understand language. He proposes a test in which the machine is confronted with philosophical questions that do not depend on any prior knowledge and yet require self-reflection to be answered appropriately.[107]\n Another variation is described as the subject-matter expert Turing test, where a machine's response cannot be distinguished from an expert in a given field. This is also known as a \"Feigenbaum test\" and was proposed by Edward Feigenbaum in a 2003 paper.[108]\n Robert French (1990) makes the case that an interrogator can distinguish human and non-human interlocutors by posing questions that reveal the low-level (i.e., unconscious) processes of human cognition, as studied by cognitive science. Such questions reveal the precise details of the human embodiment of thought and can unmask a computer unless it experiences the world as humans do.[109]\n The \"Total Turing test\"[3] variation of the Turing test, proposed by cognitive scientist Stevan Harnad,[110] adds two further requirements to the traditional Turing test. The interrogator can also test the perceptual abilities of the subject (requiring computer vision) and the subject's ability to manipulate objects (requiring robotics).[111]\n A letter published in Communications of the ACM[112] describes the concept of generating a synthetic patient population and proposes a variation of Turing test to assess the difference between synthetic and real patients. The letter states: \"In the EHR context, though a human physician can readily distinguish between synthetically generated and real live human patients, could a machine be given the intelligence to make such a determination on its own?\" and further the letter states: \"Before synthetic patient identities become a public health problem, the legitimate EHR market might benefit from applying Turing Test-like techniques to ensure greater data reliability and diagnostic value. Any new techniques must thus consider patients' heterogeneity and are likely to have greater complexity than the Allen eighth-grade-science-test is able to grade\".\n The minimum intelligent signal test was proposed by Chris McKinstry as \"the maximum abstraction of the Turing test\",[113] in which only binary responses (true/false or yes/no) are permitted, to focus only on the capacity for thought. It eliminates text chat problems like anthropomorphism bias, and does not require emulation of unintelligent human behaviour, allowing for systems that exceed human intelligence. The questions must each stand on their own, however, making it more like an IQ test than an interrogation. It is typically used to gather statistical data against which the performance of artificial intelligence programs may be measured.[114]\n The organisers of the Hutter Prize believe that compressing natural language text is a hard AI problem, equivalent to passing the Turing test. The data compression test has some advantages over most versions and variations of a Turing test, including:[citation needed]\n The main disadvantages of using data compression as a test are:\n A related approach to Hutter's prize which appeared much earlier in the late 1990s is the inclusion of compression problems in an extended Turing test.[115] or by tests which are completely derived from Kolmogorov complexity.[116]\nOther related tests in this line are presented by Hernandez-Orallo and Dowe.[117]\n Algorithmic IQ, or AIQ for short, is an attempt to convert the theoretical Universal Intelligence Measure from Legg and Hutter (based on Solomonoff's inductive inference) into a working practical test of machine intelligence.[118]\n Two major advantages of some of these tests are their applicability to nonhuman intelligences and their absence of a requirement for human testers.\n The Turing test inspired the Ebert test proposed in 2011 by film critic Roger Ebert which is a test whether a computer-based synthesised voice has sufficient skill in terms of intonations, inflections, timing and so forth, to make people laugh.[119]\n Taking advantage of large language models, in 2023 the research company AI21 Labs created an online social experiment titled \"Human or Not?\"[120][121]  It was played more than 10 million times by more than 2 million people.[122] It is the biggest Turing-style experiment to that date. The results showed that 32% of people could not distinguish between humans and machines.[123][124]\n 1990 marked the fortieth anniversary of the first publication of Turing's \"Computing Machinery and Intelligence\" paper, and saw renewed interest in the test. Two significant events occurred in that year: the first was the Turing Colloquium, which was held at the University of Sussex in April, and brought together academics and researchers from a wide variety of disciplines to discuss the Turing test in terms of its past, present, and future; the second was the formation of the annual Loebner Prize competition.\n Blay Whitby lists four major turning points in the history of the Turing test – the publication of \"Computing Machinery and Intelligence\" in 1950, the announcement of Joseph Weizenbaum's ELIZA in 1966, Kenneth Colby's creation of PARRY, which was first described in 1972, and the Turing Colloquium in 1990.[125]\n In parallel to the 2008 Loebner Prize held at the University of Reading,[126]\nthe Society for the Study of Artificial Intelligence and the Simulation of Behaviour (AISB), hosted a one-day symposium to discuss the Turing test, organised by John Barnden, Mark Bishop, Huma Shah and Kevin Warwick.[127]\nThe speakers included the Royal Institution's Director Baroness Susan Greenfield, Selmer Bringsjord, Turing's biographer Andrew Hodges, and consciousness scientist Owen Holland. No agreement emerged for a canonical Turing test, though Bringsjord expressed that a sizeable prize would result in the Turing test being passed sooner.\n"
    },
    {
        "title": "History of artificial intelligence",
        "url": "https://en.wikipedia.org/wiki/History_of_artificial_intelligence",
        "content": "\n \n The history of artificial intelligence (AI) began in antiquity, with myths, stories, and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The study of logic and formal reasoning from antiquity to the present led directly to the invention of the programmable digital computer in the 1940s, a machine based on abstract mathematical reasoning. This device and the ideas behind it inspired scientists to begin discussing the possibility of building an electronic brain.\n The field of AI research was founded at a workshop held on the campus of Dartmouth College in 1956.[1] Attendees of the workshop became the leaders of AI research for decades. Many of them predicted that machines as intelligent as humans would exist within a generation. The U.S. government provided millions of dollars with the hope of making this vision come true.[2]\n Eventually, it became obvious that researchers had grossly underestimated the difficulty of this feat.[3] In 1974, criticism from James Lighthill and pressure from the U.S. Congress led the U.S. and British Governments to stop funding undirected research into artificial intelligence. Seven years later, a visionary initiative by the Japanese Government and the success of expert systems  reinvigorated investment in AI, and by the late 1980s, the industry had grown into a billion-dollar enterprise. However, investors' enthusiasm waned in the 1990s, and the field was criticized in the press and avoided by industry (a period known as an \"AI winter\"). Nevertheless, research and funding continued to grow under other names.\n In the early 2000s, machine learning was applied to a wide range of problems in academia and industry. The success was due to the availability of powerful computer hardware, the collection of immense data sets, and the application of solid mathematical methods. Soon after, deep learning proved to be a breakthrough technology, eclipsing all other methods. The transformer architecture debuted in 2017 and was used to produce impressive generative AI applications, amongst other use cases.\n Investment in AI boomed in the 2020s. The recent AI boom, initiated by the development of transformer architecture, led to the rapid scaling and public releases of large language models (LLMs) like ChatGPT. These models exhibit human-like traits of knowledge, attention, and creativity, and have been integrated into various sectors, fueling exponential investment in AI. However, concerns about the potential risks and ethical implications of advanced AI have also emerged, prompting debate about the future of AI and its impact on society.\n In Greek mythology, Talos was a giant made of bronze who acted as guardian for the island of Crete. He would throw boulders at the ships of invaders and would complete 3 circuits around the island's perimeter daily.[4] According to pseudo-Apollodorus' Bibliotheke, Hephaestus forged Talos with the aid of a cyclops and presented the automaton as a gift to Minos.[5] In the Argonautica, Jason and the Argonauts defeated Talos by removing a plug near his foot, causing the vital ichor to flow out from his body and rendering him lifeless.[6]\n Pygmalion was a legendary king and sculptor of Greek mythology, famously represented in Ovid's Metamorphoses. In the 10th book of Ovid's narrative poem, Pygmalion becomes disgusted with women when he witnesses the way in which the Propoetides prostitute themselves. Despite this, he makes offerings at the temple of Venus asking the goddess to bring to him a woman just like a statue he carved.[7]\n In Of the Nature of Things, the Swiss alchemist Paracelsus describes a procedure that he claims can fabricate an \"artificial man\". By placing the \"sperm of a man\" in horse dung, and feeding it the \"Arcanum of Mans blood\" after 40 days, the concoction will become a living infant.[8]\n The earliest written account regarding golem-making is found in the writings of Eleazar ben Judah of Worms in the early 13th century.[9] During the Middle Ages, it was believed that the animation of a Golem could be achieved by insertion of a piece of paper with any of God's names on it, into the mouth of the clay figure.[10] Unlike legendary automata like Brazen Heads,[11] a Golem was unable to speak.[12]\n Takwin, the artificial creation of life, was a frequent topic of Ismaili alchemical manuscripts, especially those attributed to Jabir ibn Hayyan. Islamic alchemists attempted to create a broad range of life through their work, ranging from plants to animals.[13]\n In Faust: The Second Part of the Tragedy by Johann Wolfgang von Goethe, an alchemically fabricated homunculus, destined to live forever in the flask in which he was made, endeavors to be born into a full human body. Upon the initiation of this transformation, however, the flask shatters and the homunculus dies.[14]\n By the 19th century, ideas about artificial men and thinking machines became a popular theme in fiction. Notable works like Mary Shelley's Frankenstein  and Karel Čapek's R.U.R. (Rossum's Universal Robots)[15]\nexplored the concept of artificial life. Speculative essays, such as Samuel Butler's \"Darwin among the Machines\",[16] and Edgar Allan Poe's \"Maelzel's Chess Player\"[17] reflected society's growing interest in machines with artificial intelligence. AI remains a common topic in science fiction today.[18]\n Realistic humanoid automata were built by craftsman from many civilizations, including Yan Shi,[19] Hero of Alexandria,[20] Al-Jazari,[21] Haroun al-Rashid,[22] Jacques de Vaucanson,[23][24] Leonardo Torres y Quevedo,[25] Pierre Jaquet-Droz and Wolfgang von Kempelen.[26][27]\n The oldest known automata were the sacred statues of ancient Egypt and Greece.[28][29] The faithful believed that craftsman had imbued these figures with very real minds, capable of wisdom and emotion—Hermes Trismegistus wrote that \"by discovering the true nature of the gods, man has been able to reproduce it\".[30] English scholar Alexander Neckham asserted that the Ancient Roman poet Virgil had built a palace with automaton statues.[31]\n During the early modern period, these legendary automata were said to possess the magical ability to answer questions put to them. The late medieval alchemist and proto-Protestant Roger Bacon was purported to have fabricated a brazen head, having developed a legend of having been a wizard.[32][33] These legends were similar to the Norse myth of the Head of Mímir. According to legend, Mímir was known for his intellect and wisdom, and was beheaded in the Æsir-Vanir War. Odin is said to have \"embalmed\" the head with herbs and spoke incantations over it such that Mímir's head remained able to speak wisdom to Odin. Odin then kept the head near him for counsel.[34]\n Artificial intelligence is based on the assumption that the process of human thought can be mechanized. The study of mechanical—or \"formal\"—reasoning has a long history. Chinese, Indian and Greek philosophers all developed structured methods of formal deduction by the first millennium BCE. Their ideas were developed over the centuries by philosophers such as Aristotle (who gave a formal analysis of the syllogism),[35] Euclid (whose Elements was a model of formal reasoning), al-Khwārizmī (who developed algebra and gave his name to the word algorithm) and European scholastic philosophers such as William of Ockham and Duns Scotus.[36]\n Spanish philosopher Ramon Llull (1232–1315) developed several logical machines devoted to the production of knowledge by logical means;[37][38] Llull described his machines as mechanical entities that could combine basic and undeniable truths by simple logical operations, produced by the machine by mechanical meanings, in such ways as to produce all the possible knowledge.[39] Llull's work had a great influence on Gottfried Leibniz, who redeveloped his ideas.[40]\n In the 17th century, Leibniz, Thomas Hobbes and René Descartes explored the possibility that all rational thought could be made as systematic as algebra or geometry.[41] Hobbes famously wrote in Leviathan: \"For reason ... is nothing but reckoning, that is adding and subtracting\".[42] Leibniz envisioned a universal language of reasoning, the characteristica universalis, which would reduce argumentation to calculation so that \"there would be no more need of disputation between two philosophers than between two accountants. For it would suffice to take their pencils in hand, down to their slates, and to say each other (with a friend as witness, if they liked): Let us calculate.\"[43] These philosophers had begun to articulate the physical symbol system hypothesis that would become the guiding faith of AI research.\n The study of mathematical logic provided the essential breakthrough that made artificial intelligence seem plausible. The foundations had been set by such works as Boole's The Laws of Thought and Frege's Begriffsschrift.[44] Building on Frege's system, Russell and Whitehead presented a formal treatment of the foundations of mathematics in their masterpiece, the Principia Mathematica in 1913. Inspired by Russell's success, David Hilbert challenged mathematicians of the 1920s and 30s to answer this fundamental question: \"can all of mathematical reasoning be formalized?\"[36] His question was answered by Gödel's incompleteness proof,[45] Turing's machine[45] and Church's Lambda calculus.[a]\n Their answer was surprising in two ways. First, they proved that there were, in fact, limits to what mathematical logic could accomplish. But second (and more important for AI) their work suggested that, within these limits, any form of mathematical reasoning could be mechanized. The Church-Turing thesis implied that a mechanical device, shuffling symbols as simple as 0 and 1, could imitate any conceivable process of mathematical deduction.[45] The key insight was the Turing machine—a simple theoretical construct that captured the essence of abstract symbol manipulation.[48] This invention would inspire a handful of scientists to begin discussing the possibility of thinking machines.\n Calculating machines were designed or built in antiquity and throughout history by many people, including \nGottfried Leibniz,[38][49]\nJoseph Marie Jacquard,[50]\nCharles Babbage,[50][51]\nPercy Ludgate,[52]\nLeonardo Torres Quevedo,[53]\nVannevar Bush,[54]\nand others. Ada Lovelace speculated that Babbage's machine was \"a thinking or ... reasoning machine\", but warned \"It is desirable to guard against the possibility of exaggerated ideas that arise as to the powers\" of the machine.[55][56]\n The first modern computers were the massive machines of the Second World War (such as Konrad Zuse's Z3, Alan Turing's Heath Robinson and Colossus, Atanasoff and Berry's and ABC and ENIAC at the University of Pennsylvania).[57] ENIAC was based on the theoretical foundation laid by Alan Turing and developed by John von Neumann,[58] and proved to be the most influential.[57]\n The earliest research into thinking machines was inspired by a confluence of ideas that became prevalent in the late 1930s, 1940s, and early 1950s. Recent research in neurology had shown that the brain was an electrical network of neurons that fired in all-or-nothing pulses. Norbert Wiener's cybernetics described control and stability in electrical networks. Claude Shannon's information theory described digital signals (i.e., all-or-nothing signals). Alan Turing's theory of computation showed that any form of computation could be described digitally. The close relationship between these ideas suggested that it might be possible to construct an \"electronic brain\".\n In the 1940s and 50s, a handful of scientists from a variety of fields (mathematics, psychology, engineering, economics and political science) explored several research directions that would be vital to later AI research.[59] Alan Turing was among the first people to seriously investigate the theoretical possibility of \"machine intelligence\".[60] The field of \"artificial intelligence research\" was founded as an academic discipline in 1956.[61]\n In 1950 Turing published a landmark paper \"Computing Machinery and Intelligence\", in which he speculated about the possibility of creating machines that think.[63][b] In the paper, he noted that \"thinking\" is difficult to define and devised his famous Turing Test: If a machine could carry on a conversation (over a teleprinter) that was indistinguishable from a conversation with a human being, then it was reasonable to say that the machine was \"thinking\".[64] This simplified version of the problem allowed Turing to argue convincingly that a \"thinking machine\" was at least plausible and the paper answered all the most common objections to the proposition.[65] The Turing Test was the first serious proposal in the philosophy of artificial intelligence.\n Walter Pitts and Warren McCulloch analyzed networks of idealized artificial neurons and showed how they might perform simple logical functions in 1943. They were the first to describe what later researchers would call a neural network.[66] The paper was influenced by Turing's paper 'On Computable Numbers' from 1936 using similar two-state boolean 'neurons', but was the first to apply it to neuronal function.[60] One of the students inspired by Pitts and McCulloch was Marvin Minsky who was a 24-year-old graduate student at the time. In 1951 Minsky and Dean Edmonds built the first neural net machine, the SNARC.[67] Minsky would later become one of the most important leaders and innovators in AI.\n Experimental robots such as W. Grey Walter's turtles and the Johns Hopkins Beast, were built in the 1950s. These machines did not use computers, digital electronics or symbolic reasoning; they were controlled entirely by analog circuitry.[68]\n In 1951, using the Ferranti Mark 1 machine of the University of Manchester, Christopher Strachey wrote a checkers program[69] and Dietrich Prinz wrote one for chess.[70] Arthur Samuel's checkers program, the subject of his 1959 paper \"Some Studies in Machine Learning Using the Game of Checkers\", eventually achieved sufficient skill to challenge a respectable amateur.[71] Samuel's program was among the first uses of what would later be called machine learning.[72] Game AI would continue to be used as a measure of progress in AI throughout its history.\n When access to digital computers became possible in the mid-fifties, a few scientists instinctively recognized that a machine that could manipulate numbers could also manipulate symbols and that the manipulation of symbols could well be the essence of human thought. This was a new approach to creating thinking machines.[73][74]\n In 1955, Allen Newell and future Nobel Laureate Herbert A. Simon created the \"Logic Theorist\", with help from J. C. Shaw. The program would eventually prove 38 of the first 52 theorems in Russell and Whitehead's Principia Mathematica, and find new and more elegant proofs for some.[75] Simon said that they had \"solved the venerable mind/body problem, explaining how a system composed of matter can have the properties of mind.\"[76][c] The symbolic reasoning paradigm they introduced would dominate AI research and funding until the middle 90s, as well as inspire the cognitive revolution.\n The Dartmouth workshop of 1956 was a pivotal event that marked the formal inception of AI as an academic discipline.[61] It was organized by Marvin Minsky and John McCarthy, with the support of two senior scientists Claude Shannon and Nathan Rochester of IBM. The proposal for the conference stated they intended to test the assertion that \"every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it\".[77][d] The term \"Artificial Intelligence\" was introduced by John McCarthy at the workshop.[e] \nThe participants included Ray Solomonoff, Oliver Selfridge, Trenchard More, Arthur Samuel, Allen Newell and Herbert A. Simon, all of whom would create important programs during the first decades of AI research.[83][f] At the workshop Newell and Simon debuted the \"Logic Theorist\".[84] The workshop was the moment that AI gained its name, its mission, its first major success and its key players, and is widely considered the birth of AI.[g]\n In the autumn of 1956, Newell and Simon also presented the Logic Theorist at a meeting of the Special Interest Group in Information Theory at the Massachusetts Institute of Technology (MIT). At the same meeting, Noam Chomsky discussed his generative grammar, and George Miller described his landmark paper \"The Magical Number Seven, Plus or Minus Two\". Miller wrote \"I left the symposium with a conviction, more intuitive than rational, that experimental psychology, theoretical linguistics, and the computer simulation of cognitive processes were all pieces from a larger whole.\"[86][57]\n This meeting was the beginning of the \"cognitive revolution\"—an interdisciplinary paradigm shift in psychology, philosophy, computer science and neuroscience. It inspired the creation of the sub-fields of symbolic artificial intelligence, generative linguistics, cognitive science, cognitive psychology, cognitive neuroscience and the philosophical schools of computationalism and functionalism. All these fields used related tools to model the mind and results discovered in one field were relevant to the others.\n The cognitive approach allowed researchers to consider \"mental objects\" like thoughts, plans, goals, facts or memories, often analyzed using high level symbols in functional networks. These objects had been forbidden as \"unobservable\" by earlier paradigms such as behaviorism.[h] Symbolic mental objects would become the major focus of AI research and funding for the next several decades.\n The programs developed in the years after the Dartmouth Workshop were, to most people, simply \"astonishing\":[i] computers were solving algebra word problems, proving theorems in geometry and learning to speak English. Few at the time would have believed that such \"intelligent\" behavior by machines was possible at all.[90][91][89] Researchers expressed an intense optimism in private and in print, predicting that a fully intelligent machine would be built in less than 20 years.[92] Government agencies like the Defense Advanced Research Projects Agency (DARPA, then known as \"ARPA\") poured money into the field.[93] Artificial Intelligence laboratories were set up at a number of British and US universities in the latter 1950s and early 1960s.[60]\n There were many successful programs and new directions in the late 50s and 1960s. Among the most influential were these:\n Many early AI programs used the same basic algorithm. To achieve some goal (like winning a game or proving a theorem), they proceeded step by step towards it (by making a move or a deduction) as if searching through a maze, backtracking whenever they reached a dead end.[94] The principal difficulty was that, for many problems, the number of possible paths through the \"maze\" was astronomical (a situation known as a \"combinatorial explosion\"). Researchers would reduce the search space by using heuristics that would eliminate paths that were unlikely to lead to a solution.[95]\n Newell and Simon tried to capture a general version of this algorithm in a program called the \"General Problem Solver\".[96][97] Other \"searching\" programs were able to accomplish impressive tasks like solving problems in geometry and algebra, such as Herbert Gelernter's Geometry Theorem Prover (1958)[98] and Symbolic Automatic Integrator (SAINT), written by Minsky's student James Slagle in 1961.[99][100] Other programs searched through goals and subgoals to plan actions, like the STRIPS system developed at Stanford to control the behavior of the robot Shakey.[101]\n An important goal of AI research is to allow computers to communicate in natural languages like English. An early success was Daniel Bobrow's program STUDENT, which could solve high school algebra word problems.[102]\n A semantic net represents concepts (e.g. \"house\", \"door\") as nodes, and relations among concepts as links between the nodes (e.g. \"has-a\"). The first AI program to use a semantic net was written by Ross Quillian[103] and the most successful (and controversial) version was Roger Schank's Conceptual dependency theory.[104]\n Joseph Weizenbaum's ELIZA could carry out conversations that were so realistic that users occasionally were fooled into thinking they were communicating with a human being and not a computer program (see ELIZA effect). But in fact, ELIZA simply gave a canned response or repeated back what was said to it, rephrasing its response with a few grammar rules. ELIZA was the first chatbot.[105][106]\n In the late 60s, Marvin Minsky and Seymour Papert of the MIT AI Laboratory proposed that AI research should focus on artificially simple situations known as micro-worlds.[j] They pointed out that in successful sciences like physics, basic principles were often best understood using simplified models like frictionless planes or perfectly rigid bodies. Much of the research focused on a \"blocks world,\" which consists of colored blocks of various shapes and sizes arrayed on a flat surface.[107]\n This paradigm led to innovative work in machine vision by Gerald Sussman, Adolfo Guzman, David Waltz (who invented \"constraint propagation\"), and especially Patrick Winston. At the same time, Minsky and Papert built a robot arm that could stack blocks, bringing the blocks world to life. Terry Winograd's SHRDLU could communicate in ordinary English sentences about the micro-world, plan operations and execute them.[107]\n In the 1960s funding was primarily directed towards laboratories researching symbolic AI, however several people still pursued research in neural networks.\n The perceptron, a single-layer neural network was introduced in 1958 by Frank Rosenblatt[108] (who had been a schoolmate of Marvin Minsky at the Bronx High School of Science).[109] Like most AI researchers, he was optimistic about their power, predicting that a perceptron \"may eventually be able to learn, make decisions, and translate languages.\"[110] Rosenblatt was primarily funded by Office of Naval Research.[111]\n Bernard Widrow and his student Ted Hoff built ADALINE (1960) and MADALINE (1962), which had up to 1000 adjustable weights.[112][113] A group at Stanford Research Institute led by Charles A. Rosen and Alfred E. (Ted) Brain built two neural network machines named MINOS I (1960) and II (1963), mainly funded by U.S. Army Signal Corps. MINOS II[114] had 6600 adjustable weights,[115] and was controlled with an SDS 910 computer in a configuration named MINOS III (1968), which could classify symbols on army maps, and recognize hand-printed characters on Fortran coding sheets.[116][117] Most of neural network research during this early period involved building and using bespoke hardware, rather than simulation on digital computers.[k]\n However, partly due to lack of results and partly due to competition from symbolic AI research, the MINOS project ran out of funding in 1966. Rosenblatt failed to secure continued funding in the 1960s.[118] In 1969, research came to a sudden halt with the publication of Minsky and Papert's 1969 book Perceptrons.[119] It suggested that there were severe limitations to what perceptrons could do and that Rosenblatt's predictions had been grossly exaggerated. The effect of the book was that virtually no research was funded in connectionism for 10 years.[120] The competition for government funding ended with the victory of symbolic AI approaches over neural networks.[117][118]\n Minsky (who had worked on SNARC) became a staunch objector to pure connectionist AI. Widrow (who had worked on ADALINE) turned to adaptive signal processing. The SRI group (which worked on MINOS) turned to symbolic AI and robotics.[117][118]\n The main problem was the inability to train multilayered networks (versions of backpropagation had already been used in other fields but it was unknown to these researchers).[121][120] The AI community became aware of backpropogation in the 80s,[122] and, in the 21st century, neural networks would become enormously successful, fulfilling all of Rosenblatt's optimistic predictions. Rosenblatt did not live to see this, however, as he died in a boating accident in 1971.[123]\n The first generation of AI researchers made these predictions about their work:\n In June 1963, MIT received a $2.2 million grant from the newly created Advanced Research Projects Agency (ARPA, later known as DARPA). The money was used to fund project MAC which subsumed the \"AI Group\" founded by Minsky and McCarthy five years earlier. DARPA continued to provide $3 million each year until the 70s.[130] DARPA made similar grants to Newell and Simon's program at Carnegie Mellon University and to Stanford University's AI Lab, founded by John McCarthy in 1963.[131] Another important AI laboratory was established at Edinburgh University by Donald Michie in 1965.[132] These four institutions would continue to be the main centers of AI research and funding in academia for many years.[133][m]\n The money was given with few strings attached: J. C. R. Licklider, then the director of ARPA, believed that his organization should \"fund people, not projects!\" and allowed researchers to pursue whatever directions might interest them.[135]  This created a freewheeling atmosphere at MIT that gave birth to the hacker culture,[136] but this \"hands off\" approach did not last.\n In the 1970s, AI was subject to critiques and financial setbacks. AI researchers had failed to appreciate the difficulty of the problems they faced. Their tremendous optimism had raised public expectations impossibly high, and when the promised results failed to materialize, funding targeted at AI was severely reduced.[137] The lack of success indicated the techniques being used by AI researchers at the time were insufficient to achieve their goals.[138][139]\n These setbacks did not affect the growth and progress of the field, however. The funding cuts only impacted a handful of major laboratories[140] and the critiques were largely ignored.[141] General public interest in the field continued to grow,[140] the number of researchers increased dramatically,[140] and new ideas were explored in logic programming, commonsense reasoning and many other areas. Historian Thomas Haigh argued in 2023 that there was no winter,[140] and AI researcher Nils Nilsson described this period as the most \"exciting\" time to work in AI.[142]\n In the early seventies, the capabilities of AI programs were limited. Even the most impressive could only handle trivial versions of the problems they were supposed to solve;[n] all the programs were, in some sense, \"toys\".[144] AI researchers had begun to run into several limits that would be only conquered decades later, and others that still stymie the field in the 2020s:\n The agencies which funded AI research, such as the British government, DARPA and the National Research Council (NRC) became frustrated with the lack of progress and eventually cut off almost all funding for undirected AI research. The pattern began in 1966 when the Automatic Language Processing Advisory Committee (ALPAC) report criticized machine translation efforts. After spending $20 million, the NRC ended all support.[154] In 1973, the Lighthill report on the state of AI research in the UK criticized the failure of AI to achieve its \"grandiose objectives\" and led to the dismantling of AI research in that country.[155] (The report specifically mentioned the combinatorial explosion problem as a reason for AI's failings.)[139][143][s] DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at CMU and canceled an annual grant of $3 million.[157][t]\n Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues. \"Many researchers were caught up in a web of increasing exaggeration.\"[158][u] However, there was another issue: since the passage of the Mansfield Amendment in 1969, DARPA had been under increasing pressure to fund \"mission-oriented direct research, rather than basic undirected research\". Funding for the creative, freewheeling exploration that had gone on in the 60s would not come from DARPA, which instead directed money at specific projects with clear objectives, such as autonomous tanks and battle management systems.[159][v]\n The major laboratories (MIT, Stanford, CMU and Edinburgh) had been receiving generous support from their governments, and when it was withdrawn, these were the only places that were seriously impacted by the budget cuts. The thousands of researchers outside these institutions and the many more thousands that were joining the field were unaffected.[140]\n Several philosophers had strong objections to the claims being made by AI researchers. One of the earliest was John Lucas, who argued that Gödel's incompleteness theorem showed that a formal system (such as a computer program) could never see the truth of certain statements, while a human being could.[161] Hubert Dreyfus ridiculed the broken promises of the 1960s and critiqued the assumptions of AI, arguing that human reasoning actually involved very little \"symbol processing\" and a great deal of embodied, instinctive, unconscious \"know how\".[w][163] John Searle's Chinese Room argument, presented in 1980, attempted to show that a program could not be said to \"understand\" the symbols that it uses (a quality called \"intentionality\"). If the symbols have no meaning for the machine, Searle argued, then the machine can not be described as \"thinking\".[164]\n These critiques were not taken seriously by AI researchers. Problems like intractability and commonsense knowledge seemed much more immediate and serious. It was unclear what difference \"know how\" or \"intentionality\" made to an actual computer program. MIT's Minsky said of Dreyfus and Searle \"they misunderstand, and should be ignored.\"[165] Dreyfus, who also taught at MIT, was given a cold shoulder: he later said that AI researchers \"dared not be seen having lunch with me.\"[166] Joseph Weizenbaum, the author of ELIZA, was also an outspoken critic of Dreyfus' positions, but he \"deliberately made it plain that [his AI colleagues' treatment of Dreyfus] was not the way to treat a human being,\"[x] and was unprofessional and childish.[168]\n Weizenbaum began to have serious ethical doubts about AI when Kenneth Colby wrote a \"computer program which can conduct psychotherapeutic dialogue\" based on ELIZA.[169][170][y] Weizenbaum was disturbed that Colby saw a mindless program as a serious therapeutic tool. A feud began, and the situation was not helped when Colby did not credit Weizenbaum for his contribution to the program. In 1976, Weizenbaum published Computer Power and Human Reason which argued that the misuse of artificial intelligence has the potential to devalue human life.[172]\n Logic was introduced into AI research as early as 1958, by John McCarthy in his Advice Taker proposal.[173][98] In 1963, J. Alan Robinson had discovered a simple method to implement deduction on computers, the resolution and unification algorithm.[98] However, straightforward implementations, like those attempted by McCarthy and his students in the late 1960s, were especially intractable: the programs required astronomical numbers of steps to prove simple theorems.[173][174] A more fruitful approach to logic was developed in the 1970s by Robert Kowalski at the University of Edinburgh, and soon this led to the collaboration with French researchers Alain Colmerauer and Philippe Roussel [fr] who created the successful logic programming language Prolog.[175] Prolog uses a subset of logic (Horn clauses, closely related to \"rules\" and \"production rules\") that permit tractable computation. Rules would continue to be influential, providing a foundation for Edward Feigenbaum's expert systems and the continuing work by Allen Newell and Herbert A. Simon that would lead to Soar and their unified theories of cognition.[176]\n Critics of the logical approach noted, as Dreyfus had, that human beings rarely used logic when they solved problems. Experiments by psychologists like Peter Wason, Eleanor Rosch, Amos Tversky, Daniel Kahneman and others provided proof.[z] McCarthy responded that what people do is irrelevant. He argued that what is really needed are machines that can solve problems—not machines that think as people do.[aa]\n Among the critics of McCarthy's approach were his colleagues across the country at MIT. Marvin Minsky, Seymour Papert and Roger Schank were trying to solve problems like \"story understanding\" and \"object recognition\" that required a machine to think like a person. In order to use ordinary concepts like \"chair\" or \"restaurant\" they had to make all the same illogical assumptions that people normally made. Unfortunately, imprecise concepts like these are hard to represent in logic. MIT chose instead to focus on writing programs that solved a given task without using high-level abstract definitions or general theories of cognition, and measured performance by iterative testing, rather than arguments from first principles. Schank described their \"anti-logic\" approaches as scruffy, as opposed to the neat paradigm used by McCarthy, Kowalski, Feigenbaum, Newell and Simon.[177][ab]\n In 1975, in a seminal paper, Minsky noted that many of his fellow researchers were using the same kind of tool: a framework that captures all our common sense assumptions about something. For example, if we use the concept of a bird, there is a constellation of facts that immediately come to mind: we might assume that it flies, eats worms and so on (none of which are true for all birds). Minsky associated these assumptions with the general category and they could be inherited by the frames for subcategories and individuals, or over-ridden as necessary. He called these structures frames. Schank used a version of frames he called \"scripts\" to successfully answer questions about short stories in English.[178] Frames would eventually be widely used in software engineering under the name object-oriented programming.\n The logicians rose to the challenge. Pat Hayes claimed that \"most of 'frames' is just a new syntax for parts of first-order logic.\" But he noted that \"there are one or two apparently minor details which give a lot of trouble, however, especially defaults\".[179]\n Ray Reiter admitted that \"conventional logics, such as first-order\nlogic, lack the expressive power to adequately represent the knowledge required for reasoning by default\".[180] He proposed augmenting first-order logic with a closed world assumption that a conclusion holds (by default) if its contrary cannot be shown. He showed how such an assumption corresponds to the common sense assumption made in reasoning with frames. He also showed that it has its \"procedural equivalent\" as negation as failure in Prolog. The closed world assumption, as formulated by Reiter, \"is not a first-order notion. (It is a meta notion.)\"[180] However, Keith Clark showed that negation as finite failure can be understood as reasoning implicitly with definitions in first-order logic including a unique name assumption that different terms denote different individuals.[181]\n During the late 1970s and throughout the 1980s, a variety of logics and extensions of first-order logic were developed both for negation as failure in logic programming and for default reasoning more generally. Collectively, these logics have become known as non-monotonic logics.\n In the 1980s, a form of AI program called \"expert systems\" was adopted by corporations around the world and knowledge became the focus of mainstream AI research. Governments provided substantial funding, such as Japan's fifth generation computer project and the U.S. Strategic Computing Initiative. \"Overall, the AI industry boomed from a few million dollars in 1980 to billions of dollars in 1988.\"[122]\n An expert system is a program that answers questions or solves problems about a specific domain of knowledge, using logical rules that are derived from the knowledge of experts.[182]\nThe earliest examples were developed by Edward Feigenbaum and his students. Dendral, begun in 1965, identified compounds from spectrometer readings.[183][120] MYCIN, developed in 1972, diagnosed infectious blood diseases.[122] They demonstrated the feasibility of the approach.\n Expert systems restricted themselves to a small domain of specific knowledge (thus avoiding the commonsense knowledge problem)[120] and their simple design made it relatively easy for programs to be built and then modified once they were in place. All in all, the programs proved to be useful: something that AI had not been able to achieve up to this point.[184]\n In 1980, an expert system called R1 was completed at CMU for the Digital Equipment Corporation. It was an enormous success: it was saving the company 40 million dollars annually by 1986.[185] Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments.[186] An industry grew up to support them, including hardware companies like Symbolics and Lisp Machines and software companies such as IntelliCorp and Aion.[187]\n In 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings.[188] Much to the chagrin of scruffies, they initially chose Prolog as the primary computer language for the project.[189]\n Other countries responded with new programs of their own. The UK began the £350 million Alvey project.[190] A consortium of American companies formed the Microelectronics and Computer Technology Corporation (or \"MCC\") to fund large scale projects in AI and information technology.[191][190] DARPA responded as well, founding the Strategic Computing Initiative and tripling its investment in AI between 1984 and 1988.[192][193]\n The power of expert systems came from the expert knowledge they contained. They were part of a new direction in AI research that had been gaining ground throughout the 70s. \"AI researchers were beginning to suspect—reluctantly, for it violated the scientific canon of parsimony—that intelligence might very well be based on the ability to use large amounts of diverse knowledge in different ways,\"[194] writes Pamela McCorduck. \"[T]he great lesson from the 1970s was that intelligent behavior depended very much on dealing with knowledge, sometimes quite detailed knowledge, of a domain where a given task lay\".[195] Knowledge based systems and knowledge engineering became a major focus of AI research in the 1980s.[196] It was hoped that vast databases would solve the commonsense knowledge problem and provide the support that commonsense reasoning required.\n In the 1980s some researchers attempted to attack the commonsense knowledge problem directly, by creating a massive database that would contain all the mundane facts that the average person knows. Douglas Lenat, who started a database called Cyc, argued that there is no shortcut ― the only way for machines to know the meaning of human concepts is to teach them, one concept at a time, by hand.[197]\n Although symbolic knowledge representation and logical reasoning produced useful applications in the 80s and received massive amounts of funding, it was still unable to solve problems in perception, robotics, learning and common sense. A small number of scientists and engineers began to doubt that the symbolic approach would ever be sufficient for these tasks and developed other approaches, such as \"connectionism\", robotics, \"soft\" computing and reinforcement learning. Nils Nilsson called these approaches \"sub-symbolic\".\n In 1982, physicist John Hopfield was able to prove that a form of neural network (now called a \"Hopfield net\") could learn and process information, and provably converges after enough time under any fixed condition. It was a breakthrough, as it was previously thought that nonlinear networks would, in general, evolve chaotically.[198] Around the same time, Geoffrey Hinton and David Rumelhart popularized a method for training neural networks called \"backpropagation\".[ac] These two developments helped to revive the exploration of artificial neural networks.[122][199]\n Neural networks, along with several other similar models, received widespread attention after the 1986 publication of the Parallel Distributed Processing, a two volume collection of papers edited by Rumelhart and psychologist James McClelland. The new field was christened \"connectionism\" and there was a considerable debate between advocates of symbolic AI the \"connectionists\".[122] Hinton called symbols the \"luminous aether of AI\" – that is, an unworkable and misleading model of intelligence.[122] This was a direct attack on the principles that inspired the cognitive revolution.\n In 1990, Yann LeCun at Bell Labs used convolutional neural networks to recognize handwritten digits. The system was used widely in 90s, reading zip codes and personal checks. This was the first genuinely useful application of neural networks.[200][201]\n Rodney Brooks, Hans Moravec and others argued that, in order to show real intelligence, a machine needs to have a body — it needs to perceive, move, survive and deal with the world.[202] Sensorimotor skills are essential to higher level skills such as commonsense reasoning. They can't be efficiently implemented using abstract symbolic reasoning, so AI should solve the problems of perception, mobility, manipulation and survival without using symbolic representation at all. These robotics researchers advocated building intelligence \"from the bottom up\".[ad]\n A precursor to this idea was David Marr, who had come to MIT in the late 1970s from a successful background in theoretical neuroscience to lead the group studying vision. He rejected all symbolic approaches (both McCarthy's logic and Minsky's frames), arguing that AI needed to understand the physical machinery of vision from the bottom up before any symbolic processing took place. (Marr's work would be cut short by leukemia in 1980.)[204]\n In his 1990 paper \"Elephants Don't Play Chess,\"[205] robotics researcher Brooks took direct aim at the physical symbol system hypothesis, arguing that symbols are not always necessary since \"the world is its own best model. It is always exactly up to date. It always has every detail there is to be known. The trick is to sense it appropriately and often enough.\"[206]\n In the 1980s and 1990s, many cognitive scientists also rejected the symbol processing model of the mind and argued that the body was essential for reasoning, a theory called the \"embodied mind thesis\".[207]\n Soft computing uses methods that work with incomplete and imprecise information. They do not attempt to give precise, logical answers, but give results that are only \"probably\" correct. This allowed them to solve problems that precise symbolic methods could not handle. Press accounts often claimed these tools could \"think like a human\".[208][209]\n Judea Pearl's Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, an influential 1988 book[210] brought probability and decision theory into AI.[211] Fuzzy logic, developed by Lofti Zadeh in the 60s, began to be more widely used in AI and robotics. Evolutionary computation and artificial neural networks also handle imprecise information, and are classified  as \"soft\". In the 90s and early 2000s many other soft computing tools were developed and put into use, including Bayesian networks,[211] hidden Markov models,[211] information theory and stochastic modeling. These tools in turn depended on advanced mathematical techniques such as classical optimization. For a time in the 1990s and early 2000s, these soft tools were studied by a subfield of AI called \"computational intelligence\".[212]\n Reinforcement learning[213] gives an agent a reward every time it performs a desired action well, and may give negative rewards (or \"punishments\") when it performs poorly. It was described in the first half of the twentieth century by psychologists using animal models, such as Thorndike,[214][215] Pavlov[216] and Skinner.[217] In the 1950s, Alan Turing[215][218] and Arthur Samuel[215] foresaw the role of reinforcement learning in AI.\n A successful and influential research program was led by Richard Sutton and Andrew Barto beginning 1972. Their collaboration revolutionized the study of reinforcement learning and decision making over the four decades.[219][220] In 1988, Sutton described machine learning in terms of decision theory (i.e., the Markov decision process). This gave the subject a solid theoretical foundation and access to a large body of theoretical results developed in the field of operations research.[220]\n Also in 1988, Sutton and Barto developed the \"temporal difference\" (TD) learning algorithm, where the agent is rewarded only when its predictions about the future show improvement. It significantly outperformed previous algorithms.[221] TD-learning was used by Gerald Tesauro in 1992 in the program TD-Gammon, which played backgammon as well as the best human players. The program learned the game by playing against itself with zero prior knowledge.[222] In an interesting case of interdisciplinary convergence, neurologists discovered in 1997 that the dopamine reward system in brains also uses a version of the TD-learning algorithm.[223][224][225] TD learning would be become highly influential in the 21st century, used in both AlphaGo and AlphaZero.[226]\n The business community's fascination with AI rose and fell in the 1980s in the classic pattern of an economic bubble. As dozens of companies failed, the perception in the business world was that the technology was not viable.[227] The damage to AI's reputation would last into the 21st century. Inside the field there was little agreement on the reasons for AI's failure to fulfill the dream of human level intelligence that had captured the imagination of the world in the 1960s. Together, all these factors helped to fragment AI into competing subfields focused on particular problems or approaches, sometimes even under new names that disguised the tarnished pedigree of \"artificial intelligence\".[228]\n Over the next 20 years, AI consistently delivered working solutions to specific isolated problems. By the late 1990s, it was being used throughout the technology industry, although somewhat behind the scenes. The success was due to increasing computer power, by collaboration with other fields (such as mathematical optimization and statistics) and using the highest standards of scientific accountability.                                                                            By 2000, AI had achieved some of its oldest goals. The field was both more cautious and more successful than it had ever been.\n The term \"AI winter\" was coined by researchers who had survived the funding cuts of 1974 when they became concerned that enthusiasm for expert systems had spiraled out of control and that disappointment would certainly follow.[ae] Their fears were well founded: in the late 1980s and early 1990s, AI suffered a series of financial setbacks.[122]\n The first indication of a change in weather was the sudden collapse of the market for specialized AI hardware in 1987. Desktop computers from Apple and IBM had been steadily gaining speed and power and in 1987 they became more powerful than the more expensive Lisp machines made by Symbolics and others. There was no longer a good reason to buy them. An entire industry worth half a billion dollars was demolished overnight.[230]\n Eventually the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, and they were \"brittle\" (i.e., they could make grotesque mistakes when given unusual inputs). Expert systems proved useful, but only in a few special contexts.[231]\n In the late 1980s, the Strategic Computing Initiative cut funding to AI \"deeply and brutally\". New leadership at DARPA had decided that AI was not \"the next wave\" and directed funds towards projects that seemed more likely to produce immediate results.[232]\n By 1991, the impressive list of goals penned in 1981 for Japan's Fifth Generation Project had not been met. Indeed, some of them, like \"carry on a casual conversation\" would not be accomplished for another 40 years. As with other AI projects, expectations had run much higher than what was actually possible.[233][af]\n Over 300 AI companies had shut down, gone bankrupt, or been acquired by the end of 1993, effectively ending the first commercial wave of AI.[235] In 1994, HP Newquist stated in The Brain Makers that \"The immediate future of artificial intelligence—in its commercial form—seems to rest in part on the continued success of neural networks.\"[235]\n In the 1990s, algorithms originally developed by AI researchers began to appear as parts of larger systems. AI had solved a lot of very difficult problems[ag] and their solutions proved to be useful throughout the technology industry,[236][237] such as data mining, industrial robotics, logistics, speech recognition,[238] banking software,[239] medical diagnosis[239] and Google's search engine.[240][241]\n The field of AI received little or no credit for these successes in the 1990s and early 2000s. Many of AI's greatest innovations have been reduced to the status of just another item in the tool chest of computer science.[242] Nick Bostrom explains: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"[239]\n Many researchers in AI in the 1990s deliberately called their work by other names, such as informatics, knowledge-based systems, \"cognitive systems\" or computational intelligence. In part, this may have been because they considered their field to be fundamentally different from AI, but also the new names help to procure funding.[238][243][244] In the commercial world at least, the failed promises of the AI Winter continued to haunt AI research into the 2000s, as the New York Times reported in 2005: \"Computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers.\"[245]\n AI researchers began to develop and use sophisticated mathematical tools more than they ever had in the past.[246][247] Most of the new directions in AI relied heavily on mathematical models, including artificial neural networks, probabilistic reasoning, soft computing and reinforcement learning. In the 90s and 2000s, many other highly mathematical tools were adapted for AI. These tools were applied to machine learning, perception and mobility.\n There was a widespread realization that many of the problems that AI needed to solve were already being worked on by researchers in fields like statistics, mathematics, electrical engineering, economics or operations research. The shared mathematical language allowed both a higher level of collaboration with more established and successful fields and the achievement of results which were measurable and provable; AI had become a more rigorous \"scientific\" discipline.\n Another key reason for the success in the 90s was that AI researchers focussed on specific problems with verifiable solutions (an approach later derided as narrow AI). This provided useful tools in the present, rather than speculation about the future.\n A new paradigm called \"intelligent agents\" became widely accepted during the 1990s.[248][249][ah] Although earlier researchers had proposed modular \"divide and conquer\" approaches to AI,[ai] the intelligent agent did not reach its modern form until Judea Pearl, Allen Newell, Leslie P. Kaelbling, and others brought concepts from decision theory and economics into the study of AI.[250] When the economist's definition of a rational agent was married to computer science's definition of an object or module, the intelligent agent paradigm was complete.\n An intelligent agent is a system that perceives its environment and takes actions which maximize its chances of success. By this definition, simple programs that solve specific problems are \"intelligent agents\", as are human beings and organizations of human beings, such as firms. The intelligent agent paradigm defines AI research as \"the study of intelligent agents\".[aj] This is a generalization of some earlier definitions of AI: it goes beyond studying human intelligence; it studies all kinds of intelligence.\n The paradigm gave researchers license to study isolated problems and to disagree about methods, but still retain hope that their work could be combined into an agent architecture that would be capable of general intelligence.[251]\n On May 11, 1997, Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov.[252] In 2005, a Stanford robot won the DARPA Grand Challenge by driving autonomously for 131 miles along an unrehearsed desert trail. Two years later, a team from CMU won the DARPA Urban Challenge by autonomously navigating 55 miles in an urban environment while responding to traffic hazards and adhering to traffic laws.[253]\n These successes were not due to some revolutionary new paradigm, but mostly on the tedious application of engineering skill and on the tremendous increase in the speed and capacity of computers by the 90s.[ak] In fact, Deep Blue's computer was 10 million times faster than the Ferranti Mark 1 that Christopher Strachey taught to play chess in 1951.[al] This dramatic increase is measured by Moore's law, which predicts that the speed and memory capacity of computers doubles every two years. The fundamental problem of \"raw computer power\" was slowly being overcome.\n In the first decades of the 21st century, access to large amounts of data (known as \"big data\"), cheaper and faster computers and advanced machine learning techniques were successfully applied to many problems throughout the economy. A turning point was the success of deep learning around 2012 which improved the performance of machine learning on many tasks, including image and video processing, text analysis, and speech recognition.[255] Investment in AI increased along with its capabilities, and by 2016, the market for AI-related products, hardware, and software reached more than $8 billion, and the New York Times reported that interest in AI had reached a \"frenzy\".[256]\n In 2002, Ben Goertzel and others became concerned that AI had largely abandoned its original goal of producing versatile, fully intelligent machines, and argued in favor of more direct research into artificial general intelligence. By the mid-2010s several companies and institutions had been founded to pursue Artificial General Intelligence (AGI), such as OpenAI and Google's DeepMind. During the same period, new insights into superintelligence raised concerns that AI was an existential threat. The risks and unintended consequences of AI technology became an area of serious academic research after 2016.\n The success of machine learning in the 2000s depended on the availability of vast amounts of training data and faster computers.[257] Russell and Norvig wrote that the \"improvement in performance obtained by increasing the size of the data set by two or three orders of magnitude outweighs any improvement that can be made by tweaking the algorithm.\"[200] Geoffrey Hinton recalled that back in the 90s, the problem was that \"our labeled datasets were thousands of times too small. [And] our computers were millions of times too slow.\"[258] This was no longer true by 2010.\n The most useful data in the 2000s came from curated, labeled data sets created specifically for machine learning and AI. In 2007, a group at UMass Amherst released Labeled Faces in the Wild, an annotated set of images of faces that was widely used to train and test face recognition systems for the next several decades.[259] Fei-Fei Li developed ImageNet, a database of three million images captioned by volunteers using the Amazon Mechanical Turk. Released in 2009, it was a useful body of training data and a benchmark for testing for the next generation of image processing systems.[260][200] Google released word2vec in 2013 as an open source resource. It used large amounts of data text scraped from the internet and word embedding to create a numeric vector to represent each word. Users were surprised at how well it was able to capture word meanings, for example, ordinary vector addition would give equivalences like China + River = Yangtze, London+England-France = Paris.[261] This database in particular would be essential for the development of large language models in the late 2010s.\n The explosive growth of the internet gave machine learning programs access to billions of pages of text and images that could be scraped. And, for specific problems, large privately held databases contained the relevant data. McKinsey Global Institute reported that \"by 2009, nearly all sectors in the US economy had at least an average of 200 terabytes of stored data\".[262] This collection of information was known in the 2000s as big data.\n In a Jeopardy! exhibition match in February 2011, IBM's question answering system Watson defeated the two best Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.[263] Watson's expertise would have been impossible without the information available on the internet.[200]\n In 2012, AlexNet, a deep learning model,[am] developed by Alex Krizhevsky, won the ImageNet Large Scale Visual Recognition Challenge, with significantly fewer errors than the second-place winner.[265][200] Krizhevsky worked with Geoffrey Hinton at the University of Toronto.[an] This was a turning point in machine learning: over the next few years dozens of other approaches to image recognition were abandoned in favor of deep learning.[257]\n Deep learning uses a multi-layer perceptron. Although this architecture has been known since the 60s, getting it to work requires powerful hardware and large amounts of training data.[266] Before these became \navailable, improving performance of image processing systems required hand-crafted ad hoc features that were difficult to implement.[266] Deep learning was simpler and more general.[ao]\n Deep learning was applied to dozens of problems over the next few years (such as speech recognition, machine translation, medical diagnosis, and game playing). In every case it showed enormous gains in performance.[257] Investment and interest in AI boomed as a result.[257]\n It became fashionable in the 2000s to begin talking about the future of AI again and several popular books considered the possibility of superintelligent machines and what they might mean for human society. Some of this was optimistic (such as Ray Kurzweil's The Singularity is Near), but others warned that a sufficiently powerful AI was existential threat to humanity, such as Nick Bostrom and Eliezer Yudkowsky.[267] The topic became widely covered in the press and many leading intellectuals and politicians commented on the issue.\n AI programs in the 21st century are defined by their goals – the specific measures that they are designed to optimize. Nick Bostrom's influential 2005 book Superintelligence argued that, if one isn't careful about defining these goals, the machine may cause harm to humanity in the process of achieving a goal. Stuart J. Russell used the example of an intelligent robot that kills its owner to prevent it from being unplugged, reasoning \"you can't fetch the coffee if you're dead\".[268] (This problem is known by the technical term \"instrumental convergence\".) The solution is to align the machine's goal function with the goals of its owner and humanity in general. Thus, the problem of mitigating the risks and unintended consequences of AI became known as \"the value alignment problem\" or AI alignment.[269]\n At the same time, machine learning systems had begun to have disturbing unintended consequences. Cathy O'Neil explained how statistical algorithms had been among the causes of the 2008 economic crash,[270] Julia Angwin of ProPublica argued that the COMPAS system used by the criminal justice system exhibited racial bias under some measures,[271][ap] others showed that many machine learning systems exhibited some form of racial bias,[273] and there were many other examples of dangerous outcomes that had resulted from machine learning systems.[aq]\n In 2016, the election of Donald Trump and the controversy over the COMPAS system illuminated several problems with the current technological infrastructure, including misinformation, social media algorithms designed to maximize engagement, the misuse of personal data and the trustworthiness of predictive models.[274] Issues of fairness and unintended consequences became significantly more popular at AI conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The value alignment problem became a serious field of academic study.[275][ar]\n In the early 2000s, several researchers became concerned that mainstream AI was too focused on \"measurable performance in specific applications\"[277] (known as \"narrow AI\") and had abandoned AI's original goal of creating versatile, fully intelligent machines. An early critic was Nils Nilsson in 1995, and similar opinions were published by AI elder statesmen John McCarthy, Marvin Minsky, and Patrick Winston in 2007–2009. Minsky organized a symposium on \"human-level AI\" in 2004.[277] Ben Goertzel adopted the term \"artificial general intelligence\" for the new sub-field, founding a journal and holding conferences beginning in 2008.[278] The new field grew rapidly, buoyed by the continuing success of artificial neural networks and the hope that it was the key to AGI.\n Several competing companies, laboratories and foundations were founded to develop AGI in the 2010s. DeepMind was founded in 2010 by three English scientists, Demis Hassabis, Shane Legg and Mustafa Suleyman, with funding from Peter Thiel and later Elon Musk. The founders and financiers were deeply concerned about AI safety and the existential risk of AI. DeepMind's founders had a personal connection with Yudkowsky and Musk was among those who was actively raising the alarm.[279] Hassabis was both worried about the dangers of AGI and optimistic about its power; he hoped they could \"solve AI, then solve everything else.\"[280] The New York Times wrote in 2023 \"At the heart of this competition is a brain-stretching paradox. The people who say they are most worried about AI are among the most determined to create it and enjoy its riches. They have justified their ambition with their strong belief that they alone can keep AI from endangering Earth.\"[279]\n In 2012, Geoffrey Hinton (who been leading neural network research since the 80s) was approached by Baidu, which wanted to hire him and all his students for an enormous sum. Hinton decided to hold an auction and, at a Lake Tahoe AI conference, they sold themselves to Google for a price of $44 million. Hassabis took notice and sold DeepMind to Google in 2014, on the condition that it would not accept military contracts and would be overseen by an ethics board.[279]\n Larry Page of Google, unlike Musk and Hassabis, was an optimist about the future of AI. Musk and Paige became embroiled in an argument about the risk of AGI at Musk's 2015 birthday party. They had been friends for decades but stopped speaking to each other shortly afterwards. Musk attended the one and only meeting of the DeepMind's ethics board, where it became clear that Google was uninterested in mitigating the harm of AGI. Frustrated by his lack of influence he founded OpenAI in 2015, enlisting Sam Altman to run it and hiring top scientists. OpenAI began as a non-profit, \"free from the economic incentives that were driving Google and other corporations.\"[279] Musk became frustrated again and left the company in 2018. OpenAI turned to Microsoft for continued financial support and Altman and OpenAI formed a for-profit version of the company with more than $1 billion in financing.[279]\n In 2021, Dario Amodei  and 14 other scientists left OpenAI over concerns that the company was putting profits above safety. They formed Anthropic, which soon had $6 billion in financing from Microsoft and Google.[279]\n The AI boom started with the initial development of key architectures and algorithms such as the transformer architecture in 2017, leading to the scaling and development of large language models exhibiting human-like traits of knowledge, attention and creativity. The new AI era began around 2020–2023, with the public release of scaled large language models (LLMs) such as ChatGPT.[281]\n In 2017, the transformer architecture was proposed by Google researchers. It exploits an attention mechanism and became widely used in large language models.[282]\n Large language models, based on the transformer, were developed by AGI companies: OpenAI released GPT-3 in 2020, and DeepMind released Gato in 2022. These are foundation models: they are trained on vast quantities of unlabeled data and can be adapted to a wide range of downstream tasks.[citation needed]\n These models can discuss a huge number of topics and display general knowledge. The question naturally arises: are these models an example of artificial general intelligence? Bill Gates was skeptical of the new technology and the hype that surrounded AGI. However, Altman presented him with a live demo of ChatGPT4 passing an advanced biology test. Gates was convinced.[279] In 2023, Microsoft Research tested the model with a large variety of tasks, and concluded that \"it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system\".[283]\n In 2024, OpenAI o3, a type of advanced reasoning model developed by OpenAI was announced. On the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) benchmark developed by François Chollet in 2019, the model achieved an unofficial score of 87.5% on the semi-private test, surpassing the typical human score of 84%. The benchmark is supposed to be a necessary, but not sufficient test for AGI. Speaking of the benchmark, Chollet has said \"You’ll know AGI is here when the exercise of creating tasks that are easy for regular humans but hard for AI becomes simply impossible.\"[284]\n Investment in AI grew exponentially after 2020, with venture capital funding for generative AI companies increasing dramatically. Total AI investments rose from $18 billion in 2014 to $119 billion in 2021, with generative AI accounting for approximately 30% of investments by 2023.[285] According to metrics from 2017 to 2021, the United States outranked the rest of the world in terms of venture capital funding, number of startups, and AI patents granted.[286] The commercial AI scene became dominated by American Big Tech companies, whose investments in this area surpassed those from U.S.-based venture capitalists.[287] OpenAI's valuation reached $86 billion by early 2024,[288] while NVIDIA's market capitalization surpassed $3.3 trillion by mid-2024, making it the world's largest company by market capitalization as the demand for AI-capable GPUs surged.[289]\n 15.ai, launched in March 2020[290] by an anonymous MIT researcher,[291][292] was one of the earliest examples of generative AI gaining widespread public attention during the initial stages of the AI boom.[293] The free web application demonstrated the ability to clone character voices using neural networks with minimal training data, requiring as little as 15 seconds of audio to reproduce a voice—a capability later corroborated by OpenAI in 2024.[294] The service went viral on social media platforms in early 2021,[295][296] allowing users to generate speech for characters from popular media franchises, and became particularly notable for its pioneering role in popularizing AI voice synthesis for creative content and memes.[297]\n Contemporary AI systems are now becoming human-competitive at general tasks, and we must ask ourselves: Should we let machines flood our information channels with propaganda and untruth? Should we automate away all the jobs, including the fulfilling ones? Should we develop nonhuman minds that might eventually outnumber, outsmart, obsolete and replace us? Should we risk loss of control of our civilization? Such decisions must not be delegated to unelected tech leaders. Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable. This confidence must be well justified and increase with the magnitude of a system’s potential effects. OpenAI’s recent statement regarding artificial general intelligence, states that \"At some point, it may be important to get independent review before starting to train future systems, and for the most advanced efforts to agree to limit the rate of growth of compute used for creating new models.\" We agree. That point is now.\n Therefore, we call on all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4. This pause should be public and verifiable, and include all key actors. If such a pause cannot be enacted quickly, governments should step in and institute a moratorium.\n ChatGPT was launched on November 30, 2022, marking a pivotal moment in artificial intelligence's public adoption. Within days of its release it went viral, gaining over 100 million users in two months and becoming the fastest-growing consumer software application in history.[299] The chatbot's ability to engage in human-like conversations, write code, and generate creative content captured public imagination and led to rapid adoption across various sectors including education, business, and research.[300] ChatGPT's success prompted unprecedented responses from major technology companies—Google declared a \"code red\" and rapidly launched Gemini (formerly known as Google Bard), while Microsoft incorporated the technology into Bing Chat.[301] The rapid adoption of these AI technologies sparked intense debate about their implications. Notable AI researchers and industry leaders voiced both optimism and concern about the accelerating pace of development. In March 2023, over 20,000 signatories, including computer scientist Yoshua Bengio, Elon Musk, and Apple co-founder Steve Wozniak, signed an open letter calling for a pause in advanced AI development, citing \"profound risks to society and humanity.\"[302] However, other prominent researchers like Juergen Schmidhuber took a more optimistic view, emphasizing that the majority of AI research aims to make \"human lives longer and healthier and easier.\"[303]\n By mid-2024, however, the financial sector began to scrutinize AI companies more closely, particularly questioning their capacity to produce a return on investment commensurate with their massive valuations. Some prominent investors raised concerns about market expectations becoming disconnected from fundamental business realities. Jeremy Grantham, co-founder of GMO LLC, warned investors to \"be quite careful\" and drew parallels to previous technology-driven market bubbles.[304] Similarly, Jeffrey Gundlach, CEO of DoubleLine Capital, explicitly compared the AI boom to the dot-com bubble of the late 1990s, suggesting that investor enthusiasm might be outpacing realistic near-term capabilities and revenue potential.[305] These concerns were amplified by the substantial market capitalizations of AI-focused companies, many of which had yet to demonstrate sustainable profitability models.\n In March 2024, Anthropic released the Claude 3 family of large language models, including Claude 3 Haiku, Sonnet, and Opus.[306] The models demonstrated significant improvements in capabilities across various benchmarks, with Claude 3 Opus notably outperforming leading models from OpenAI and Google.[307] In June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated improved performance compared to the larger Claude 3 Opus, particularly in areas such as coding, multistep workflows, and image analysis.[308]\n In 2024, the Royal Swedish Academy of Sciences awarded Nobel Prizes in recognition of groundbreaking contributions to artificial intelligence. The recipients included:\n .\n"
    },
    {
        "title": "Timeline of artificial intelligence",
        "url": "https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence",
        "content": "\n \n This is a timeline of artificial intelligence, sometimes alternatively called synthetic intelligence.\n"
    },
    {
        "title": "Progress in artificial intelligence",
        "url": "https://en.wikipedia.org/wiki/Progress_in_artificial_intelligence",
        "content": "Progress in artificial intelligence (AI) refers to the advances, milestones, and breakthroughs that have been achieved in the field of artificial intelligence over time. AI is a multidisciplinary branch of computer science that aims to create machines and systems capable of performing tasks that typically require human intelligence. AI applications have been used in a wide range of fields including medical diagnosis, finance, robotics, law, video games, agriculture, and scientific discovery. However, many AI applications are not perceived as AI: \"A lot of cutting-edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"[1][2] \"Many thousands of AI applications are deeply embedded in the infrastructure of every industry.\"[3] In the late 1990s and early 2000s, AI technology became widely used as elements of larger systems,[3][4] but the field was rarely credited for these successes at the time.\n Kaplan and Haenlein structure artificial intelligence along three evolutionary stages:\n To allow comparison with human performance, artificial intelligence can be evaluated on constrained and well-defined problems. Such tests have been termed subject-matter expert Turing tests. Also, smaller problems provide more achievable goals and there are an ever-increasing number of positive results.\n Humans still substantially outperform both GPT-4 and models trained on the ConceptARC benchmark that scored 60% on most, and 77% on one category, while humans 91% on all and 97% on one category.[5]\n There are many useful abilities that can be described as showing some form of intelligence. This gives better insight into the comparative success of artificial intelligence in different areas.\n AI, like electricity or the steam engine, is a general-purpose technology. There is no consensus on how to characterize which tasks AI tends to excel at.[15] Some versions of Moravec's paradox observe that humans are more likely to outperform machines in areas such as physical dexterity that have been the direct target of natural selection.[16] While projects such as AlphaZero have succeeded in generating their own knowledge from scratch, many other machine learning projects require large training datasets.[17][18] Researcher Andrew Ng has suggested, as a \"highly imperfect rule of thumb\", that \"almost anything a typical human can do with less than one second of mental thought, we can probably now or in the near future automate using AI.\"[19]\n Games provide a high-profile benchmark for assessing rates of progress; many games have a large professional player base and a well-established competitive rating system. AlphaGo brought the era of classical board-game benchmarks to a close when Artificial Intelligence proved their competitive edge over humans in 2016. Deep Mind's AlphaGo AI software program defeated the world's best professional Go Player Lee Sedol.[20] Games of imperfect knowledge provide new challenges to AI in the area of game theory; the most prominent milestone in this area was brought to a close by Libratus' poker victory in 2017.[21][22] E-sports continue to provide additional benchmarks; Facebook AI, Deepmind, and others have engaged with the popular StarCraft franchise of videogames.[23][24]\n Broad classes of outcome for an AI test may be given as:\n In his famous Turing test, Alan Turing picked language, the defining feature of human beings, for its basis.[66] The Turing test is now considered too exploitable to be a meaningful benchmark.[67]\n The Feigenbaum test, proposed by the inventor of expert systems, tests a machine's knowledge and expertise about a specific subject.[68] A paper by Jim Gray of Microsoft in 2003 suggested extending the Turing test to speech understanding, speaking and recognizing objects and behavior.[69]\n Proposed \"universal intelligence\" tests aim to compare how well machines, humans, and even non-human animals perform on problem sets that are generic as possible. At an extreme, the test suite can contain every possible problem, weighted by Kolmogorov complexity; however, these problem sets tend to be dominated by impoverished pattern-matching exercises where a tuned AI can easily exceed human performance levels.[70][71][72][73][74]\n According to OpenAI, in 2023 ChatGPT GPT-4 scored the 90th percentile on the Uniform Bar Exam. On the SATs, GPT-4 scored the 89th percentile on math, and the 93rd percentile in Reading & Writing. On the GREs, it scored on the 54th percentile on the writing test, 88th percentile on the quantitative section, and 99th percentile on the verbal section. It scored in the 99th to 100th percentile on the 2020 USA Biology Olympiad semifinal exam. It scored a perfect \"5\" on several AP exams.[75]\n Independent researchers found in 2023 that ChatGPT GPT-3.5 \"performed at or near the passing threshold\" for the three parts of the United States Medical Licensing Examination. GPT-3.5 was also assessed to attain a low, but passing, grade from exams for four law school courses at the University of Minnesota.[75] GPT-4 passed a text-based radiology board–style examination.[76][77]\n Many competitions and prizes, such as the Imagenet Challenge, promote research in artificial intelligence. The most common areas of competition include general machine intelligence, conversational behavior, data-mining, robotic cars, and robot soccer as well as conventional games.[78]\n An expert poll around 2016, conducted by Katja Grace of the Future of Humanity Institute and associates, gave median estimates of 3 years for championship Angry Birds, 4 years for the World Series of Poker, and 6 years for StarCraft. On more subjective tasks, the poll gave 6 years for folding laundry as well as an average human worker, 7–10 years for expertly answering 'easily Googleable' questions, 8 years for average speech transcription, 9 years for average telephone banking, and 11 years for expert songwriting, but over 30 years for writing a New York Times bestseller or winning the Putnam math competition.[79][80][81]\n An AI defeated a grandmaster in a regulation tournament game for the first time in 1988; rebranded as Deep Blue, it beat the reigning human world chess champion in 1997 (see Deep Blue versus Garry Kasparov).[82]\n AlphaGo defeated a European Go champion in October 2015, and Lee Sedol in March 2016, one of the world's top players (see AlphaGo versus Lee Sedol). According to Scientific American and other sources, most observers had expected superhuman Computer Go performance to be at least a decade away.[85][86][87]\n AI pioneer and economist Herbert A. Simon inaccurately predicted in 1965: \"Machines will be capable, within twenty years, of doing any work a man can do\". Similarly, in 1970 Marvin Minsky wrote that \"Within a generation... the problem of creating artificial intelligence will substantially be solved.\"[93]\n Four polls conducted in 2012 and 2013 suggested that the median estimate among experts for when AGI would arrive was 2040 to 2050, depending on the poll.[94][95]\n The Grace poll around 2016 found results varied depending on how the question was framed. Respondents asked to estimate \"when unaided machines can accomplish every task better and more cheaply than human workers\" gave an aggregated median answer of 45 years and a 10% chance of it occurring within 9 years. Other respondents asked to estimate \"when all occupations are fully automatable. That is, when for any occupation, machines could be built to carry out the task better and more cheaply than human workers\" estimated a median of 122 years and a 10% probability of 20 years. The median response for when \"AI researcher\" could be fully automated was around 90 years. No link was found between seniority and optimism, but Asian researchers were much more optimistic than North American researchers on average; Asians predicted 30 years on average for \"accomplish every task\", compared with the 74 years predicted by North Americans.[79][80][81]\n"
    },
    {
        "title": "AI winter",
        "url": "https://en.wikipedia.org/wiki/AI_winter",
        "content": "\nIn the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research.[1] The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or even decades later.\n The term first appeared in 1984 as the topic of a public debate at the annual meeting of AAAI (then called the \"American Association of Artificial Intelligence\").[2] Roger Schank and Marvin Minsky—two leading AI researchers who experienced the \"winter\" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. They described a chain reaction, similar to a \"nuclear winter\", that would begin with pessimism in the AI community, followed by pessimism in the press, followed by a severe cutback in funding, followed by the end of serious research.[2] Three years later the billion-dollar AI industry began to collapse.\n There were two major \"winters\" approximately 1974–1980 and 1987–2000,[3] and several smaller episodes, including the following:\n Enthusiasm and optimism about AI has generally increased since its low point in the early 1990s. Beginning about 2012, interest in artificial intelligence (and especially the sub-field of machine learning) from the research and corporate communities led to a dramatic increase in funding and investment, leading to the current (as of 2024[update]) AI boom.\n Natural language processing (NLP) research has its roots in the early 1930s and began its existence with the work on machine translation (MT).[4] However, significant advancements and applications began to emerge after the publication of Warren Weaver's influential memorandum, Machine translation of languages: fourteen essays in 1949.[5] The memorandum generated great excitement within the research community. In the following years, notable events unfolded: IBM embarked on the development of the first machine, MIT appointed its first full-time professor in machine translation, and several conferences dedicated to MT took place. The culmination came with the public demonstration of the Georgetown–IBM machine, which garnered widespread attention in respected newspapers in 1954.[6]\n Just like all AI booms that have been followed by desperate AI winters, the media tended to exaggerate the significance of these developments. Headlines about the Georgetown–IBM experiment proclaimed phrases like \"The bilingual machine,\" \"Robot brain translates Russian into King's English,\"[7] and \"Polyglot brainchild.\"[8] However, the actual demonstration involved the translation of a curated set of only 49 Russian sentences into English, with the machine's vocabulary limited to just 250 words.[6] To put things into perspective, a 2006 study made by Paul Nation found that humans need a vocabulary of around 8,000 to 9,000-word families to comprehend written texts with 98% accuracy.[9]\n During the Cold War, the US government was particularly interested in the automatic, instant translation of Russian documents and scientific reports. The government aggressively supported efforts at machine translation starting in 1954. Another factor that propelled the field of mechanical translation was the interest shown by the Central Intelligence Agency (CIA). During that period, the CIA firmly believed in the importance of developing machine translation capabilities and supported such initiatives. They also recognized that this program had implications that extended beyond the interests of the CIA and the intelligence community.[6]\n \nAt the outset, the researchers were optimistic. Noam Chomsky's new work in grammar was streamlining the translation process and there were \"many predictions of imminent 'breakthroughs'\".[10] However, researchers had underestimated the profound difficulty of word-sense disambiguation. In order to translate a sentence, a machine needed to have some idea what the sentence was about, otherwise it made mistakes. An apocryphal[11] example is \"the spirit is willing but the flesh is weak.\" Translated back and forth with Russian, it became \"the vodka is good but the meat is rotten.\"[12] Later researchers would call this the commonsense knowledge problem.\n By 1964, the National Research Council had become concerned about the lack of progress and formed the Automatic Language Processing Advisory Committee (ALPAC) to look into the problem. They concluded, in a famous 1966 report, that machine translation was more expensive, less accurate and slower than human translation. After spending some 20 million dollars, the NRC ended all support. Careers were destroyed and research ended.[2][10]\n Machine translation shared the same path with NLP from the rule-based approaches through the statistical approaches up to the neural network approaches, which have in 2023 culminated in large language models.\n Simple networks or circuits of connected units, including Walter Pitts and Warren McCulloch's neural network for logic and Marvin Minsky's SNARC system, have failed to deliver the promised results and were abandoned in the late 1950s. Following the success of programs such as the Logic Theorist and the General Problem Solver,[13] algorithms for manipulating symbols seemed more promising at the time as means to achieve logical reasoning viewed at the time as the essence of intelligence, either natural or artificial. \n Interest in perceptrons, invented by Frank Rosenblatt, was kept alive only by the sheer force of his personality.[14]\nHe optimistically predicted that the perceptron \"may eventually be able to learn, make decisions, and translate languages\".[15]\nMainstream research into perceptrons ended partially because the 1969 book Perceptrons by Marvin Minsky and Seymour Papert emphasized the limits of what perceptrons could do.[16] While it was already known that multilayered perceptrons are not subject to the criticism, nobody in the 1960s knew how to train a multilayered perceptron. Backpropagation was still years away.[17]\n Major funding for projects neural network approaches was difficult to find in the 1970s and early 1980s.[18] Important theoretical work continued despite the lack of funding. The \"winter\" of neural network approach came to an end in the middle 1980s, when the work of John Hopfield, David Rumelhart and others revived large scale interest.[19] Rosenblatt did not live to see this, however, as he died in a boating accident shortly after Perceptrons was published.[15]\n In 1973, professor Sir James Lighthill was asked by the UK Parliament to evaluate the state of AI research in the United Kingdom. His report, now called the Lighthill report, criticized the utter failure of AI to achieve its \"grandiose objectives\". He concluded that nothing being done in AI could not be done in other sciences. He specifically mentioned the problem of \"combinatorial explosion\" or \"intractability\", which implied that many of AI's most successful algorithms would grind to a halt on real world problems and were only suitable for solving \"toy\" versions.[20]\n The report was contested in a debate broadcast in the BBC \"Controversy\" series in 1973. The debate \"The general purpose robot is a mirage\" from the Royal Institution was Lighthill versus the team of Donald Michie, John McCarthy and Richard Gregory.[21] McCarthy later wrote that \"the combinatorial explosion problem has been recognized in AI from the beginning\".[22]\n The report led to the complete dismantling of AI research in the UK.[20] AI research continued in only a few universities (Edinburgh, Essex and Sussex). Research would not revive on a large scale until 1983, when Alvey (a research project of the British Government) began to fund AI again from a war chest of £350 million in response to the Japanese Fifth Generation Project (see below). Alvey had a number of UK-only requirements which did not sit well internationally, especially with US partners, and lost Phase 2 funding.\n During the 1960s, the Defense Advanced Research Projects Agency (then known as \"ARPA\", now known as \"DARPA\") provided millions of dollars for AI research with few strings attached. J. C. R. Licklider, the founding director of DARPA's computing division, believed in \"funding people, not projects\"[23] and he and several successors allowed AI's leaders (such as Marvin Minsky, John McCarthy, Herbert A. Simon or Allen Newell) to spend it almost any way they liked.\n This attitude changed after the passage of Mansfield Amendment in 1969, which required DARPA to fund \"mission-oriented direct research, rather than basic undirected research\".[24] Pure undirected research of the kind that had gone on in the 1960s would no longer be funded by DARPA. Researchers now had to show that their work would soon produce some useful military technology. AI research proposals were held to a very high standard. The situation was not helped when the Lighthill report and DARPA's own study (the American Study Group) suggested that most AI research was unlikely to produce anything truly useful in the foreseeable future. DARPA's money was directed at specific projects with identifiable goals, such as autonomous tanks and battle management systems. By 1974, funding for AI projects was hard to find.[24]\n AI researcher Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues: \"Many researchers were caught up in a web of increasing exaggeration. Their initial promises to DARPA had been much too optimistic. Of course, what they delivered stopped considerably short of that. But they felt they couldn't in their next proposal promise less than in the first one, so they promised more.\"[25] The result, Moravec claims, is that some of the staff at DARPA had lost patience with AI research. \"It was literally phrased at DARPA that 'some of these people were going to be taught a lesson [by] having their two-million-dollar-a-year contracts cut to almost nothing!'\" Moravec told Daniel Crevier.[26]\n While the autonomous tank project was a failure, the battle management system (the Dynamic Analysis and Replanning Tool) proved to be enormously successful, saving billions in the first Gulf War, repaying all of DARPAs investment in AI[27] and justifying DARPA's pragmatic policy.[28]\n As described in:[29]\n In 1971, the Defense Advanced Research Projects Agency (DARPA) began an ambitious five-year experiment in speech understanding. The goals of the project were to provide recognition of utterances from a limited vocabulary in near-real time. Three organizations finally demonstrated systems at the conclusion of the project in 1976. These were Carnegie-Mellon University (CMU), who actually demonstrated two systems [HEARSAY-II and HARPY]; Bolt, Beranek and Newman (BBN); and System Development Corporation with Stanford Research Institute (SDC/SRI) The system that came closest to satisfying the original project goals was the CMU HARPY system. The relatively high performance of the HARPY system was largely achieved through 'hard-wiring' information about possible utterances into the system's knowledge base. Although HARPY made some interesting contributions, its dependence on extensive pre-knowledge limited the applicability of the approach to other signal-understanding tasks. DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at Carnegie Mellon University. DARPA had hoped for, and felt it had been promised, a system that could respond to voice commands from a pilot. The SUR team had developed a system which could recognize spoken English, but only if the words were spoken in a particular order. DARPA felt it had been duped and, in 1974, they cancelled a three million dollar a year contract.[30]\n Many years later, several successful commercial speech recognition systems would use the technology developed by the Carnegie Mellon team (such as hidden Markov models) and the market for speech recognition systems would reach $4 billion by 2001.[31]\n For a description of Hearsay-II see Hearsay-II, The Hearsay-II Speech Understanding System: Integrating Knowledge to Resolve Uncertainty and A Retrospective View of the Hearsay-II Architecture which appear in Blackboard Systems.[32]\n Reddy gives a review of progress in speech understanding at the end of the DARPA project in a 1976 article in Proceedings of the IEEE.[33]\n \nThomas Haigh argues that activity in the domain of AI did not slow down, even as funding from DoD was being redirected, mostly in the wake of congressional legislation meant to separate military and academic activities.[34] That indeed professional interest was growing throughout the 70s. Using the membership count of ACM's SIGART, the Special Interest Group on Artificial Intelligence, as a proxy for interest in the subject, the author writes:[34]  (...) I located two data sources, neither of which supports the idea of a broadly based AI winter during the 1970s. One is membership of ACM's SIGART, the major venue for sharing news and research abstracts during the 1970s. When the Lighthill report was published in 1973 the fast-growing group had 1,241 members, approximately twice the level in 1969. The next five years are conventionally thought of as the darkest part of the first AI winter. Was the AI community shrinking? No! By mid-1978 SIGART membership had almost tripled, to 3,500. Not only was the group growing faster than ever, it was increasing proportionally faster than ACM as a whole which had begun to plateau (expanding by less than 50% over the entire period from 1969 to 1978). One in every 11 ACM members was in SIGART. In the 1980s, a form of AI program called an \"expert system\" was adopted by corporations around the world. The first commercial expert system was XCON, developed at Carnegie Mellon for Digital Equipment Corporation, and it was an enormous success: it was estimated to have saved the company 40 million dollars over just six years of operation. Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments. An industry grew up to support them, including software companies like Teknowledge and Intellicorp (KEE), and hardware companies like Symbolics and LISP Machines Inc. who built specialized computers, called LISP machines, that were optimized to process the programming language LISP, the preferred language for AI research in the USA.[35][36]\n In 1987, three years after Minsky and Schank's prediction, the market for specialized LISP-based AI hardware collapsed. Workstations by companies like Sun Microsystems offered a powerful alternative to LISP machines and companies like Lucid offered a LISP environment for this new class of workstations. The performance of these general workstations became an increasingly difficult challenge for LISP Machines. Companies like Lucid and Franz LISP offered increasingly powerful versions of LISP that were portable to all UNIX systems. For example, benchmarks were published showing workstations maintaining a performance advantage over LISP machines.[37] Later desktop computers built by Apple and IBM would also offer a simpler and more popular architecture to run LISP applications on. By 1987, some of them had become as powerful as the more expensive LISP machines. The desktop computers had rule-based engines such as CLIPS available.[38] These alternatives left consumers with no reason to buy an expensive machine specialized for running LISP. An entire industry worth half a billion dollars was replaced in a single year.[39]\n By the early 1990s, most commercial LISP companies had failed, including Symbolics, LISP Machines Inc., Lucid Inc., etc. Other companies, like Texas Instruments and Xerox, abandoned the field. A small number of customer companies (that is, companies using systems written in LISP and developed on LISP machine platforms) continued to maintain systems. In some cases, this maintenance involved the assumption of the resulting support work.[40]\n By the early 1990s, the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, they were \"brittle\" (i.e., they could make grotesque mistakes when given unusual inputs), and they fell prey to problems (such as the qualification problem) that had been identified years earlier in research in nonmonotonic logic. Expert systems proved useful, but only in a few special contexts.[41][42] Another problem dealt with the computational hardness of truth maintenance efforts for general knowledge. KEE used an assumption-based approach supporting multiple-world scenarios that was difficult to understand and apply.\n The few remaining expert system shell companies were eventually forced to downsize and search for new markets and software paradigms, like case-based reasoning or universal database access. The maturation of Common Lisp saved many systems such as ICAD which found application in knowledge-based engineering. Other systems, such as Intellicorp's KEE, moved from LISP to a C++ (variant) on the PC and helped establish object-oriented technology (including providing major support for the development of UML (see UML Partners).\n In 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth Generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings. By 1991, the impressive list of goals penned in 1981 had not been met. According to HP Newquist in The Brain Makers, \"On June 1, 1992, The Fifth Generation Project ended not with a successful roar, but with a whimper.\"[40] As with other AI projects, expectations had run much higher than what was actually possible.[43][44]\n In 1983, in response to the fifth generation project, DARPA again began to fund AI research through the Strategic Computing Initiative. As originally proposed the project would begin with practical, achievable goals, which even included artificial general intelligence as long-term objective. The program was under the direction of the Information Processing Technology Office (IPTO) and was also directed at supercomputing and microelectronics. By 1985 it had spent $100 million and 92 projects were underway at 60 institutions, half in industry, half in universities and government labs. AI research was well-funded by the SCI.[45]\n Jack Schwarz, who ascended to the leadership of IPTO in 1987, dismissed expert systems as \"clever programming\" and cut funding to AI \"deeply and brutally\", \"eviscerating\" SCI. Schwarz felt that DARPA should focus its funding only on those technologies which showed the most promise, in his words, DARPA should \"surf\", rather than \"dog paddle\", and he felt strongly AI was not \"the next wave\". Insiders in the program cited problems in communication, organization and integration. A few projects survived the funding cuts, including pilot's assistant and an autonomous land vehicle (which were never delivered) and the DART battle management system, which (as noted above) was successful.[46]\n A survey of reports from the early 2000s suggests that AI's reputation was still poor:\n Many researchers in AI in the mid 2000s deliberately called their work by other names, such as informatics, machine learning, analytics, knowledge-based systems, business rules management, cognitive systems, intelligent systems, intelligent agents or computational intelligence, to indicate that their work emphasizes particular tools or is directed at a particular sub-problem. Although this may be partly because they consider their field to be fundamentally different from AI, it is also true that the new names help to procure funding by avoiding the stigma of false promises attached to the name \"artificial intelligence\".[49][50]\n In the late 1990s and early 21st century, AI technology became widely used as elements of larger systems,[51][52] but the field is rarely credited for these successes. In 2006, Nick Bostrom explained that \"a lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"[53] Rodney Brooks stated around the same time that \"there's this stupid myth out there that AI has failed, but AI is around you every second of the day.\"[54]\n AI has reached the highest levels of interest and funding in its history in the early 2020s by every possible measure, including: \npublications,[55] \npatent applications,[56]\ntotal investment ($50 billion in 2022),[57] and \njob openings (800,000 U.S. job openings in 2022).[58]\nThe successes of the current \"AI spring\" or \"AI boom\" are advances in language translation (in particular, Google Translate), image recognition (spurred by the ImageNet training database) as commercialized by Google Image Search, and in game-playing systems such as AlphaZero (chess champion) and AlphaGo (go champion), and Watson (Jeopardy champion). A turning point was in 2012 when AlexNet (a deep learning network) won the ImageNet Large Scale Visual Recognition Challenge with half as many errors as the second place winner.[59]\n The 2022 release of OpenAI's AI chatbot ChatGPT which as of January 2023 has over 100 million users,[60] has reinvigorated the discussion about artificial intelligence and its effects on the world.[61][62]\n"
    },
    {
        "title": "AI boom",
        "url": "https://en.wikipedia.org/wiki/AI_boom",
        "content": "\n The AI boom[1][2] is an ongoing period of rapid progress in the field of artificial intelligence (AI) that started in the late 2010s before gaining international prominence in the early 2020s. Examples include protein folding prediction led by Google DeepMind as well as large language models and generative AI applications developed by OpenAI. This period is sometimes referred to as an AI spring, to contrast it with previous AI winters.[3][4]\n In 2012, a University of Toronto research team used artificial neural networks and deep learning techniques to lower the error rate below 25% for the first time during the ImageNet challenge for object recognition in computer vision. The event catalyzed the AI boom later that decade, when many alumni of the ImageNet challenge became leaders in the tech industry.[5][6] In March 2016, AlphaGo beat Lee Sedol in a five-game match, marking the first time a computer Go program had beaten a 9-dan professional without handicap. This match led to significant increase in public interest in AI.[7] The generative AI race began in earnest in 2016 or 2017 following the founding of OpenAI and earlier advances made in graphical processing units (GPUs), the amount and quality of training data, generative adversarial networks, diffusion models and transformer architectures.[8][9]\n In 2018, the Artificial Intelligence Index, an initiative from Stanford University, reported a global explosion of commercial and research efforts in AI. Europe published the largest number of papers in the field that year, followed by China and North America.[10] Technologies such as AlphaFold led to more accurate predictions of protein folding and improved the process of drug development.[11] Economists and lawmakers began to discuss the potential impact of AI more frequently.[12][13] By 2022, large language models (LLMs) saw increased usage in chatbot applications; text-to-image-models could generate images that appeared to be human-made;[14] and speech synthesis software was able to replicate human speech efficiently.[15]\n According to metrics from 2017 to 2021, the United States outranks the rest of the world in terms of venture capital funding, the number of startups, and patents granted in AI.[16][17] Scientists who have immigrated to the U.S. play an outsized role in the country's development of AI technology.[18][19] Many of them were educated in China, prompting debates about national security concerns amid worsening relations between the two countries.[20]\n Experts have framed AI development as a competition for economic and geopolitical advantage between the United States and China.[21] In 2021, an analyst for the Council on Foreign Relations outlined ways that the U.S. could maintain its position amid progress made by China.[22][23] In 2023, an analyst at the Center for Strategic and International Studies advocated for the U.S. to use its dominance in AI technology to drive its foreign policy instead of relying on trade agreements.[16]\n The AlphaFold 2 score of more than 90 in CASP's global distance test (GDT) is considered a significant achievement in computational biology[24] and great progress towards a decades-old grand challenge of biology.[25] Nobel Prize winner and structural biologist Venki Ramakrishnan called the result \"a stunning advance on the protein folding problem\",[24] adding that \"It has occurred decades before many people in the field would have predicted.\"[26][27]\n The ability to predict protein structures accurately based on the constituent amino acid sequence is expected to accelerate drug discovery and enable a better understanding of diseases.[25][28][29]\n Text-to-image models captured widespread public attention when OpenAI announced DALL-E, a transformer system, in January 2021.[30] A successor capable of generating complex and realistic images, DALL-E 2, was unveiled in April 2022.[31] An alternative text-to-image model, Midjourney, was released in July 2022.[32] Another alternative, open-source model Stable Diffusion, released in August 2022.[33]\n Following other text-to-image models, language model-powered text-to-video platforms such as Runway,[34] OpenAI's Sora, DAMO,[35] Make-A-Video,[36] Imagen Video[37] and Phenaki[38] can generate video from text as well as image prompts.[39]\n GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text.[40] The tool has been credited with spurring and accelerating the AI boom following its release.[41][42][43] An upgraded version called GPT-3.5 was used in ChatGPT, which later garnered attention for its detailed responses and articulate answers across many domains of knowledge.[44] A new version called GPT-4 was released on March 14, 2023, and was used in the Microsoft Bing search engine.[45][46] Other language models have been released, such as PaLM and Gemini by Google and LLaMA by Meta Platforms.\n In January 2023, DeepL Write, an AI-based tool to improve monolingual texts, was released.[47] In December 2023 Google released the Gemini language model.[48]\n In 2016, Google DeepMind unveiled WaveNet, a deep learning network that produced English, Mandarin, and piano music.[49]\n 15.ai, a free text-to-speech web application launched in March 2020, was an early development in the AI boom that used AI for voice synthesis. The platform could generate convincing character voices using as little as 15 seconds of training data.[50]  The application gained widespread attention in early 2021 for its ability to synthesize emotionally expressive speech from popular fictional characters,[51][52] becoming particularly influential in online content creation.[53][54]\n By April 2024, full length original songs generated by a pseudonymous creator named \"Glorb\" using the voices of cartoon characters from the SpongeBob SquarePants cartoon were being listened to millions of times on Spotify and YouTube. Glorb is not affiliated with the copyright holder or the original voice performers.[55][56]\n ElevenLabs allowed users to upload voice samples and create audio that sounds similar to the samples. The company was criticized[by whom?] after controversial[among whom?] statements were generated based on the vocal styles of celebrities, public officials, and other famous individuals,[57] raising concerns[among whom?] that the technology could make deepfakes even more convincing.[58] An unofficial song created using the voices of musicians Drake and The Weeknd raised questions[among whom?] about the ethics and legality of similar software.[59]\n Electricity consumed by hardware used for AI has increased demands on power grids, which has lead to prolonged use of fossil fuel power plants which would otherwise have been deactivated.[60][61][62]\n Microsoft, Google, and Amazon have all invested in existing or proposed nuclear power plants to meet these demands.[63][64] In September 2024, Microsoft signed a deal with Constellation Energy to purchase power from a reactor at Three Mile Island which had been shut down in 2019. The reactor is set to reopen in 2028 to provide power to Microsoft's data centers. The reactor is next to the unit which caused the worst nuclear power disaster in US history in 1979.[65][66][67]\n During the AI boom, different groups emerged, ranging from the ones that want to accelerate AI development as quickly as possible to those that are more concerned about AI safety and would like to \"decelerate\".[68]\n Some economists have been optimistic about the potential of the current wave of AI to boost productivity and economic growth.  Notably, Stanford University economist Erik Brynjolfsson, in a series of articles has argued for an \"AI-powered Productivity Boom\"[69]  and a \"Coming Productivity Boom\".[70] At the same time, others like Northwestern University economist Robert Gordon remain more pessimistic.[71] Brynjolfsson and Gordon have made a formal bet, registered at long bets, about the rate of productivity growth in the 2020s, to be resolved at the end of the decade.\n Big Tech companies view the AI boom as both opportunity and threat; Alphabet's Google, for example, realized that ChatGPT could be an innovator's dilemma-like replacement for Google Search. The company merged DeepMind and Google Brain, a rival internal unit, to accelerate its AI research.[72]\n The market capitalization of Nvidia, whose GPUs are in high demand to train and use generative AI models, rose to over US$3.3 trillion, making it the world's largest company by market capitalization as of June 19 2024.[73]\n In 2023, San Francisco's population increased for the first time in years, with the boom cited as a contributing factor.[74]\n Machine learning resources, hardware or software can be bought and licensed off-the-shelf or as cloud platform services.[75] This enables wide and publicly available uses, spreading AI skills.[76] Over half of businesses consider AI to be a top organizational priority and to be the most crucial technological advancement in many decades.[77]\n Across industries, generative AI tools are becoming widely available through the AI boom and are increasingly used in businesses across regions.[78] A main area of use is data analytics. Seen as an incremental change, machine learning improves industry performance.[79] Businesses report AI to be most useful in increased process efficiency, improved decision-making and strengthening of existing services and products.[80] Through adoption, AI has already positively influenced revenue generation in multiple business functions. Businesses have experienced revenue increases of up to 16%, mainly in manufacturing, risk management and research and development.[81]\n AI and generative AI investments have been increasing with the boom, increasing from $18 billion in 2014 to $119 billion in 2021. Most notably, the share of generative AI investments was around 30% in 2023.[82] Further, generative AI businesses have seen considerable venture capital investments even though regulatory and economic outlooks remain in question.[83]\n Tech giants capture the bulk of the monetary gains from AI and act as major suppliers to or customers of private users and other businesses.[84][85]\n Inaccuracy, cybersecurity and intellectual property infringement are considered to be the main risks associated with the boom, although not many actively attempt to mitigate the risk.[86] Large language models have been criticized for reproducing biases inherited from their training data, including discriminatory biases related to ethnicity or gender.[87] As a dual-use technology, AI carries risks of misuse by malicious actors.[88] As AI becomes more sophisticated, it may eventually become cheaper and more efficient than human workers, which could cause technological unemployment and a transition period of economic turmoil.[89][12] Public reaction to the AI boom has been mixed, with some hailing the new possibilities that AI creates, its sophistication and potential for benefiting humanity;[90][91] while others denounced it for threatening job security[92][93] and for giving 'uncanny' or flawed responses.[94]\n The commercial AI scene is dominated by American Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft, whose investments in this area have surpassed those from U.S.-based venture capitalists.[95][96][97] Some of these players already own the vast majority of existing cloud infrastructure, AI chips, and computing power from data centers, allowing them to entrench further in the marketplace.[98][99] AI-related patents have been hoarded by the largest American tech companies as well, with IBM leading the way with 1,200.[100]\n Tech companies such as Meta, OpenAI and Nvidia have been sued by artists, writers, journalists, and software developers for using their work to train AI models.[101][102] Early generative AI chatbots, such as the GPT-1, used the BookCorpus, and books are still the best source of training data for producing high-quality language models. ChatGPT aroused suspicion that its sources included libraries of pirated content after the chatbot produced detailed summaries of every part of Sarah Silverman's The Bedwetter and verbatim excerpts of paywalled content from The New York Times.[103][104]\n The ability to generate convincing, personalized messages as well as realistic images may facilitate large-scale misinformation, manipulation, and propaganda.[105]\n On April 19, 2024, as part of an ongoing feud with fellow rapper Kendrick Lamar, the artist Drake released the diss track Taylor Made Freestyle, which feature generated vocals imitating the voices of Tupac Shakur and Snoop Dogg.[106] Shakur's estate threatened to sue over the use of Shakur's likeness,[107] saying that it constituted a violation of Shakur's personality rights.\n On May 20, 2024, following the release of a demo of updates to OpenAI's ChatGPT Voice Mode feature a week earlier,[108][109] actor Scarlett Johansson issued a statement[110][111] in relation to the \"Sky\" voice shown in the demo, accusing OpenAI of producing it to be very similar to her own, and her portrayal of the artificial intelligence voice assistant Samantha in the film Her (2013), despite Johansson refusing an earlier offer from the company to provide her voice for the system. The unnamed voice actress who voiced Sky has stated she was coached to sound like Johansson,[citation needed] and used her natural speaking voice.[112]\n Several incidents involving sharing of non-consensual deepfake pornography have occurred. In late January 2024, deepfake images of American musician Taylor Swift proliferated. Several experts have warned that deepfake pornography is more quickly created and disseminated, due to the relative ease of using the technology.[113] Canada introduced federal legislation targeting sharing of non-consensual sexually explicit AI-generated photos; most provinces already had such laws.[114] In the United States, the DEFIANCE Act was introduced in March 2024.[115]\n A large amount of electricity is needed to power generative AI products,[116] making it more difficult for companies to achieve net zero emissions. From 2019 to 2024, Google's greenhouse gas emissions increased by 50%.[117]\n AI is expected by researchers of the Center for AI Safety to improve the \"accessibility, success rate, scale, speed, stealth and potency of cyberattacks\", potentially causing \"significant geopolitical turbulence\" if it reinforces attack more than defense.[88][118] Concerns have been raised about the potential capability of future AI systems to engineer particularly lethal and contagious pathogens.[119][120]\n The AI boom is said to have started an arms race in which large companies are competing against each other to have the most powerful AI model on the market, with speed and profit prioritized over safety and user protection.[121][122][123]\n Rapid progress in artificial intelligence has also sparked interest in whether some future AI systems will be sentient or otherwise worthy of moral consideration,[124] and whether they should be granted rights.[125]\n Industry leaders have further warned in the statement on AI risk of extinction that humanity might irreversibly lose control over a sufficiently advanced artificial general intelligence.[126]\n"
    },
    {
        "title": "Glossary of artificial intelligence",
        "url": "https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence",
        "content": "\n This glossary of artificial intelligence is a list of definitions of terms and concepts relevant to the study of artificial intelligence (AI), its subdisciplines, and related fields. Related glossaries include Glossary of computer science, Glossary of robotics, and Glossary of machine vision.\n Pronounced \"A-star\". Also abduction. Also adaptive network-based fuzzy inference system. Also artificial emotional intelligence or emotion AI. Also fuzzy string searching. Also argumentation system. Also machine intelligence. Also simply AI planning. Also self-driving car, robot car, and driverless car. Also backward reasoning. Also stochastic Hopfield network with hidden units. Also propositional satisfiability problem; abbreviated SATISFIABILITY or SAT. Also bagging or bootstrapping. Also self-learning know-how system. Also exhaustive search or generate and test. Also smartbot, talkbot, chatterbot, bot, IM bot, interactive agent, conversational interface, or artificial conversational entity. Also clustering. Also artificial creativity, mechanical creativity, creative computing, or creative computation. Also theoretical neuroscience or mathematical neuroscience. Also algorithmic number theory. Also statistical computing. Also conlang. Also recombination. Also dataset. Also enterprise data warehouse (EDW). Also theory of choice. Also epigenetic robotics. Also conversational agent (CA). Also dimension reduction. Also decentralized artificial intelligence. Also dilution. Also interface agent. Also representation learning. Also first-order predicate calculus or predicate logic. Also forward reasoning. Also friendly AI or FAI. Also graph search. Also cognitive augmentation, machine augmented intelligence, and enhanced intelligence. Also virtual assistant or personal digital assistant. Also logic tree. Also Clique Tree. Also mathematical programming. Also computer audition (CA). Also mechatronic engineering. Also self-organized system. Also entity identification, entity chunking, and entity extraction. Also brain–computer interface (BCI), neural-control interface (NCI), mind-machine interface (MMI), direct neural interface (DNI), or brain–machine interface (BMI). Also neuromorphic computing. Also non-deterministic polynomial-time hardness. Also Ockham's razor or Ocham's razor. Also ontology extraction, ontology generation, or ontology acquisition. Also pathing. Also first-order logic, predicate logic, and first-order predicate calculus. Also rationality principle. Also propositional logic, statement logic, sentential calculus, sentential logic, and zeroth-order logic. Also random decision forest. Also frame network. Also reasoning engine, rules engine, or simply reasoner. Also weak supervision. Also simply SLD resolution. Also sparse coding or SDL. Also simply the singularity. Abbreviated H+ or h+. Also tree search. Also narrow AI."
    },
    {
        "title": "Intelligence",
        "url": "https://en.wikipedia.org/wiki/Intelligence",
        "content": "\n Intelligence has been defined in many ways: the capacity for abstraction, logic, understanding, self-awareness, learning, emotional knowledge, reasoning, planning, creativity, critical thinking, and problem-solving. It can be described as the ability to perceive or infer information; and to retain it as knowledge to be applied to adaptive behaviors within an environment or context.[1]\n The term rose to prominence during the early 1900s.[2][3] Most psychologists believe that intelligence can be divided into various domains or competencies.[4]\n Intelligence has been long-studied in humans, and across numerous disciplines. It has also been observed in the cognition of non-human animals.[5] Some researchers have suggested that plants exhibit forms of intelligence, though this remains controversial.[6][7][8]\n Intelligence in computers or other machines is called artificial intelligence.\n The word intelligence derives from the Latin nouns intelligentia or intellēctus, which in turn stem from the verb intelligere, to comprehend or perceive. In the Middle Ages, the word intellectus became the scholarly technical term for understanding and a translation for the Greek philosophical term nous. This term, however, was strongly linked to the metaphysical and cosmological theories of teleological scholasticism, including theories of the immortality of the soul, and the concept of the active intellect (also known as the active intelligence). This approach to the study of nature was strongly rejected by  early modern philosophers such as Francis Bacon, Thomas Hobbes, John Locke, and David Hume, all of whom preferred \"understanding\" (in place of \"intellectus\" or \"intelligence\") in their English philosophical works.[9][10] Hobbes for example, in his Latin De Corpore, used \"intellectus intelligit\", translated in the English version as \"the understanding understandeth\", as a typical example of a logical absurdity.[11] \"Intelligence\" has therefore become less common in English language philosophy, but it has later been taken up (with the scholastic theories that it now implies) in more contemporary psychology.[12]\n There is controversy over how to define intelligence. Scholars describe its constituent abilities in various ways, and differ in the degree to which they conceive of intelligence as quantifiable.[13]\n A consensus report called Intelligence: Knowns and Unknowns, published in 1995 by the Board of Scientific Affairs of the American Psychological Association, states:\n Individuals differ from one another in their ability to understand complex ideas, to adapt effectively to the environment, to learn from experience, to engage in various forms of reasoning, to overcome obstacles by taking thought. Although these individual differences can be substantial, they are never entirely consistent: a given person's intellectual performance will vary on different occasions, in different domains, as judged by different criteria. Concepts of \"intelligence\" are attempts to clarify and organize this complex set of phenomena. Although considerable clarity has been achieved in some areas, no such conceptualization has yet answered all the important questions, and none commands universal assent. Indeed, when two dozen prominent theorists were recently asked to define intelligence, they gave two dozen, somewhat different, definitions.[14] Psychologists and learning researchers also have suggested definitions of intelligence such as the following:\n \"Intelligence is a force, F, that acts so as to maximize future freedom of action. It acts to maximize future freedom of action, or keep options open, with some strength T, with the diversity of possible accessible futures, S, up to some future time horizon, τ. In short, intelligence doesn't like to get trapped\".\n Human intelligence is the intellectual power of humans, which is marked by complex cognitive feats and high levels of motivation and self-awareness.[23][24] Intelligence enables humans to remember descriptions of things and use those descriptions in future behaviors. It gives humans the cognitive abilities to learn, form concepts, understand, and reason, including the capacities to recognize patterns, innovate, plan, solve problems, and employ language to communicate. These cognitive abilities can be organized into frameworks like fluid vs. crystallized and the Unified Cattell-Horn-Carroll model,[4] which contains abilities like fluid reasoning, perceptual speed, verbal abilities, and others.\n Intelligence is different from learning. Learning refers to the act of retaining facts and information or abilities and being able to recall them for future use. Intelligence, on the other hand, is the cognitive ability of someone to perform these and other processes. \n There have been various attempts to quantify intelligence via psychometric testing. Prominent among these are the various Intelligence Quotient (IQ) tests, which were first developed in the early 20th century to screen children for intellectual disability.[25] Over time, IQ tests became more pervasive, being used to screen immigrants, military recruits, and job applicants.[26] As the tests became more popular, belief that IQ tests measure a fundamental and unchanging attribute that all humans possess became widespread.[25]\n An influential theory that promoted the idea that IQ measures a fundamental quality possessed by every person is the theory of General Intelligence, or g factor.[27] The g factor is a construct that summarizes the correlations observed between an individual's scores on a range of cognitive tests. \n Today, most psychologists agree that IQ measures at least some aspects of human intelligence, particularly the ability to thrive in an academic context.[28] However, many psychologists question the validity of IQ tests as a measure of intelligence as a whole.[28][29]\n There is debate about the heritability of IQ, that is, what proportion of differences in IQ test performance between individuals are explained by genetic or environmental factors.[30][31] The scientific consensus is that genetics does not explain average differences in IQ test performance between racial groups.[32][33][34]\n Emotional intelligence is thought to be the ability to convey emotion to others in an understandable way as well as to read the emotions of others accurately.[35] Some theories imply that a heightened emotional intelligence could also lead to faster generating and processing of emotions in addition to the accuracy.[36] In addition, higher emotional intelligence is thought to help us manage emotions, which is beneficial for our problem-solving skills. Emotional intelligence is important to our mental health and has ties to social intelligence.[35]\n Social intelligence is the ability to understand the social cues and motivations of others and oneself in social situations. It is thought to be distinct to other types of intelligence, but has relations to emotional intelligence. Social intelligence has coincided with other studies that focus on how we make judgements of others, the accuracy with which we do so, and why people would be viewed as having positive or negative social character. There is debate as to whether or not these studies and social intelligence come from the same theories or if there is a distinction between them, and they are generally thought to be of two different schools of thought.[37]\n Moral intelligence is the capacity to understand right from wrong and to behave based on the value that is believed to be right.[38] It is considered a distinct form of intelligence, independent to both emotional and cognitive intelligence.[39]\n Concepts of \"book smarts\" and \"street smart\" are contrasting views based on the premise that some people have knowledge gained through academic study, but may lack the experience to sensibly apply that knowledge, while others have knowledge gained through practical experience, but may lack accurate information usually gained through study by which to effectively apply that knowledge. Artificial intelligence researcher Hector Levesque has noted that:\n Given the importance of learning through text in our own personal lives and in our culture, it is perhaps surprising how utterly dismissive we tend to be of it. It is sometimes derided as being merely \"book knowledge\", and having it is being \"book smart\". In contrast, knowledge acquired through direct experience and apprenticeship is called \"street knowledge\", and having it is being \"street smart\".[40] Although humans have been the primary focus of intelligence researchers, scientists have also attempted to investigate animal intelligence, or more broadly, animal cognition. These researchers are interested in studying both mental ability in a particular species, and comparing abilities between species. They study various measures of problem solving, as well as numerical and verbal reasoning abilities. Some challenges include defining intelligence so it has the same meaning across species, and operationalizing a measure that accurately compares mental ability across species and contexts.[41]\n Wolfgang Köhler's research on the intelligence of apes is an example of research in this area, as is Stanley Coren's book, The Intelligence of Dogs.[42] Non-human animals particularly noted and studied for their intelligence include chimpanzees, bonobos (notably the language-using Kanzi) and other great apes, dolphins, elephants and to some extent parrots, rats and ravens.[43]\n Cephalopod intelligence provides an important comparative study. Cephalopods appear to exhibit characteristics of significant intelligence, yet their nervous systems differ radically from those of backboned animals. Vertebrates such as mammals, birds, reptiles and fish have shown a fairly high degree of intellect that varies according to each species. The same is true with arthropods.[44]\n Evidence of a general factor of intelligence has been observed in non-human animals. First described in humans, the g factor has since been identified in a number of non-human species.[45]\n Cognitive ability and intelligence cannot be measured using the same, largely verbally dependent, scales developed for humans. Instead, intelligence is measured using a variety of interactive and observational tools focusing on innovation, habit reversal, social learning, and responses to novelty. Studies have shown that g is responsible for 47% of the individual variance in cognitive ability measures in primates[45] and between 55% and 60% of the variance in mice (Locurto, Locurto). These values are similar to the accepted variance in IQ explained by g in humans (40–50%).[46]\n It has been argued that plants should also be classified as intelligent based on their ability to sense and model external and internal environments and adjust their morphology, physiology and phenotype accordingly to ensure self-preservation and reproduction.[47][48]\n A counter argument is that intelligence is commonly understood to involve the creation and use of persistent memories as opposed to computation that does not involve learning. If this is accepted as definitive of intelligence, then it includes the artificial intelligence of robots capable of \"machine learning\", but excludes those purely autonomic sense-reaction responses that can be observed in many plants. Plants are not limited to automated sensory-motor responses, however, they are capable of discriminating positive and negative experiences and of \"learning\" (registering memories) from their past experiences. They are also capable of communication, accurately computing their circumstances, using sophisticated cost–benefit analysis and taking tightly controlled actions to mitigate and control the diverse environmental stressors.[7][8][49]\n Scholars studying artificial intelligence have proposed definitions of intelligence that include the intelligence demonstrated by machines. Some of these definitions are meant to be general enough to encompass human and other animal intelligence as well. An intelligent agent can be defined as a system that perceives its environment and takes actions which maximize its chances of success.[50] Kaplan and Haenlein define artificial intelligence as \"a system's ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation\".[51] Progress in artificial intelligence can be demonstrated in benchmarks ranging from games to practical tasks such as protein folding.[52] Existing AI lags humans in terms of general intelligence, which is sometimes defined as the \"capacity to learn how to carry out a huge range of tasks\".[53]\n Mathematician Olle Häggström defines intelligence in terms of \"optimization power\", an agent's capacity for efficient cross-domain optimization of the world according to the agent's preferences, or more simply the ability to \"steer the future into regions of possibility ranked high in a preference ordering\". In this optimization framework, Deep Blue has the power to \"steer a chessboard's future into a subspace of possibility which it labels as 'winning', despite attempts by Garry Kasparov to steer the future elsewhere.\"[54] Hutter and Legg, after surveying the literature, define intelligence as \"an agent's ability to achieve goals in a wide range of environments\".[55][56] While cognitive ability is sometimes measured as a one-dimensional parameter, it could also be represented as a \"hypersurface in a multidimensional space\" to compare systems that are good at different intellectual tasks.[57] Some skeptics believe that there is no meaningful way to define intelligence, aside from \"just pointing to ourselves\".[58]\n"
    },
    {
        "title": "Computer",
        "url": "https://en.wikipedia.org/wiki/Computer",
        "content": "\n A computer is a machine that can be programmed to automatically carry out sequences of arithmetic or logical operations (computation). Modern digital electronic computers can perform generic sets of operations known as programs. These programs enable computers to perform a wide range of tasks. The term computer system may refer to a nominally complete computer that includes the hardware, operating system, software, and peripheral equipment needed and used for full operation; or to a group of computers that are linked and function together, such as a computer network or computer cluster.\n A broad range of industrial and consumer products use computers as control systems, including simple special-purpose devices like microwave ovens and remote controls, and factory devices like industrial robots. Computers are at the core of general-purpose devices such as personal computers and mobile devices such as smartphones. Computers power the Internet, which links billions of computers and users.\n Early computers were meant to be used only for calculations. Simple manual instruments like the abacus have aided people in doing calculations since ancient times. Early in the Industrial Revolution, some mechanical devices were built to automate long, tedious tasks, such as guiding patterns for looms. More sophisticated electrical machines did specialized analog calculations in the early 20th century. The first digital electronic calculating machines were developed during World War II, both electromechanical and using thermionic valves. The first semiconductor transistors in the late 1940s were followed by the silicon-based MOSFET (MOS transistor) and monolithic integrated circuit chip technologies in the late 1950s, leading to the microprocessor and the microcomputer revolution in the 1970s. The speed, power, and versatility of computers have been increasing dramatically ever since then, with transistor counts increasing at a rapid pace (Moore's law noted that counts doubled every two years), leading to the Digital Revolution during the late 20th and early 21st centuries.\n Conventionally, a modern computer consists of at least one processing element, typically a central processing unit (CPU) in the form of a microprocessor, together with some type of computer memory, typically semiconductor memory chips. The processing element carries out arithmetic and logical operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices include input devices (keyboards, mice, joysticks, etc.), output devices (monitors, printers, etc.), and input/output devices that perform both functions (e.g. touchscreens). Peripheral devices allow information to be retrieved from an external source, and they enable the results of operations to be saved and retrieved.\n It was not until the mid-20th century that the word acquired its modern definition; according to the Oxford English Dictionary, the first known use of the word computer was in a different sense, in a 1613 book called The Yong Mans Gleanings by the English writer Richard Brathwait: \"I haue  [sic] read the truest computer of Times, and the best Arithmetician that euer  [sic] breathed, and he reduceth thy dayes into a short number.\" This usage of the term referred to a human computer, a person who carried out calculations or computations. The word continued to have the same meaning until the middle of the 20th century. During the latter part of this period, women were often hired as computers because they could be paid less than their male counterparts.[1] By 1943, most human computers were women.[2]\n The Online Etymology Dictionary gives the first attested use of computer in the 1640s, meaning 'one who calculates'; this is an \"agent noun from compute (v.)\". The Online Etymology Dictionary states that the use of the term to mean \"'calculating machine' (of any type) is from 1897.\" The Online Etymology Dictionary indicates that the \"modern use\" of the term, to mean 'programmable digital electronic computer' dates from \"1945 under this name; [in a] theoretical [sense] from 1937, as Turing machine\".[3] The name has remained, although modern computers are capable of many higher-level functions.\n Devices have been used to aid computation for thousands of years, mostly using one-to-one correspondence with fingers. The earliest counting device was most likely a form of tally stick. Later record keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented counts of items, likely livestock or grains, sealed in hollow unbaked clay containers.[a][4] The use of counting rods is one example.\n The abacus was initially used for arithmetic tasks. The Roman abacus was developed from devices used in Babylonia as early as 2400 BCE. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European counting house, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.[5]\n The Antikythera mechanism is believed to be the earliest known mechanical analog computer, according to Derek J. de Solla Price.[6] It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to approximately c. 100 BCE. Devices of comparable complexity to the Antikythera mechanism would not reappear until the fourteenth century.[7]\n Many mechanical aids to calculation and measurement were constructed for astronomical and navigation use. The planisphere was a star chart invented by Abū Rayhān al-Bīrūnī in the early 11th century.[8] The astrolabe was invented in the Hellenistic world in either the 1st or 2nd centuries BCE and is often attributed to Hipparchus. A combination of the planisphere and dioptra, the astrolabe was effectively an analog computer capable of working out several different kinds of problems in spherical astronomy. An astrolabe incorporating a mechanical calendar computer[9][10] and gear-wheels was invented by Abi Bakr of Isfahan, Persia in 1235.[11] Abū Rayhān al-Bīrūnī invented the first mechanical geared lunisolar calendar astrolabe,[12] an early fixed-wired knowledge processing machine[13] with a gear train and gear-wheels,[14] c. 1000 AD.\n The sector, a calculating instrument used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots, was developed in the late 16th century and found application in gunnery, surveying and navigation.\n The planimeter was a manual instrument to calculate the area of a closed figure by tracing over it with a mechanical linkage.\n The slide rule was invented around 1620–1630, by the English clergyman William Oughtred, shortly after the publication of the concept of the logarithm. It is a hand-operated analog computer for doing multiplication and division. As slide rule development progressed, added scales provided reciprocals, squares and square roots, cubes and cube roots, as well as transcendental functions such as logarithms and exponentials, circular and hyperbolic trigonometry and other functions. Slide rules with special scales are still used for quick performance of routine calculations, such as the E6B circular slide rule used for time and distance calculations on light aircraft.\n In the 1770s, Pierre Jaquet-Droz, a Swiss watchmaker, built a mechanical doll (automaton) that could write holding a quill pen. By switching the number and order of its internal wheels different letters, and hence different messages, could be produced. In effect, it could be mechanically \"programmed\" to read instructions. Along with two other complex machines, the doll is at the Musée d'Art et d'Histoire of Neuchâtel, Switzerland, and still operates.[15]\n In 1831–1835, mathematician and engineer Giovanni Plana devised a Perpetual Calendar machine, which through a system of pulleys and cylinders could predict the perpetual calendar for every year from 0 CE (that is, 1 BCE) to 4000 CE, keeping track of leap years and varying day length. The tide-predicting machine invented by the Scottish scientist Sir William Thomson in 1872 was of great utility to navigation in shallow waters. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location.\n The differential analyser, a mechanical analog computer designed to solve differential equations by integration, used wheel-and-disc mechanisms to perform the integration. In 1876, Sir William Thomson had already discussed the possible construction of such calculators, but he had been stymied by the limited output torque of the ball-and-disk integrators.[16] In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output. The torque amplifier was the advance that allowed these machines to work. Starting in the 1920s, Vannevar Bush and others developed mechanical differential analyzers.\n In the 1890s, the Spanish engineer Leonardo Torres Quevedo began to develop a series of advanced analog machines that could solve real and complex roots of polynomials,[17][18][19][20] which were published in 1901 by the Paris Academy of Sciences.[21]\n Charles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the \"father of the computer\",[22] he conceptualized and invented the first mechanical computer in the early 19th century.\n After working on his difference engine he announced his invention in 1822, in a paper to the Royal Astronomical Society, titled \"Note on the application of machinery to the computation of astronomical and mathematical tables\".[23] He also designed to aid in navigational calculations, in 1833 he realized that a much more general design, an analytical engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. The engine would incorporate an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete.[24][25]\n The machine was about a century ahead of its time. All the parts for his machine had to be made by hand – this was a major problem for a device with thousands of parts. Eventually, the project was dissolved with the decision of the British Government to cease funding. Babbage's failure to complete the analytical engine can be chiefly attributed to political and financial difficulties as well as his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. Nevertheless, his son, Henry Babbage, completed a simplified version of the analytical engine's computing unit (the mill) in 1888. He gave a successful demonstration of its use in computing tables in 1906.\n In his work Essays on Automatics published in 1914, Leonardo Torres Quevedo wrote a brief history of Babbage's efforts at constructing a mechanical Difference Engine and Analytical Engine. The paper contains a design of a machine capable to calculate formulas like \n\n\n\n\na\n\nx\n\n\n(\ny\n−\nz\n\n)\n\n2\n\n\n\n\n{\\displaystyle a^{x}(y-z)^{2}}\n\n, for a sequence of sets of values. The whole machine was to be controlled by a read-only program, which was complete with provisions for conditional branching. He also introduced the idea of floating-point arithmetic.[26][27][28] In 1920, to celebrate the 100th anniversary of the invention of the arithmometer, Torres presented in Paris the Electromechanical Arithmometer, which allowed a user to input arithmetic problems through a keyboard, and computed and printed the results,[29][30][31][32] demonstrating the feasibility of an electromechanical analytical engine.[33]\n During the first half of the 20th century, many scientific computing needs were met by increasingly sophisticated analog computers, which used a direct mechanical or electrical model of the problem as a basis for computation. However, these were not programmable and generally lacked the versatility and accuracy of modern digital computers.[34] The first modern analog computer was a tide-predicting machine, invented by Sir William Thomson (later to become Lord Kelvin) in 1872. The differential analyser, a mechanical analog computer designed to solve differential equations by integration using wheel-and-disc mechanisms, was conceptualized in 1876 by James Thomson, the elder brother of the more famous Sir William Thomson.[16]\n The art of mechanical analog computing reached its zenith with the differential analyzer, built by H. L. Hazen and Vannevar Bush at MIT starting in 1927. This built on the mechanical integrators of James Thomson and the torque amplifiers invented by H. W. Nieman. A dozen of these devices were built before their obsolescence became obvious. By the 1950s, the success of digital electronic computers had spelled the end for most analog computing machines, but analog computers remained in use during the 1950s in some specialized applications such as education (slide rule) and aircraft (control systems).\n Claude Shannon's 1937 master's thesis laid the foundations of digital computing, with his insight of applying Boolean algebra to the analysis and synthesis of switching circuits being the basic concept which underlies all electronic digital computers.[35][36]\n By 1938, the United States Navy had developed an electromechanical analog computer small enough to use aboard a submarine. This was the Torpedo Data Computer, which used trigonometry to solve the problem of firing a torpedo at a moving target. During World War II similar devices were developed in other countries as well.\n Early digital computers were electromechanical; electric switches drove mechanical relays to perform the calculation. These devices had a low operating speed and were eventually superseded by much faster all-electric computers, originally using vacuum tubes. The Z2, created by German engineer Konrad Zuse in 1939 in Berlin, was one of the earliest examples of an electromechanical relay computer.[37]\n In 1941, Zuse followed his earlier machine up with the Z3, the world's first working electromechanical programmable, fully automatic digital computer.[40][41] The Z3 was built with 2000 relays, implementing a 22 bit word length that operated at a clock frequency of about 5–10 Hz.[42] Program code was supplied on punched film while data could be stored in 64 words of memory or supplied from the keyboard. It was quite similar to modern machines in some respects, pioneering numerous advances such as floating-point numbers. Rather than the harder-to-implement decimal system (used in Charles Babbage's earlier design), using a binary system meant that Zuse's machines were easier to build and potentially more reliable, given the technologies available at that time.[43] The Z3 was not itself a universal computer but could be extended to be Turing complete.[44][45]\n Zuse's next computer, the Z4, became the world's first commercial computer; after initial delay due to the Second World War, it was completed in 1950 and delivered to the ETH Zurich.[46] The computer was manufactured by Zuse's own company, Zuse KG, which was founded in 1941 as the first company with the sole purpose of developing computers in Berlin.[46] The Z4 served as the inspiration for the construction of the ERMETH, the first Swiss computer and one of the first in Europe.[47]\n \nPurely electronic circuit elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. The engineer Tommy Flowers, working at the Post Office Research Station in London in the 1930s, began to explore the possible use of electronics for the telephone exchange. Experimental equipment that he built in 1934 went into operation five years later, converting a portion of the telephone exchange network into an electronic data processing system, using thousands of vacuum tubes.[34] In the US, John Vincent Atanasoff and Clifford E. Berry of Iowa State University developed and tested the Atanasoff–Berry Computer (ABC) in 1942,[48] the first \"automatic electronic digital computer\".[49] This design was also all-electronic and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory.[50]\n During World War II, the British code-breakers at Bletchley Park achieved a number of successes at breaking encrypted German military communications. The German encryption machine, Enigma, was first attacked with the help of the electro-mechanical bombes which were often run by women.[51][52] To crack the more sophisticated German Lorenz SZ 40/42 machine, used for high-level Army communications, Max Newman and his colleagues commissioned Flowers to build the Colossus.[50] He spent eleven months from early February 1943 designing and building the first Colossus.[53] After a functional test in December 1943, Colossus was shipped to Bletchley Park, where it was delivered on 18 January 1944[54] and attacked its first message on 5 February.[50]\n Colossus was the world's first electronic digital programmable computer.[34] It used a large number of valves (vacuum tubes). It had paper-tape input and was capable of being configured to perform a variety of boolean logical operations on its data, but it was not Turing-complete. Nine Mk II Colossi were built (The Mk I was converted to a Mk II making ten machines in total). Colossus Mark I contained 1,500 thermionic valves (tubes), but Mark II with 2,400 valves, was both five times faster and simpler to operate than Mark I, greatly speeding the decoding process.[55][56]\n The ENIAC[57] (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the U.S. Although the ENIAC was similar to the Colossus, it was much faster, more flexible, and it was Turing-complete. Like the Colossus, a \"program\" on the ENIAC was defined by the states of its patch cables and switches, a far cry from the stored program electronic machines that came later. Once a program was written, it had to be mechanically set into the machine with manual resetting of plugs and switches. The programmers of the ENIAC were six women, often known collectively as the \"ENIAC girls\".[58][59]\n It combined the high speed of electronics with the ability to be programmed for many complex problems. It could add or subtract 5000 times a second, a thousand times faster than any other machine. It also had modules to multiply, divide, and square root. High speed memory was limited to 20 words (about 80 bytes). Built under the direction of John Mauchly and J. Presper Eckert at the University of Pennsylvania, ENIAC's development and construction lasted from 1943 to full operation at the end of 1945. The machine was huge, weighing 30 tons, using 200 kilowatts of electric power and contained over 18,000 vacuum tubes, 1,500 relays, and hundreds of thousands of resistors, capacitors, and inductors.[60]\n The principle of the modern computer was proposed by Alan Turing in his seminal 1936 paper,[61] On Computable Numbers. Turing proposed a simple device that he called \"Universal Computing machine\" and that is now known as a universal Turing machine. He proved that such a machine is capable of computing anything that is computable by executing instructions (program) stored on tape, allowing the machine to be programmable. The fundamental concept of Turing's design is the stored program, where all the instructions for computing are stored in memory. Von Neumann acknowledged that the central concept of the modern computer was due to this paper.[62] Turing machines are to this day a central object of study in theory of computation. Except for the limitations imposed by their finite memory stores, modern computers are said to be Turing-complete, which is to say, they have algorithm execution capability equivalent to a universal Turing machine.\n Early computing machines had fixed programs. Changing its function required the re-wiring and re-structuring of the machine.[50] With the proposal of the stored-program computer this changed. A stored-program computer includes by design an instruction set and can store in memory a set of instructions (a program) that details the computation. The theoretical basis for the stored-program computer was laid out by Alan Turing in his 1936 paper. In 1945, Turing joined the National Physical Laboratory and began work on developing an electronic stored-program digital computer. His 1945 report \"Proposed Electronic Calculator\" was the first specification for such a device. John von Neumann at the University of Pennsylvania also circulated his First Draft of a Report on the EDVAC in 1945.[34]\n The Manchester Baby was the world's first stored-program computer. It was built at the University of Manchester in England by Frederic C. Williams, Tom Kilburn and Geoff Tootill, and ran its first program on 21 June 1948.[63] It was designed as a testbed for the Williams tube, the first random-access digital storage device.[64] Although the computer was described as \"small and primitive\" by a 1998 retrospective, it was the first working machine to contain all of the elements essential to a modern electronic computer.[65] As soon as the Baby had demonstrated the feasibility of its design, a project began at the university to develop it into a practically useful computer, the Manchester Mark 1.\n The Mark 1 in turn quickly became the prototype for the Ferranti Mark 1, the world's first commercially available general-purpose computer.[66] Built by Ferranti, it was delivered to the University of Manchester in February 1951. At least seven of these later machines were delivered between 1953 and 1957, one of them to Shell labs in Amsterdam.[67] In October 1947 the directors of British catering company J. Lyons & Company decided to take an active role in promoting the commercial development of computers. Lyons's LEO I computer, modelled closely on the Cambridge EDSAC of 1949, became operational in April 1951[68] and ran the world's first routine office computer job.\n The concept of a field-effect transistor was proposed by Julius Edgar Lilienfeld in 1925. John Bardeen and Walter Brattain, while working under William Shockley at Bell Labs, built the first working transistor, the point-contact transistor, in 1947, which was followed by Shockley's bipolar junction transistor in 1948.[69][70] From 1955 onwards, transistors replaced vacuum tubes in computer designs, giving rise to the \"second generation\" of computers. Compared to vacuum tubes, transistors have many advantages: they are smaller, and require less power than vacuum tubes, so give off less heat. Junction transistors were much more reliable than vacuum tubes and had longer, indefinite, service life. Transistorized computers could contain tens of thousands of binary logic circuits in a relatively compact space. However, early junction transistors were relatively bulky devices that were difficult to manufacture on a mass-production basis, which limited them to a number of specialized applications.[71]\n At the University of Manchester, a team under the leadership of Tom Kilburn designed and built a machine using the newly developed transistors instead of valves.[72] Their first transistorized computer and the first in the world, was operational by 1953, and a second version was completed there in April 1955. However, the machine did make use of valves to generate its 125 kHz clock waveforms and in the circuitry to read and write on its magnetic drum memory, so it was not the first completely transistorized computer. That distinction goes to the Harwell CADET of 1955,[73] built by the electronics division of the Atomic Energy Research Establishment at Harwell.[73][74]\n The metal–oxide–silicon field-effect transistor (MOSFET), also known as the MOS transistor, was invented at Bell Labs between 1955 and 1960[75][76][77][78][79][80] and was the first truly compact transistor that could be miniaturized and mass-produced for a wide range of uses.[71] With its high scalability,[81] and much lower power consumption and higher density than bipolar junction transistors,[82] the MOSFET made it possible to build high-density integrated circuits.[83][84] In addition to data processing, it also enabled the practical use of MOS transistors as memory cell storage elements, leading to the development of MOS semiconductor memory, which replaced earlier magnetic-core memory in computers. The MOSFET led to the microcomputer revolution,[85] and became the driving force behind the computer revolution.[86][87] The MOSFET is the most widely used transistor in computers,[88][89] and is the fundamental building block of digital electronics.[90]\n The next great advance in computing power came with the advent of the integrated circuit (IC).\nThe idea of the integrated circuit was first conceived by a radar scientist working for the Royal Radar Establishment of the Ministry of Defence, Geoffrey W.A. Dummer. Dummer presented the first public description of an integrated circuit at the Symposium on Progress in Quality Electronic Components in Washington, D.C., on 7 May 1952.[91]\n The first working ICs were invented by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor.[92] Kilby recorded his initial ideas concerning the integrated circuit in July 1958, successfully demonstrating the first working integrated example on 12 September 1958.[93] In his patent application of 6 February 1959, Kilby described his new device as \"a body of semiconductor material ... wherein all the components of the electronic circuit are completely integrated\".[94][95] However, Kilby's invention was a hybrid integrated circuit (hybrid IC), rather than a monolithic integrated circuit (IC) chip.[96] Kilby's IC had external wire connections, which made it difficult to mass-produce.[97]\n Noyce also came up with his own idea of an integrated circuit half a year later than Kilby.[98] Noyce's invention was the first true monolithic IC chip.[99][97] His chip solved many practical problems that Kilby's had not. Produced at Fairchild Semiconductor, it was made of silicon, whereas Kilby's chip was made of germanium. Noyce's monolithic IC was fabricated using the planar process, developed by his colleague Jean Hoerni in early 1959. In turn, the planar process was based on Carl Frosch and Lincoln Derick work on semiconductor surface passivation by silicon dioxide.[100][101][102][103][104][105]\n Modern monolithic ICs are predominantly MOS (metal–oxide–semiconductor) integrated circuits, built from MOSFETs (MOS transistors).[106] The earliest experimental MOS IC to be fabricated was a 16-transistor chip built by Fred Heiman and Steven Hofstein at RCA in 1962.[107] General Microelectronics later introduced the first commercial MOS IC in 1964,[108] developed by Robert Norman.[107] Following the development of the self-aligned gate (silicon-gate) MOS transistor by Robert Kerwin, Donald Klein and John Sarace at Bell Labs in 1967, the first silicon-gate MOS IC with self-aligned gates was developed by Federico Faggin at Fairchild Semiconductor in 1968.[109] The MOSFET has since become the most critical device component in modern ICs.[106]\n The development of the MOS integrated circuit led to the invention of the microprocessor,[110][111] and heralded an explosion in the commercial and personal use of computers. While the subject of exactly which device was the first microprocessor is contentious, partly due to lack of agreement on the exact definition of the term \"microprocessor\", it is largely undisputed that the first single-chip microprocessor was the Intel 4004,[112] designed and realized by Federico Faggin with his silicon-gate MOS IC technology,[110] along with Ted Hoff, Masatoshi Shima and Stanley Mazor at Intel.[b][114] In the early 1970s, MOS IC technology enabled the integration of more than 10,000 transistors on a single chip.[84]\n System on a Chip (SoCs) are complete computers on a microchip (or chip) the size of a coin.[115] They may or may not have integrated RAM and flash memory. If not integrated, the RAM is usually placed directly above (known as Package on package) or below (on the opposite side of the circuit board) the SoC, and the flash memory is usually placed right next to the SoC. This is done to improve data transfer speeds, as the data signals do not have to travel long distances. Since ENIAC in 1945, computers have advanced enormously, with modern SoCs (such as the Snapdragon 865) being the size of a coin while also being hundreds of thousands of times more powerful than ENIAC, integrating billions of transistors, and consuming only a few watts of power.\n The first mobile computers were heavy and ran from mains power. The 50 lb (23 kg) IBM 5100 was an early example. Later portables such as the Osborne 1 and Compaq Portable were considerably lighter but still needed to be plugged in. The first laptops, such as the Grid Compass, removed this requirement by incorporating batteries – and with the continued miniaturization of computing resources and advancements in portable battery life, portable computers grew in popularity in the 2000s.[116] The same developments allowed manufacturers to integrate computing resources into cellular mobile phones by the early 2000s.\n These smartphones and tablets run on a variety of operating systems and recently became the dominant computing device on the market.[117] These are powered by System on a Chip (SoCs), which are complete computers on a microchip the size of a coin.[115]\n Computers can be classified in a number of different ways, including:\n The term hardware covers all of those parts of a computer that are tangible physical objects. Circuits, computer chips, graphic cards, sound cards, memory (RAM), motherboard, displays, power supplies, cables, keyboards, printers and \"mice\" input devices are all hardware.\n A general-purpose computer has four main components: the arithmetic logic unit (ALU), the control unit, the memory, and the input and output devices (collectively termed I/O). These parts are interconnected by buses, often made of groups of wires. Inside each of these parts are thousands to trillions of small electrical circuits which can be turned off or on by means of an electronic switch. Each circuit represents a bit (binary digit) of information so that when the circuit is on it represents a \"1\", and when off it represents a \"0\" (in positive logic representation). The circuits are arranged in logic gates so that one or more of the circuits may control the state of one or more of the other circuits.\n When unprocessed data is sent to the computer with the help of input devices, the data is processed and sent to output devices. The input devices may be hand-operated or automated. The act of processing is mainly regulated by the CPU. Some examples of input devices are:\n The means through which computer gives output are known as output devices. Some examples of output devices are:\n The control unit (often called a control system or central controller) manages the computer's various components; it reads and interprets (decodes) the program instructions, transforming them into control signals that activate other parts of the computer.[d] Control systems in advanced computers may change the order of execution of some instructions to improve performance.\n A key component common to all CPUs is the program counter, a special memory cell (a register) that keeps track of which location in memory the next instruction is to be read from.[e]\n The control system's function is as follows— this is a simplified description, and some of these steps may be performed concurrently or in a different order depending on the type of CPU:\n Since the program counter is (conceptually) just another set of memory cells, it can be changed by calculations done in the ALU. Adding 100 to the program counter would cause the next instruction to be read from a place 100 locations further down the program. Instructions that modify the program counter are often known as \"jumps\" and allow for loops (instructions that are repeated by the computer) and often conditional instruction execution (both examples of control flow).\n The sequence of operations that the control unit goes through to process an instruction is in itself like a short computer program, and indeed, in some more complex CPU designs, there is another yet smaller computer called a microsequencer, which runs a microcode program that causes all of these events to happen.\n The control unit, ALU, and registers are collectively known as a central processing unit (CPU). Early CPUs were composed of many separate components. Since the 1970s, CPUs have typically been constructed on a single MOS integrated circuit chip called a microprocessor.\n The ALU is capable of performing two classes of operations: arithmetic and logic.[122] The set of arithmetic operations that a particular ALU supports may be limited to addition and subtraction, or might include multiplication, division, trigonometry functions such as sine, cosine, etc., and square roots. Some can operate only on whole numbers (integers) while others use floating point to represent real numbers, albeit with limited precision. However, any computer that is capable of performing just the simplest operations can be programmed to break down the more complex operations into simple steps that it can perform. Therefore, any computer can be programmed to perform any arithmetic operation—although it will take more time to do so if its ALU does not directly support the operation. An ALU may also compare numbers and return Boolean truth values (true or false) depending on whether one is equal to, greater than or less than the other (\"is 64 greater than 65?\"). Logic operations involve Boolean logic: AND, OR, XOR, and NOT. These can be useful for creating complicated conditional statements and processing Boolean logic.\n Superscalar computers may contain multiple ALUs, allowing them to process several instructions simultaneously.[123] Graphics processors and computers with SIMD and MIMD features often contain ALUs that can perform arithmetic on vectors and matrices.\n A computer's memory can be viewed as a list of cells into which numbers can be placed or read. Each cell has a numbered \"address\" and can store a single number. The computer can be instructed to \"put the number 123 into the cell numbered 1357\" or to \"add the number that is in cell 1357 to the number that is in cell 2468 and put the answer into cell 1595.\" The information stored in memory may represent practically anything. Letters, numbers, even computer instructions can be placed into memory with equal ease. Since the CPU does not differentiate between different types of information, it is the software's responsibility to give significance to what the memory sees as nothing but a series of numbers.\n In almost all modern computers, each memory cell is set up to store binary numbers in groups of eight bits (called a byte). Each byte is able to represent 256 different numbers (28 = 256); either from 0 to 255 or −128 to +127. To store larger numbers, several consecutive bytes may be used (typically, two, four or eight). When negative numbers are required, they are usually stored in two's complement notation. Other arrangements are possible, but are usually not seen outside of specialized applications or historical contexts. A computer can store any kind of information in memory if it can be represented numerically. Modern computers have billions or even trillions of bytes of memory.\n The CPU contains a special set of memory cells called registers that can be read and written to much more rapidly than the main memory area. There are typically between two and one hundred registers depending on the type of CPU. Registers are used for the most frequently needed data items to avoid having to access main memory every time data is needed. As data is constantly being worked on, reducing the need to access main memory (which is often slow compared to the ALU and control units) greatly increases the computer's speed.\n Computer main memory comes in two principal varieties:\n RAM can be read and written to anytime the CPU commands it, but ROM is preloaded with data and software that never changes, therefore the CPU can only read from it. ROM is typically used to store the computer's initial start-up instructions. In general, the contents of RAM are erased when the power to the computer is turned off, but ROM retains its data indefinitely. In a PC, the ROM contains a specialized program called the BIOS that orchestrates loading the computer's operating system from the hard disk drive into RAM whenever the computer is turned on or reset. In embedded computers, which frequently do not have disk drives, all of the required software may be stored in ROM. Software stored in ROM is often called firmware, because it is notionally more like hardware than software. Flash memory blurs the distinction between ROM and RAM, as it retains its data when turned off but is also rewritable. It is typically much slower than conventional ROM and RAM however, so its use is restricted to applications where high speed is unnecessary.[f]\n In more sophisticated computers there may be one or more RAM cache memories, which are slower than registers but faster than main memory. Generally computers with this sort of cache are designed to move frequently needed data into the cache automatically, often without the need for any intervention on the programmer's part.\n I/O is the means by which a computer exchanges information with the outside world.[125] Devices that provide input or output to the computer are called peripherals.[126] On a typical personal computer, peripherals include input devices like the keyboard and mouse, and output devices such as the display and printer. Hard disk drives, floppy disk drives and optical disc drives serve as both input and output devices. Computer networking is another form of I/O.\nI/O devices are often complex computers in their own right, with their own CPU and memory. A graphics processing unit might contain fifty or more tiny computers that perform the calculations necessary to display 3D graphics.[citation needed] Modern desktop computers contain many smaller computers that assist the main CPU in performing I/O. A 2016-era flat screen display contains its own computer circuitry.\n While a computer may be viewed as running one gigantic program stored in its main memory, in some systems it is necessary to give the appearance of running several programs simultaneously. This is achieved by multitasking i.e. having the computer switch rapidly between running each program in turn.[127] One means by which this is done is with a special signal called an interrupt, which can periodically cause the computer to stop executing instructions where it was and do something else instead. By remembering where it was executing prior to the interrupt, the computer can return to that task later. If several programs are running \"at the same time\". then the interrupt generator might be causing several hundred interrupts per second, causing a program switch each time. Since modern computers typically execute instructions several orders of magnitude faster than human perception, it may appear that many programs are running at the same time even though only one is ever executing in any given instant. This method of multitasking is sometimes termed \"time-sharing\" since each program is allocated a \"slice\" of time in turn.[128]\n Before the era of inexpensive computers, the principal use for multitasking was to allow many people to share the same computer. Seemingly, multitasking would cause a computer that is switching between several programs to run more slowly, in direct proportion to the number of programs it is running, but most programs spend much of their time waiting for slow input/output devices to complete their tasks. If a program is waiting for the user to click on the mouse or press a key on the keyboard, then it will not take a \"time slice\" until the event it is waiting for has occurred. This frees up time for other programs to execute so that many programs may be run simultaneously without unacceptable speed loss.\n Some computers are designed to distribute their work across several CPUs in a multiprocessing configuration, a technique once employed in only large and powerful machines such as supercomputers, mainframe computers and servers. Multiprocessor and multi-core (multiple CPUs on a single integrated circuit) personal and laptop computers are now widely available, and are being increasingly used in lower-end markets as a result.\n Supercomputers in particular often have highly unique architectures that differ significantly from the basic stored-program architecture and from general-purpose computers.[g] They often feature thousands of CPUs, customized high-speed interconnects, and specialized computing hardware. Such designs tend to be useful for only specialized tasks due to the large scale of program organization required to use most of the available resources at once. Supercomputers usually see usage in large-scale simulation, graphics rendering, and cryptography applications, as well as with other so-called \"embarrassingly parallel\" tasks.\n Software refers to parts of the computer which do not have a material form, such as programs, data, protocols, etc. Software is that part of a computer system that consists of encoded information or computer instructions, in contrast to the physical hardware from which the system is built. Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media. It is often divided into system software and application software. Computer hardware and software require each other and neither can be realistically used on its own. When software is stored in hardware that cannot easily be modified, such as with BIOS ROM in an IBM PC compatible computer, it is sometimes called \"firmware\".\n There are thousands of different programming languages—some intended for general purpose, others useful for only highly specialized applications.\n The defining feature of modern computers which distinguishes them from all other machines is that they can be programmed. That is to say that some type of instructions (the program) can be given to the computer, and it will process them. Modern computers based on the von Neumann architecture often have machine code in the form of an imperative programming language. In practical terms, a computer program may be just a few instructions or extend to many millions of instructions, as do the programs for word processors and web browsers for example. A typical modern computer can execute billions of instructions per second (gigaflops) and rarely makes a mistake over many years of operation. Large computer programs consisting of several million instructions may take teams of programmers years to write, and due to the complexity of the task almost certainly contain errors.\n This section applies to most common RAM machine–based computers.\n In most cases, computer instructions are simple: add one number to another, move some data from one location to another, send a message to some external device, etc. These instructions are read from the computer's memory and are generally carried out (executed) in the order they were given. However, there are usually specialized instructions to tell the computer to jump ahead or backwards to some other place in the program and to carry on executing from there. These are called \"jump\" instructions (or branches). Furthermore, jump instructions may be made to happen conditionally so that different sequences of instructions may be used depending on the result of some previous calculation or some external event. Many computers directly support subroutines by providing a type of jump that \"remembers\" the location it jumped from and another instruction to return to the instruction following that jump instruction.\n Program execution might be likened to reading a book. While a person will normally read each word and line in sequence, they may at times jump back to an earlier place in the text or skip sections that are not of interest. Similarly, a computer may sometimes go back and repeat the instructions in some section of the program over and over again until some internal condition is met. This is called the flow of control within the program and it is what allows the computer to perform tasks repeatedly without human intervention.\n Comparatively, a person using a pocket calculator can perform a basic arithmetic operation such as adding two numbers with just a few button presses. But to add together all of the numbers from 1 to 1,000 would take thousands of button presses and a lot of time, with a near certainty of making a mistake. On the other hand, a computer may be programmed to do this with just a few simple instructions. The following example is written in the MIPS assembly language:\n Once told to run this program, the computer will perform the repetitive addition task without further human intervention. It will almost never make a mistake and a modern PC can complete the task in a fraction of a second.\n In most computers, individual instructions are stored as machine code with each instruction being given a unique number (its operation code or opcode for short). The command to add two numbers together would have one opcode; the command to multiply them would have a different opcode, and so on. The simplest computers are able to perform any of a handful of different instructions; the more complex computers have several hundred to choose from, each with a unique numerical code. Since the computer's memory is able to store numbers, it can also store the instruction codes. This leads to the important fact that entire programs (which are just lists of these instructions) can be represented as lists of numbers and can themselves be manipulated inside the computer in the same way as numeric data. The fundamental concept of storing programs in the computer's memory alongside the data they operate on is the crux of the von Neumann, or stored program, architecture.[130][131] In some cases, a computer might store some or all of its program in memory that is kept separate from the data it operates on. This is called the Harvard architecture after the Harvard Mark I computer. Modern von Neumann computers display some traits of the Harvard architecture in their designs, such as in CPU caches.\n While it is possible to write computer programs as long lists of numbers (machine language) and while this technique was used with many early computers,[h] it is extremely tedious and potentially error-prone to do so in practice, especially for complicated programs. Instead, each basic instruction can be given a short name that is indicative of its function and easy to remember – a mnemonic such as ADD, SUB, MULT or JUMP. These mnemonics are collectively known as a computer's assembly language. Converting programs written in assembly language into something the computer can actually understand (machine language) is usually done by a computer program called an assembler.\n Programming languages provide various ways of specifying programs for computers to run. Unlike natural languages, programming languages are designed to permit no ambiguity and to be concise. They are purely written languages and are often difficult to read aloud. They are generally either translated into machine code by a compiler or an assembler before being run, or translated directly at run time by an interpreter. Sometimes programs are executed by a hybrid method of the two techniques.\n Machine languages and the assembly languages that represent them (collectively termed low-level programming languages) are generally unique to the particular architecture of a computer's central processing unit (CPU). For instance, an ARM architecture CPU (such as may be found in a smartphone or a hand-held videogame) cannot understand the machine language of an x86 CPU that might be in a PC.[i] Historically a significant number of other CPU architectures were created and saw extensive use, notably including the MOS Technology 6502 and 6510 in addition to the Zilog Z80.\n Although considerably easier than in machine language, writing long programs in assembly language is often difficult and is also error prone. Therefore, most practical programs are written in more abstract high-level programming languages that are able to express the needs of the programmer more conveniently (and thereby help reduce programmer error). High level languages are usually \"compiled\" into machine language (or sometimes into assembly language and then into machine language) using another computer program called a compiler.[j] High level languages are less related to the workings of the target computer than assembly language, and more related to the language and structure of the problem(s) to be solved by the final program. It is therefore often possible to use different compilers to translate the same high level language program into the machine language of many different types of computer. This is part of the means by which software like video games may be made available for different computer architectures such as personal computers and various video game consoles.\n Program design of small programs is relatively simple and involves the analysis of the problem, collection of inputs, using the programming constructs within languages, devising or using established procedures and algorithms, providing data for output devices and solutions to the problem as applicable.[132] As problems become larger and more complex, features such as subprograms, modules, formal documentation, and new paradigms such as object-oriented programming are encountered.[133] Large programs involving thousands of line of code and more require formal software methodologies.[134] The task of developing large software systems presents a significant intellectual challenge.[135] Producing software with an acceptably high reliability within a predictable schedule and budget has historically been difficult;[136] the academic and professional discipline of software engineering concentrates specifically on this challenge.[137]\n Errors in computer programs are called \"bugs\". They may be benign and not affect the usefulness of the program, or have only subtle effects. However, in some cases they may cause the program or the entire system to \"hang\", becoming unresponsive to input such as mouse clicks or keystrokes, to completely fail, or to crash.[138] Otherwise benign bugs may sometimes be harnessed for malicious intent by an unscrupulous user writing an exploit, code designed to take advantage of a bug and disrupt a computer's proper execution. Bugs are usually not the fault of the computer. Since computers merely execute the instructions they are given, bugs are nearly always the result of programmer error or an oversight made in the program's design.[k] Admiral Grace Hopper, an American computer scientist and developer of the first compiler, is credited for having first used the term \"bugs\" in computing after a dead moth was found shorting a relay in the Harvard Mark II computer in September 1947.[139]\n Computers have been used to coordinate information between multiple physical locations since the 1950s. The U.S. military's SAGE system was the first large-scale example of such a system, which led to a number of special-purpose commercial systems such as Sabre.[140]\n In the 1970s, computer engineers at research institutions throughout the United States began to link their computers together using telecommunications technology. The effort was funded by ARPA (now DARPA), and the computer network that resulted was called the ARPANET.[141] The technologies that made the Arpanet possible spread and evolved. In time, the network spread beyond academic and military institutions and became known as the Internet.\n The emergence of networking involved a redefinition of the nature and boundaries of computers. Computer operating systems and applications were modified to include the ability to define and access the resources of other computers on the network, such as peripheral devices, stored information, and the like, as extensions of the resources of an individual computer. Initially these facilities were available primarily to people working in high-tech environments, but in the 1990s, computer networking become almost ubiquitous, due to the spread of applications like e-mail and the World Wide Web, combined with the development of cheap, fast networking technologies like Ethernet and ADSL.\n The number of computers that are networked is growing phenomenally. A very large proportion of personal computers regularly connect to the Internet to communicate and receive information. \"Wireless\" networking, often utilizing mobile phone networks, has meant networking is becoming increasingly ubiquitous even in mobile computing environments.\n A computer does not need to be electronic, nor even have a processor, nor RAM, nor even a hard disk. While popular usage of the word \"computer\" is synonymous with a personal electronic computer,[l] a typical modern definition of a computer is: \"A device that computes, especially a programmable [usually] electronic machine that performs high-speed mathematical or logical operations or that assembles, stores, correlates, or otherwise processes information.\"[142] According to this definition, any device that processes information qualifies as a computer.\n There is active research to make unconventional computers out of many promising new types of technology, such as optical computers, DNA computers, neural computers, and quantum computers. Most computers are universal, and are able to calculate any computable function, and are limited only by their memory capacity and operating speed. However different designs of computers can give very different performance for particular problems; for example quantum computers can potentially break some modern encryption algorithms (by quantum factoring) very quickly.\n There are many types of computer architectures:\n Of all these abstract machines, a quantum computer holds the most promise for revolutionizing computing.[143] Logic gates are a common abstraction which can apply to most of the above digital or analog paradigms. The ability to store and execute lists of instructions called programs makes computers extremely versatile, distinguishing them from calculators. The Church–Turing thesis is a mathematical statement of this versatility: any computer with a minimum capability (being Turing-complete) is, in principle, capable of performing the same tasks that any other computer can perform. Therefore, any type of computer (netbook, supercomputer, cellular automaton, etc.) is able to perform the same computational tasks, given enough time and storage capacity.\n A computer will solve problems in exactly the way it is programmed to, without regard to efficiency, alternative solutions, possible shortcuts, or possible errors in the code. Computer programs that learn and adapt are part of the emerging field of artificial intelligence and machine learning. Artificial intelligence based products generally fall into two major categories: rule-based systems and pattern recognition systems. Rule-based systems attempt to represent the rules used by human experts and tend to be expensive to develop. Pattern-based systems use data about a problem to generate conclusions. Examples of pattern-based systems include voice recognition, font recognition, translation and the emerging field of on-line marketing.\n As the use of computers has spread throughout society, there are an increasing number of careers involving computers.\n The need for computers to work well together and to be able to exchange information has spawned the need for many standards organizations, clubs and societies of both a formal and informal nature.\n \n"
    },
    {
        "title": "Computer science",
        "url": "https://en.wikipedia.org/wiki/Computer_science",
        "content": "\n Computer science is the study of computation, information, and automation.[1][2][3] Computer science spans theoretical disciplines (such as algorithms, theory of computation, and information theory) to applied disciplines (including the design and implementation of hardware and software).[4][5][6]\n Algorithms and data structures are central to computer science.[7]\nThe theory of computation concerns abstract models of computation and general classes of problems that can be solved using them. The fields of cryptography and computer security involve studying the means for secure communication and preventing security vulnerabilities. Computer graphics and computational geometry address the generation of images. Programming language theory considers different ways to describe computational processes, and database theory concerns the management of repositories of data. Human–computer interaction investigates the interfaces through which humans and computers interact, and software engineering focuses on the design and principles behind developing software. Areas such as operating systems, networks and embedded systems investigate the principles and design behind complex systems. Computer architecture describes the construction of computer components and computer-operated equipment. Artificial intelligence and machine learning aim to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. Within artificial intelligence, computer vision aims to understand and process image and video data, while natural language processing aims to understand and process textual and linguistic data.\n The fundamental concern of computer science is determining what can and cannot be automated.[2][8][3][9][10] The Turing Award is generally recognized as the highest distinction in computer science.[11][12]\n The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.[16]\n Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623.[17] In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner.[18] Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry[note 1] when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine.[19] He started developing this machine in 1834, and \"in less than two years, he had sketched out many of the salient features of the modern computer\".[20] \"A crucial step was the adoption of a punched card system derived from the Jacquard loom\"[20] making it infinitely programmable.[note 2] In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer.[21] Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published[22] the 2nd of the only two designs for mechanical analytical engines in history. In 1914, the Spanish engineer Leonardo Torres Quevedo published his Essays on Automatics,[23] and designed, inspired by Babbage, a theoretical electromechanical calculating machine which was to be controlled by a read-only program. The paper also introduced the idea of floating-point arithmetic.[24][25] In 1920, to celebrate the 100th anniversary of the invention of the arithmometer, Torres presented in Paris the Electromechanical Arithmometer, a prototype that demonstrated the feasibility of an electromechanical analytical engine,[26] on which commands could be typed and the results printed automatically.[27] In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business[28] to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as \"Babbage's dream come true\".[29]\n \nDuring the 1940s, with the development of new and more powerful computing machines such as the Atanasoff–Berry computer and ENIAC, the term computer came to refer to the machines rather than their human predecessors.[30] As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan's West Side was IBM's first laboratory devoted to pure science. The lab is the forerunner of IBM's Research Division, which today operates research facilities around the world.[31] Ultimately, the close relationship between IBM and Columbia University was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946.[32] Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s.[33][34] The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science department in the United States was formed at Purdue University in 1962.[35] Since practical computers became available, many applications of computing have become distinct areas of study in their own rights. Although first proposed in 1956,[36] the term \"computer science\" appears in a 1959 article in Communications of the ACM,[37]\nin which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921.[38] Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.[37]\nHis efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962.[39] Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed.[40] Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy,[41] to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.\n In the early days of computing, a number of terms for the practitioners of the field of computing were suggested (albeit facetiously) in the Communications of the ACM—turingineer, turologist, flow-charts-man, applied meta-mathematician, and applied epistemologist.[42] Three months later in the same journal, comptologist was suggested, followed next year by hypologist.[43] The term computics has also been suggested.[44] In Europe, terms derived from contracted translations of the expression \"automatic information\" (e.g. \"informazione automatica\" in Italian) or \"information and mathematics\" are often used, e.g. informatique (French), Informatik (German), informatica (Italian, Dutch), informática (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (πληροφορική, which means informatics) in Greek. Similar words have also been adopted in the UK (as in the School of Informatics, University of Edinburgh).[45] \"In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain.\"[46]\n A folkloric quotation, often attributed to—but almost certainly not first formulated by—Edsger Dijkstra, states that \"computer science is no more about computers than astronomy is about telescopes.\"[note 3] The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been exchange of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as cognitive science, linguistics, mathematics, physics, biology, Earth science, statistics, philosophy, and logic.\n Computer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science.[33] Early computer science was strongly influenced by the work of mathematicians such as Kurt Gödel, Alan Turing, John von Neumann, Rózsa Péter and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.[36]\n The relationship between computer science and software engineering is a contentious issue, which is further muddied by disputes over what the term \"software engineering\" means, and how computer science is defined.[47] David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.[48]\n The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.\n \nDespite the word science in its name, there is debate over whether or not computer science is a discipline of science,[49] mathematics,[50] or engineering.[51] Allen Newell and Herbert A. Simon argued in 1975,  Computer science is an empirical discipline. We would have called it an experimental science, but like astronomy, economics, and geology, some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method. Nonetheless, they are experiments. Each new machine that is built is an experiment. Actually constructing the machine poses a question to nature; and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available.[51]  It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science.[51] Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering.[51] They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena.[51]\n Proponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs that can be deductively reasoned through mathematical formal methods.[51] Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems.[51]\n A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics.[52] Peter Denning's working group argued that they are theory, abstraction (modeling), and design.[33] Amnon H. Eden described them as the \"rationalist paradigm\" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the \"technocratic paradigm\" (which might be found in engineering approaches, most prominently in software engineering), and the \"scientific paradigm\" (which approaches computer-related artifacts from the empirical perspective of natural sciences,[53] identifiable in some branches of artificial intelligence).[54]\nComputer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.[55]\n As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.[56][57]\nCSAB, formerly called Computing Sciences Accreditation Board—which is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)[58]—identifies four areas that it considers crucial to the discipline of computer science: theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture. In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, human–computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science.[56]\n Theoretical computer science is mathematical and abstract in spirit, but it derives its motivation from practical and everyday computation. It aims to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.\n According to Peter Denning, the fundamental question underlying computer science is, \"What can be automated?\"[3] Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.\n The famous P = NP? problem, one of the Millennium Prize Problems,[59] is an open problem in the theory of computation.\n Information theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.[60]\nCoding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.\n[61]\n Data structures and algorithms are the studies of commonly used computational methods and their computational efficiency.\n Programming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.\n Formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems.[62] The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.\n Computer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.\n Information can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier – whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. What is the lower bound on the complexity of fast Fourier transform algorithms? is one of the unsolved problems in theoretical computer science.\n Scientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE,[63] as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.[64]\n Human–computer interaction (HCI) is the field of study and research concerned with the design and use of computer systems, mainly based on the analysis of the interaction between humans and computer interfaces. HCI has several subfields that focus on the relationship between emotions, social behavior and brain activity with computers.\n Software engineering is the study of designing, implementing, and modifying the software in order to ensure it is of high quality, affordable, maintainable, and fast to build. It is a systematic approach to software design, involving the application of engineering practices to software. Software engineering deals with the organizing and analyzing of software—it does not just deal with the creation or manufacture of new software, but its internal arrangement and maintenance. For example software testing, systems engineering, technical debt and software development processes.\n Artificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question \"Can computers think?\", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.\n Computer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory.[65] Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems. The term \"architecture\" in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks Jr., members of the Machine Organization department in IBM's main research center in 1959.\n Concurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other.[66] A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the parallel random access machine model.[67] When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.[68]\n This branch of computer science aims to manage networks between computers worldwide.\n Computer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users.\n Historical cryptography is the art of writing and deciphering secret messages. Modern cryptography is the scientific study of problems relating to distributed computations that can be attacked.[69] Technologies studied in modern cryptography include symmetric and asymmetric encryption, digital signatures, cryptographic hash functions, key-agreement protocols, blockchain, zero-knowledge proofs, and garbled circuits.\n A database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.\n The philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:[70]\n Programming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:\n Many languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.[76]\n Conferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications.[77][78] One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.[79]\n"
    },
    {
        "title": "Software",
        "url": "https://en.wikipedia.org/wiki/Software",
        "content": "\n Software consists of computer programs that instruct the execution of a computer.[1] Software also includes design documents and specifications.\n The history of software is closely tied to the development of digital computers in the mid-20th century. Early programs were written in the machine language specific to the hardware. The introduction of high-level programming languages in 1958 allowed for more human-readable instructions, making software development easier and more portable across different computer architectures. Software in a programming language is run through a compiler or interpreter to execute on the architecture's hardware. Over time, software has become complex, owing to developments in networking, operating systems, and databases.\n Software can generally be categorized into two main types:\n The rise of cloud computing has introduced the new software delivery model Software as a Service (SaaS). In SaaS, applications are hosted by a provider and accessed over the Internet.\n The process of developing software involves several stages. The stages include software design, programming, testing, release, and maintenance. Software quality assurance and security are critical aspects of software development, as bugs and security vulnerabilities can lead to system failures and security breaches. Additionally, legal issues such as software licenses and intellectual property rights play a significant role in the distribution of software products.\n The first use of the word software is credited to mathematician John Wilder Tukey in 1958.[3]\nThe first programmable computers, which appeared at the end of the 1940s,[4] were programmed in machine language. Machine language is difficult to debug and not portable across different computers.[5] Initially, hardware resources were more expensive than human resources.[6] As programs became complex, programmer productivity became the bottleneck. The introduction of high-level programming languages in 1958 hid the details of the hardware and expressed the underlying algorithms into the code .[7][8] Early languages include Fortran, Lisp, and COBOL.[8]\n There are two main types of software:\n Software can also be categorized by how it is deployed. Traditional applications are purchased with a perpetual license for a specific version of the software, downloaded, and run on hardware belonging to the purchaser.[17]  The rise of the Internet and cloud computing enabled a new model, software as a service (SaaS),[18] in which the provider hosts the software (usually built on top of rented infrastructure or platforms)[19] and provides the use of the software to customers, often in exchange for a subscription fee.[17] By 2023, SaaS products—which are usually delivered via a web application—had become the primary method that companies deliver applications.[20]\n Software companies aim to deliver a high-quality product on time and under budget. A challenge is that software development effort estimation is often inaccurate.[21] Software development begins by conceiving the project, evaluating its feasibility, analyzing the business requirements, and making a software design.[22][23] Most software projects speed up their development by reusing or incorporating existing software, either in the form of commercial off-the-shelf (COTS) or open-source software.[24][25] Software quality assurance is typically a combination of manual code review by other engineers[26] and automated software testing. Due to time constraints, testing cannot cover all aspects of the software's intended functionality, so developers often focus on the most critical functionality.[27] Formal methods are used in some safety-critical systems to prove the correctness of code,[28] while user acceptance testing helps to ensure that the product meets customer expectations.[29] There are a variety of software development methodologies, which vary from completing all steps in order to concurrent and iterative models.[30] Software development is driven by requirements taken from prospective users, as opposed to maintenance, which is driven by events such as a change request.[31]\n Frequently, software is released in an incomplete state when the development team runs out of time or funding.[32] Despite testing and quality assurance, virtually all software contains bugs where the system does not work as intended. Post-release software maintenance is necessary to remediate these bugs when they are found and keep the software working as the environment changes over time.[33] New features are often added after the release. Over time, the level of maintenance becomes increasingly restricted before being cut off entirely when the product is withdrawn from the market.[34] As software ages, it becomes known as legacy software and can remain in use for decades, even if there is no one left who knows how to fix it.[35] Over the lifetime of the product, software maintenance is estimated to comprise 75 percent or more of the total development cost.[36][37]\n Completing a software project involves various forms of expertise, not just in software programmers but also testing, documentation writing, project management, graphic design, user experience, user support, marketing, and fundraising.[38][39][23]\n Software quality is defined as meeting the stated requirements as well as customer expectations.[40] Quality is an overarching term that can refer to a code's correct and efficient behavior, its reusability and portability, or the ease of modification.[41] It is usually more cost-effective to build quality into the product from the beginning rather than try to add it later in the development process.[42] Higher quality code will reduce lifetime cost to both suppliers and customers as it is more reliable and easier to maintain.[43][44] Software failures in safety-critical systems can be very serious including death.[43] By some estimates, the cost of poor quality software can be as high as 20 to 40 percent of sales.[45]  Despite developers' goal of delivering a product that works entirely as intended, virtually all software contains bugs.[46]\n The rise of the Internet also greatly increased the need for computer security as it enabled malicious actors to conduct cyberattacks remotely.[47][48] If a bug creates a security risk, it is called a vulnerability.[49][50] Software patches are often released to fix identified vulnerabilities, but those that remain unknown (zero days) as well as those that have not been patched are still liable for exploitation.[51] Vulnerabilities vary in their ability to be exploited by malicious actors,[49] and the actual risk is dependent on the nature of the vulnerability as well as the value of the surrounding system.[52]  Although some vulnerabilities can only be used for denial of service attacks that compromise a system's availability, others allow the attacker to inject and run their own code (called malware), without the user being aware of it.[49] To thwart cyberattacks, all software in the system must be designed to withstand and recover from external attack.[48] Despite efforts to ensure security, a significant fraction of computers are infected with malware.[53]\n Programming languages are the format in which software is written. Since the 1950s, thousands of different programming languages have been invented; some have been in use for decades, while others have fallen into disuse.[54] Some definitions classify machine code—the exact instructions directly implemented by the hardware—and assembly language—a more human-readable alternative to machine code whose statements can be translated one-to-one into machine code—as programming languages.[55] Programs written in the high-level programming languages used to create software share a few main characteristics: knowledge of machine code is not necessary to write them, they can be ported to other computer systems, and they are more concise and human-readable than machine code.[56] They must be both human-readable and capable of being translated into unambiguous instructions for computer hardware.[57]\n The invention of high-level programming languages was simultaneous with the compilers needed to translate them automatically into machine code.[58] Most programs do not contain all the resources needed to run them and rely on external libraries. Part of the compiler's function is to link these files in such a way that the program can be executed by the hardware. Once compiled, the program can be saved as an object file and the loader (part of the operating system) can take this saved file and execute it as a process on the computer hardware.[59] Some programming languages use an interpreter instead of a compiler. An interpreter converts the program into machine code at run time, which makes them 10 to 100 times slower than compiled programming languages.[60][61]\n Software is often released with the knowledge that it is incomplete or contains bugs. Purchasers knowingly buy it in this state, which has led to a legal regime where liability for software products is significantly curtailed compared to other products.[62]\n Source code is protected by copyright law that vests the owner with the exclusive right to copy the code. The underlying ideas or algorithms are not protected by copyright law, but are often treated as a trade secret and concealed by such methods as non-disclosure agreements.[63] Software copyright has been recognized since the mid-1970s and is vested in the company that makes the software, not the employees or contractors who wrote it.[64] The use of most software is governed by an agreement (software license) between the copyright holder and the user. Proprietary software is usually sold under a restrictive license that limits copying and reuse (often enforced with tools such as digital rights management (DRM)).[65] Open-source licenses, in contrast, allow free use and redistribution of software with few conditions.[64] Most open-source licenses used for software require that modifications be released under the same license, which can create complications when open-source software is reused in proprietary projects.[66]\n Patents give an inventor an exclusive, time-limited license for a novel product or process.[67] Ideas about what software could accomplish are not protected by law and concrete implementations are instead covered by copyright law. In some countries, a requirement for the claimed invention to have an effect on the physical world may also be part of the requirements for a software patent to be held valid.[68] Software patents have been historically controversial. Before the 1998 case State Street Bank & Trust Co. v. Signature Financial Group, Inc., software patents were generally not recognized in the United States. In that case, the Supreme Court decided that business processes could be patented.[69] Patent applications are complex and costly, and lawsuits involving patents can drive up the cost of products.[70] Unlike copyrights, patents generally only apply in the jurisdiction where they were issued.[71]\n Engineer Capers Jones writes that \"computers and software are making profound changes to every aspect of human life: education, work, warfare, entertainment, medicine, law, and everything else\".[73] It has become ubiquitous in everyday life in developed countries.[74] In many cases, software augments the functionality of existing technologies such as household appliances and elevators.[75] Software also spawned entirely new technologies such as the Internet, video games, mobile phones, and GPS.[75][76] New methods of communication, including email, forums, blogs, microblogging, wikis, and social media, were enabled by the Internet.[77] Massive amounts of knowledge exceeding any paper-based library are now available with a quick web search.[76] Most creative professionals have switched to software-based tools such as computer-aided design, 3D modeling, digital image editing, and computer animation.[72] Almost every complex device is controlled by software.[76]\n"
    },
    {
        "title": "Machine perception",
        "url": "https://en.wikipedia.org/wiki/Machine_perception",
        "content": "Machine perception is the capability of a computer system to interpret data in a manner that is similar to the way humans use their senses to relate to the world around them.[1][2][3] The basic method that the computers take in and respond to their environment is through the attached hardware.  Until recently input was limited to a keyboard, or a mouse, but advances in technology, both in hardware and software, have allowed computers to take in sensory input in a way similar to humans.[1][2]\n Machine perception allows the computer to use this sensory input, as well as conventional computational means of gathering information, to gather information with greater accuracy and to present it in a way that is more comfortable for the user.[1] These include computer vision, machine hearing, machine touch, and machine smelling, as artificial scents are, at a chemical compound, molecular, atomic level, indiscernible and identical.[4][5]\n The end goal of machine perception is to give machines the ability to see, feel and perceive the world as humans do and therefore for them to be able to explain in a human way why they are making their decisions, to warn us when it is failing and more importantly, the reason why it is failing.[6] This purpose is very similar to the proposed purposes for artificial intelligence generally, except that machine perception would only grant machines limited sentience, rather than bestow upon machines full consciousness, self-awareness, and intentionality.\n Computer vision is a field that includes methods for acquiring, processing, analyzing, and understanding images and high-dimensional data from the real world to produce numerical or symbolic information, e.g., in the forms of decisions.  Computer vision has many applications already in use today such as facial recognition, geographical modeling, and even aesthetic judgment.[7]\n However, machines still struggle to interpret visual impute accurately if it is blurry or if the viewpoint at which stimuli are viewed varies often. Computers also struggle to determine the proper nature of some stimulus if overlapped by or seamlessly touching another stimulus. This refers to the Principle of Good Continuation. Machines also struggle to perceive and record stimulus functioning according to the Apparent Movement principle which is a field of research in Gestalt psychology.\n Machine hearing, also known as machine listening or computer audition is the ability of a computer or machine to take in and process sound data such as speech or music.[8][9]\nThis area has a wide range of application including music recording and compression, speech synthesis and speech recognition.[10]\nMoreover, this technology allows the machine to replicate the human brain's ability to selectively focus on a specific sound against many other competing sounds and background noise. This ability is called \"auditory scene analysis\". The technology enables the machine to segment several streams occurring at the same time.[8][11][12]\nMany commonly used devices such as a smartphones, voice translators and cars make use of some form of machine hearing. Present technology still has challenges in speech segmentation. This  means it is occasionally unable to correctly split words within sentences especially when spoken in an atypical accent.\n Machine touch is an area of machine perception where tactile information is processed by a machine or computer.  Applications include tactile perception of surface properties and dexterity whereby tactile information can enable intelligent reflexes and interaction with the environment.[13] Though this could possibly be done through measuring when and where friction occurs and also the nature and intensity of the friction, machines however still do not have any way of measuring few ordinary physical human experiences including physical pain. For example, scientists have yet to invent a mechanical substitute for the Nociceptors in the body and brain that are responsible for noticing and measuring physical human discomfort and suffering.\n Scientists are developing computers known as machine olfaction which can recognize and measure smells as well. Airborne chemicals are sensed and classified with a device sometimes known as an electronic nose.[14][15]\n The electronic tongue is an instrument that measures and compares tastes. As per the IUPAC technical report, an “electronic tongue” as analytical instrument including an array of non-selective chemical sensors with partial specificity to different solution components and an appropriate pattern recognition instrument, capable to recognize quantitative and qualitative compositions of simple and complex solutions[16][17]\n Chemical compounds responsible for taste are detected by human taste receptors. Similarly, the multi-electrode sensors of electronic instruments detect the same dissolved organic and inorganic compounds. Like human receptors, each sensor has a spectrum of reactions different from the other. The information given by each sensor is complementary, and the combination of all sensors' results generates a unique fingerprint. Most of the detection thresholds of sensors are similar to or better than human receptors.\n In the biological mechanism, taste signals are transduced by nerves in the brain into electric signals. E-tongue sensors process is similar: they generate electric signals as voltammetric and potentiometric variations.\n Other than those listed above, some of the future hurdles that the science of machine perception still has to overcome include, but are not limited to:\n - Embodied cognition - The theory that cognition is a full body experience, and therefore can only exist, and therefore be measure and analyzed, in fullness if all required human abilities and processes are working together through a mutually aware and supportive systems network.\n - The Moravec's paradox (see the link)\n - The Principle of similarity - The ability young children develop to determine what family a newly introduced stimulus falls under even when the said stimulus is different from the members with which the child usually associates said family with. (An example could be a child figuring that a chihuahua is a dog and house pet rather than vermin.)\n - The Unconscious inference: The natural human behavior of determining if a new stimulus is dangerous or not, what it is, and then how to relate to it without ever requiring any new conscious effort.\n - The innate human ability to follow the likelihood principle in order to learn from circumstances and others over time.\n - The recognition-by-components theory - being able to mentally analyze and break even complicated mechanisms into manageable parts with which to interact with. For example: A person seeing both the cup and the handle parts that make up a mug full of hot cocoa, in order to use the handle to hold the mug so as to avoid being burned.\n - The free energy principle - determining long before hand how much energy one can safely delegate to being aware of things outside one's self without the loss of the needed energy one requires for sustaining their life and function satisfactorily. This allows one to become both optimally aware of the world around them self without depleting their energy so much that they experience damaging stress, decision fatigue, and/or exhaustion.\n"
    },
    {
        "title": "Applications of artificial intelligence",
        "url": "https://en.wikipedia.org/wiki/Applications_of_AI",
        "content": "\n Artificial intelligence (AI) has been used in applications throughout industry and academia. In a manner analogous to electricity or computers, AI serves as a general-purpose technology. AI programs are designed to simulate human perception and understanding. These systems are capable of adapting to new information and responding to changing situations. Machine learning has been used for various scientific and commercial purposes[1] including language translation, image recognition, decision-making,[2][3] credit scoring, and e-commerce.\n Machine learning is has been used for recommendation systems in for determining which posts should show up in social media feeds.[4][5] Various types of social media analysis also make use of machine learning[6][7] and there is research into its use for (semi-)automated tagging/enhancement/correction of online misinformation and related filter bubbles.[8][9][10]\n AI has been used to customize shopping options and personalize offers.[11] Online gambling companies have used AI for targeting gamblers.[12]\n Intelligent personal assistants use AI to understand many natural language requests in other ways than rudimentary commands. Common examples are Apple's Siri, Amazon's Alexa, and a more recent AI, ChatGPT by OpenAI.[13]\n Bing Chat has used artificial intelligence as part of its search engine.[14]\n Machine learning can be used to combat spam, scams, and phishing. It can scrutinize the contents of spam and phishing attacks to attempt to identify malicious elements.[15] Some models built via machine learning algorithms have over 90% accuracy in distinguishing between spam and legitimate emails.[16] These models can be refined using new data and evolving spam tactics. Machine learning also analyzes traits such as sender behavior, email header information, and attachment types, potentially enhancing spam detection.[17]\n Speech translation technology attempts to convert one language's spoken words into another language. This potentially reduces language barriers in global commerce and cross-cultural exchange, enabling speakers of various languages to communicate with one another.[18]\n AI has been used to automatically translate spoken language and textual content in products such as Microsoft Translator, Google Translate, and DeepL Translator.[19] Additionally, research and development are in progress to decode and conduct animal communication.[20][21]\n Meaning is conveyed not only by text, but also through usage and context (see semantics and pragmatics). As a result, the two primary categorization approaches for machine translations are statistical and neural machine translations (NMTs). The old method of performing translation was to use a statistical machine translation (SMT) methodology to forecast the best probable output with specific algorithms. However, with NMT, the approach employs dynamic algorithms to achieve better translations based on context.[22]\n AI has been used in facial recognition systems. Some examples are Apple's Face ID and Android's Face Unlock, which are used to secure mobile devices.[23]\n Image labeling has been used by Google Image Labeler to detect products in photos and to allow people to search based on a photo. Image labeling has also been demonstrated to generate speech to describe images to blind people.[24] Facebook's DeepFace identifies human faces in digital images.\n Games have been a major application[relevant?] of AI's capabilities since the 1950s. In the 21st century, AIs have beaten human players in many games, including chess (Deep Blue), Jeopardy! (Watson),[25] Go (AlphaGo),[26][27][28][29][30][31][32] poker (Pluribus[33] and Cepheus),[34] E-sports (StarCraft),[35][36] and general game playing (AlphaZero[37][38][39] and MuZero).[40][41][42][43]\n Kuki AI is a set of chatbots and other apps which were designed for entertainment and as a marketing tool.[44][45] Character.ai is another example of a chatbot being used for recreation.\n AI for Good is a platform launched in 2017 by the International Telecommunication Union (ITU) agency of the United Nations (UN). The goal of the platform is to use AI to help achieve the UN's Sustainable Development Goals.[citation needed]\n The University of Southern California launched the Center for Artificial Intelligence in Society, with the goal of using AI to address problems such as homelessness. Stanford researchers use AI to analyze satellite images to identify high poverty areas.[46]\n In agriculture, AI has been proposed as a way for farmers to identify areas that need irrigation, fertilization, or pesticide treatments to increase yields, thereby improving efficiency.[47] AI has been used to attempt to classify livestock pig call emotions,[20] automate greenhouses,[48] detect diseases and pests,[49] and optimize irrigation.[50]\n Cyber security companies are adopting neural networks, machine learning, and natural language processing to improve their systems.[51]\n Applications of AI in cyber security include:\n AI elevates teaching, focusing on significant issues like the knowledge nexus and educational equality. The evolution of AI in education and technology should be used to improve human capabilities in relationships where they do not replace humans. UNESCO recognizes the future of AI in education as an instrument to reach Sustainable Development Goal 4, called \"Inclusive and Equitable Quality Education.” [56]\n The World Economic Forum also stresses AI's contribution to students' overall improvement and transforming teaching into a more enjoyable process.[56]\n Personalized Learning\n AI driven tutoring systems, such as Khan Academy, Duolingo and Carnegie Learning are the forefoot of delivering personalized education.[57]\n These platforms leverage AI algorithms to analyze individual learning patterns, strengths, and weaknesses, enabling the customization of content and Algorithm to suit each student's pace and style of learning.[57]\n Administrative Efficiency\n In educational institutions, AI is increasingly used to automate routine tasks like attendance tracking, grading and marking, which allows educators to devote more time to interactive teaching and direct student engagement.[58]\n Furthermore, AI tools are employed to monitor student progress, analyze learning behaviors, and predict academic challenges, facilitating timely and proactive interventions for students who may be at risk of falling behind.[58]\n Ethical and Privacy Concerns\n Despite the benefits, the integration of AI in education raises significant ethical and privacy concerns, particularly regarding the handling of sensitive student data.[57]\n It is imperative that AI systems in education are designed and operated with a strong emphasis on transparency, security, and respect for privacy to maintain trust and uphold the integrity of educational practices.[57]\n Much of the regulation will be influenced by the AI Act, the world’s first comprehensive AI law. [59]\n Financial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation. The use of AI in banking began in 1987 when Security Pacific National Bank launched a fraud prevention task-force to counter the unauthorized use of debit cards.[60] Kasisto and Moneystream use AI.\n Banks use AI to organize operations for bookkeeping, investing in stocks, and managing properties. AI can adapt to changes during non-business hours.[61] AI is used to combat fraud and financial crimes by monitoring behavioral patterns for any abnormal changes or anomalies.[62][63][64]\n The use of AI in applications such as online trading and decision-making has changed major economic theories.[65] For example, AI-based buying and selling platforms estimate personalized demand and supply curves, thus enabling individualized pricing. AI systems reduce information asymmetry in the market and thus make markets more efficient.[66] The application of artificial intelligence in the financial industry can alleviate the financing constraints of non-state-owned enterprises, especially for smaller and more innovative enterprises.[67]\n Algorithmic trading involves the use of AI systems to make trading decisions at speeds orders of magnitude greater than any human is capable of, making millions of trades in a day without human intervention. Such high-frequency trading represents a fast-growing sector. Many banks, funds, and proprietary trading firms now have entire portfolios that are AI-managed. Automated trading systems are typically used by large institutional investors but include smaller firms trading with their own AI systems.[68]\n Large financial institutions use AI to assist with their investment practices. BlackRock's AI engine, Aladdin, is used both within the company and by clients to help with investment decisions. Its functions include the use of natural language processing to analyze text such as news, broker reports, and social media feeds. It then gauges the sentiment on the companies mentioned and assigns a score. Banks such as UBS and Deutsche Bank use SQREEM (Sequential Quantum Reduction and Extraction Model) to mine data to develop consumer profiles and match them with wealth management products.[69]\n Online lender Upstart uses machine learning for underwriting.[70]\n ZestFinance's Zest Automated Machine Learning (ZAML) platform is used for credit underwriting. This platform uses machine learning to analyze data including purchase transactions and how a customer fills out a form to score borrowers. The platform is particularly useful to assign credit scores to those with limited credit histories.[71]\n AI makes continuous auditing possible. Potential benefits include reducing audit risk, increasing the level of assurance, and reducing audit duration.[72][quantify]\n Continuous auditing with AI allows real-time monitoring and reporting of financial activities and provides businesses with timely insights that can lead to quick decision making.[73]\n AI software, such as LaundroGraph which uses contemporary suboptimal datasets, could be used for anti-money laundering (AML).[74][75]\n In the 1980s, AI started to become prominent in finance as expert systems were commercialized. For example, Dupont created 100 expert systems, which helped them to save almost $10 million per year.[76] One of the first systems was the Pro-trader expert system that predicted the 87-point drop in the Dow Jones Industrial Average in 1986. \"The major junctions of the system were to monitor premiums in the market, determine the optimum investment strategy, execute transactions when appropriate and modify the knowledge base through a learning mechanism.\"[77]\n One of the first expert systems to help with financial plans was PlanPowerm and Client Profiling System, created by Applied Expert Systems (APEX). It was launched in 1986. It helped create personal financial plans for people.[78]\n In the 1990s AI was applied to fraud detection. In 1993 FinCEN Artificial Intelligence System (FAIS) launched. It was able to review over 200,000 transactions per week and over two years it helped identify 400 potential cases of money laundering equal to $1 billion.[79] These expert systems were later replaced by machine learning systems.[80]\n AI can enhance entrepreneurial activity and AI is one of the most dynamic areas for start-ups, with significant venture capital flowing into AI.[81]\n AI facial recognition systems are used for mass surveillance, notably in China.[82][83] In 2019, Bengaluru, India deployed AI-managed traffic signals. This system uses cameras to monitor traffic density and adjust signal timing based on the interval needed to clear traffic.[84]\n Various countries are deploying AI military applications.[85] The main applications enhance command and control, communications, sensors, integration and interoperability.[86] Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles.[85] AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams.[86]\n AI has been used in military operations in Iraq, Syria, Israel and Ukraine.[85][87][88][89]\n AI in healthcare is often used for classification, to evaluate a CT scan or electrocardiogram or to identify high-risk patients for population health. AI is helping with the high-cost problem of dosing. One study suggested that AI could save $16 billion. In 2016, a study reported that an AI-derived formula derived the proper dose of immunosuppressant drugs to give to transplant patients.[90] Current research has indicated that non-cardiac vascular illnesses are also being treated with artificial intelligence (AI). For certain disorders, AI algorithms can aid in diagnosis, recommended treatments, outcome prediction, and patient progress tracking. As AI technology advances, it is anticipated that it will become more significant in the healthcare industry.[91]\n The early detection of diseases like cancer is made possible by AI algorithms, which diagnose diseases by analyzing complex sets of medical data. For example, the IBM Watson system might be used to comb through massive data such as medical records and clinical trials to help diagnose a problem.[92] Microsoft's AI project Hanover helps doctors choose cancer treatments from among the more than 800 medicines and vaccines.[93][94] Its goal is to memorize all the relevant papers to predict which (combinations of) drugs will be most effective for each patient. Myeloid leukemia is one target. Another study reported on an AI that was as good as doctors in identifying skin cancers.[95] Another project monitors multiple high-risk patients by asking each patient questions based on data acquired from doctor/patient interactions.[96] In one study done with transfer learning, an AI diagnosed eye conditions similar to an ophthalmologist and recommended treatment referrals.[97]\n Another study demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel judged better than a surgeon.[98]\n Artificial neural networks are used as clinical decision support systems for medical diagnosis,[99] such as in concept processing technology in EMR software.\n Other healthcare tasks thought suitable for an AI that are in development include:\n AI-enabled chatbots decrease the need for humans to perform basic call center tasks.[115]\n Machine learning in sentiment analysis can spot fatigue in order to prevent overwork.[115] Similarly, decision support systems can prevent industrial disasters and make disaster response more efficient.[116] For manual workers in material handling, predictive analytics may be used to reduce musculoskeletal injury.[117] Data collected from wearable sensors can improve workplace health surveillance, risk assessment, and research.[116][how?]\n AI can auto-code workers' compensation claims.[118][119] AI-enabled virtual reality systems can enhance safety training for hazard recognition.[116] AI can more efficiently detect accident near misses, which are important in reducing accident rates, but are often underreported.[120]\n AlphaFold 2 can determine the 3D structure of a (folded) protein in hours rather than the months required by earlier automated approaches and was used to provide the likely structures of all proteins in the human body and essentially all proteins known to science (more than 200 million).[121][122][123][124]\n Machine learning has been used for drug design.[125] It has also been used for predicting molecular properties and exploring large chemical/reaction spaces.[126] Computer-planned syntheses via computational reaction networks, described as a platform that combines \"computational synthesis with AI algorithms to predict molecular properties\",[127] have been used to explore the origins of life on Earth,[128] drug-syntheses and developing routes for recycling 200 industrial waste chemicals into important drugs and agrochemicals (chemical synthesis design).[129] There is research about which types of computer-aided chemistry would benefit from machine learning.[130] It can also be used for \"drug discovery and development, drug repurposing, improving pharmaceutical productivity, and clinical trials\".[131] It has been used for the design of proteins with prespecified functional sites.[132][133]\n It has been used with databases for the development of a 46-day process to design, synthesize and test a drug which inhibits enzymes of a particular gene, DDR1. DDR1 is involved in cancers and fibrosis which is one reason for the high-quality datasets that enabled these results.[134]\n There are various types of applications for machine learning in decoding human biology, such as helping to map gene expression patterns to functional activation patterns[135] or identifying functional DNA motifs.[136] It is widely used in genetic research.[137]\n There also is some use of machine learning in synthetic biology,[138][139] disease biology,[139] nanotechnology (e.g. nanostructured materials and bionanotechnology),[140][141] and materials science.[142][143][144]\n There are also prototype robot scientists, including robot-embodied ones like the two Robot Scientists, which show a form of \"machine learning\" not commonly associated with the term.[145][146]\n Similarly, there is research and development of biological \"wetware computers\" that can learn (e.g. for use as biosensors) and/or implantation into an organism's body (e.g. for use to control prosthetics).[147][148][149] Polymer-based artificial neurons operate directly in biological environments and define biohybrid neurons made of artificial and living components.[150][151]\n Moreover, if whole brain emulation is possible via both scanning and replicating the, at least, bio-chemical brain – as premised in the form of digital replication in The Age of Em, possibly using physical neural networks – that may have applications as or more extensive than e.g. valued human activities and may imply that society would face substantial moral choices, societal risks and ethical problems[152][153] such as whether (and how) such are built, sent through space and used compared to potentially competing e.g. potentially more synthetic and/or less human and/or non/less-sentient types of artificial/semi-artificial intelligence.[additional citation(s) needed] An alternative or additive approach to scanning are types of reverse engineering of the brain.[154][155]\n A subcategory of artificial intelligence is embodied,[156][157] some of which are mobile robotic systems that each consist of one or multiple robots that are able to learn in the physical world.\n However, biological computers, even if both highly artificial and intelligent, are typically distinguished from synthetic, often silicon-based, computers – they could however be combined or used for the design of either. Moreover, many tasks may be carried out inadequately by artificial intelligence even if its algorithms were transparent, understood, bias-free, apparently effective, and goal-aligned and its trained data sufficiently large and cleansed – such as in cases were the underlying or available metrics, values or data are inappropriate. Computer-aided is a phrase used to describe human activities that make use of computing as tool in more comprehensive activities and systems such as AI for narrow tasks or making use of such without substantially relying on its results (see also: human-in-the-loop).[citation needed] A study described the biological as a limitation of AI with \"as long as the biological system cannot be understood, formalized, and imitated, we will not be able to develop technologies that can mimic it\" and that if it was understood this does not mean there being \"a technological solution to imitate natural intelligence\".[158] Technologies that integrate biology and are often AI-based include biorobotics.\n Artificial intelligence is used in astronomy to analyze increasing amounts of available data[159][160] and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights\" for example for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy.[161] It could also be used for activities in space such as space exploration, including analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance,[162] and more autonomous operation.[163][164][165][160]\n In the search for extraterrestrial intelligence (SETI), machine learning has been used in attempts to identify artificially generated electromagnetic waves in available data[166][167] – such as real-time observations[168] – and other technosignatures, e.g. via anomaly detection.[169] In ufology, the SkyCAM-5 project headed by Prof. Hakan Kayal[170] and the Galileo Project headed by Avi Loeb use machine learning to attempt to detect and classify types of UFOs.[171][172][173][174][175] The Galileo Project also seeks to detect two further types of potential extraterrestrial technological signatures with the use of AI: 'Oumuamua-like interstellar objects, and non-manmade artificial satellites.[176][177]\n Machine learning can also be used to produce datasets of spectral signatures of molecules that may be involved in the atmospheric production or consumption of particular chemicals – such as phosphine possibly detected on Venus – which could prevent miss assignments and, if accuracy is improved, be used in future detections and identifications of molecules on other planets.[178]\n In April 2024, the Scientific Advice Mechanism to the European Commission published advice[179] including a comprehensive evidence review of the opportunities and challenges posed by artificial intelligence in scientific research.\n As benefits, the evidence review[180] highlighted:\n As challenges:\n Machine learning can help to restore and attribute ancient texts.[181] It can help to index texts for example to enable better and easier searching[182] and classification of fragments.[183]\n \nArtificial intelligence can also be used to investigate genomes to uncover genetic history, such as interbreeding between archaic and modern humans by which for example the past existence of a ghost population, not Neanderthal or Denisovan, was inferred.[184]  \nIt can also be used for \"non-invasive and non-destructive access to internal structures of archaeological remains\".[185]  A deep learning system was reported to learn intuitive physics from visual data (of virtual 3D environments) based on an unpublished approach inspired by studies of visual cognition in infants.[186][187] Other researchers have developed a machine learning algorithm that could discover sets of basic variables of various physical systems and predict the systems' future dynamics from video recordings of their behavior.[188][189] In the future, it may be possible that such can be used to automate the discovery of physical laws of complex systems.[188]\n AI could be used for materials optimization and discovery such as the discovery of stable materials and the prediction of their crystal structure.[190][191][192]\n In November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.[193][194][195]\n Machine learning is used in diverse types of reverse engineering. For example, machine learning has been used to reverse engineer a composite material part, enabling unauthorized production of high quality parts,[196] and for quickly understanding the behavior of malware.[197][198][199] It can be used to reverse engineer artificial intelligence models.[200] It can also design components by engaging in a type of reverse engineering of not-yet existent virtual components such as inverse molecular design for particular desired functionality[201] or protein design for prespecified functional sites.[132][133] Biological network reverse engineering could model interactions in a human understandable way, e.g. bas on time series data of gene expression levels.[202]\n AI is a mainstay of law-related professions. Algorithms and machine learning do some tasks previously done by entry-level lawyers.[203] While its use is common, it is not expected to replace most work done by lawyers in the near future.[204]\n The electronic discovery industry uses machine learning to reduce manual searching.[205]\n Law enforcement has begun using facial recognition systems (FRS) to identify suspects from visual data. FRS results have proven to be more accurate when compared to eyewitness results. Furthermore, FRS has shown to have much a better ability to identify individuals when video clarity and visibility are low in comparison to human participants. [206]\n COMPAS is a commercial system used by U.S. courts to assess the likelihood of recidivism.[207]\n One concern relates to algorithmic bias, AI programs may become biased after processing data that exhibits bias.[208] ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than that of white defendants.[207]\n In 2019, the city of Hangzhou, China established a pilot program artificial intelligence-based Internet Court to adjudicate disputes related to ecommerce and internet-related intellectual property claims.[209]: 124  Parties appear before the court via videoconference and AI evaluates the evidence presented and applies relevant legal standards.[209]: 124 \n Another application of AI is in human resources. AI can screen resumes and rank candidates based on their qualifications, predict candidate success in given roles, and automate repetitive communication tasks via chatbots.[210]\n AI has simplified the recruiting /job search process for both recruiters and job seekers. According to Raj Mukherjee from Indeed, 65% of job searchers search again within 91 days after hire. An AI-powered engine streamlines the complexity of job hunting by assessing information on job skills, salaries, and user tendencies, matching job seekers to the most relevant positions. Machine intelligence calculates appropriate wages and highlights resume information for recruiters using NLP, which extracts relevant words and phrases from text. Another application is an AI resume builder that compiles a CV in 5 minutes.[211] Chatbots assist website visitors and refine workflows.\n AI underlies avatars (automated online assistants) on web pages.[212] It can reduce operation and training costs.[212] Pypestream automated customer service for its mobile application to streamline communication with customers.[213]\n A Google app analyzes language and converts speech into text. The platform can identify angry customers through their language and respond appropriately.[214] Amazon uses a chatbot for customer service that can perform tasks like checking the status of an order, cancelling orders, offering refunds and connecting the customer with a human representative.[215] Generative AI (GenAI), such as ChatGPT, is increasingly used in business to automate tasks and enhance decision-making.[216]\n In the hospitality industry, AI is used to reduce repetitive tasks, analyze trends, interact with guests, and predict customer needs.[217] AI hotel services come in the form of a chatbot,[218] application, virtual voice assistant and service robots.\n AI applications analyze media content such as movies, TV programs, advertisement videos or user-generated content. The solutions often involve computer vision.\n Typical scenarios include the analysis of images using object recognition or face recognition techniques, or the analysis of video for scene recognizing scenes, objects or faces. AI-based media analysis can facilitate media search, the creation of descriptive keywords for content, content policy monitoring (such as verifying the suitability of content for a particular TV viewing time), speech to text for archival or other purposes, and the detection of logos, products or celebrity faces for ad placement.\n Deep-fakes can be used for comedic purposes but are better known for fake news and hoaxes.\n Deepfakes can portray individuals in harmful or compromising situations, causing significant reputational damage and emotional distress, especially when the content is defamatory or violates personal ethics. While defamation and false light laws offer some recourse, their focus on false statements rather than fabricated images or videos often leaves victims with limited legal protection and a challenging burden of proof.[232]\n In January 2016,[233] the Horizon 2020 program financed the InVID Project[234][235] to help journalists and researchers detect fake documents, made available as browser plugins.[236][237]\n In June 2016, the visual computing group of the Technical University of Munich and from Stanford University developed Face2Face,[238] a program that animates photographs of faces, mimicking the facial expressions of another person. The technology has been demonstrated animating the faces of people including Barack Obama and Vladimir Putin. Other methods have been demonstrated based on deep neural networks, from which the name deep fake was taken.\n In September 2018, U.S. Senator Mark Warner proposed to penalize social media companies that allow sharing of deep-fake documents on their platforms.[239]\n In 2018, Darius Afchar and Vincent Nozick found a way to detect faked content by analyzing the mesoscopic properties of video frames.[240] DARPA gave 68 million dollars to work on deep-fake detection.[240]\n Audio deepfakes[241][242] and AI software capable of detecting deep-fakes and cloning human voices have been developed.[243][244]\n Respeecher is a program that enables one person to speak with the voice of another.\n AI algorithms have been used to detect deepfake videos.[245][246]\n Artificial intelligence is also starting to be used in video production, with tools and software being developed that utilize generative AI in order to create new video, or alter existing video. Some of the major tools that are being used in these processes currently are DALL-E, Mid-journey, and Runway.[247]  Way mark Studios utilized the tools offered by both DALL-E and Mid-journey to create a fully AI generated film called The Frost in the summer of 2023.[247] Way mark Studios is experimenting with using these AI tools to generate advertisements and commercials for companies in mere seconds.[247]  Yves Bergquist, a director of the AI & Neuroscience in Media Project at USC's Entertainment Technology Center, says post production crews in Hollywood are already using generative AI, and predicts that in the future more companies will embrace this new technology.[248]\n AI has been used to compose music of various genres.\n David Cope created an AI called Emily Howell that managed to become well known in the field of algorithmic computer music.[249] The algorithm behind Emily Howell is registered as a US patent.[250]\n In 2012, AI Iamus created the first complete classical album.[251]\n AIVA (Artificial Intelligence Virtual Artist), composes symphonic music, mainly classical music for film scores.[252] It achieved a world first by becoming the first virtual composer to be recognized by a musical professional association.[253]\n Melomics creates computer-generated music for stress and pain relief.[254]\n At Sony CSL Research Laboratory, the Flow Machines software creates pop songs by learning music styles from a huge database of songs. It can compose in multiple styles.\n The Watson Beat uses reinforcement learning and deep belief networks to compose music on a simple seed input melody and a select style. The software was open sourced[255] and musicians such as Taryn Southern[256] collaborated with the project to create music.\n South Korean singer, Hayeon's, debut song, \"Eyes on You\" was composed using AI which was supervised by real composers, including NUVO.[257]\n Narrative Science sells computer-generated news and reports. It summarizes sporting events based on statistical data from the game. It also creates financial reports and real estate analyses.[258] Automated Insights generates personalized recaps and previews for Yahoo Sports Fantasy Football.[259]\n Yseop, uses AI to turn structured data into natural language comments and recommendations. Yseop writes financial reports, executive summaries, personalized sales or marketing documents and more in multiple languages, including English, Spanish, French, and German.[260]\n TALESPIN made up stories similar to the fables of Aesop. The program started with a set of characters who wanted to achieve certain goals. The story narrated their attempts to satisfy these goals.[citation needed] Mark Riedl and Vadim Bulitko asserted that the essence of storytelling was experience management, or \"how to balance the need for a coherent story progression with user agency, which is often at odds\".[261]\n While AI storytelling focuses on story generation (character and plot), story communication also received attention. In 2002, researchers developed an architectural framework for narrative prose generation. They faithfully reproduced text variety and complexity on stories such as Little Red Riding Hood.[262] In 2016, a Japanese AI co-wrote a short story and almost won a literary prize.[263]\n South Korean company Hanteo Global uses a journalism bot to write articles.[264]\n Literary authors are also exploring uses of AI. An example is David Jhave Johnston's work ReRites (2017-2019), where the poet created a daily rite of editing the poetic output of a neural network to create a series of performances and publications.\n In 2010, artificial intelligence used baseball statistics to automatically generate news articles. This was launched by The Big Ten Network using software from Narrative Science.[265]\n After being unable to cover every Minor League Baseball game with a large team, Associated Press collaborated with Automated Insights in 2016 to create game recaps that were automated by artificial intelligence.[266]\n UOL in Brazil expanded the use of AI in its writing. Rather than just generating news stories, they programmed the AI to include commonly searched words on Google.[266]\n El Pais, a Spanish news site that covers many things including sports, allows users to make comments on each news article. They use the Perspective API to moderate these comments and if the software deems a comment to contain toxic language, the commenter must modify it in order to publish it.[266]\n A local Dutch media group used AI to create automatic coverage of amateur soccer, set to cover 60,000 games in just a single season. NDC partnered with United Robots to create this algorithm and cover what would have never been possible before without an extremely large team.[266]\n Lede AI has been used in 2023 to take scores from high school football games to generate stories automatically for the local newspaper. This was met with significant criticism from readers for the very robotic diction that was published. With some descriptions of games being a \"close encounter of the athletic kind,\" readers were not pleased and let the publishing company, Gannett, know on social media. Gannett has since halted their used of Lede AI until they come up with a solution for what they call an experiment.[267]\n  Millions of its articles have been edited by bots[271] which however are usually not artificial intelligence software. Many AI platforms use Wikipedia data,[272] mainly for training machine learning applications. There is research and development of various artificial intelligence applications for Wikipedia such as for identifying outdated sentences,[273] detecting covert vandalism[274] or recommending articles and tasks to new editors.\n Machine translation (see above) has also be used for translating Wikipedia articles and could play a larger role in creating, updating, expanding, and generally improving articles in the future. A content translation tool allows editors of some Wikipedias to more easily translate articles across several select languages.[275][276]\n In video games, AI is routinely used to generate behavior in non-player characters (NPCs). In addition, AI is used for pathfinding. Some researchers consider NPC AI in games to be a \"solved problem\" for most production tasks.[who?] Games with less typical AI include the AI director of Left 4 Dead (2008) and the neuroevolutionary training of platoons in Supreme Commander 2 (2010).[277][278] AI is also used in Alien Isolation (2014) as a way to control the actions the Alien will perform next.[279]\n Kinect, which provides a 3D body–motion interface for the Xbox 360 and the Xbox One, uses algorithms that emerged from AI research.[280][which?]\n AI has been used to produce visual art. The first AI art program, called AARON, was developed by Harold Cohen in 1968[281] with the goal of being able to code the act of drawing. It started by creating simple black and white drawings, and later to painting using special brushes and dyes that were chosen by the program itself without mediation from Cohen.[282]\n AI platforms such as \"DALL-E\",[283] Stable Diffusion,[283] Imagen,[284] and Midjourney[285] have been used for generating visual images from inputs such as text or other images.[286] Some AI tools allow users to input images and output changed versions of that image, such as to display an object or product in different environments. AI image models can also attempt to replicate the specific styles of artists, and can add visual complexity to rough sketches.\n Since their design in 2014, generative adversarial networks (GANs) have been used by AI artists. GAN computer programming, generates technical images through machine learning frameworks that surpass the need for human operators.[281] Examples of GAN programs that generate art include Artbreeder and DeepDream.\n In addition to the creation of original art, research methods that utilize AI have been generated to quantitatively analyze digital art collections. Although the main goal of the large-scale digitization of artwork in the past few decades was to allow for accessibility and exploration of these collections, the use of AI in analyzing them has brought about new research perspectives.[287]\nTwo computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art.[288] While distant viewing includes the analysis of large collections, close reading involves one piece of artwork.\n AI has been in use since the early 2000s, most notably by a system designed by Pixar called \"Genesis\".[289] It was designed to learn algorithms and create 3D models for its characters and props. Notable movies that used this technology included Up and The Good Dinosaur.[290] AI has been used less ceremoniously in recent years. In 2023, it was revealed Netflix of Japan was using AI to generate background images for their upcoming show to be met with backlash online.[291]  In recent years, motion capture became an easily accessible form of AI animation. For example, Move AI is a program built to capture any human movement and reanimate it in its animation program using learning AI.[292]\n Power electronics converters are used in renewable energy, energy storage, electric vehicles and high-voltage direct current transmission. These converters are failure-prone, which can interrupt service and require costly maintenance or catastrophic consequences in mission critical applications.[citation needed] AI can guide the design process for reliable power electronics converters, by calculating exact design parameters that ensure the required lifetime.[293]\n The U.S. Department of Energy underscores AI's pivotal role in realizing national climate goals. With AI, the ambitious target of achieving net-zero greenhouse gas emissions across the economy becomes feasible. AI also helps make room for wind and solar on the grid by avoiding congestion and increasing grid reliability. [294]\n Machine learning can be used for energy consumption prediction and scheduling, e.g. to help with renewable energy intermittency management (see also: smart grid and climate change mitigation in the power grid).[295][296][297][298][125]\n Many telecommunications companies make use of heuristic search to manage their workforces. For example, BT Group deployed heuristic search[299] in an application that schedules 20,000 engineers. Machine learning is also used for speech recognition (SR), including of voice-controlled devices, and SR-related transcription, including of videos.[300][301]\n Artificial intelligence has been combined with digital spectrometry by IdeaCuria Inc.,[302][303] enable applications such as at-home water quality monitoring.\n In the 1990s, early artificial intelligence tools controlled Tamagotchis and Giga Pets, the Internet, and the first widely released robot, Furby. Aibo was a domestic robot in the form of a robotic dog with intelligent features and autonomy.\n Mattel created an assortment of AI-enabled toys that \"understand\" conversations, give intelligent responses, and learn.[304]\n Oil and gas companies have used artificial intelligence tools to automate functions, foresee equipment issues, and increase oil and gas output.[305][306]\n AI in transport is expected to provide safe, efficient, and reliable transportation while minimizing the impact on the environment and communities. The major development challenge is the complexity of transportation systems that involves independent components and parties, with potentially conflicting objectives.[307]\n AI-based fuzzy logic controllers operate gearboxes. For example, the 2006 Audi TT, VW Touareg [citation needed] and VW Caravell feature the DSP transmission. A number of Škoda variants (Škoda Fabia) include a fuzzy logic-based controller. Cars have AI-based driver-assist features such as self-parking and adaptive cruise control.\n There are also prototypes of autonomous automotive public transport vehicles such as electric mini-buses[308][309][310][311] as well as autonomous rail transport in operation.[312][313][314]\n There also are prototypes of autonomous delivery vehicles, sometimes including delivery robots.[315][316][317][318][319][320][321]\n Transportation's complexity means that in most cases training an AI in a real-world driving environment is impractical. Simulator-based testing can reduce the risks of on-road training.[322]\n AI underpins self-driving vehicles. Companies involved with AI include Tesla, Waymo, and General Motors. AI-based systems control functions such as braking, lane changing, collision prevention, navigation and mapping.[323]\n Autonomous trucks are in the testing phase. The UK government passed legislation to begin testing of autonomous truck platoons in 2018.[324] A group of autonomous trucks follow closely behind each other. German corporation Daimler is testing its Freightliner Inspiration.[325]\n Autonomous vehicles require accurate maps to be able to navigate between destinations.[326] Some autonomous vehicles do not allow human drivers (they have no steering wheels or pedals).[327]\n AI has been used to optimize traffic management, which reduces wait times, energy use, and emissions by as much as 25 percent.[328]\n Smart traffic lights have been developed at Carnegie Mellon since 2009.  Professor Stephen Smith has started a company since then Surtrac that has installed smart traffic control systems in 22 cities.  It costs about $20,000 per intersection to install. Drive time has been reduced by 25% and traffic jam waiting time has been reduced by 40% at the intersections it has been installed.[329]\n The Royal Australian Air Force (RAAF) Air Operations Division (AOD) uses AI for expert systems. AIs operate as surrogate operators for combat and training simulators, mission management aids, support systems for tactical decision making, and post processing of the simulator data into symbolic summaries.[330]\n Aircraft simulators use AI for training aviators. Flight conditions can be simulated that allow pilots to make mistakes without risking themselves or expensive aircraft. Air combat can also be simulated.\n AI can also be used to operate planes analogously to their control of ground vehicles. Autonomous drones can fly independently or in swarms.[331]\n AOD uses the Interactive Fault Diagnosis and Isolation System, or IFDIS, which is a rule-based expert system using information from TF-30 documents and expert advice from mechanics that work on the TF-30. This system was designed to be used for the development of the TF-30 for the F-111C. The system replaced specialized workers. The system allowed regular workers to communicate with the system and avoid mistakes, miscalculations, or having to speak to one of the specialized workers.\n Speech recognition allows traffic controllers to give verbal directions to drones.\n Artificial intelligence supported design of aircraft,[332] or AIDA, is used to help designers in the process of creating conceptual designs of aircraft. This program allows the designers to focus more on the design itself and less on the design process. The software also allows the user to focus less on the software tools. The AIDA uses rule-based systems to compute its data. This is a diagram of the arrangement of the AIDA modules. Although simple, the program is proving effective.\n In 2003 a Dryden Flight Research Center project created software that could enable a damaged aircraft to continue flight until a safe landing can be achieved.[333] The software compensated for damaged components by relying on the remaining undamaged components.[334]\n The 2016 Intelligent Autopilot System combined apprenticeship learning and behavioral cloning whereby the autopilot observed low-level actions required to maneuver the airplane and high-level strategy used to apply those actions.[335]\n Neural networks are used by situational awareness systems in ships and boats.[336] There also are autonomous boats.\n Autonomous ships that monitor the ocean, AI-driven satellite data analysis, passive acoustics[337] or remote sensing and other applications of environmental monitoring make use of machine learning.[338][339][340][165]\n For example, \"Global Plastic Watch\" is an AI-based satellite monitoring-platform for analysis/tracking of plastic waste sites to help prevention of plastic pollution – primarily ocean pollution – by helping identify who and where mismanages plastic waste, dumping it into oceans.[341][342]\n Machine learning can be used to spot early-warning signs of disasters and environmental issues, possibly including natural pandemics,[343][344] earthquakes,[345][346][347] landslides,[348] heavy rainfall,[349] long-term water supply vulnerability,[350] tipping-points of ecosystem collapse,[351] cyanobacterial bloom outbreaks,[352] and droughts.[353][354][355]\n AI can be used for real-time code completion, chat, and automated test generation. These tools are typically integrated with editors and IDEs as plugins. They differ in functionality, quality, speed, and approach to privacy.[356] Code suggestions could be incorrect, and should be carefully reviewed by software developers before accepted.\n GitHub Copilot is an artificial intelligence model developed by GitHub and OpenAI that is able to autocomplete code in multiple programming languages.[357] Price for individuals: $10/mo or $100/yr, with one free month trial.\n Tabnine was created by Jacob Jackson and was originally owned by Tabnine company. In late 2019, Tabnine was acquired by Codota.[358] Tabnine tool is available as plugin to most popular IDEs. It offers multiple pricing options, including limited \"starter\" free version.[359]\n CodiumAI by CodiumAI, small startup in Tel Aviv, offers automated test creation. Currently supports Python, JS, and TS.[360]\n Ghostwriter by Replit offers code completion and chat.[361] They have multiple pricing plans, including a free one and a \"Hacker\" plan for $7/month.\n CodeWhisperer by Amazon collects individual users' content, including files open in the IDE. They claim to focus on security both during transmission and when storing.[362] Individual plan is free, professional plan is $19/user/month.\n Other tools: SourceGraph Cody, CodeCompleteFauxPilot, Tabby[356]\n AI can be used to create other AIs. For example, around November 2017, Google's AutoML project to evolve new neural net topologies created NASNet, a system optimized for ImageNet and POCO F1. NASNet's performance exceeded all previously published performance on ImageNet.[363]\n Machine learning has been used for noise-cancelling in quantum technology,[364] including quantum sensors.[365] Moreover, there is substantial research and development of using quantum computers with machine learning algorithms. For example, there is a prototype, photonic, quantum memristive device for neuromorphic (quantum-)computers (NC)/artificial neural networks and NC-using quantum materials with some variety of potential neuromorphic computing-related applications,[366][367] and quantum machine learning is a field with some variety of applications under development. AI could be used for quantum simulators which may have the application of solving physics and chemistry[368][369] problems as well as for quantum annealers for training of neural networks for AI applications.[370] There may also be some usefulness in chemistry, e.g. for drug discovery, and in materials science, e.g. for materials optimization/discovery (with possible relevance to quantum materials manufacturing[191][192]).[371][372][373][better source needed]\n AI researchers have created many tools to solve the most difficult problems in computer science. Many of their inventions have been adopted by mainstream computer science and are no longer considered AI. All of the following were originally developed in AI laboratories:[374]\n An optical character reader is used in the extraction of data in business documents like invoices and receipts. It can also be used in business contract documents e.g. employment agreements to extract critical data like employment terms, delivery terms, termination clauses, etc.[375]\n Artificial intelligence in architecture describes the use of artificial intelligence in automation, design and planning in the architectural process or in assisting human skills in the field of architecture. Artificial Intelligence is thought to potentially lead to and ensue major changes in architecture.[376][377][378]\n AI in architecture has created a way for architects to create things beyond human understanding. AI implementation of machine learning text-to-render technologies, like DALL-E and stable Diffusion, gives power to visualization complex.[379]\n AI allows designers to demonstrate their creativity and even invent new ideas while designing. In future, AI will not replace architects; instead, it will improve the speed of translating ideas sketching.[379]\n"
    },
    {
        "title": "Search engine",
        "url": "https://en.wikipedia.org/wiki/Web_search_engine",
        "content": "\n A search engine is a software system that provides hyperlinks to web pages and other relevant information on the Web in response to a user's query. The user inputs a query within a web browser or a mobile app, and the search results are often a list of hyperlinks, accompanied by textual summaries and images. Users also have the option of limiting the search to a specific type of results, such as images, videos, or news.\n For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query is based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, but some content is not accessible to crawlers.\n There have been many search engines since the dawn of the Web in the 1990s, but Google Search became the dominant one in the 2000s and has remained so. It currently has a 90% global market share. Other search engines with a smaller market share include Bing at 4%, Yandex at 2%, and Yahoo at 1%. Other search engines not listed have less than a 3% market share.[1][2] The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google.\n In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk.[3] He called it a memex. He described the system in an article titled \"As We May Think\" that was published in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5]\n Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7]\n The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10]\n Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12]\n The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually.\n The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor.\n In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18]\n In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format.\n JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered.\n One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor.\n The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages.\n Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search.\n In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his Rankdex technology for the Baidu search engine, which was founded by him in China and launched in 2000.\n In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31]\n Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[citation needed]\n Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[32] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000.\n Around 2000, Google's search engine rose to prominence.[33] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker.\n By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions.\n Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot).\n Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology.\n As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex.\n \n A search engine maintains the following processes in near real time:[34]\n Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[36]\n Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are made in a public database, made available for web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[35] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis.\n Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[35] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot.\n Typically when a user enters a query into a search engine it is a few keywords.[37] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[35] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing.\n Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results.\nFor example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[38] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[35] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases you search for.\n The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[35] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work.\n Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[39]\n Local search is the process that optimizes the efforts of local businesses. They focus on change to make sure all searches are consistent. It is important because many people determine where they plan to go and what to buy based on their searches.[40]\n As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[2] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[41]\n In Russia, Yandex has a market share of 62.6%, compared to Google's 28.3%. Yandex is the second most used search engine on smartphones in Asia and Europe.[42] In China, Baidu is the most popular search engine.[43] South Korea-based search portal Naver is used for 62.8% of online searches in the country.[44] Yahoo! Japan and Yahoo! Taiwan are the most popular choices for Internet searches in Japan and Taiwan, respectively.[45] China is one of few countries where Google is not in the top three web search engines for market share. Google was previously more popular in China, but withdrew significantly after a disagreement with the government over censorship and a cyberattack. Bing, however, is in the top three web search engines with a market share of 14.95%. Baidu is top with 49.1% of the market share.[46][failed verification]\n Most countries' markets in the European Union are dominated by Google, except for the Czech Republic, where Seznam is a strong competitor.[47]\n The search engine Qwant is based in Paris, France, where it attracts most of its 50 million monthly registered users from.\n Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[48][49] and the underlying assumptions about the technology.[50] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[51] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal.\n Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[52] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[49]\n Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons.\n Several scholars have studied the cultural changes triggered by search engines,[53] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[54] climate change denial,[55] and conspiracy theories.[56]\n There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[57] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behaviour and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[58][59][60] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalisation in search,[60] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[61][59]\n The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[62]\n While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[63] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[64]\n Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign.\n Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[65]\n In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[66] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this.\n The first web search engine was Archie, created in 1990[67] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.\n The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: Some administrator decides that he wants to make files available from his computer. He sets up a program on his computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, he or she connects to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol.\n Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them.\n Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file.\n Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[68]\n In 1993, the University of Nevada System Computing Services group developed Veronica.[67] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[68]\n The World Wide Web Wanderer, developed by Matthew Gray in 1993[69] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database.\n Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained.\n In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways.\n ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth.  The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[68]\n Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[68]\n Excite was the first serious commercial search engine which launched in 1995.[70] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million.\n Some of the first analysis of web searching was conducted on search logs from Excite[71][72]\n In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos.\n As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory.\n The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites.\n At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine.\n Search engines on the web are sites enriched with facility to search the content stored on other sites.  There is difference in the way various search engines work, but they all perform three basic tasks.[73]\n The process begins when a user enters a query statement into the system through the interface provided.\n There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two.\n Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine.\n Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index.\n In both cases, when you query a search engine to locate information, you're actually searching through the index that the search engine has created —you are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated.\n So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for.\n One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing.\n Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking.\n Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity.\n Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[74]\n"
    },
    {
        "title": "Google Search",
        "url": "https://en.wikipedia.org/wiki/Google_Search",
        "content": "\n Google Search (also known simply as Google or Google.com) is a search engine operated by Google. It allows users to search for information on the Web by entering keywords or phrases. Google Search uses algorithms to analyze and rank websites based on their relevance to the search query. It is the most popular search engine worldwide.\n Google Search is the most-visited website in the world. As of 2020, Google Search has a 92% share of the global search engine market.[3] Approximately 26.75% of Google's monthly global traffic comes from the United States, 4.44% from India, 4.4% from Brazil, 3.92% from the United Kingdom and 3.84% from Japan according to data provided by Similarweb.[4]\n The order of search results returned by Google is based, in part, on a priority rank system called \"PageRank\". Google Search also provides many different options for customized searches, using symbols to include, exclude, specify or require certain search behavior, and offers specialized interactive experiences, such as flight status and package tracking, weather forecasts, currency, unit, and time conversions, word definitions, and more.\n The main purpose of Google Search is to search for text in publicly accessible documents offered by web servers, as opposed to other data, such as images or data contained in databases. It was originally developed in 1996 by Larry Page, Sergey Brin, and Scott Hassan.[5][6][7] The search engine would also be set up in the garage of Susan Wojcicki's Menlo Park home.[8] In 2011, Google introduced \"Google Voice Search\" to search for spoken, rather than typed, words.[9] In 2012, Google introduced a semantic search feature named Knowledge Graph.\n Analysis of the frequency of search terms may indicate economic, social and health trends.[10] Data about the frequency of use of search terms on Google can be openly inquired via Google Trends and have been shown to correlate with flu outbreaks and unemployment levels, and provide the information faster than traditional reporting methods and surveys. As of mid-2016, Google's search engine has begun to rely on deep neural networks.[11]\n In August 2024, a US judge in Virginia ruled that Google's search engine held an illegal monopoly over Internet search.[12][13] The court found that Google maintained its market dominance by paying large amounts to phone-makers and browser-developers to make Google its default search engine.[14]\n Google indexes hundreds of terabytes of information from web pages.[15] For websites that are currently down or otherwise not available, Google provides links to cached versions of the site, formed by the search engine's latest indexing of that page.[16] Additionally, Google indexes some file types, being able to show users PDFs, Word documents, Excel spreadsheets, PowerPoint presentations, certain Flash multimedia content, and plain text files.[17] Users can also activate \"SafeSearch\", a filtering technology aimed at preventing explicit and pornographic content from appearing in search results.[18]\n Despite Google search's immense index, sources generally assume that Google is only indexing less than 5% of the total Internet, with the rest belonging to the deep web, inaccessible through its search tools.[15][19][20]\n In 2012, Google changed its search indexing tools to demote sites that had been accused of piracy.[21] In October 2016, Gary Illyes, a webmaster trends analyst with Google, announced that the search engine would be making a separate, primary web index dedicated for mobile devices, with a secondary, less up-to-date index for desktop use. The change was a response to the continued growth in mobile usage, and a push for web developers to adopt a mobile-friendly version of their websites.[22][23] In December 2017, Google began rolling out the change, having already done so for multiple websites.[24]\n In August 2009, Google invited web developers to test a new search architecture, codenamed \"Caffeine\", and give their feedback. The new architecture provided no visual differences in the user interface, but added significant speed improvements and a new \"under-the-hood\" indexing infrastructure. The move was interpreted in some quarters as a response to Microsoft's recent release of an upgraded version of its own search service, renamed Bing, as well as the launch of Wolfram Alpha, a new search engine based on \"computational knowledge\".[25][26] Google announced completion of \"Caffeine\" on June 8, 2010, claiming 50% fresher results due to continuous updating of its index.[27]\n With \"Caffeine\", Google moved its back-end indexing system away from MapReduce and onto Bigtable, the company's distributed database platform.[28][29]\n In August 2018, Danny Sullivan from Google announced a broad core algorithm update. As per current analysis done by the industry leaders Search Engine Watch and Search Engine Land, the update was to drop down the medical and health-related websites that were not user friendly and were not providing good user experience. This is why the industry experts named it \"Medic\".[30]\n Google reserves very high standards for YMYL (Your Money or Your Life) pages. This is because misinformation can affect users financially, physically, or emotionally. Therefore, the update targeted particularly those YMYL pages that have low-quality content and misinformation. This resulted in the algorithm targeting health and medical-related websites more than others. However, many other websites from other industries were also negatively affected.[31]\n By 2012, it handled more than 3.5 billion searches per day.[32] In 2013 the European Commission found that Google Search favored Google's own products, instead of the best result for consumers' needs.[33] In February 2015 Google announced a major change to its mobile search algorithm which would favor mobile friendly over other websites. Nearly 60% of Google searches come from mobile phones. Google says it wants users to have access to premium quality websites. Those websites which lack a mobile-friendly interface would be ranked lower and it is expected that this update will cause a shake-up of ranks. Businesses who fail to update their websites accordingly could see a dip in their regular websites traffic.[34]\n Google's rise was largely due to a patented algorithm called PageRank which helps rank web pages that match a given search string.[35] When Google was a Stanford research project, it was nicknamed BackRub because the technology checks backlinks to determine a site's importance. Other keyword-based methods to rank search results, used by many search engines that were once more popular than Google, would check how often the search terms occurred in a page, or how strongly associated the search terms were within each resulting page. The PageRank algorithm instead analyzes human-generated links assuming that web pages linked from many important pages are also important. The algorithm computes a recursive score for pages, based on the weighted sum of other pages linking to them. PageRank is thought to correlate well with human concepts of importance. In addition to PageRank, Google, over the years, has added many other secret criteria for determining the ranking of resulting pages. This is reported to comprise over 250 different indicators,[36][37] the specifics of which are kept secret to avoid difficulties created by scammers and help Google maintain an edge over its competitors globally.\n PageRank was influenced by a similar page-ranking and site-scoring algorithm earlier used for RankDex, developed by Robin Li in 1996. Larry Page's patent for PageRank filed in 1998 includes a citation to Li's earlier patent. Li later went on to create the Chinese search engine Baidu in 2000.[38][39]\n In a potential hint of Google's future direction of their Search algorithm, Google's then chief executive Eric Schmidt, said in a 2007 interview with the Financial Times: \"The goal is to enable Google users to be able to ask the question such as 'What shall I do tomorrow?' and 'What job shall I take?'\".[40] Schmidt reaffirmed this during a 2010 interview with The Wall Street Journal: \"I actually think most people don't want Google to answer their questions, they want Google to tell them what they should be doing next.\"[41]\n Because Google is the most popular search engine, many webmasters attempt to influence their website's Google rankings. An industry of consultants has arisen to help websites increase their rankings on Google and other search engines. This field, called search engine optimization, attempts to discern patterns in search engine listings, and then develop a methodology for improving rankings to draw more searchers to their clients' sites. Search engine optimization encompasses both \"on page\" factors (like body copy, title elements, H1 heading elements and image alt attribute values) and Off Page Optimization factors (like anchor text and PageRank). The general idea is to affect Google's relevance algorithm by incorporating the keywords being targeted in various places \"on page\", in particular the title element and the body copy (note: the higher up in the page, presumably the better its keyword prominence and thus the ranking). Too many occurrences of the keyword, however, cause the page to look suspect to Google's spam checking algorithms. Google has published guidelines for website owners who would like to raise their rankings when using legitimate optimization consultants.[42] It has been hypothesized, and, allegedly, is the opinion of the owner of one business about which there have been numerous complaints, that negative publicity, for example, numerous consumer complaints, may serve as well to elevate page rank on Google Search as favorable comments.[43] The particular problem addressed in The New York Times article, which involved DecorMyEyes, was addressed shortly thereafter by an undisclosed fix in the Google algorithm. According to Google, it was not the frequently published consumer complaints about DecorMyEyes which resulted in the high ranking but mentions on news websites of events which affected the firm such as legal actions against it. Google Search Console helps to check for websites that use duplicate or copyright content.[44]\n In 2013, Google significantly upgraded its search algorithm with \"Hummingbird\". Its name was derived from the speed and accuracy of the hummingbird.[45] The change was announced on September 26, 2013, having already been in use for a month.[46] \"Hummingbird\" places greater emphasis on natural language queries, considering context and meaning over individual keywords.[45] It also looks deeper at content on individual pages of a website, with improved ability to lead users directly to the most appropriate page rather than just a website's homepage.[47] The upgrade marked the most significant change to Google search in years, with more \"human\" search interactions[48] and a much heavier focus on conversation and meaning.[45] Thus, web developers and writers were encouraged to optimize their sites with natural writing rather than forced keywords, and make effective use of technical web development for on-site navigation.[49]\n In 2023, drawing on internal Google documents disclosed as part of the United States v. Google LLC (2020) antitrust case, technology reporters claimed that Google Search was \"bloated and overmonetized\"[50] and that the \"semantic matching\" of search queries put advertising profits before quality.[51] Wired withdrew Megan Gray's piece after Google complained about alleged inaccuracies, while the author reiterated that «As stated in court, \"A goal of Project Mercury was to increase commercial queries\"».[52]\n In March 2024, Google announced a significant update to its core search algorithm and spam targeting, which is expected to wipe out 40 percent of all spam results.[53] On March 20th, it was confirmed that the roll out of the spam update was complete.[54]\n On September 10, 2024, the European-based EU Court of Justice found that Google held an illegal monopoly with the way the company showed favoritism to its shopping search, and could not avoid paying €2.4 billion.[55] The EU Court of Justice referred to Google's treatment of rival shopping searches as \"discriminatory\" and in violation of the Digital Markets Act.[55]\n At the top of the search page, the approximate result count and the response time two digits behind decimal is noted. Of search results, page titles and URLs, dates, and a preview text snippet for each result appears. Along with web search results, sections with images, news, and videos may appear.[56] The length of the previewed text snipped was experimented with in 2015 and 2017.[57][58]\n \"Universal search\" was launched by Google on May 16, 2007, as an idea that merged the results from different kinds of search types into one. Prior to Universal search, a standard Google search would consist of links only to websites. Universal search, however, incorporates a wide variety of sources, including websites, news, pictures, maps, blogs, videos, and more, all shown on the same search results page.[59][60] Marissa Mayer, then-vice president of search products and user experience, described the goal of Universal search as \"we're attempting to break down the walls that traditionally separated our various search properties and integrate the vast amounts of information available into one simple set of search results.[61]\n In June 2017, Google expanded its search results to cover available job listings. The data is aggregated from various major job boards and collected by analyzing company homepages. Initially only available in English, the feature aims to simplify finding jobs suitable for each user.[62][63]\n In May 2009, Google announced that they would be parsing website microformats to populate search result pages with \"Rich snippets\". Such snippets include additional details about results, such as displaying reviews for restaurants and social media accounts for individuals.[64]\n In May 2016, Google expanded on the \"Rich snippets\" format to offer \"Rich cards\", which, similarly to snippets, display more information about results, but shows them at the top of the mobile website in a swipeable carousel-like format.[65] Originally limited to movie and recipe websites in the United States only, the feature expanded to all countries globally in 2017.[66]\n The Knowledge Graph is a knowledge base used by Google to enhance its search engine's results with information gathered from a variety of sources.[67] This information is presented to users in a box to the right of search results.[68] Knowledge Graph boxes were added to Google's search engine in May 2012,[67] starting in the United States, with international expansion by the end of the year.[69] The information covered by the Knowledge Graph grew significantly after launch, tripling its original size within seven months,[70] and being able to answer \"roughly one-third\" of the 100 billion monthly searches Google processed in May 2016.[71] The information is often used as a spoken answer in Google Assistant[72] and Google Home searches.[73] The Knowledge Graph has been criticized for providing answers without source attribution.[71]\n A Google Knowledge Panel[74] is a feature integrated into Google search engine result pages, designed to present a structured overview of entities such as individuals, organizations, locations, or objects directly within the search interface. This feature leverages data from Google's Knowledge Graph,[75] a database that organizes and interconnects information about entities, enhancing the retrieval and presentation of relevant content to users.\n The content within a Knowledge Panel[76] is derived from various sources, including Wikipedia and other structured databases, ensuring that the information displayed is both accurate and contextually relevant. For instance, querying a well-known public figure may trigger a Knowledge Panel displaying essential details such as biographical information, birthdate, and links to social media profiles or official websites.\n The primary objective of the Google Knowledge Panel is to provide users with immediate, factual answers, reducing the need for extensive navigation across multiple web pages.\n In May 2017, Google enabled a new \"Personal\" tab in Google Search, letting users search for content in their Google accounts' various services, including email messages from Gmail and photos from Google Photos.[77][78]\n Google Discover, previously known as Google Feed, is a personalized stream of articles, videos, and other news-related content. The feed contains a \"mix of cards\" which show topics of interest based on users' interactions with Google, or topics they choose to follow directly.[79] Cards include, \"links to news stories, YouTube videos, sports scores, recipes, and other content based on what [Google] determined you're most likely to be interested in at that particular moment.\"[79] Users can also tell Google they're not interested in certain topics to avoid seeing future updates.\n Google Discover launched in December 2016[80] and received a major update in July 2017.[81] Another major update was released in September 2018, which renamed the app from Google Feed to Google Discover, updated the design, and adding more features.[82]\n Discover can be found on a tab in the Google app and by swiping left on the home screen of certain Android devices. As of 2019, Google will not allow political campaigns worldwide to target their advertisement to people to make them vote.[83]\n At the 2023 Google I/O event in May, Google unveiled Search Generative Experience (SGE), an experimental feature in Google Search available through Google Labs which produces AI-generated summaries in response to search prompts.[84] This was part of Google's wider efforts to counter the unprecedented rise of generative AI technology, ushered by OpenAI's launch of ChatGPT, which sent Google executives to a panic due to its potential threat to Google Search.[85] Google added the ability to generate images in October.[86] At I/O in 2024, the feature was upgraded and renamed AI Overviews.[87]\n AI Overviews was rolled out to users in the United States in May 2024.[87] The feature faced public criticism in the first weeks of its rollout after errors from the tool went viral online. These included results suggesting users add glue to pizza or eat rocks,[88] or incorrectly claiming Barack Obama is Muslim.[89] Google described these viral errors as \"isolated examples\", maintaining that most AI Overviews provide accurate information.[88][90] Two weeks after the rollout of AI Overviews, Google made technical changes and scaled back the feature, pausing its use for some health-related queries and limiting its reliance on social media posts.[91] Scientific American has criticised the system on environmental grounds, as such a search uses 30 times more energy than a conventional one.[92] It has also been criticized for condensing information from various sources, making it less likely for people to view full articles and websites. When it was announced in May 2024, Danielle Coffey, CEO of the News/Media Alliance was quoted as saying \"This will be catastrophic to our traffic, as marketed by Google to further satisfy user queries, leaving even less incentive to click through so that we can monetize our content.\"[93]\n In August 2024, AI Overviews were rolled out in the UK, India, Japan, Indonesia, Mexico and Brazil, with local language support.[94] On October 28, 2024, AI Overviews was rolled out to 100 more countries, including Australia and New Zealand.[95]\n In late June 2011, Google introduced a new look to the Google homepage in order to boost the use of the Google+ social tools.[96]\n One of the major changes was replacing the classic navigation bar with a black one. Google's digital creative director Chris Wiggins explains: \"We're working on a project to bring you a new and improved Google experience, and over the next few months, you'll continue to see more updates to our look and feel.\"[97] The new navigation bar has been negatively received by a vocal minority.[98]\n In November 2013, Google started testing yellow labels for advertisements displayed in search results, to improve user experience. The new labels, highlighted in yellow color, and aligned to the left of each sponsored link help users differentiate between organic and sponsored results.[99]\n On December 15, 2016, Google rolled out a new desktop search interface that mimics their modular mobile user interface. The mobile design consists of a tabular design that highlights search features in boxes. and works by imitating the desktop Knowledge Graph real estate, which appears in the right-hand rail of the search engine result page, these featured elements frequently feature Twitter carousels, People Also Search For, and Top Stories (vertical and horizontal design) modules. The Local Pack and Answer Box were two of the original features of the Google SERP that were primarily showcased in this manner, but this new layout creates a previously unseen level of design consistency for Google results.[100]\n Google offers a \"Google Search\" mobile app for Android and iOS devices.[101] The mobile apps exclusively feature Google Discover and a \"Collections\" feature, in which the user can save for later perusal any type of search result like images, bookmarks or map locations into groups.[102] Android devices were introduced to a preview of the feed, perceived as related to Google Now, in December 2016,[103] while it was made official on both Android and iOS in July 2017.[104][105]\n In April 2016, Google updated its Search app on Android to feature \"Trends\"; search queries gaining popularity appeared in the autocomplete box along with normal query autocompletion.[106] The update received significant backlash, due to encouraging search queries unrelated to users' interests or intentions, prompting the company to issue an update with an opt-out option.[107] In September 2017, the Google Search app on iOS was updated to feature the same functionality.[108]\n In December 2017, Google released \"Google Go\", an app designed to enable use of Google Search on physically smaller and lower-spec devices in multiple languages. A Google blog post about designing \"India-first\" products and features explains that it is \"tailor-made for the millions of people in [India and Indonesia] coming online for the first time\".[109]\n Google Search consists of a series of localized websites. The largest of those, the google.com site, is the top most-visited website in the world.[110] Some of its features include a definition link for most searches including dictionary words, the number of results you got on your search, links to other searches (e.g. for words that Google believes to be misspelled, it provides a link to the search results using its proposed spelling), the ability to filter results to a date range,[111] and many more.\n Google search accepts queries as normal text, as well as individual keywords.[112] It automatically corrects apparent misspellings by default (while offering to use the original spelling as a selectable alternative), and provides the same results regardless of capitalization.[112] For more customized results, one can use a wide variety of operators, including, but not limited to:[113][114]\n Google also offers a Google Advanced Search page with a web interface to access the advanced features without needing to remember the special operators.[115]\n Google applies query expansion to submitted search queries, using techniques to deliver results that it considers \"smarter\" than the query users actually submitted. This technique involves several steps, including:[116]\n In 2008, Google started to give users autocompleted search suggestions in a list below the search bar while typing, originally with the approximate result count previewed for each listed search suggestion.[117]\n Google's homepage includes a button labeled \"I'm Feeling Lucky\". This feature originally allowed users to type in their search query, click the button and be taken directly to the first result, bypassing the search results page. Clicking it while leaving the search box empty opens Google's archive of Doodles.[118] With the 2010 announcement of Google Instant, an automatic feature that immediately displays relevant results as users are typing in their query, the \"I'm Feeling Lucky\" button disappears, requiring that users opt-out of Instant results through search settings to keep using the \"I'm Feeling Lucky\" functionality.[119] In 2012, \"I'm Feeling Lucky\" was changed to serve as an advertisement for Google services; users hover their computer mouse over the button, it spins and shows an emotion (\"I'm Feeling Puzzled\" or \"I'm Feeling Trendy\", for instance), and, when clicked, takes users to a Google service related to that emotion.[120]\n Tom Chavez of \"Rapt\", a firm helping to determine a website's advertising worth, estimated in 2007 that Google lost $110 million in revenue per year due to use of the button, which bypasses the advertisements found on the search results page.[121]\n Besides the main text-based search-engine function of Google search, it also offers multiple quick, interactive features. These include, but are not limited to:[122][123][124]\n During Google's developer conference, Google I/O, in May 2013, the company announced that users on Google Chrome and ChromeOS would be able to have the browser initiate an audio-based search by saying \"OK Google\", with no button presses required. After having the answer presented, users can follow up with additional, contextual questions; an example include initially asking \"OK Google, will it be sunny in Santa Cruz this weekend?\", hearing a spoken answer, and reply with \"how far is it from here?\"[125][126] An update to the Chrome browser with voice-search functionality rolled out a week later, though it required a button press on a microphone icon rather than \"OK Google\" voice activation.[127] Google released a browser extension for the Chrome browser, named with a \"beta\" tag for unfinished development, shortly thereafter.[128] In May 2014, the company officially added \"OK Google\" into the browser itself;[129] they removed it in October 2015, citing low usage, though the microphone icon for activation remained available.[130] In May 2016, 20% of search queries on mobile devices were done through voice.[131]\n In addition to its tool for searching web pages, Google also provides services for searching images, Usenet newsgroups, news websites, videos (Google Videos), searching by locality, maps, and items for sale online. Google Videos allows searching the World Wide Web for video clips.[132] The service evolved from Google Video, Google's discontinued video hosting service that also allowed to search the web for video clips.[132]\n In 2012, Google has indexed over 30 trillion web pages, and received 100 billion queries per month.[133] It also caches much of the content that it indexes. Google operates other tools and services including Google News, Google Shopping, Google Maps, Google Custom Search, Google Earth, Google Docs, Picasa (discontinued), Panoramio (discontinued), YouTube, Google Translate, Google Blog Search and Google Desktop Search (discontinued[134]).\n There are also products available from Google that are not directly search-related. Gmail, for example, is a webmail application, but still includes search features; Google Browser Sync does not offer any search facilities, although it aims to organize your browsing time.\n In 2009, Google claimed that a search query requires altogether about 1 kJ or 0.0003 kW·h,[135] which is enough to raise the temperature of one liter of water by 0.24 °C. According to green search engine Ecosia, the industry standard for search engines is estimated to be about 0.2 grams of CO2 emission per search.[136] Google's 40,000 searches per second translate to 8 kg CO2 per second or over 252 million kilos of CO2 per year.[137]\n On certain occasions, the logo on Google's webpage will change to a special version, known as a \"Google Doodle\". This is a picture, drawing, animation, or interactive game that includes the logo. It is usually done for a special event or day although not all of them are well known.[138] Clicking on the Doodle links to a string of Google search results about the topic. The first was a reference to the Burning Man Festival in 1998,[139][140] and others have been produced for the birthdays of notable people like Albert Einstein, historical events like the interlocking Lego block's 50th anniversary and holidays like Valentine's Day.[141] Some Google Doodles have interactivity beyond a simple search, such as the famous \"Google Pac-Man\" version that appeared on May 21, 2010.\n Google has been criticized for placing long-term cookies on users' machines to store preferences, a tactic which also enables them to track a user's search terms and retain the data for more than a year.[142]\n Since 2012, Google Inc. has globally introduced encrypted connections for most of its clients, to bypass governative blockings of the commercial and IT services.[143]\n In 2003, The New York Times complained about Google's indexing, claiming that Google's caching of content on its site infringed its copyright for the content.[144] In both Field v. Google and Parker v. Google, the United States District Court of Nevada ruled in favor of Google.[145][146]\n A 2019 New York Times article on Google Search showed that images of child sexual abuse had been found on Google and that the company had been reluctant at times to remove them.[147]\n Google flags search results with the message \"This site may harm your computer\" if the site is known to install malicious software in the background or otherwise surreptitiously. For approximately 40 minutes on January 31, 2009, all search results were mistakenly classified as malware and could therefore not be clicked; instead a warning message was displayed and the user was required to enter the requested URL manually. The bug was caused by human error.[148][149][150][151] The URL of \"/\" (which expands to all URLs) was mistakenly added to the malware patterns file.[149][150]\n In 2007, a group of researchers observed a tendency for users to rely exclusively on Google Search for finding information, writing that \"With the Google interface the user gets the impression that the search results imply a kind of totality. ... In fact, one only sees a small part of what one could see if one also integrates other research tools.\"[152]\n In 2011, Google Search query results have been shown by Internet activist Eli Pariser to be tailored to users, effectively isolating users in what he defined as a filter bubble. Pariser holds algorithms used in search engines such as Google Search responsible for catering \"a personal ecosystem of information\".[153] Although contrasting views have mitigated the potential threat of \"informational dystopia\" and questioned the scientific nature of Pariser's claims,[154] filter bubbles have been mentioned to account for the surprising results of the U.S. presidential election in 2016 alongside fake news and echo chambers, suggesting that Facebook and Google have designed personalized online realities in which \"we only see and hear what we like\".[155]\n In 2012, the US Federal Trade Commission fined Google US$22.5 million for violating their agreement not to violate the privacy of users of Apple's Safari web browser.[156] The FTC was also continuing to investigate if Google's favoring of their own services in their search results violated antitrust regulations.[157]\n In a November 2023 disclosure, during the ongoing antitrust trial against Google, an economics professor at the University of Chicago revealed that Google pays Apple 36% of all search advertising revenue generated when users access Google through the Safari browser. This revelation reportedly caused Google's lead attorney to cringe visibly.[citation needed] The revenue generated from Safari users has been kept confidential, but the 36% figure suggests that it is likely in the tens of billions of dollars.\n Both Apple and Google have argued that disclosing the specific terms of their search default agreement would harm their competitive positions. However, the court ruled that the information was relevant to the antitrust case and ordered its disclosure. This revelation has raised concerns about the dominance of Google in the search engine market and the potential anticompetitive effects of its agreements with Apple.[158]\n Google search engine robots are programmed to use algorithms that understand and predict human behavior. The book, Race After Technology: Abolitionist Tools for the New Jim Code[159] by Ruha Benjamin talks about human bias as a behavior that the Google search engine can recognize. In 2016, some users Google searched \"three Black teenagers\" and images of criminal mugshots of young African American teenagers came up. Then, the users searched \"three White teenagers\" and were presented with photos of smiling, happy teenagers. They also searched for \"three Asian teenagers\", and very revealing photos of Asian girls and women appeared. Benjamin concluded that these results reflect human prejudice and views on different ethnic groups. A group of analysts explained the concept of a racist computer program: \"The idea here is that computers, unlike people, can't be racist but we're increasingly learning that they do in fact take after their makers ... Some experts believe that this problem might stem from the hidden biases in the massive piles of data that the algorithms process as they learn to recognize patterns ... reproducing our worst values\".[159]\n On August 5, 2024, Google lost a lawsuit which started in 2020 in D.C. Circuit Court, with Judge Amit Mehta finding that the company had an illegal monopoly over Internet search.[160] This monopoly was held to be in violation of Section 2 of the Sherman Act.[161] Google has said it will appeal the ruling[162], though they did propose to loosen search deals with Apple and others requiring them to set Google as the default search engine.[163]\n As people talk about \"googling\" rather than searching, the company has taken some steps to defend its trademark, in an effort to prevent it from becoming a generic trademark.[164][165] This has led to lawsuits, threats of lawsuits, and the use of euphemisms, such as calling Google Search a famous web search engine.[166]\n Until May 2013, Google Search had offered a feature to translate search queries into other languages. A Google spokesperson told Search Engine Land that \"Removing features is always tough, but we do think very hard about each decision and its implications for our users. Unfortunately, this feature never saw much pick up\".[167]\n Instant search was announced in September 2010 as a feature that displayed suggested results while the user typed in their search query, initially only in select countries or to registered users.[168] The primary advantage of the new system was its ability to save time, with Marissa Mayer, then-vice president of search products and user experience, proclaiming that the feature would save 2–5 seconds per search, elaborating that \"That may not seem like a lot at first, but it adds up. With Google Instant, we estimate that we'll save our users 11 hours with each passing second!\"[169] Matt Van Wagner of Search Engine Land wrote that \"Personally, I kind of like Google Instant and I think it represents a natural evolution in the way search works\", and also praised Google's efforts in public relations, writing that \"With just a press conference and a few well-placed interviews, Google has parlayed this relatively minor speed improvement into an attention-grabbing front-page news story\".[170] The upgrade also became notable for the company switching Google Search's underlying technology from HTML to AJAX.[171]\n Instant Search could be disabled via Google's \"preferences\" menu for those who didn't want its functionality.[172]\n The publication 2600: The Hacker Quarterly compiled a list of words that Google Instant did not show suggested results for, with a Google spokesperson giving the following statement to Mashable:[173]\n There are several reasons you may not be seeing search queries for a particular topic. Among other things, we apply a narrow set of removal policies for pornography, violence, and hate speech. It's important to note that removing queries from Autocomplete is a hard problem, and not as simple as blacklisting particular terms and phrases.\n In search, we get more than one billion searches each day. Because of this, we take an algorithmic approach to removals, and just like our search algorithms, these are imperfect. We will continue to work to improve our approach to removals in Autocomplete, and are listening carefully to feedback from our users.\n \nOur algorithms look not only at specific words, but compound queries based on those words, and across all languages. So, for example, if there's a bad word in Russian, we may remove a compound word including the transliteration of the Russian word into English. We also look at the search results themselves for given queries. So, for example, if the results for a particular query seem pornographic, our algorithms may remove that query from Autocomplete, even if the query itself wouldn't otherwise violate our policies. This system is neither perfect nor instantaneous, and we will continue to work to make it better. PC Magazine discussed the inconsistency in how some forms of the same topic are allowed; for instance, \"lesbian\" was blocked, while \"gay\" was not, and \"cocaine\" was blocked, while \"crack\" and \"heroin\" were not. The report further stated that seemingly normal words were also blocked due to pornographic innuendos, most notably \"scat\", likely due to having two completely separate contextual meanings, one for music and one for a sexual practice.[174]\n On July 26, 2017, Google removed Instant results, due to a growing number of searches on mobile devices, where interaction with search, as well as screen sizes, differ significantly from a computer.[175][176]\n \"Instant previews\" allowed previewing screenshots of search results' web pages without having to open them. The feature was introduced in November 2010 to the desktop website and removed in April 2013 citing low usage.[177][178]\n Various search engines provide encrypted Web search facilities. In May 2010 Google rolled out SSL-encrypted web search.[179] The encrypted search was accessed at encrypted.google.com[180] However, the web search is encrypted via Transport Layer Security (TLS) by default today, thus every search request should be automatically encrypted if TLS is supported by the web browser.[181] On its support website, Google announced that the address encrypted.google.com would be turned off April 30, 2018, stating that all Google products and most new browsers use HTTPS connections as the reason for the discontinuation.[182]\n Google Real-Time Search was a feature of Google Search in which search results also sometimes included real-time information from sources such as Twitter, Facebook, blogs, and news websites.[183] The feature was introduced on December 7, 2009,[184] and went offline on July 2, 2011, after the deal with Twitter expired.[185] Real-Time Search included Facebook status updates beginning on February 24, 2010.[186] A feature similar to Real-Time Search was already available on Microsoft's Bing search engine, which showed results from Twitter and Facebook.[187] The interface for the engine showed a live, descending \"river\" of posts in the main region (which could be paused or resumed), while a bar chart metric of the frequency of posts containing a certain search term or hashtag was located on the right hand corner of the page above a list of most frequently reposted posts and outgoing links. Hashtag search links were also supported, as were \"promoted\" tweets hosted by Twitter (located persistently on top of the river) and thumbnails of retweeted image or video links.\n In January 2011, geolocation links of posts were made available alongside results in Real-Time Search. In addition, posts containing syndicated or attached shortened links were made searchable by the link: query option. In July 2011, Real-Time Search became inaccessible, with the Real-Time link in the Google sidebar disappearing and a custom 404 error page generated by Google returned at its former URL. Google originally suggested that the interruption was temporary and related to the launch of Google+;[188] they subsequently announced that it was due to the expiry of a commercial arrangement with Twitter to provide access to tweets.[189]\n"
    },
    {
        "title": "Recommender system",
        "url": "https://en.wikipedia.org/wiki/Recommendation_systems",
        "content": "\n A recommender system (RecSys), or a recommendation system (sometimes replacing system with terms such as platform, engine, or algorithm), is a subclass of information filtering system that provides suggestions for items that are most pertinent to a particular user.[1][2][3] Recommender systems are particularly useful when an individual needs to choose an item from a potentially overwhelming number of items that a service may offer.[1][4]\n Typically, the suggestions refer to various decision-making processes, such as what product to purchase, what music to listen to, or what online news to read.[1]\nRecommender systems are used in a variety of areas, with commonly recognised examples taking the form of playlist generators for video and music services, product recommenders for online stores, or content recommenders for social media platforms and open web content recommenders.[5][6] These systems can operate using a single type of input, like music, or multiple inputs within and across platforms like news, books and search queries. There are also popular recommender systems for specific topics like restaurants and online dating. Recommender systems have also been developed to explore research articles and experts,[7] collaborators,[8] and financial services.[9]\n A content discovery platform is an implemented software recommendation platform which uses recommender system tools. It utilizes user metadata in order to discover and recommend appropriate content, whilst reducing ongoing maintenance and development costs. A content discovery platform delivers personalized content to websites, mobile devices and set-top boxes. A large range of content discovery platforms currently exist for various forms of content ranging from news articles and academic journal articles[10] to television.[11] As operators compete to be the gateway to home entertainment, personalized television is a key service differentiator. Academic content discovery has recently become another area of interest, with several companies being established to help academic researchers keep up to date with relevant academic content and serendipitously discover new content.[10]\n Recommender systems usually make use of either or both collaborative filtering and content-based filtering, as well as other systems such as knowledge-based systems. Collaborative filtering approaches build a model from a user's past behavior (items previously purchased or selected and/or numerical ratings given to those items) as well as similar decisions made by other users. This model is then used to predict items (or ratings for items) that the user may have an interest in.[12] Content-based filtering approaches utilize a series of discrete, pre-tagged characteristics of an item in order to recommend additional items with similar properties.[13]\n The differences between collaborative and content-based filtering can be demonstrated by comparing two early music recommender systems, Last.fm and Pandora Radio.\n Each type of system has its strengths and weaknesses. In the above example, Last.fm requires a large amount of information about a user to make accurate recommendations. This is an example of the cold start problem, and is common in collaborative filtering systems.[15][16][17][18][19][20] Whereas Pandora needs very little information to start, it is far more limited in scope (for example, it can only make recommendations that are similar to the original seed).\n Recommender systems are a useful alternative to search algorithms since they help users discover items they might not have found otherwise. Of note, recommender systems are often implemented using search engines indexing non-traditional data.\n Recommender systems have been the focus of several granted patents,[21][22][23][24][25] and there are more than 50 software libraries[26] that support the development of recommender systems including LensKit,[27][28] RecBole,[29] ReChorus[30] and RecPack.[31]\n Elaine Rich created the first recommender system in 1979, called Grundy.[32][33] She looked for a way to recommend users books they might like. Her idea was to create a system that asks users specific questions and classifies them into classes of preferences, or \"stereotypes\", depending on their answers. Depending on users' stereotype membership, they would then get recommendations for books they might like.\n Another early recommender system, called a \"digital bookshelf\", was described in a 1990 technical report by Jussi Karlgren at Columbia University,\n[34]\nand implemented at scale and worked through in technical reports and publications from 1994 onwards by Jussi Karlgren, then at SICS,[35][36]\nand research groups led by Pattie Maes at MIT,[37] Will Hill at Bellcore,[38] and Paul Resnick, also at MIT,[39][4] whose work with GroupLens was awarded the 2010 ACM Software Systems Award.\n Montaner provided the first overview of recommender systems from an intelligent agent perspective.[40] Adomavicius provided a new, alternate overview of recommender systems.[41]  Herlocker provides an additional overview of evaluation techniques for recommender systems,[42] and Beel et al. discussed the problems of offline evaluations.[43] Beel et al. have also provided literature surveys on available research paper recommender systems and existing challenges.[44][45]\n One approach to the design of recommender systems that has wide use is collaborative filtering.[46] Collaborative filtering is based on the assumption that people who agreed in the past will agree in the future, and that they will like similar kinds of items as they liked in the past. The system generates recommendations using only information about rating profiles for different users or items. By locating peer users/items with a rating history similar to the current user or item, they generate recommendations using this neighborhood. Collaborative filtering methods are classified as memory-based and model-based. A well-known example of memory-based approaches is the user-based algorithm,[47] while that of model-based approaches is matrix factorization (recommender systems).[48]\n A key advantage of the collaborative filtering approach is that it does not rely on machine analyzable content and therefore it is capable of accurately recommending complex items such as movies without requiring an \"understanding\" of the item itself. Many algorithms have been used in measuring user similarity or item similarity in recommender systems. For example, the k-nearest neighbor (k-NN) approach[49] and the Pearson Correlation as first implemented by Allen.[50]\n When building a model from a user's behavior, a distinction is often made between explicit and implicit forms of data collection.\n Examples of explicit data collection include the following:\n Examples of implicit data collection include the following:\n Collaborative filtering approaches often suffer from three problems: cold start, scalability, and sparsity.[52]\n One of the most famous examples of collaborative filtering is item-to-item collaborative filtering (people who buy x also buy y), an algorithm popularized by Amazon.com's recommender system.[54]\n Many social networks originally used collaborative filtering to recommend new friends, groups, and other social connections by examining the network of connections between a user and their friends.[1] Collaborative filtering is still used as part of hybrid systems.\n Another common approach when designing recommender systems is content-based filtering. Content-based filtering methods are based on a description of the item and a profile of the user's preferences.[55][56] These methods are best suited to situations where there is known data on an item (name, location, description, etc.), but not on the user. Content-based recommenders treat recommendation as a user-specific classification problem and learn a classifier for the user's likes and dislikes based on an item's features.\n In this system, keywords are used to describe the items, and a user profile is built to indicate the type of item this user likes. In other words, these algorithms try to recommend items similar to those that a user liked in the past or is examining in the present. It does not rely on a user sign-in mechanism to generate this often temporary profile. In particular, various candidate items are compared with items previously rated by the user, and the best-matching items are recommended. This approach has its roots in information retrieval and information filtering research.\n To create a user profile, the system mostly focuses on two types of information:\n Basically, these methods use an item profile (i.e., a set of discrete attributes and features) characterizing the item within the system. To abstract the features of the items in the system, an item presentation algorithm is applied. A widely used algorithm is the tf–idf representation (also called vector space representation).[57] The system creates a content-based profile of users based on a weighted vector of item features. The weights denote the importance of each feature to the user and can be computed from individually rated content vectors using a variety of techniques. Simple approaches use the average values of the rated item vector while other sophisticated methods use machine learning techniques such as Bayesian Classifiers, cluster analysis, decision trees, and artificial neural networks in order to estimate the probability that the user is going to like the item.[58]\n A key issue with content-based filtering is whether the system can learn user preferences from users' actions regarding one content source and use them across other content types. When the system is limited to recommending content of the same type as the user is already using, the value from the recommendation system is significantly less than when other content types from other services can be recommended. For example, recommending news articles based on news browsing is useful. Still, it would be much more useful when music, videos, products, discussions, etc., from different services, can be recommended based on news browsing. To overcome this, most content-based recommender systems now use some form of the hybrid system.\n Content-based recommender systems can also include opinion-based recommender systems. In some cases, users are allowed to leave text reviews or feedback on the items. These user-generated texts are implicit data for the recommender system because they are potentially rich resources of both feature/aspects of the item and users' evaluation/sentiment to the item. Features extracted from the user-generated reviews are improved metadata of items, because as they also reflect aspects of the item like metadata, extracted features are widely concerned by the users. Sentiments extracted from the reviews can be seen as users' rating scores on the corresponding features. Popular approaches of opinion-based recommender system utilize various techniques including text mining, information retrieval, sentiment analysis (see also Multimodal sentiment analysis) and deep learning.[59]\n Most recommender systems now use a hybrid approach, combining collaborative filtering, content-based filtering, and other approaches. There is no reason why several different techniques of the same type could not be hybridized. Hybrid approaches can be implemented in several ways: by making content-based and collaborative-based predictions separately and then combining them; by adding content-based capabilities to a collaborative-based approach (and vice versa); or by unifying the approaches into one model.[41] Several studies that empirically compared the performance of the hybrid with the pure collaborative and content-based methods and demonstrated that the hybrid methods can provide more accurate recommendations than pure approaches. These methods can also be used to overcome some of the common problems in recommender systems such as cold start and the sparsity problem, as well as the knowledge engineering bottleneck in knowledge-based approaches.[60]\n Netflix is a good example of the use of hybrid recommender systems.[61] The website makes recommendations by comparing the watching and searching habits of similar users (i.e., collaborative filtering) as well as by offering movies that share characteristics with films that a user has rated highly (content-based filtering).\n Some hybridization techniques include:\n These recommender systems use the interactions of a user within a session[63] to generate recommendations. Session-based recommender systems are used at YouTube[64] and Amazon.[65] These are particularly useful when history (such as past clicks, purchases) of a user is not available or not relevant in the current user session. Domains, where session-based recommendations are particularly relevant, include video, e-commerce, travel, music and more. Most instances of session-based recommender systems rely on the sequence of recent interactions within a session without requiring any additional details (historical, demographic) of the user. Techniques for session-based recommendations are mainly based on generative sequential models such as recurrent neural networks,[63][66] transformers,[67] and other deep-learning-based approaches.[68][69]\n The recommendation problem can be seen as a special instance of a reinforcement learning problem whereby the user is the environment upon which the agent, the recommendation system acts upon in order to receive a reward, for instance, a click or engagement by the user.[64][70][71] One aspect of reinforcement learning that is of particular use in the area of recommender systems is the fact that the models or policies can be learned by providing a reward to the recommendation agent. This is in contrast to traditional learning techniques which rely on supervised learning approaches that are less flexible, reinforcement learning recommendation techniques allow to potentially train models that can be optimized directly on metrics of engagement, and user interest.[72]\n Multi-criteria recommender systems (MCRS) can be defined as recommender systems that incorporate preference information upon multiple criteria. Instead of developing recommendation techniques based on a single criterion value, the overall preference of user u for the item i, these systems try to predict a rating for unexplored items of u by exploiting preference information on multiple criteria that affect this overall preference value. Several researchers approach MCRS as a multi-criteria decision making (MCDM) problem, and apply MCDM methods and techniques to implement MCRS systems.[73] See this chapter[74] for an extended introduction.\n The majority of existing approaches to recommender systems focus on recommending the most relevant content to users using contextual information, yet do not take into account the risk of disturbing the user with unwanted notifications. It is important to consider the risk of upsetting the user by pushing recommendations in certain circumstances, for instance, during a professional meeting, early morning, or late at night. Therefore, the performance of the recommender system depends in part on the degree to which it has incorporated the risk into the recommendation process. One option to manage this issue is DRARS, a system which models the context-aware recommendation as a bandit problem. This system combines a content-based technique and a contextual bandit algorithm.[75]\n Mobile recommender systems make use of internet-accessing smartphones to offer personalized, context-sensitive recommendations. This is a particularly difficult area of research as mobile data is more complex than data that recommender systems often have to deal with. It is heterogeneous, noisy, requires spatial and temporal auto-correlation, and has validation and generality problems.[76]\n There are three factors that could affect the mobile recommender systems and the accuracy of prediction results: the context, the recommendation method and privacy.[77] Additionally, mobile recommender systems suffer from a transplantation problem – recommendations may not apply in all regions (for instance, it would be unwise to recommend a recipe in an area where all of the ingredients may not be available).\n One example of a mobile recommender system are the approaches taken by companies such as Uber and Lyft to generate driving routes for taxi drivers in a city.[76] This system uses GPS data of the routes that taxi drivers take while working, which includes location (latitude and longitude), time stamps, and operational status (with or without passengers). It uses this data to recommend a list of pickup points along a route, with the goal of optimizing occupancy times and profits.\n Generative recommenders (GR) represent an approach that transforms recommendation tasks into sequential transduction problems, where user actions are treated like tokens in a generative modeling framework. In one method, known as HSTU (Hierarchical Sequential Transduction Units),[78] high-cardinality, non-stationary, and streaming datasets are efficiently processed as sequences, enabling the model to learn from trillions of parameters and to handle user action histories orders of magnitude longer than before. By turning all of the system’s varied data into a single stream of tokens and using a custom self-attention approach instead of traditional neural network layers, generative recommenders make the model much simpler and less memory-hungry. As a result, it can improve recommendation quality in test simulations and in real-world tests, while being faster than previous Transformer-based systems when handling long lists of user actions. Ultimately, this approach allows the model’s performance to grow steadily as more computing power is used, laying a foundation for efficient and scalable “foundation models” for recommendations.\n One of the events that energized research in recommender systems was the Netflix Prize. From 2006 to 2009, Netflix sponsored a competition, offering a grand prize of $1,000,000 to the team that could take an offered dataset of over 100 million movie ratings and return recommendations that were 10% more accurate than those offered by the company's existing recommender system. This competition energized the search for new and more accurate algorithms. On 21 September 2009, the grand prize of US$1,000,000 was given to the BellKor's Pragmatic Chaos team using tiebreaking rules.[79]\n The most accurate algorithm in 2007 used an ensemble method of 107 different algorithmic approaches, blended into a single prediction. As stated by the winners, Bell et al.:[80]\n \nPredictive accuracy is substantially improved when blending multiple predictors. Our experience is that most efforts should be concentrated in deriving substantially different approaches, rather than refining a single technique.  Consequently, our solution is an ensemble of many methods. Many benefits accrued to the web due to the Netflix project. Some teams have taken their technology and applied it to other markets. Some members from the team that finished second place founded Gravity R&D, a recommendation engine that's active in the RecSys community.[79][81] 4-Tell, Inc. created a Netflix project–derived solution for ecommerce websites.\n A number of privacy issues arose around the dataset offered by Netflix for the Netflix Prize competition. Although the data sets were anonymized in order to preserve customer privacy, in 2007 two researchers from the University of Texas were able to identify individual users by matching the data sets with film ratings on the Internet Movie Database (IMDb).[82] As a result, in December 2009, an anonymous Netflix user sued Netflix in Doe v. Netflix, alleging that Netflix had violated United States fair trade laws and the Video Privacy Protection Act by releasing the datasets.[83] This, as well as concerns from the Federal Trade Commission, led to the cancellation of a second Netflix Prize competition in 2010.[84]\n Evaluation is important in assessing the effectiveness of recommendation algorithms. To measure the effectiveness of recommender systems, and compare different approaches, three types of evaluations are available: user studies, online evaluations (A/B tests), and offline evaluations.[43]\n The commonly used metrics are the mean squared error and root mean squared error, the latter having been used in the Netflix Prize. The information retrieval metrics such as precision and recall or DCG are useful to assess the quality of a recommendation method. Diversity, novelty, and coverage are also considered as important aspects in evaluation.[85] However, many of the classic evaluation measures are highly criticized.[86]\n Evaluating the performance of a recommendation algorithm on a fixed test dataset will always be extremely challenging as it is impossible to accurately predict the reactions of real users to the recommendations. Hence any metric that computes the effectiveness of an algorithm in offline data will be imprecise.\n User studies are rather a small scale. A few dozens or hundreds of users are presented recommendations created by different recommendation approaches, and then the users judge which recommendations are best.\n In A/B tests, recommendations are shown to typically thousands of users of a real product, and the recommender system randomly picks at least two different recommendation approaches to generate recommendations. The effectiveness is measured with implicit measures of effectiveness such as conversion rate or click-through rate.\n Offline evaluations are based on historic data, e.g. a dataset that contains information about how users previously rated movies.[87]\n The effectiveness of recommendation approaches is then measured based on how well a recommendation approach can predict the users' ratings in the dataset. While a rating is an explicit expression of whether a user liked a movie, such information is not available in all domains. For instance, in the domain of citation recommender systems, users typically do not rate a citation or recommended article. In such cases, offline evaluations may use implicit measures of effectiveness. For instance, it may be assumed that a recommender system is effective that is able to recommend as many articles as possible that are contained in a research article's reference list. However, this kind of offline evaluations is seen critical by many researchers.[88][89][90][43] For instance, it has been shown that results of offline evaluations have low correlation with results from user studies or A/B tests.[90][91] A dataset popular for offline evaluation has been shown to contain duplicate data and thus to lead to wrong conclusions in the evaluation of algorithms.[92] Often, results of so-called offline evaluations do not correlate with actually assessed user-satisfaction.[93] This is probably because offline training is highly biased toward the highly reachable items, and offline testing data is highly influenced by the outputs of the online recommendation module.[88][94] Researchers have concluded that the results of offline evaluations should be viewed critically.[95]\n Typically, research on recommender systems is concerned with finding the most accurate recommendation algorithms. However, there are a number of factors that are also important.\n Recommender systems are notoriously difficult to evaluate offline, with some researchers claiming that this has led to a reproducibility crisis in recommender systems publications. The topic of reproducibility seems to be a recurrent issue in some Machine Learning publication venues, but does not have a considerable effect beyond the world of scientific publication. In the context of recommender systems a 2019 paper surveyed a small number of hand-picked publications applying deep learning or neural methods to the top-k recommendation problem, published in top conferences (SIGIR, KDD, WWW, RecSys, IJCAI), has shown that on average less than 40% of articles could be reproduced by the authors of the survey, with as little as 14% in some conferences. The articles considers a number of potential problems in today's research scholarship and suggests improved scientific practices in that area.[108][109][110]\nMore recent work on benchmarking a set of the same methods came to qualitatively very different results[111] whereby neural methods were found to be among the best performing methods. Deep learning and neural methods for recommender systems have been used in the winning solutions in several recent recommender system challenges, WSDM,[112] RecSys Challenge.[113]\nMoreover, neural and deep learning methods are widely used in industry where they are extensively tested.[114][64][65] The topic of reproducibility is not new in recommender systems. By 2011, Ekstrand, Konstan, et al. criticized that \"it is currently difficult to reproduce and extend recommender systems research results,\" and that evaluations are \"not handled consistently\".[115] Konstan and Adomavicius conclude that \"the Recommender Systems research community is facing a crisis where a significant number of papers present results that contribute little to collective knowledge [...] often because the research lacks the [...] evaluation to be properly judged and, hence, to provide meaningful contributions.\"[116] As a consequence, much research about recommender systems can be considered as not reproducible.[117] Hence, operators of recommender systems find little guidance in the current research for answering the question, which recommendation approaches to use in a recommender systems. Said and Bellogín conducted a study of papers published in the field, as well as benchmarked some of the most popular frameworks for recommendation and found large inconsistencies in results, even when the same algorithms and data sets were used.[118] Some researchers demonstrated that minor variations in the recommendation algorithms or scenarios led to strong changes in the effectiveness of a recommender system. They conclude that seven actions are necessary to improve the current situation:[117] \"(1) survey other research fields and learn from them, (2) find a common understanding of reproducibility, (3) identify and understand the determinants that affect reproducibility, (4) conduct more comprehensive experiments (5) modernize publication practices, (6) foster the development and use of recommendation frameworks, and (7) establish best-practice guidelines for recommender-systems research.\"\n Artificial intelligence (AI) applications in recommendation systems are the advanced methodologies that leverage AI technologies, to enhance the performance recommendation engines. The AI-based recommender can analyze complex data sets, learning from user behavior, preferences, and interactions to generate highly accurate and personalized content or product suggestions.[119] The integration of AI in recommendation systems has marked a significant evolution from traditional recommendation methods. Traditional methods often relied on inflexible algorithms that could suggest items based on general user trends or apparent similarities in content. In comparison, AI-powered systems have the capability to detect patterns and subtle distinctions that may be overlooked by traditional methods.[120] These systems can adapt to specific individual preferences, thereby offering recommendations that are more aligned with individual user needs. This approach marks a shift towards more personalized, user-centric suggestions.\n Recommendation systems widely adopt AI techniques such as machine learning, deep learning, and natural language processing.[121] These advanced methods enhance system capabilities to predict user preferences and deliver personalized content more accurately. Each technique contributes uniquely. The following sections will introduce specific AI models utilized by a recommendation system by illustrating their theories and functionalities.[citation needed]\n Collaborative filtering (CF) is one of the most commonly used recommendation system algorithms. It generates personalized suggestions for users based on explicit or implicit behavioral patterns to form predictions.[122] Specifically, it relies on external feedback such as star ratings, purchasing history and so on to make judgments. CF make predictions about users' preference based on similarity measurements. Essentially, the underlying theory is: \"if user A is similar to user B, and if A likes item C, then it is likely that B also likes item C.\"\n There are many models available for collaborative filtering. For AI-applied collaborative filtering, a common model is called K-nearest neighbors. The ideas are as follows:\n An artificial neural network (ANN), is a deep learning model structure which aims to mimic a human brain. They comprise a series of neurons, each responsible for receiving and processing information transmitted from other interconnected neurons.[123] Similar to a human brain, these neurons will change activation state based on incoming signals (training input and backpropagated output), allowing the system to adjust activation weights during the network learning phase. ANN is usually designed to be a black-box model. Unlike regular machine learning where the underlying theoretical components are formal and rigid, the collaborative effects of neurons are not entirely clear, but modern experiments has shown the predictive power of ANN.\n ANN is widely used in recommendation systems for its power to utilize various data. Other than feedback data, ANN can incorporate non-feedback data which are too intricate for collaborative filtering to learn, and the unique structure allows ANN to identify extra signal from non-feedback data to boost user experience.[121] Following are some examples:\n The Two-Tower model is a neural architecture[124] commonly employed in large-scale recommendation systems, particularly for candidate retrieval tasks.[125] It consists of two neural networks:\n The outputs of the two towers are fixed-length embeddings that represent users and items in a shared vector space. A similarity metric, such as dot product or cosine similarity, is used to measure relevance between a user and an item.\n This model is highly efficient for large datasets as embeddings can be pre-computed for items, allowing rapid retrieval during inference. It is often used in conjunction with ranking models for end-to-end recommendation pipelines.\n Natural language processing is a series of AI algorithms to make natural human language accessible and analyzable to a machine.[126] It is a fairly modern technique inspired by the growing amount of textual information. For application in recommendation system, a common case is the Amazon customer review. Amazon will analyze the feedbacks comments from each customer and report relevant data to other customers for reference. The recent years have witnessed the development of various text analysis models, including latent semantic analysis (LSA), singular value decomposition (SVD), latent Dirichlet allocation (LDA), etc. Their uses have consistently aimed to provide customers with more precise and tailored recommendations.\n An emerging market for content discovery platforms is academic content.[127][128] Approximately 6000 academic journal articles are published daily, making it increasingly difficult for researchers to balance time management with staying up to date with relevant research.[10] Though traditional tools academic search tools such as Google Scholar or PubMed provide a readily accessible database of journal articles, content recommendation in these cases are performed in a 'linear' fashion, with users setting 'alarms' for new publications based on keywords, journals or particular authors.\n Google Scholar provides an 'Updates' tool that suggests articles by using a statistical model that takes a researchers' authorized paper and citations as input.[10] Whilst these recommendations have been noted to be extremely good, this poses a problem with early career researchers which may be lacking a sufficient body of work to produce accurate recommendations.[10]\n In contrast to an engagement-based ranking system employed by social media and other digital platforms, a bridging-based ranking optimizes for content that is unifying instead of polarizing.[129][130] Examples include Polis and Remesh which have been used around the world to help find more consensus around specific political issues.[130] Twitter has also used this approach for managing its community notes,[131] which YouTube planned to pilot in 2024.[132][133] Aviv Ovadya also argues for implementing bridging-based algorithms in major platforms by empowering deliberative groups that are representative of the platform's users to control the design and implementation of the algorithm.[134]\n As the connected television landscape continues to evolve, search and recommendation are seen as having an even more pivotal role in the discovery of content.[135] With broadband-connected devices, consumers are projected to have access to content from linear broadcast sources as well as internet television. Therefore, there is a risk that the market could become fragmented, leaving it to the viewer to visit various locations and find what they want to watch in a way that is time-consuming and complicated for them. By using a search and recommendation engine, viewers are provided with a central 'portal' from which to discover content from several sources in just one location.\n"
    },
    {
        "title": "YouTube",
        "url": "https://en.wikipedia.org/wiki/YouTube",
        "content": "\n YouTube  is an American social media and online video sharing platform owned by Google. YouTube was founded on February 14, 2005, by Steve Chen, Chad Hurley, and Jawed Karim, three former employees of PayPal. Headquartered in San Bruno, California, it is the second-most-visited website in the world, after Google Search. In January 2024, YouTube had more than 2.7 billion monthly active users, who collectively watched more than one billion hours of videos every day.[7] As of May 2019[update], videos were being uploaded to the platform at a rate of more than 500 hours of content per minute,[8][9] and as of 2023[update], there were approximately 14 billion videos in total.[10]\n On October 9, 2006, YouTube was purchased by Google for $1.65 billion (equivalent to $2.31 billion in 2023).[11] Google expanded YouTube's business model of generating revenue from advertisements alone, to offering paid content such as movies and exclusive content produced by and for YouTube. It also offers YouTube Premium, a paid subscription option for watching content without ads. YouTube incorporated Google's AdSense program, generating more revenue for both YouTube and approved content creators. In 2023, YouTube's advertising revenue totaled $31.7 billion, a 2% increase from the $31.1 billion reported in 2022.[12] From Q4 2023 to Q3 2024, YouTube's combined revenue from advertising and subscriptions exceeded $50 billion.[13]\n Since its purchase by Google, YouTube has expanded beyond the core website into mobile apps, network television, and the ability to link with other platforms. Video categories on YouTube include music videos, video clips, news, short and feature films, songs, documentaries, movie trailers, teasers, TV spots, live streams, vlogs, and more. Most content is generated by individuals, including collaborations between \"YouTubers\" and corporate sponsors. Established media, news, and entertainment corporations have also created and expanded their visibility to YouTube channels to reach greater audiences.\n YouTube has had unprecedented social impact, influencing popular culture, internet trends, and creating multimillionaire celebrities. Despite its growth and success, the platform has been criticized for its facilitation of the spread of misinformation and copyrighted content, routinely violating its users' privacy, excessive censorship, endangering the safety of children and their well-being, and for its inconsistent implementation of platform guidelines.\n YouTube was founded by Steve Chen, Chad Hurley, and Jawed Karim. The trio were early employees of PayPal, which left them enriched after the company was bought by eBay.[14] Hurley had studied design at the Indiana University of Pennsylvania, and Chen and Karim studied computer science together at the University of Illinois Urbana-Champaign.[15]\n According to a story that has often been repeated in the media, Hurley and Chen developed the idea for YouTube during the early months of 2005, after they had experienced difficulty sharing videos that had been shot at a dinner party at Chen's apartment in San Francisco. Karim did not attend the party and denied that it had occurred, but Chen remarked that the idea that YouTube was founded after a dinner party \"was probably very strengthened by marketing ideas around creating a story that was very digestible\".[16]\n Karim said the inspiration for YouTube came from the Super Bowl XXXVIII halftime show controversy when Janet Jackson's breast was briefly exposed by Justin Timberlake during the halftime show. Karim could not easily find video clips of the incident and the 2004 Indian Ocean Tsunami online, which led to the idea of a video-sharing site.[17][18] Hurley and Chen said that the original idea for YouTube was a video version of an online dating service and had been influenced by the website Hot or Not.[16][19] They created posts on Craigslist asking attractive women to upload videos of themselves to YouTube in exchange for a $100 reward.[20] Difficulty in finding enough dating videos led to a change of plans, with the site's founders deciding to accept uploads of any video.[21]\n YouTube began as a venture capital–funded technology startup. Between November 2005 and April 2006, the company raised money from various investors, with Sequoia Capital and Artis Capital Management being the largest two.[14][22] YouTube's early headquarters were situated above a pizzeria and a Japanese restaurant in San Mateo, California.[23] In February 2005, the company activated www.youtube.com.[24] The first video was uploaded on April 23, 2005. Titled \"Me at the zoo\", it shows co-founder Jawed Karim at the San Diego Zoo and can still be viewed on the site.[25][26] The same day, the company launched a public beta and by November, a Nike ad featuring Ronaldinho became the first video to reach one million total views.[27][28] The site launched officially on December 15, 2005, by which time the site was receiving 8 million views a day.[29][30] Clips at the time were limited to 100 megabytes, as little as 30 seconds of footage.[31]\n YouTube was not the first video-sharing site on the Internet; Vimeo was launched in November 2004, though that site remained a side project of its developers from CollegeHumor.[32] The week of YouTube's launch, NBCUniversal Saturday Night Live ran a skit \"Lazy Sunday\" by The Lonely Island. Besides helping to bolster ratings and long-term viewership for Saturday Night Live, \"Lazy Sunday\"'s status as an early viral video helped establish YouTube as an important website.[33] Unofficial uploads of the skit to YouTube drew in more than five million collective views by February 2006 before they were removed when NBCUniversal requested it two months later based on copyright concerns.[34] Despite eventually being taken down, these duplicate uploads of the skit helped popularize YouTube's reach and led to the upload of more third-party content.[35][36] The site grew rapidly; in July 2006, the company announced that more than 65,000 new videos were being uploaded every day and that the site was receiving 100 million video views per day.[37]\n The choice of the name youtube.com led to problems for a similarly named website, utube.com. That site's owner, Universal Tube & Rollform Equipment, filed a lawsuit against YouTube in November 2006, after being regularly overloaded by people looking for YouTube. Universal Tube subsequently changed its website to www.utubeonline.com.[38][39]\n On October 9, 2006, Google announced that they had acquired YouTube for $1.65 billion in Google stock.[40][41] The deal was finalized on November 13, 2006.[42][43] Google's acquisition launched newfound interest in video-sharing sites; IAC, which now owned Vimeo, focused on supporting the content creators to distinguish itself from YouTube.[32] It is at this time YouTube issued the slogan \"Broadcast Yourself.\"\nThe company experienced rapid growth. The Daily Telegraph wrote that in 2007, YouTube consumed as much bandwidth as the entire Internet in 2000.[44] By 2010, the company had reached a market share of around 43% and more than 14 billion views of videos, according to comScore.[45] That year, the company simplified its interface to increase the time users would spend on the site.[46] In 2011, more than three billion videos were being watched each day with 48 hours of new videos uploaded every minute.[47][48][49] However, most of these views came from a relatively small number of videos; according to a software engineer at that time, 30% of videos accounted for 99% of views on the site.[50] That year, the company again changed its interface and at the same time, introduced a new logo with a darker shade of red.[51][52] A subsequent interface change, designed to unify the experience across desktop, TV, and mobile, was rolled out in 2013.[53] By that point, more than 100 hours were being uploaded every minute, increasing to 300 hours by November 2014.[54][55]\n During that time, the company also went through some organizational changes. In October 2006, YouTube moved to a new office in San Bruno, California.[56] Hurley announced that he would be stepping down as chief executive officer of YouTube to take an advisory role and that Salar Kamangar would take over as head of the company in October 2010.[57] In December 2009, YouTube partnered with Vevo.[58] In April 2010, Lady Gaga's \"Bad Romance\" became the most-viewed video, becoming the first video to reach 200 million views on May 9, 2010.[59]\n YouTube faced a major lawsuit by Viacom International in 2011 that nearly resulted in the discontinuation of the website. The lawsuit was filed due to alleged copyright infringement of Viacom's material by YouTube. However, the United States Court of Appeals for the Second Circuit ruled that YouTube was not liable, and thus YouTube won the case in 2012.[60]\n Susan Wojcicki was appointed CEO of YouTube in February 2014.[61] In January 2016, YouTube expanded its headquarters in San Bruno by purchasing an office park for $215 million. The complex has 51,468 square metres (554,000 square feet) of space and can house up to 2,800 employees.[62] YouTube officially launched the \"polymer\" redesign of its user interfaces based on Material Design language as its default, as well a redesigned logo that is built around the service's play button emblem in August 2017.[63]\n Through this period, YouTube tried several new ways to generate revenue beyond advertisements. In 2013, YouTube launched a pilot program for content providers to offer premium, subscription-based channels.[64][65] This effort was discontinued in January 2018 and relaunched in June, with US$4.99 channel subscriptions.[66][67] These channel subscriptions complemented the existing Super Chat ability, launched in 2017, which allows viewers to donate between $1 and $500 to have their comment highlighted.[68] In 2014, YouTube announced a subscription service known as \"Music Key\", which bundled ad-free streaming of music content on YouTube with the existing Google Play Music service.[69] The service continued to evolve in 2015 when YouTube announced YouTube Red, a new premium service that would offer ad-free access to all content on the platform (succeeding the Music Key service released the previous year), premium original series, and films produced by YouTube personalities, as well as background playback of content on mobile devices. YouTube also released YouTube Music, a third app oriented towards streaming and discovering the music content hosted on the YouTube platform.[70][71][72]\n The company also attempted to create products appealing to specific viewers. YouTube released a mobile app known as YouTube Kids in 2015, designed to provide an experience optimized for children. It features a simplified user interface, curated selections of channels featuring age-appropriate content, and parental control features.[73] Also in 2015, YouTube launched YouTube Gaming—a video gaming-oriented vertical and app for videos and live streaming, intended to compete with the Amazon.com-owned Twitch.[74]\n The company was attacked on April 3, 2018, when a shooting occurred at YouTube's headquarters in San Bruno, California, which wounded four and resulted in the death of the shooter.[75]\n By February 2017, one billion hours of YouTube videos were being watched every day, and 400 hours worth of videos were uploaded every minute.[7][76] Two years later, the uploads had risen to more than 500 hours per minute.[8] During the COVID-19 pandemic, when most of the world was under stay-at-home orders, usage of services like YouTube significantly increased. One data firm[which?] estimated that YouTube was accounting for 15% of all internet traffic, twice its pre-pandemic level.[77] In response to EU officials requesting that such services reduce bandwidth as to make sure medical entities had sufficient bandwidth to share information, YouTube and Netflix said they would reduce streaming quality for at least thirty days as to cut bandwidth use of their services by 25% to comply with the EU's request.[78] YouTube later announced that they would continue with this move worldwide: \"We continue to work closely with governments and network operators around the globe to do our part to minimize stress on the system during this unprecedented situation.\"[79]\n After a 2018 complaint alleging violations of the Children's Online Privacy Protection Act (COPPA),[80] the company was fined $170 million by the FTC for collecting personal information from minors under the age of 13.[81] YouTube was also ordered to create systems to increase children's privacy.[82][83] Following criticisms of its implementation of those systems, YouTube started treating all videos designated as \"made for kids\" as liable under COPPA on January 6, 2020.[84][85] Joining the YouTube Kids app, the company created a supervised mode, designed more for tweens, in 2021.[86] Additionally, to compete with TikTok and Instagram Reels, YouTube released YouTube Shorts, a short-form video platform.[87]\n During that period, YouTube entered disputes with other tech companies. For over a year, in 2018 and 2019, no YouTube app was available for Amazon Fire products.[88] In 2020, Roku removed the YouTube TV app from its streaming store after the two companies were unable to reach an agreement.[89]\n After testing earlier in 2021, YouTube removed public display of dislike counts on videos in November 2021, claiming the reason for the removal was, based on its internal research, that users often used the dislike feature as a form of cyberbullying and brigading.[90] While some users praised the move as a way to discourage trolls, others felt that hiding dislikes would make it harder for viewers to recognize clickbait or unhelpful videos and that other features already existed for creators to limit bullying. YouTube co-founder Jawed Karim referred to the update as \"a stupid idea\", and that the real reason behind the change was \"not a good one, and not one that will be publicly disclosed.\" He felt that users' ability on a social platform to identify harmful content was essential, saying, \"The process works, and there's a name for it: the wisdom of the crowds. The process breaks when the platform interferes with it. Then, the platform invariably declines.\"[91][92][93] Shortly after the announcement, software developer Dmitry Selivanov created Return YouTube Dislike, an open-source, third-party browser extension for Chrome and Firefox that allows users to see a video's number of dislikes.[94] In a letter published on January 25, 2022, by then YouTube CEO Susan Wojcicki, acknowledged that removing public dislike counts was a controversial decision, but reiterated that she stands by this decision, claiming that \"it reduced dislike attacks.\"[95]\n In 2022, YouTube launched an experiment where the company would show users who watched longer videos on TVs a long chain of short unskippable adverts, intending to consolidate all ads into the beginning of a video. Following public outrage over the unprecedented amount of unskippable ads, YouTube \"ended\" the experiment on September 19 of that year.[96] In October, YouTube announced that they would be rolling out customizable user handles in addition to channel names, which would also become channel URLs.[97]\n On February 16, 2023, Wojcicki announced that she would step down as CEO, with Neal Mohan named as her successor. Wojcicki took on an advisory role for Google and parent company Alphabet.[98] Wojcicki died a year and a half later from non-small-cell lung cancer, on August 9, 2024.[99]\n In late October 2023, YouTube began cracking down on the use of ad blockers on the platform. Users of ad blockers may be given a pop-up warning saying \"Video player will be blocked after 3 videos\". Users of ad blockers are shown a message asking them to allow ads or inviting them to subscribe to the ad-free YouTube Premium subscription plan. YouTube says that the use of ad blockers violates its terms of service.[100][101] In April 2024, YouTube announced it would be \"strengthening our enforcement on third-party apps that violate YouTube's Terms of Service, specifically ad-blocking apps\".[102] Starting in June 2024, Google Chrome announced that it would be replacing Manifest V2 in favor of Manifest V3, effectively killing support for most ad-blockers.[103] Manifest V3 allows YouTube to inject the ads directly into the video, instead of having the ad as a separate file which can be blocked.[104]\n In September 2023, YouTube announced an in-app gaming platform called Playables. It was made accessible to all users in May 2024, expanding from an initial offering limited to premium subscribers. In December 2024, YouTube began testing a new multiplayer feature for that service, supporting multiplayer functionality across desktop and mobile devices. As of December 2024[update] the Playables catalog has over 130 games in various genres including trivia, action and sports.[105][106]\n In December 2024, YouTube introduced new guidelines in India prohibiting videos with clickbait titles to enhance content quality and combat misinformation. The platform aims to penalize creators using misleading or sensationalized titles, with potential actions including video removal or channel suspension.[107]\n YouTube has been led by a CEO since its founding in 2005, beginning with Chad Hurley, who led the company until 2010. After Google's acquisition of YouTube, the CEO role was retained. Salar Kamangar took over Hurley's position and kept the job until 2014. He was replaced by Susan Wojcicki, who later resigned in 2023.[98] The current CEO is Neal Mohan, who was appointed on February 16, 2023.[98]\n YouTube offers different features based on user verification, such as standard or basic features like uploading videos, creating playlists, and using YouTube Music, with limits based on daily activity (verification via phone number or channel history increases feature availability and daily usage limits); intermediate or additional features like longer videos (over 15 minutes), live streaming, custom thumbnails, and creating podcasts; advanced features like content ID appeals, embedding live streams, applying for monetization, clickable links, adding chapters, and pinning comments on videos or posts.[108]\n In January 2012, it was estimated that visitors to YouTube spent an average of 15 minutes a day on the site, in contrast to the four or five hours a day spent by a typical US citizen watching television.[109] In 2017, viewers on average watched YouTube on mobile devices for more than an hour every day.[110]\n In December 2012, two billion views were removed from the view counts of Universal and Sony music videos on YouTube, prompting a claim by The Daily Dot that the views had been deleted due to a violation of the site's terms of service, which ban the use of automated processes to inflate view counts. That was disputed by Billboard, which said that the two billion views had been moved to Vevo, since the videos were no longer active on YouTube.[111][112] On August 5, 2015, YouTube patched the formerly notorious behavior which caused a video's view count to freeze at \"301\" (later \"301+\") until the actual count was verified to prevent view count fraud.[113] YouTube view counts once again updated in real time.[114] Since September 2019, subscriber counts are abbreviated. Only three leading digits of channels' subscriber counts are indicated publicly, compromising the function of third-party real-time indicators such as that of Social Blade. Exact counts remain available to channel operators inside YouTube Studio.[115]\n On November 11, 2021, after testing out this change in March of the same year, YouTube announced it would start hiding dislike counts on videos, making them invisible to viewers. The company stated the decision was in response to experiments which confirmed that smaller YouTube creators were more likely to be targeted in dislike brigading and harassment. Creators will still be able to see the number of likes and dislikes in the YouTube Studio dashboard tool, according to YouTube.[116][117][118]\n YouTube has an estimated 14 billion videos[10] with about 5% of those never having a view and just over 85% having fewer than 1,000 views.[119]\n YouTube has faced numerous challenges and criticisms in its attempts to deal with copyright, including the site's first viral video, Lazy Sunday, which had to be taken down, due to copyright concerns.[33] At the time of uploading a video, YouTube users are shown a message asking them not to violate copyright laws.[120] Despite this advice, many unauthorized clips of copyrighted material remain on YouTube. YouTube does not view videos before they are posted online, and it is left to copyright holders to issue a DMCA takedown notice pursuant to the terms of the Online Copyright Infringement Liability Limitation Act. Any successful complaint about copyright infringement results in a YouTube copyright strike. Three successful complaints for copyright infringement against a user account will result in the account and all of its uploaded videos being deleted.[121][122] From 2007 to 2009 organizations including Viacom, Mediaset, and the English Premier League have filed lawsuits against YouTube, claiming that it has done too little to prevent the uploading of copyrighted material.[123][124][125]\n In August 2008, a US court ruled in Lenz v. Universal Music Corp. that copyright holders cannot order the removal of an online file without first determining whether the posting reflected fair use of the material.[126] YouTube's owner Google announced in November 2015 that they would help cover the legal cost in select cases where they believe fair use defenses apply.[127]\n In the 2011 case of Smith v. Summit Entertainment LLC, professional singer Matt Smith sued Summit Entertainment for the wrongful use of copyright takedown notices on YouTube.[128] He asserted seven causes of action, and four were ruled in Smith's favor.[129] In April 2012, a court in Hamburg ruled that YouTube could be held responsible for copyrighted material posted by its users.[130] On November 1, 2016, the dispute with GEMA was resolved, with Google content ID being used to allow advertisements to be added to videos with content protected by GEMA.[131]\n In April 2013, it was reported that Universal Music Group and YouTube have a contractual agreement that prevents content blocked on YouTube by a request from UMG from being restored, even if the uploader of the video files a DMCA counter-notice.[132][133] As part of YouTube Music, Universal and YouTube signed an agreement in 2017, which was followed by separate agreements other major labels, which gave the company the right to advertising revenue when its music was played on YouTube.[134] By 2019, creators were having videos taken down or demonetized when Content ID identified even short segments of copyrighted music within a much longer video, with different levels of enforcement depending on the record label.[135] Experts noted that some of these clips said qualified for fair use.[135]\n In June 2007, YouTube began trials of a system for automatic detection of uploaded videos that infringe copyright. Google CEO Eric Schmidt regarded this system as necessary for resolving lawsuits such as the one from Viacom, which alleged that YouTube profited from content that it did not have the right to distribute.[136] The system, which was initially called \"Video Identification\"[137][138] and later became known as Content ID,[139] creates an ID File for copyrighted audio and video material, and stores it in a database. When a video is uploaded, it is checked against the database, and flags the video as a copyright violation if a match is found.[140] When this occurs, the content owner has the choice of blocking the video to make it unviewable, tracking the viewing statistics of the video, or adding advertisements to the video.\n An independent test in 2009 uploaded multiple versions of the same song to YouTube and concluded that while the system was \"surprisingly resilient\" in finding copyright violations in the audio tracks of videos, it was not infallible.[141] The use of Content ID to remove material automatically has led to controversy in some cases, as the videos have not been checked by a human for fair use.[142] If a YouTube user disagrees with a decision by Content ID, it is possible to fill in a form disputing the decision.[143]\n Before 2016, videos were not monetized until the dispute was resolved. Since April 2016, videos continue to be monetized while the dispute is in progress, and the money goes to whoever won the dispute.[144] Should the uploader want to monetize the video again, they may remove the disputed audio in the \"Video Manager\".[145] YouTube has cited the effectiveness of Content ID as one of the reasons why the site's rules were modified in December 2010 to allow some users to upload videos of unlimited length.[146]\n In 2021, two accounts linked to RT Deutsch, the German channel of the Russian RT network were removed as well for breaching YouTube's policies relating to COVID-19.[147] Russia threatened to ban YouTube after the platform deleted two German RT channels in September 2021.[148] Shortly after the Russian invasion of Ukraine in 2022, YouTube removed all channels funded by the Russian state.[149] YouTube expanded the removal of Russian content from its site to include channels described as 'pro-Russian'. In June 2022, the War Gonzo channel run by Russian military blogger and journalist Semyon Pegov was deleted.[150]\n In July 2023, YouTube removed the channel of British journalist Graham Phillips, active in covering the War in Donbas from 2014.[151] In August 2023, a Moscow court fined Google 3 million rubles, around $35,000, for not deleting what it said was \"fake news about the war in Ukraine\".[152]\n In October 2024, a Russian court has fined its parent company Google a grand total of 2 undecillion rubles (equivalent to US$20 decillion) for restricting Russian state media channels on YouTube.[153] The fine imposed by Russia is far greater than the world's total GDP, estimated at US$110 trillion by the International Monetary Fund.[154] News agency TASS reported that Google is allowed to return to the Russian market only if it complies with the court's decision.[155] Kremlin spokesperson Dmitry Peskov labeled the court decision as \"symbolic\" and warned Google that it “should not be restricting the actions of our broadcasters on its platform.”[156]\n YouTube featured an April Fools prank on the site on April 1 of every year from 2008 to 2016. In 2008, all links to videos on the main page were redirected to Rick Astley's music video \"Never Gonna Give You Up\", a prank known as \"rickrolling\".[157][158] The next year, when clicking on a video on the main page, the whole page turned upside down, which YouTube claimed was a \"new layout\".[159] In 2010, YouTube temporarily released a \"TEXTp\" mode which rendered video imagery into ASCII art letters \"in order to reduce bandwidth costs by $1 per second.\"[160]\n The next year, the site celebrated its \"100th anniversary\" with a range of sepia-toned silent, early 1900s-style films, including a parody of Keyboard Cat.[161] In 2012, clicking on the image of a DVD next to the site logo led to a video about a purported option to order every YouTube video for home delivery on DVD.[162] In 2013, YouTube teamed up with satirical newspaper company The Onion to claim in an uploaded video that the video-sharing website was launched as a contest which had finally come to an end, and would shut down for ten years before being re-launched in 2023, featuring only the winning video. The video starred several YouTube celebrities, including Antoine Dodson. A video of two presenters announcing the nominated videos streamed live for 12 hours.[163][164]\n In 2014, YouTube announced that it was responsible for the creation of all viral video trends, and revealed previews of upcoming trends, such as \"Clocking\", \"Kissing Dad\", and \"Glub Glub Water Dance\".[165] The next year, YouTube added a music button to the video bar that played samples from \"Sandstorm\" by Darude.[166] In 2016, YouTube introduced an option to watch every video on the platform in 360-degree mode with Snoop Dogg.[167]\n YouTube Premium (formerly YouTube Red) is YouTube's premium subscription service. It offers advertising-free streaming, access to original programming, and background and offline video playback on mobile devices.[168] YouTube Premium was originally announced on November 12, 2014, as \"Music Key\", a subscription music streaming service, and was intended to integrate with and replace the existing Google Play Music \"All Access\" service.[169][170][171] On October 28, 2015, the service was relaunched as YouTube Red, offering ad-free streaming of all videos and access to exclusive original content.[172][173][174] As of November 2016[update], the service has 1.5 million subscribers, with a further million on a free-trial basis.[175] As of June 2017[update], the first season of YouTube Originals had received 250 million views in total.[176]\n YouTube Kids is an American children's video app developed by YouTube, a subsidiary of Google. The app was developed in response to parental and government scrutiny on the content available to children. The app provides a version of the service-oriented towards children, with curated selections of content, parental control features, and filtering of videos deemed inappropriate viewing for children aged under 13, 8 or 5 depending on the age grouping chosen. First released on February 15, 2015, as an Android and iOS mobile app, the app has since been released for LG, Samsung, and Sony smart TVs, as well as for Android TV. On May 27, 2020, it became available on Apple TV. As of September 2019, the app is available in 69 countries, including Hong Kong and Macau, and one province. YouTube launched a web-based version of YouTube Kids on August 30, 2019.\n On September 28, 2016, YouTube named Lyor Cohen, the co-founder of 300 Entertainment and former Warner Music Group executive, the Global Head of Music.[177]\n In early 2018, Cohen began hinting at the possible launch of YouTube's new subscription music streaming service, a platform that would compete with other services such as Spotify and Apple Music.[178] On May 22, 2018, the music streaming platform named \"YouTube Music\" was launched.[179][180]\n YouTube Movies & TV is a video on demand service that offers movies and television shows for purchase or rental, depending on availability, along with a selection of movies (encompassing between 100 and 500 titles overall) that are free to stream, with interspersed ad breaks. YouTube began offering free-to-view movie titles to its users in November 2018; selections of new movies are added and others removed, unannounced each month.[181]\n In March 2021, Google announced plans to gradually deprecate the Google Play Movies & TV app, and eventually migrate all users to the YouTube app's Movies & TV store to view, rent and purchase movies and TV shows (first affecting Roku, Samsung, LG, and Vizio smart TV users on July 15).[182][183] Google Play Movies & TV formally shut down on January 17, 2024, with the web version of that platform migrated to YouTube as an expansion of the Movies & TV store to desktop users. (Other functions of Google Play Movies & TV were integrated into the Google TV service.)[184]\n On November 1, 2022, YouTube launched Primetime Channels, a channel store platform offering third-party subscription streaming add-ons sold a la carte through the YouTube website and app, competing with similar subscription add-on stores operated by Apple, Prime Video and Roku. The add-ons can be purchased through the YouTube Movies & TV hub or through the official YouTube channels of the available services; subscribers of YouTube TV add-ons that are sold through Primetime Channels can also access their content via the YouTube app and website. A total of 34 streaming services (including Paramount+, Showtime, Starz, MGM+, AMC+ and ViX+) were initially available for purchase.[185][186]\n NFL Sunday Ticket, as part of a broader residential distribution deal with Google signed in December 2022 that also made it available to YouTube TV subscribers, was added to Primetime Channels as a standalone add-on on August 16, 2023.[187][188] The ad-free tier of Max was added to Primetime Channels on December 12, 2023, coinciding with YouTube TV converting its separate HBO (for base plan subscribers) and HBO Max (for all subscribers) linear/VOD add-ons into a single combined Max offering.[189][190][note 1]\n On February 28, 2017, in a press announcement held at YouTube Space Los Angeles, YouTube announced YouTube TV, an over-the-top MVPD-style subscription service that would be available for United States customers at a price of US$65 per month. Initially launching in five major markets (New York City, Los Angeles, Chicago, Philadelphia and San Francisco) on April 5, 2017,[191][192] the service offers live streams of programming from the five major broadcast networks (ABC, CBS, The CW, Fox and NBC, along with selected MyNetworkTV affiliates and independent stations in certain markets), as well as approximately 60 cable channels owned by companies such as The Walt Disney Company, Paramount Global, Fox Corporation, NBCUniversal, Allen Media Group and Warner Bros. Discovery (including among others Bravo, USA Network, Syfy, Disney Channel, CNN, Cartoon Network, E!, Fox Sports 1, Freeform, FX and ESPN).[193][194]\n Subscribers can also receive premium cable channels (including HBO (via a combined Max add-on that includes in-app and log-in access to the service), Cinemax, Showtime, Starz and MGM+) and other subscription services (such as NFL Sunday Ticket, MLB.tv, NBA League Pass, Curiosity Stream and Fox Nation) as optional add-ons for an extra fee, and can access YouTube Premium original content.[193][194] In September 2022, YouTube TV began allowing customers to purchase most of its premium add-ons (excluding certain services such as NBA League Pass and AMC+) without an existing subscription to its base package.[195]\n In September 2016, YouTube Go was announced,[196] as an Android app created for making YouTube easier to access on mobile devices in emerging markets. It was distinct from the company's main Android app and allowed videos to be downloaded and shared with other users. It also allowed users to preview videos, share downloaded videos through Bluetooth, and offered more options for mobile data control and video resolution.[197]\n In February 2017, YouTube Go was launched in India, and expanded in November 2017 to 14 other countries, including Nigeria, Indonesia, Thailand, Malaysia, Vietnam, the Philippines, Kenya, and South Africa.[198][199] On February 1, 2018, it was rolled out in 130 countries worldwide, including Brazil, Mexico, Turkey, and Iraq. Before it shut down, the app was available to around 60% of the world's population.[200][201] In May 2022, Google announced that they would be shutting down YouTube Go in August 2022.[202]\n In September 2020, YouTube announced that it would be launching a beta version of a new platform of 15-second videos, similar to TikTok, called YouTube Shorts.[203][204] The platform was first tested in India but as of March 2021 has expanded to other countries including the United States with videos now able to be up to 1 minute long.[205] The platform is not a standalone app, but is integrated into the main YouTube app. Like TikTok, it gives users access to built-in creative tools, including the possibility of adding licensed music to their videos.[206] The platform had its global beta launch in July 2021.[207]\n In 2018, YouTube started testing a new feature initially called \"YouTube Reels\".[208] The feature was nearly identical to Instagram Stories and Snapchat Stories. YouTube later renamed the feature \"YouTube Stories\". It was only available to creators who had more than 10,000 subscribers and could only be posted/seen in the YouTube mobile app.[209] On May 25, 2023, YouTube announced that they would be shutting down this feature on June 26, 2023.[210][211]\n In November 2016, YouTube released YouTube VR, a dedicated version with an interface for VR devices, for Google's Daydream mobile VR platform on Android.[212] In November 2018, YouTube VR was released on the Oculus Store for the Oculus Go headset.[212] YouTube VR was updated since for compatibility with successive Quest devices, and was ported to Pico 4.[213]\n YouTube VR allows for access to all YouTube-hosted videos, but particularly supports headset access for 360° and 180°-degree video (both in 2D and stereoscopic 3D). Starting with the Oculus Quest, the app was updated for compatibility with mixed-reality passthrough modes on VR headsets. In April 2024, YouTube VR was updated to support 8K SDR video on Meta Quest 3.[214]\n YouTube, a video sharing platform, has faced various criticisms over the years, particularly regarding content moderation, offensive content, and monetization. YouTube has faced criticism over aspects of its operations,[215] its recommendation algorithms perpetuating videos that promote conspiracy theories and falsehoods,[216] hosting videos ostensibly targeting children but containing violent or sexually suggestive content involving popular characters,[217] videos of minors attracting pedophilic activities in their comment sections,[218] and fluctuating policies on the types of content that is eligible to be monetized with advertising.[215]\n Since its founding in 2005, the American video-sharing website YouTube has been faced with a growing number of privacy issues, including allegations that it allows users to upload unauthorized copyrighted material and allows personal information from young children to be collected without their parents' consent.\n YouTube has been censored, filtered, or banned for a variety of reasons, including:[225]\n Access to specific videos is sometimes prevented due to copyright and intellectual property protection laws (e.g. in Germany), violations of hate speech, and preventing access to videos judged inappropriate for youth,[226] which is also done by YouTube with the YouTube Kids app and with \"restricted mode\".[227] Businesses, schools, government agencies, and other private institutions often block social media sites, including YouTube, due to its bandwidth limitations[228][229] and the site's potential for distraction.[225][230]\n As of 2018[update], public access to YouTube is blocked in many countries, including China, North Korea, Iran, Turkmenistan,[231] Uzbekistan,[232][233] Tajikistan, Eritrea, Sudan and South Sudan. In some countries, YouTube is blocked for more limited periods of time such as during periods of unrest, the run-up to an election, or in response to upcoming political anniversaries. In cases where the entire site is banned due to one particular video, YouTube will often agree to remove or limit access to that video in order to restore service.[225]\n Reports emerged that since October 2019, comments posted with Chinese characters insulting the Chinese Communist Party (共匪 \"communist bandit\" or 五毛 \"50 Cent Party\", referring to state-sponsored commentators) were being automatically deleted within 15 seconds.[234]\n Specific incidents where YouTube has been blocked include:\n Private individuals[267] and large production corporations[268] have used YouTube to grow their audiences. Indie creators have built grassroots followings numbering in the thousands at very little cost or effort, while mass retail and radio promotion proved problematic.[267] Concurrently, old media celebrities moved into the website at the invitation of a YouTube management that witnessed early content creators accruing substantial followings and perceived audience sizes potentially larger than that attainable by television.[268] While YouTube's revenue-sharing \"Partner Program\" made it possible to earn a substantial living as a video producer—its top five hundred partners each earning more than $100,000 annually[269] and its ten highest-earning channels grossing from $2.5 million to $12 million[270]—in 2012 CMU business editor characterized YouTube as \"a free-to-use ... promotional platform for the music labels.\"[271] In 2013 Forbes' Katheryn Thayer asserted that digital-era artists' work must not only be of high quality, but must elicit reactions on the YouTube platform and social media.[272] Videos of the 2.5% of artists categorized as \"mega\", \"mainstream\" and \"mid-sized\" received 90.3% of the relevant views on YouTube and Vevo in that year.[273] By early 2013, Billboard had announced that it was factoring YouTube streaming data into calculation of the Billboard Hot 100 and related genre charts.[274]\n Observing that face-to-face communication of the type that online videos convey has been \"fine-tuned by millions of years of evolution\", TED curator Chris Anderson referred to several YouTube contributors and asserted that \"what Gutenberg did for writing, online video can now do for face-to-face communication.\"[275] Anderson asserted that it is not far-fetched to say that online video will dramatically accelerate scientific advance, and that video contributors may be about to launch \"the biggest learning cycle in human history.\"[275] In education, for example, the Khan Academy grew from YouTube video tutoring sessions for founder Salman Khan's cousin into what Forbes' Michael Noer called \"the largest school in the world,\" with technology poised to disrupt how people learn.[276] YouTube was awarded a 2008 George Foster Peabody Award,[277] the website being described as a Speakers' Corner that \"both embodies and promotes democracy.\"[278] The Washington Post reported that a disproportionate share of YouTube's most-subscribed channels feature minorities, contrasting with mainstream television in which the stars are largely white.[279] A Pew Research Center study reported the development of \"visual journalism\", in which citizen eyewitnesses and established news organizations share in content creation.[280] The study also concluded that YouTube was becoming an important platform by which people acquire news.[281]\n YouTube has enabled people to more directly engage with government, such as in the CNN/YouTube presidential debates (2007) in which ordinary people submitted questions to U.S. presidential candidates via YouTube video, with a techPresident co-founder saying that Internet video was changing the political landscape.[282] Describing the Arab Spring (2010–2012), sociologist Philip N. Howard quoted an activist's succinct description that organizing the political unrest involved using \"Facebook to schedule the protests, Twitter to coordinate, and YouTube to tell the world.\"[283] In 2012, more than a third of the U.S. Senate introduced a resolution condemning Joseph Kony 16 days after the \"Kony 2012\" video was posted to YouTube, with resolution co-sponsor Senator Lindsey Graham remarking that the video \"will do more to lead to (Kony's) demise than all other action combined.\"[284]\n Conversely, YouTube has also allowed government to more easily engage with citizens, the White House's official YouTube channel being the seventh top news organization producer on YouTube in 2012[287] and in 2013 a healthcare exchange commissioned Obama impersonator Iman Crosson's YouTube music video spoof to encourage young Americans to enroll in the Affordable Care Act (Obamacare)-compliant health insurance.[288] In February 2014, U.S. President Obama held a meeting at the White House with leading YouTube content creators not only to promote awareness of Obamacare[289] but more generally to develop ways for government to better connect with the \"YouTube Generation.\"[285] Whereas YouTube's inherent ability to allow presidents to directly connect with average citizens was noted, the YouTube content creators' new media savvy was perceived necessary to better cope with the website's distracting content and fickle audience.[285]\n Some YouTube videos have themselves had a direct effect on world events, such as Innocence of Muslims (2012) which spurred protests and related anti-American violence internationally.[290] TED curator Chris Anderson described a phenomenon by which geographically distributed individuals in a certain field share their independently developed skills in YouTube videos, thus challenging others to improve their own skills, and spurring invention and evolution in that field.[275] Journalist Virginia Heffernan stated in The New York Times that such videos have \"surprising implications\" for the dissemination of culture and even the future of classical music.[291]\n A 2017 article in The New York Times Magazine posited that YouTube had become \"the new talk radio\" for the far right.[292] Almost a year before YouTube's January 2019 announcement that it would begin a \"gradual change\" of \"reducing recommendations of borderline content and content that could misinform users in harmful ways\",[293] Zeynep Tufekci had written in The New York Times that, \"(g)iven its billion or so users, YouTube may be one of the most powerful radicalizing instruments of the 21st century\".[294] Under YouTube's changes to its recommendation engine, the most-recommended channel evolved from conspiracy theorist Alex Jones (2016) to Fox News (2019).[295] According to a 2020 study, viewership of far-right videos on YouTube peaked in 2017 and \"a growing body of journalistic evidence\" suggested that YouTube was radicalizing young men through its recommendation engine, but that such evidence was \"fraught with a bias towards sensationalism\". It also found more \"mainstream-adjacent Conservative creators\" gaining over alt-right and extremist videos by 2020.[296] A 2022 study found that \"despite widespread concerns that YouTube's algorithms send people down 'rabbit holes' with recommendations to extremist videos, little systematic evidence exists to support this conjecture\", and that such exposure was \"heavily concentrated among a small group of people with high prior levels of gender and racial resentment.\"[297] A 2024 study by the Institute for Strategic Dialogue found that YouTube frequently recommended Christian videos and right-leaning and culturally conservative \"culture war\" videos by Fox News and male lifestyle influencers to accounts that did not show an interest in such topics.[298]\n The Legion of Extraordinary Dancers[299] and the YouTube Symphony Orchestra[300] selected their membership based on individual video performances.[275][300] Further, the cyber-collaboration charity video \"We Are the World 25 for Haiti (YouTube edition)\" was formed by mixing performances of 57 globally distributed singers into a single musical work,[301] with The Tokyo Times noting the \"We Pray for You\" YouTube cyber-collaboration video as an example of a trend to use crowdsourcing for charitable purposes.[302]\nThe anti-bullying It Gets Better Project expanded from a single YouTube video directed to discouraged or suicidal LGBT teens,[303] that within two months drew video responses from hundreds including U.S. President Barack Obama, Vice President Biden, White House staff, and several cabinet secretaries.[304] Similarly, in response to fifteen-year-old Amanda Todd's video \"My story: Struggling, bullying, suicide, self-harm\", legislative action was undertaken almost immediately after her suicide to study the prevalence of bullying and form a national anti-bullying strategy.[305] In May 2018, after London Metropolitan Police claimed that drill music videos glamorizing violence gave rise to gang violence, YouTube deleted 30 videos.[306]\n Prior to 2020, Google did not provide detailed figures for YouTube's running costs, and YouTube's revenues in 2007 were noted as \"not material\" in a regulatory filing.[307] In June 2008, a Forbes magazine article projected the 2008 revenue at $200 million, noting progress in advertising sales.[308] In 2012, YouTube's revenue from its ads program was estimated at $3.7 billion.[309] In 2013, it nearly doubled and estimated to hit $5.6 billion according to e-Marketer,[309][310] while others estimated $4.7 billion.[309] The vast majority of videos on YouTube are free to view and supported by advertising.[64] In May 2013, YouTube introduced a trial scheme of 53 subscription channels with prices ranging from $0.99 to $6.99 a month.[311] The move was seen as an attempt to compete with other providers of online subscription services such as Netflix, Amazon Prime, and Hulu.[64]\n Google first published exact revenue numbers for YouTube in February 2020 as part of Alphabet's 2019 financial report. According to Google, YouTube had made US$15.1 billion in ad revenue in 2019, in contrast to US$8.1 billion in 2017 and US$11.1 billion in 2018. YouTube's revenues made up nearly 10% of the total Alphabet revenue in 2019.[312][313] These revenues accounted for approximately 20 million subscribers combined between YouTube Premium and YouTube Music subscriptions, and 2 million subscribers to YouTube TV.[314]\n YouTube had $29.2 billion ads revenue in 2022, up by $398 million from the prior year.[315] In Q2 2024, ad revenue rose to $8.66 billion, up 13% on Q1.[316]\n YouTube entered into a marketing and advertising partnership with NBC in June 2006.[317] In March 2007, it struck a deal with BBC for three channels with BBC content, one for news and two for entertainment.[318] In November 2008, YouTube reached an agreement with MGM, Lions Gate Entertainment, and CBS, allowing the companies to post full-length films and television episodes on the site, accompanied by advertisements in a section for U.S. viewers called \"Shows\". The move was intended to create competition with websites such as Hulu, which features material from NBC, Fox, and Disney.[319][320] In November 2009, YouTube launched a version of \"Shows\" available to UK viewers, offering around 4,000 full-length shows from more than 60 partners.[321] In January 2010, YouTube introduced an online film rentals service,[322] which is only available to users in the United States, Canada, and the UK as of 2010.[323][324][needs update] The service offers over 6,000 films.[325]\n In March 2017, the government of the United Kingdom pulled its advertising campaigns from YouTube, after reports that its ads had appeared on videos containing extremist content. The government demanded assurances that its advertising would \"be delivered safely and appropriately\". The Guardian newspaper, as well as other major British and U.S. brands, similarly suspended their advertising on YouTube in response to their advertising appearing near offensive content. Google stated that it had \"begun an extensive review of our advertising policies and have made a public commitment to put in place changes that give brands more control over where their ads appear\".[326][327] In early April 2017, the YouTube channel h3h3Productions presented evidence claiming that a Wall Street Journal article had fabricated screenshots showing major brand advertising on an offensive video containing Johnny Rebel music overlaid on a Chief Keef music video, citing that the video itself had not earned any ad revenue for the uploader. The video was retracted after it was found that the ads had been triggered by the use of copyrighted content in the video.[328][329]\n On April 6, 2017, YouTube announced that to \"ensure revenue only flows to creators who are playing by the rules\", it would change its practices to require that a channel undergo a policy compliance review, and have at least 10,000-lifetime views, before they may join the Partner Program.[330]\n In May 2007, YouTube launched its Partner Program (YPP), a system based on AdSense which allows the uploader of the video to share the revenue produced by advertising on the site.[331] YouTube typically takes 45 percent of the advertising revenue from videos in the Partner Program, with 55 percent going to the uploader.[332][333]\n There are over two million members of the YouTube Partner Program.[334] According to TubeMogul, in 2013 a pre-roll advertisement on YouTube (one that is shown before the video starts) cost advertisers on average $7.60 per 1000 views. Usually, no more than half of the eligible videos have a pre-roll advertisement, due to a lack of interested advertisers.[335]\n YouTube's policies restrict certain forms of content from being included in videos being monetized with advertising, including videos containing violence, strong language, sexual content, \"controversial or sensitive subjects and events, including subjects related to war, political conflicts, natural disasters and tragedies, even if graphic imagery is not shown\" (unless the content is \"usually newsworthy or comedic and the creator's intent is to inform or entertain\"),[336] and videos whose user comments contain \"inappropriate\" content.[337] In 2013, YouTube introduced an option for channels with at least a thousand subscribers to require a paid subscription for viewers to watch videos.[338][339] In April 2017, YouTube set an eligibility requirement of 10,000 lifetime views for a paid subscription.[340] On January 16, 2018, the eligibility requirement for monetization was changed to 4,000 hours of watch-time within the past 12 months and 1,000 subscribers.[340] The move was seen as an attempt to ensure that videos being monetized did not lead to controversy, but was criticized for penalizing smaller YouTube channels.[341]\n YouTube Play Buttons, a part of the YouTube Creator Rewards, are a recognition by YouTube of its most popular channels.[342] The trophies made of nickel plated copper-nickel alloy, golden plated brass, silver plated metal, ruby, and red tinted crystal glass are given to channels with at least one hundred thousand, a million, ten million, fifty million subscribers, and one hundred million subscribers, respectively.[343][344]\n YouTube's policies on \"advertiser-friendly content\" restrict what may be incorporated into videos being monetized; this includes strong violence, language,[345] sexual content, and \"controversial or sensitive subjects and events, including subjects related to war, political conflicts, natural disasters and tragedies, even if graphic imagery is not shown\", unless the content is \"usually newsworthy or comedic and the creator's intent is to inform or entertain\".[346] In September 2016, after introducing an enhanced notification system to inform users of these violations, YouTube's policies were criticized by prominent users, including Philip DeFranco and Vlogbrothers. DeFranco argued that not being able to earn advertising revenue on such videos was \"censorship by a different name\". A YouTube spokesperson stated that while the policy itself was not new, the service had \"improved the notification and appeal process to ensure better communication to our creators\".[347][348][349] Boing Boing reported in 2019 that LGBT keywords resulted in demonetization.[350]\n In the United States as of November 2020, and June 2021 worldwide,[351] YouTube reserves the right to monetize any video on the platform, even if their uploader is not a member of the YouTube Partner Program. This will occur on channels whose content is deemed \"advertiser-friendly\", and all revenue will go directly to Google without any share given to the uploader.[352]\n The majority of YouTube's advertising revenue goes to the publishers and video producers who hold the rights to their videos; the company retains 45% of the ad revenue.[353] In 2010, it was reported that nearly a third of the videos with advertisements were uploaded without permission of the copyright holders. YouTube gives an option for copyright holders to locate and remove their videos or to have them continue running for revenue.[354] In May 2013, Nintendo began enforcing its copyright ownership and claiming the advertising revenue from video creators who posted screenshots of its games.[355] In February 2015, Nintendo agreed to share the revenue with the video creators through the Nintendo Creators Program.[356][357][358] On March 20, 2019, Nintendo announced on Twitter that the company will end the Creators program. Operations for the program ceased on March 20, 2019.[359][360]\n"
    },
    {
        "title": "Amazon (company)",
        "url": "https://en.wikipedia.org/wiki/Amazon_(company)",
        "content": "\n Amazon.com, Inc.,[1] doing business as Amazon (/ˈæməzɒn/, AM-ə-zon; UK also /ˈæməzən/, AM-ə-zən), is an American multinational technology company engaged in e-commerce, cloud computing, online advertising, digital streaming, and artificial intelligence.[5] Founded in 1994 by Jeff Bezos in Bellevue, Washington,[6] the company originally started as an online marketplace for books but gradually expanded its offerings to include a wide range of product categories, referred to as \"The Everything Store\".[7] Today, Amazon is considered one of the Big Five American technology companies, the other four being Alphabet,[a] Apple, Meta,[b] and Microsoft.\n The company has multiple subsidiaries, including Amazon Web Services, providing cloud computing; Zoox, a self-driving car division; Kuiper Systems, a satellite Internet provider; and Amazon Lab126, a computer hardware R&D provider. Other subsidiaries include Ring, Twitch, IMDb, and Whole Foods Market. Its acquisition of Whole Foods in August 2017 for US$13.4 billion substantially increased its market share and presence as a physical retailer.[8] Amazon also distributes a variety of downloadable and streaming content through its Amazon Prime Video, MGM+, Amazon Music, Twitch, Audible and Wondery[9] units. It publishes books through its publishing arm, Amazon Publishing, film and television content through Amazon MGM Studios, including the Metro-Goldwyn-Mayer studio, which was acquired in March 2022, and owns Brilliance Audio and Audible, which produce and distribute audiobooks, respectively. Amazon also produces consumer electronics—most notably, Kindle e-readers, Echo devices, Fire tablets, and Fire TVs.\n Amazon has a reputation as a disruptor of industries through technological innovation and aggressive reinvestment of profits into capital expenditures.[10][11][12][13] As of 2023[update], it is the world's largest online retailer and marketplace, smart speaker provider, cloud computing service through AWS,[14] live-streaming service through Twitch, and Internet company as measured by revenue and market share.[15] In 2021, it surpassed Walmart as the world's largest retailer outside of China, driven in large part by its paid subscription plan, Amazon Prime, which has close to 200 million subscribers worldwide.[16][17] It is the second-largest private employer in the United States[18] and the second-largest company in the World and in the U.S. by revenue as of 2024.[19] As of October 2024, Amazon is the 12th-most visited website in the world and 84% of its traffic comes from the United States.[20][21] Amazon is also the global leader in research and development spending, with R&D expenditure of US$73 billion in 2022.[22] Amazon has been criticized on various grounds, including but not limited to customer data collection practices, a toxic work culture, censorship, tax avoidance, and anti-competitive practices. \n Amazon was founded on July 5, 1994, by Jeff Bezos after he relocated from New York City to Bellevue, Washington, near Seattle, to operate an online bookstore. Bezos chose the Seattle area for its abundance of technical talent from Microsoft and the University of Washington, as well as its smaller population for sales tax purposes and the proximity to a major book distribution warehouse in Roseburg, Oregon. Bezos also considered several other options, including Portland, Oregon, and Boulder, Colorado.[23] The company, originally named Cadabra, was founded in the converted garage of Bezos's house for symbolic reasons and was renamed to Amazon in November 1994.[24] The Amazon website launched for public sales on July 16, 1995, and initially sourced its books directly from wholesalers and publishers.[23][25]\n Amazon went public in May 1997. It began selling music and videos in 1998, and began international operations by acquiring online sellers of books in the United Kingdom and Germany. In the subsequent year, it initiated the sale of a diverse range of products, including music, video games, consumer electronics, home improvement items, software, games, and toys.[26][27]\n In 2002, it launched Amazon Web Services (AWS), which initially focused on providing APIs for web developers to build web applications on top of Amazon's ecommerce platform.[28][29] In 2004, AWS was expanded to provide website popularity statistics and web crawler data from the Alexa Web Information Service.[30] AWS later shifted toward providing enterprise services with Simple Storage Service (S3) in 2006,[31] and Elastic Compute Cloud (EC2) in 2008,[32] allowing companies to rent data storage and computing power from Amazon. In 2006, Amazon also launched the Fulfillment by Amazon program, which allowed individuals and small companies (called \"third-party sellers\") to sell products through Amazon's warehouses and fulfillment infrastructure.[33]\n Amazon purchased the Whole Foods Market supermarket chain in 2017.[34] It is the leading e-retailer in the United States with approximately US$178 billion net sales in 2017. It has over 300 million active customer accounts globally.[35]\n Amazon saw large growth during the COVID-19 pandemic, hiring more than 100,000 staff in the United States and Canada.[36] Some Amazon workers in the US, France, and Italy protested the company's decision to \"run normal shifts\" due to COVID-19's ease of spread in warehouses.[37][38] In Spain, the company faced legal complaints over its policies,[39] while a group of US Senators wrote an open letter to Bezos expressing concerns about workplace safety.[40]\n On February 2, 2021, Bezos announced that he would step down as CEO to become executive chair of Amazon's board. The transition officially took place on July 5, 2021, with former CEO of AWS Andy Jassy replacing him as CEO.[41][42] In January 2023, Amazon cut over 18,000 jobs, primarily in consumer retail and its human resources division in an attempt to cut costs.[43]\n On November 8, 2023, a plan was adopted for Jeff Bezos to sell approximately 50 million shares of the company over the next year (the deadline for the entire sales plan is January 31, 2025). The first step was the sale of 12 million shares for about $2 billion.[44]\n On February 26, 2024, Amazon became a component of the Dow Jones Industrial Average.[45]\n On December 19, 2024, Amazon workers, led by the International Brotherhood of Teamsters labor union, went on strike against Amazon in at least four US states, with workers in other facilities in the United States being welcomed to join the strike as well.[46][47]\n Amazon.com is an e-commerce platform that sells many product lines, including media (books, movies, music, and software), apparel, baby products, consumer electronics, beauty products, gourmet food, groceries, health and personal care products, industrial & scientific supplies, kitchen items, jewelry, watches, lawn and garden items, musical instruments, sporting goods, tools, automotive items, toys and games, and farm supplies[49] and consulting services.[50] Amazon websites are country-specific (for example, amazon.com for the US and amazon.co.uk for UK) though some offer international shipping.[51]\n Visits to amazon.com grew from 615 million annual visitors in 2008,[52] to more than 2 billion per month in 2022.[citation needed] The e-commerce platform is the 12th most visited website in the world.[21]\n In February 2024, Amazon announced its first chatbot was first “Rufus” in the US and in July, it was widely available to all customers in the US.[53]\n “Rufus” is now available in the US, India and the UK which helps the shoppers get product recommendations, get shopping list advice, compare products and see what other customers have responded to their specific questions.[54]\n Results generated by Amazon's search engine are partly determined by promotional fees.[55] The company's localized storefronts, which differ in selection and prices, are differentiated by top-level domain and country code:\n In 2000, US toy retailer Toys \"R\" Us entered into a 10-year agreement with Amazon, valued at $50 million per year plus a cut of sales, under which Toys \"R\" Us would be the exclusive supplier of toys and baby products on the service, and the chain's website would redirect to Amazon's Toys & Games category. In 2004, Toys \"R\" Us sued Amazon, claiming that because of a perceived lack of variety in Toys \"R\" Us stock, Amazon had knowingly allowed third-party sellers to offer items on the service in categories that Toys \"R\" Us had been granted exclusivity. In 2006, a court ruled in favor of Toys \"R\" Us, giving it the right to unwind its agreement with Amazon and establish its independent e-commerce website. The company was later awarded $51 million in damages.[66][67][68]\n In 2001, Amazon entered into a similar agreement with Borders, under which Amazon would comanage Borders.com as a co-branded service.[69] Borders pulled out of the arrangement in 2007, with plans to also launch its own online store.[70]\n On October 18, 2011, Amazon.com announced a partnership with DC Comics for the exclusive digital rights to many popular comics, including Superman, Batman, Green Lantern, Sandman, and Watchmen. The partnership has caused well-known bookstores like Barnes & Noble to remove these titles from their shelves.[71]\n In November 2013, Amazon announced a partnership with the United States Postal Service to begin delivering orders on Sundays. The service, included in Amazon's standard shipping rates, initiated in metropolitan areas of Greater Los Angeles and New York because of the high-volume and inability to deliver in a timely way, with plans to expand into Dallas, Houston, New Orleans and Phoenix by 2014.[72]\n In June 2017, Nike agreed to sell products through Amazon in exchange for better policing of counterfeit goods.[73][74] This proved unsuccessful and Nike withdrew from the partnership in November 2019.[74][75] Companies including IKEA and Birkenstock also stopped selling through Amazon around the same time, citing similar frustrations over business practices and counterfeit goods.[76]\n In September 2017, Amazon ventured with one of its sellers JV Appario Retail owned by Patni Group which has recorded a total income of US$104.44 million (₹759 crore) in financial year 2017–2018.[77]\n As of October 11, 2017[update], Amazon Fresh sold a range of Booths branded products for home delivery in selected areas.[78]\n In November 2018, Amazon reached an agreement with Apple Inc. to sell selected products through the service, via the company and selected Apple Authorized Resellers. As a result of this partnership, only Apple Authorized Resellers may sell Apple products on Amazon effective January 4, 2019.[79][80]\n On November 7, 2024, Amazon is reportedly discussing a second multi-billion dollar investment in AI startup Anthropic, following its initial $4 billion investment.[81]\n Amazon sells many products under its own brand names, including phone chargers, batteries, and diaper wipes. The AmazonBasics brand was introduced in 2009, and now features hundreds of product lines, including smartphone cases, computer mice, batteries, dumbbells, and dog crates. Amazon owned 34 private-label brands as of 2019. These brands account for 0.15% of Amazon's global sales, whereas the average for other large retailers is 18%.[82] Other Amazon retail brands include Presto!, Mama Bear, and Amazon Essentials.[83]\n Amazon derives many of its sales (around 40% in 2008) from third-party sellers who sell products on Amazon.[84] Some other large e-commerce sellers use Amazon to sell their products in addition to selling them through their websites. The sales are processed through Amazon.com and end up at individual sellers for processing and order fulfillment and Amazon leases space for these retailers. Small sellers of used and new goods go to Amazon Marketplace to offer goods at a fixed price.[85]\n Publishers can sign up as affiliates and receive a commission for referring customers to Amazon by placing links to Amazon on their websites if the referral results in a sale. Worldwide, Amazon has \"over 900,000 members\" in its affiliate programs.[86] In the middle of 2014, the Amazon Affiliate Program is used by 1.2% of all websites and it is the second most popular advertising network after Google Ads.[87] It is frequently used by websites and non-profits to provide a way for supporters to earn them a commission.[88]\n Associates can access the Amazon catalog directly on their websites by using the Amazon Web Services (AWS) XML service. A new affiliate product, aStore, allows Associates to embed a subset of Amazon products within another website, or linked to another website. In June 2010, Amazon Seller Product Suggestions was launched to provide more transparency to sellers by recommending specific products to third-party sellers to sell on Amazon. Products suggested are based on customers' browsing history.[89]\n Amazon allows users to submit reviews to the web page of each product. Reviewers must rate the product on a rating scale from one to five stars. Amazon provides a badging option for reviewers which indicates the real name of the reviewer (based on confirmation of a credit card account) or which indicates that the reviewer is one of the top reviewers by popularity. As of December 16, 2020, Amazon removed the ability of sellers and customers to comment on product reviews and purged their websites of all posted product review comments. In an email to sellers, Amazon gave its rationale for removing this feature: \"...the comments feature on customer reviews was rarely used.\" The remaining review response options are to indicate whether the reader finds the review helpful or to report that it violates Amazon policies (abuse). If a review is given enough \"helpful\" hits, it appears on the front page of the product. In 2010, Amazon was reported as being the largest single source of Internet consumer reviews.[90]\n When publishers asked Bezos why Amazon would publish negative reviews, he defended the practice by claiming that Amazon.com was \"taking a different approach...we want to make every book available—the good, the bad and the ugly...to let truth loose\".[91]\n There have been cases of positive reviews being written and posted by public relations companies on behalf of their clients[92] and instances of writers using pseudonyms to leave negative reviews of their rivals' works.\n The Amazon sales rank (ASR) indicates the popularity of a product sold on any Amazon locale. It is a relative indicator of popularity that is updated hourly. Effectively, it is a \"best sellers list\" for the millions of products stocked by Amazon.[93] While the ASR has no direct effect on the sales of a product, it is used by Amazon to determine which products to include in its bestsellers lists.[93] Products that appear in these lists enjoy additional exposure on the Amazon website and this may lead to an increase in sales. In particular, products that experience large jumps (up or down) in their sales ranks may be included within Amazon's lists of \"movers and shakers\"; such a listing provides additional exposure that might lead to an increase in sales.[94] For competitive reasons, Amazon does not release actual sales figures to the public. However, Amazon has now begun to release point of sale data via the BookScan service to verified authors.[95] While the ASR has been the source of much speculation by publishers, manufacturers, and marketers, Amazon itself does not release the details of its sales rank calculation algorithm. Some companies have analyzed Amazon sales data to generate sales estimates based on the ASR,[96] though Amazon states:\n Please keep in mind that our sales rank figures are simply meant to be a guide of general interest for the customer and not definitive sales information for publishers—we assume you have this information regularly from your distribution sources In November 2015, Amazon opened a physical Amazon Books store in University Village in Seattle. The store was 5,500 square feet and prices for all products matched those on its website.[98] Amazon opened its tenth physical bookstore in 2017;[99] media speculation at the time suggested that Amazon planned to eventually roll out 300 to 400 bookstores around the country.[98] All of its locations were closed in 2022 along with other retail locations under the \"Amazon 4-Star\" brand.[100]\n In July 2016, the company announced that it was opening a 1,100,000 ft (335,280.0 m) square foot facility in Palmer Township in the Lehigh Valley region of eastern Pennsylvania. As of 2024, Amazon is Lehigh Valley region's third-largest employer.[101][102]\n In August 2019, Amazon applied to have a liquor store in San Francisco, as a means to ship beer and alcohol within the city.[103]\n In 2020, Amazon Fresh opened several physical stores in the US and the United Kingdom.[104]\n Amazon has a number of products and services available, including its digital assistant Alexa, Amazon Music, and Prime Video for music and videos respectively, the Amazon Appstore for Android apps, the Kindle line of electronic paper e-readers, Fire and Fire HD color LCD tablets. Audible provides audiobooks for purchase and listening.\n In September 2021, Amazon announced the launch of Astro, its first household robot, powered by its Alexa smart home technology. This can be remote-controlled when not at home, to check on pets, people, or home security. It will send owners a notification if it detects something unusual.[105]\n In January 2023, Amazon announced the launch of RXPass, a prescription drug delivery service. It allows U.S. Amazon Prime members to pay a $5 monthly fee for access to 60 medications. The service was launched immediately after the announcement except in states with specific prescription delivery requirements. Beneficiaries of government healthcare programs such as Medicare and Medicaid will not be able to sign up for RXPass.[106]\n Amazon owns over 100 subsidiaries, including Amazon Web Services, Audible, Diapers.com, Goodreads, IMDb, Kiva Systems (now Amazon Robotics), One Medical, Shopbop, Teachstreet, Twitch, Zappos, and Zoox.[107]\n Bezos separately owns The Washington Post (through Nash Holdings, LLC), Blue Origin, Bezos Expeditions, Altos Labs, and other companies.\n Amazon Live is an American video e-commerce live-streaming service created by Amazon Inc. to compete with live-streaming services. The service allows users to stream live videos promoting or sponsoring products.[108] Users (mainly celebrities or Internet influencers) have the option to livestream on Amazon and add tags to additionally add context to the products they're selling or promoting. Other users can join in and type in messages to send to a global chat on the livestream.[108]\n In 2019 Amazon launched an integrated platform into the Amazon website and application. In 2023 roughly a billion total viewers watch Amazon Live across the United States and India. The platform has also been integrated into Amazon Freevee and Amazon Prime Video.[109]\n Amazon Web Services (AWS) is a subsidiary of Amazon that provides on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered pay-as-you-go basis. These cloud computing web services provide distributed computing processing capacity and software tools via AWS server farms. As of 2021 Q4, AWS has 33% market share for cloud infrastructure while the next two competitors Microsoft Azure and Google Cloud have 21%, and 10% respectively, according to Synergy Group.[110]\n Audible is a seller and producer of spoken audio entertainment, information, and educational programming on the Internet. Audible sells digital audiobooks, radio and television programs, and audio versions of magazines and newspapers. Through its production arm, Audible Studios, Audible has also become the world's largest producer of downloadable audiobooks. On January 31, 2008, Amazon announced it would buy Audible for about $300 million. The deal closed in March 2008 and Audible became a subsidiary of Amazon.[111]\n Goodreads is a \"social cataloging\" website founded in December 2006 and launched in January 2007 by Otis Chandler, a software engineer, and entrepreneur, and Elizabeth Khuri. The website allows individuals to freely search Goodreads' extensive user-populated database of books, annotations, and reviews. Users can sign up and register books to generate library catalogs and reading lists. They can also create their groups of book suggestions and discussions. In December 2007, the site had over 650,000 members, and over a million books had been added. Amazon bought the company in March 2013.[112]\n Ring is a home automation company founded by Jamie Siminoff in 2013. It is primarily known for its Wi-Fi powered smart doorbells, but manufactures other devices such as security cameras. Amazon bought Ring for US$1 billion in 2018.[113]\n Twitch is a live streaming platform for video, primarily oriented towards video gaming content. Twitch was acquired by Amazon in August 2014 for $970 million.[114] The site's rapid growth had been boosted primarily by the prominence of major esports competitions on the service, leading GameSpot senior esports editor Rod Breslau to have described the service as \"the ESPN of esports\".[115] As of 2015[update], the service had over 1.5 million broadcasters and 100 million monthly viewers.[116]\n Whole Foods Market is an American supermarket chain exclusively featuring foods without artificial preservatives, colors, flavors, sweeteners, and hydrogenated fats.[117] Amazon acquired Whole Foods for $13.7 billion in August 2017.[118][119][8]\n Other Amazon subsidiaries include:\n Amazon also has investments in renewable energy, including plans to fund four small nuclear reactors at the Xe-100 reactor site in Eastern Washington, and plans to expand its position into the Canadian market through an investment in a new plant in Alberta.[151]\n Amazon uses many different transportation services to deliver packages. Amazon-branded services include:\n Amazon directly employs people to work at its warehouses, bulk distribution centers, staffed \"Amazon Hub Locker+\" locations, and delivery stations where drivers pick up packages. As of December 2020, it is not hiring delivery drivers as employees.[154]\n Rakuten Intelligence estimated that in 2020 in the United States, the proportion of last-mile deliveries was 56% by Amazon's directly contracted services (mostly in urban areas), 30% by the United States Postal Service (mostly in rural areas), and 14% by UPS.[155] In April 2021, Amazon reported to investors it had increased its in-house delivery capacity by 50% in the last 12 months (which included the first year of the COVID-19 pandemic in the United States).[156]\n Amazon first launched its distribution network in 1997 with two fulfillment centers in Seattle and New Castle, Delaware. Amazon has several types of distribution facilities consisting of cross-dock centers, fulfillment centers, sortation centers, delivery stations, Prime now hubs, and Prime air hubs. There are 75 fulfillment centers and 25 sortation centers with over 125,000 employees.[157][158] Employees are responsible for five basic tasks: unpacking and inspecting incoming goods; placing goods in storage and recording their location; picking goods from their computer recorded locations to make up an individual shipment; sorting and packing orders; and shipping. A computer that records the location of goods and maps out routes for pickers plays a key role: employees carry hand-held computers which communicate with the central computer and monitor their rate of progress. Some warehouses are partially automated with systems built by Amazon Robotics.[159]\n In September 2006, Amazon launched a program called FBA (Fulfillment By Amazon) whereby it could handle storage, packing and distribution of products and services for small sellers.[33]\n As of June 2022[update], Amazon's board of directors were:[160]\n The 10 largest shareholder of Amazon in early 2024 were:[56]\n Amazon.com is primarily a retail site with a sales revenue model; Amazon takes a small percentage of the sale price of each item that is sold through its website while also allowing companies to advertise their products by paying to be listed as featured products.[161] As of 2018[update], Amazon.com is ranked eighth on the Fortune 500 rankings of the largest United States corporations by total revenue.[162] In Forbes Global 2000 2023 Amazon ranked 36th.[163]\n For the fiscal year 2021, Amazon reported earnings of US$33.36 billion, with an annual revenue of US$469.82 billion, an increase of 21.7% over the previous fiscal cycle. Since 2007 sales increased from 14.835 billion to 469.822 billion, due to continued business expansion.[citation needed]\n Amazon's market capitalization went over US$1 trillion again in early February 2020 after the announcement of the fourth quarter 2019 results.[164]\n During his tenure, Jeff Bezos had become renowned for his annual shareholder letters, which have gained similar notability to those of Warren Buffett.[183] These annual letters gave an \"invaluable window\" into the famously \"secretive\" company, and revealed Bezos's perspectives and strategic focus.[183][184] A common theme of these letters is Bezos's desire to instill customer-centricity (in his words, \"customer obsession\") at all levels of Amazon, notably by making all senior executives field customer support queries for a short time at Amazon call centers. He also read many emails addressed by customers to his public email address.[185] One of Bezos's most well-known internal memos was his mandate for \"all teams\" to \"expose their data and functionality\" through service interfaces \"designed from the ground up to be externalizable\". This process, commonly known as a service-oriented architecture (SOA), resulted in mandatory dogfooding of services that would later be commercialized as part of AWS.[citation needed]\n Amazon lobbies the United States federal government and state governments on multiple issues such as the enforcement of sales taxes on online sales, transportation safety, privacy and data protection and intellectual property. According to regulatory filings, Amazon.com focuses its lobbying on the United States Congress, the Federal Communications Commission and the Federal Reserve. Amazon.com spent roughly $3.5 million, $5 million and $9.5 million on lobbying, in 2013, 2014 and 2015, respectively.[186] In 2019, it spent $16.8 million and had a team of 104 lobbyists.[187]\n Amazon.com was a corporate member of the American Legislative Exchange Council (ALEC) until it dropped membership following protests at its shareholders' meeting on May 24, 2012.[188]\n In 2014, Amazon expanded its lobbying practices as it prepared to lobby the Federal Aviation Administration to approve its drone delivery program, hiring the Akin Gump Strauss Hauer & Feld lobbying firm in June.[189] Amazon and its lobbyists have visited with Federal Aviation Administration officials and aviation committees in Washington, D.C. to explain its plans to deliver packages.[190] In September 2020 this moved one step closer with the granting of a critical certificate by the FAA.[191]\n Amazon has attracted criticism for its actions, including: supplying law enforcement with facial recognition surveillance tools;[192] forming cloud computing partnerships with the CIA;[193] leading customers away from bookshops;[194] adversely impacting the environment;[195] placing a low priority on warehouse conditions for workers;[196][197] actively opposing unionization efforts;[198] remotely deleting content purchased by Amazon Kindle users; taking public subsidies; seeking to patent its 1-Click technology; engaging in anti-competitive actions and price discrimination;[199][200] and reclassifying LGBTQ books as adult content.[201][202] Criticism has also concerned various decisions over whether to censor or publish content such as the WikiLeaks website, works containing libel, anti-LGBT merchandise, and material facilitating dog fighting, cockfighting, or pedophile activities. An article published by Time in the wake of social media website Parler's termination of service by Amazon Web Service highlights the power companies like Amazon now have over the internet.[203] In December 2011, Amazon faced a backlash from small businesses for running a one-day deal to promote its new Price Check app. Shoppers who used the app to check prices in a brick-and-mortar store were offered a 5% discount to purchase the same item from Amazon.[204] Companies like Groupon, eBay and Taap have countered Amazon's promotion by offering $10 off from their products.[205][206]\n The company has also faced accusations of putting undue pressure on suppliers to maintain and extend its profitability. One effort to squeeze the most vulnerable book publishers was known within the company as the Gazelle Project, after Bezos suggested, according to Brad Stone, \"that Amazon should approach these small publishers the way a cheetah would pursue a sickly gazelle.\"[55] In July 2014, the Federal Trade Commission launched a lawsuit against the company alleging it was promoting in-app purchases to children, which were being transacted without parental consent.[207] In 2019, Amazon banned selling skin-lightening products after pushback from Minnesota health and environmental activists.[208] In 2022, a lawsuit filed by state attorney-general Letitia James was dismissed by the New York state court of appeals.[209] After the COVID-19 pandemic, Amazon faced criticism for complying, under pressure from the Biden Administration, to \"reduce the visibility” of books critical of the COVID-19 vaccine,[210][211] which was revealed after Rep. Jim Jordan (acting on behalf of the House Judiciary Committee) subpoenaed emails between the company and the Biden Administration.[212]\n Amazon Prime has been criticized for its vehicles systemically double parking, blocking bike lanes, and otherwise violating traffic laws while dropping off packages, contributing to traffic congestion and endangering other road users.[213][214][215][216]\n Jane Friedman[217] discovered six listings of books fraudulently using her name, on Amazon and Goodreads. Amazon and Goodreads resisted removing the fraudulent titles until the author's complaints went viral on social media, in a blog post titled \"I Would Rather See My Books Get Pirated Than This (Or: Why Goodreads and Amazon Are Becoming Dumpster Fires).\"[218][219][220][221]\n In 2024, following years of criticism for providing law enforcement footage in the custody of Ring (a home security company owned by Amazon) without a warrant, Ring has halted this practice.[222] It received cautious praise from privacy-focused organizations such as the Electronic Frontier Foundation for this change.[222]\n"
    },
    {
        "title": "Netflix",
        "url": "https://en.wikipedia.org/wiki/Netflix",
        "content": "\n Netflix is an American subscription video on-demand over-the-top streaming service. The service primarily distributes original and acquired films and television shows from various genres, and it is available internationally in multiple languages.[6]\n Launched in 2007, nearly a decade after Netflix, Inc. began its pioneering DVD-by-mail movie rental service, Netflix is the most-subscribed video on demand streaming media service, with 282.7 million paid memberships in more than 190 countries as of 2024.[5][7] By 2022, \"Netflix Original\" productions accounted for half of its library in the United States and the namesake company had ventured into other categories, such as video game publishing of mobile games through its flagship service. As of 2023, Netflix is the 23rd most-visited website in the world, with 23.66% of its traffic coming from the United States, followed by the United Kingdom at 5.84%, and Brazil at 5.64%.[8][9]\n \n Netflix was founded by Marc Randolph and Reed Hastings on August 29, 1997, in Scotts Valley, California. Hastings, a computer scientist and mathematician, was a co-founder of Pure Software, which was acquired by Rational Software that year for $750 million, the then biggest acquisition in Silicon Valley history.[10] Randolph had worked as a marketing director for Pure Software after Pure Atria acquired a company where Randolph worked. He was previously a co-founder of MicroWarehouse, a computer mail-order company, as well as vice president of marketing for Borland.[11][12]\n Hastings and Randolph came up with the idea for Netflix while carpooling between their homes in Santa Cruz, California, and Pure Atria's headquarters in Sunnyvale.[13] Patty McCord, later head of human resources at Netflix, was also in the carpool group.[14] Randolph admired Amazon and wanted to find a large category of portable items to sell over the Internet using a similar model. Hastings and Randolph considered and rejected selling and renting VHS as too expensive to stock and too delicate to ship.[11] When they heard about DVDs, first introduced in the United States in early 1997, they tested the concept of selling or renting DVDs by mail, by mailing a compact disc to Hastings's house in Santa Cruz.[11] When the CD arrived intact, they decided to enter the $16 billion Home-video sales and rental industry.[11][13] Hastings is often quoted saying that he decided to start Netflix after being fined $40 at a Blockbuster store for being late to return a copy of Apollo 13.[13] Hastings invested $2.5 million into Netflix from the sale of Pure Atria.[15][13] Netflix launched as the first DVD rental and sales website with 30 employees and 925 titles available—nearly all DVDs published.[13][16][17] Randolph and Hastings met with Jeff Bezos, where Amazon offered to acquire Netflix for between $14 and $16 million. Fearing competition from Amazon, Randolph at first thought the offer was fair, but Hastings, who owned 70% of the company, turned it down on the plane ride home.[18][19]\n Initially, Netflix offered a per-rental model for each DVD but introduced a monthly subscription concept in September 1999.[20] The per-rental model was dropped by early 2000, allowing the company to focus on the business model of flat-fee unlimited rentals without due dates, late fees, shipping and handling fees, or per-title rental fees.[21] In September 2000, during the dot-com bubble, while Netflix was suffering losses, Hastings and Randolph offered to sell the company to Blockbuster for $50 million. John Antioco, CEO of Blockbuster, thought the offer was a joke and declined, saying, \"The dot-com hysteria is completely overblown.\"[22][23] While Netflix experienced fast growth in early 2001, the continued effects of the dot-com bubble collapse and the September 11 attacks caused the company to hold off plans for its initial public offering (IPO) and to lay off one-third of its 120 employees.[24]\n DVD players were a popular gift for holiday sales in late 2001, and demand for DVD subscription services were \"growing like crazy\", according to chief talent officer Patty McCord.[25] The company went public on May 23, 2002, selling 5.5 million shares of common stock at US$15.00 per share.[26] In 2003, Netflix was issued a patent by the U.S. Patent & Trademark Office to cover its subscription rental service and several extensions.[27] Netflix posted its first profit in 2003, earning $6.5 million on revenues of $272 million; by 2004, profit had increased to $49 million on over $500 million in revenues.[28] In 2005, 35,000 different films were available, and Netflix shipped 1 million DVDs out every day.[29]\n In 2004, Blockbuster introduced a DVD rental service, which not only allowed users to check out titles through online sites but allowed for them to return them at brick and-mortar stores.[30] By 2006, Blockbuster's service reached two million users, and while trailing Netflix's subscriber count, was drawing business away from Netflix. Netflix lowered fees in 2007.[28] While it was an urban legend that Netflix ultimately \"killed\" Blockbuster in the DVD rental market, Blockbuster's debt load and internal disagreements hurt the company.[30]\n On April 4, 2006, Netflix filed a patent infringement lawsuit in which it demanded a jury trial in the United States District Court for the Northern District of California, alleging that Blockbuster's online DVD rental subscription program violated two patents held by Netflix. The first cause of action alleged Blockbuster's infringement of copying the \"dynamic queue\" of DVDs available for each customer, Netflix's method of using the ranked preferences in the queue to send DVDs to subscribers, and Netflix's method permitting the queue to be updated and reordered.[31] The second cause of action alleged infringement of the subscription rental service as well as Netflix's methods of communication and delivery.[32] The companies settled their dispute on June 25, 2007; terms were not disclosed.[33][34][35][36]\n On October 1, 2006, Netflix announced the Netflix Prize, $1,000,000 to the first developer of a video-recommendation algorithm that could beat its existing algorithm Cinematch, at predicting customer ratings by more than 10%. On September 21, 2009, it awarded the $1,000,000 prize to team \"BellKor's Pragmatic Chaos\".[37] Cinematch, launched in 2000, was a system that recommended movies to its users, many of which might have been entirely new to the user.[38][39]\n Through its division Red Envelope Entertainment, Netflix licensed and distributed independent films such as Born into Brothels and Sherrybaby. In late 2006, Red Envelope Entertainment also expanded into producing original content with filmmakers such as John Waters.[40] Netflix closed Red Envelope Entertainment in 2008.[41][42]\n In January 2007, the company launched a streaming media service, introducing video on demand via the Internet. However, at that time it only had 1,000 films available for streaming, compared to 70,000 available on DVD.[43] The company had for some time considered offering movies online, but it was only in the mid-2000s that data speeds and bandwidth costs had improved sufficiently to allow customers to download movies from the internet. The original idea was a \"Netflix box\" that could download movies overnight, and be ready to watch the next day. By 2005, Netflix had acquired movie rights and designed the box and service. But after witnessing how popular streaming services such as YouTube were despite the lack of high-definition content, the concept of using a hardware device was scrapped and replaced with a streaming concept.[44]\n In February 2007, Netflix delivered its billionth DVD, a copy of Babel to a customer in Texas.[45][46] In April 2007, Netflix recruited ReplayTV founder Anthony Wood, to build a \"Netflix Player\" that would allow streaming content to be played directly on a television rather than a desktop or laptop.[47] Hastings eventually shut down the project to help encourage other hardware manufacturers to include built-in Netflix support, which would be spun off as the digital media player product Roku.[48][49][50]\n In January 2008, all rental-disc subscribers became entitled to unlimited streaming at no additional cost. This change came in a response to the introduction of Hulu and to Apple's new video-rental services.[51][52][page needed] In August 2008, the Netflix database was corrupted and the company was not able to ship DVDs to customers for 3 days, leading the company to move all its data to the Amazon Web Services cloud.[53] In November 2008, Netflix began offering subscribers rentals on Blu-ray and discontinued its sale of used DVDs.[54] In 2009, Netflix streams overtook DVD shipments.[55]\n On January 6, 2010, Netflix agreed with Warner Bros. to delay new release rentals to 28 days after the DVDs became available for sale, in an attempt to help studios sell physical copies, and similar deals involving Universal Pictures and 20th Century Fox were reached on April 9.[56][57][58] In July 2010, Netflix signed a deal to stream movies of Relativity Media.[59] In August 2010, Netflix reached a five-year deal worth nearly $1 billion to stream films from Paramount, Lionsgate and Metro-Goldwyn-Mayer. The deal increased Netflix's annual spending fees, adding roughly $200 million per year. It spent $117 million in the first six months of 2010 on streaming, up from $31 million in 2009.[60] On September 22, 2010, Netflix launched in Canada, its first international market.[61][62] In November 2010, Netflix began offering a standalone streaming service separate from DVD rentals.[63]\n In 2010, Netflix acquired the rights to Breaking Bad, produced by Sony Pictures Television, after the show's third season, at a point where original broadcaster AMC had expressed the possibility of cancelling the show. Sony pushed Netflix to release Breaking Bad in time for the fourth season, which as a result, greatly expanded the show's audience on AMC due to new viewers bingeing on the Netflix past episodes, and doubling the viewership by the time of the fifth season. Breaking Bad is considered the first such show to have this \"Netflix effect\".[64]\n In January 2011, Netflix announced agreements with several manufacturers to include branded Netflix buttons on the remote controls of devices compatible with the service, such as Blu-ray players.[65] By May 2011, Netflix had become the largest source of Internet streaming traffic in North America, accounting for 30% of traffic during peak hours.[66][67][68][69]\n On July 12, 2011, Netflix announced that it would separate its existing subscription plans into two separate plans: one covering the streaming and the other DVD rental services.[70][71] The cost for streaming would be $7.99 per month, while DVD rental would start at the same price.[72] On September 11, 2011, Netflix expanded to countries in Latin America.[73][74][75] On September 18, 2011, Netflix announced its intentions to rebrand and restructure its DVD home media rental service as an independent subsidiary called Qwikster, separating DVD rental and streaming services.[76][77][78][79][80] On September 26, 2011, Netflix announced a content deal with DreamWorks Animation.[81] On October 10, 2011, Netflix announced that it would retain its DVD service under the name Netflix and that its streaming and DVD-rental plans would remain branded together, citing customer dissatisfaction with the split.[82][83]\n In October 2011. Netflix and The CW signed a multi-year output deal for its television shows.[84] On January 9, 2012, Netflix started its expansion to Europe, launching in the United Kingdom and Ireland.[85] In February 2012, Netflix reached a multi-year agreement with The Weinstein Company.[86][87] In March 2012, Netflix acquired the domain name DVD.com.[88] By 2016, Netflix rebranded its DVD-by-mail service under the name DVD.com, A Netflix Company.[89][90] In April 2012, Netflix filed with the Federal Election Commission (FEC) to form a political action committee (PAC) called FLIXPAC.[91] Netflix spokesperson Joris Evers tweeted that the intent was to \"engage on issues like net neutrality, bandwidth caps, UBB and VPPA\".[92][93] In June 2012, Netflix signed a deal with Open Road Films.[94][95]\n On August 23, 2012, Netflix and The Weinstein Company signed a multi-year output deal for RADiUS-TWC films.[96][97] In September 2012, Epix signed a five-year streaming deal with Netflix. For the initial two years of this agreement, first-run and back-catalog content from Epix was exclusive to Netflix. Epix films came to Netflix 90 days after premiering on Epix.[98] These included films from Paramount, Metro-Goldwyn-Mayer and Lionsgate.[99][100]\n On October 18, 2012, Netflix launched in Denmark, Finland, Norway and Sweden.[101][102] On December 4, 2012, Netflix and Disney announced an exclusive multi-year agreement for first-run United States subscription television rights to Walt Disney Studios' animated and live-action films, with classics such as Dumbo, Alice in Wonderland and Pocahontas available immediately and others available on Netflix beginning in 2016.[103] Direct-to-video releases were made available in 2013.[104][105]\n On January 14, 2013, Netflix signed an agreement with Time Warner's Turner Broadcasting System and Warner Bros. Television to distribute Cartoon Network, Warner Bros. Animation, and Adult Swim content, as well as TNT's Dallas, beginning in March 2013. The rights to these programs were given to Netflix shortly after deals with Viacom to stream Nickelodeon and Nick Jr. Channel programs expired.[106]\n For cost reasons, Netflix stated that it would limit its expansion in 2013,[107] adding only one new market—the Netherlands—in September of that year. This expanded its availability to 40 territories.[108][109]\n In 2011, Netflix began its efforts into original content development. In March, it made a straight-to-series order from MRC for the political drama House of Cards, led by Kevin Spacey, outbidding U.S. cable networks. This marked the first instance of a first-run television series being specifically commissioned by the service.[110] In November the same year, Netflix added two more significant productions to its roster: the comedy-drama Orange Is the New Black, adapted from Piper Kerman's memoir,[111] and a new season of the previously cancelled Fox sitcom Arrested Development.[112] Netflix acquired the U.S. rights to the Norwegian drama Lilyhammer after its television premiere on Norway's NRK1 on January 25, 2012. Notably departing from the traditional broadcast television model of weekly episode premieres, Netflix chose to release the entire first season on February 8 of the same year.[113][114]\n House of Cards was released by Netflix on February 1, 2013, marketed as the first \"Netflix Original\" production.[115] Later that month, Netflix announced an agreement with DreamWorks Animation to commission children's television series based on its properties, beginning with Turbo: F.A.S.T., a spin-off of its film Turbo.[116][117] Orange is the New Black would premiere in July 2013; Netflix stated that Orange is the New Black had been its most-watched original series so far, with all of them having \"an audience comparable with successful shows on cable and broadcast TV.\"[118][119]\n On March 13, 2013, Netflix added a Facebook sharing feature, letting United States subscribers access \"Watched by your friends\" and \"Friends' Favorites\" by agreeing.[120] This was not legal until the Video Privacy Protection Act was modified in early 2013.[121] On August 1, 2013, Netflix reintroduced the \"Profiles\" feature that permits accounts to accommodate up to five user profiles.[122][123][124][125]\n In November 2013, Marvel Television and ABC Studios announced Netflix had ordered a slate of four television series based on the Marvel Comics characters Daredevil, Jessica Jones, Iron Fist and Luke Cage. Each of the four series received an initial order of 13 episodes, and Netflix also ordered a Defenders miniseries that would tie them together. Daredevil and Jessica Jones premiered in 2015.[126][127][128] The Luke Cage series premiered on September 30, 2016, followed by Iron Fist on March 17, 2017, and The Defenders on August 18, 2017.[129][130] Marvel owner Disney later entered into other content agreements with Netflix, including acquiring its animated Star Wars series Star Wars: The Clone Wars, and a new sixth season.[131]\n In February 2014, Netflix began to enter into agreements with U.S. internet service providers, beginning with Comcast (whose customers had repeatedly complained of frequent buffering when streaming Netflix), in order to provide the service a direct connection to their networks.[132][133][134] In April 2014, Netflix signed Arrested Development creator Mitchell Hurwitz and his production firm The Hurwitz Company to a multi-year deal to create original projects for the service.[135] In May 2014, Netflix & Sony Pictures Animation had a major multi-deal to acquired streaming rights to produce films.[136] It also began to introduce an updated logo, with a flatter appearance and updated typography.[137]\n In September 2014, Netflix expanded into six new European markets, including Austria, Belgium, France, Germany, Luxembourg, and Switzerland.[138] On September 10, 2014, Netflix participated in Internet Slowdown Day by deliberately slowing down its speed in support of net neutrality regulations in the United States.[139] In October 2014, Netflix announced a four-film deal with Adam Sandler and his Happy Madison Productions.[140]\n In April 2015, following the launch of Daredevil, Netflix director of content operations Tracy Wright announced that Netflix had added support for audio description, and had begun to work with its partners to add descriptions to its other original series over time.[141][142] The following year, as part of a settlement with the American Council of the Blind, Netflix agreed to provide descriptions for its original series within 30 days of their premiere, and add screen reader support and the ability to browse content by availability of descriptions.[143]\n In March 2015, Netflix expanded to Australia and New Zealand.[144][145] In September 2015, Netflix launched in Japan, its first country in Asia.[146][147][148] In October 2015, Netflix launched in Italy, Portugal, and Spain.[149]\n In January 2016, at the Consumer Electronics Show, Netflix announced a major international expansion of its service into 130 additional countries. It then had become available worldwide except China, Syria, North Korea, Kosovo and Crimea.[150] In May 2016, Netflix created a tool called Fast.com to determine the speed of an Internet connection.[151] It received praise for being \"simple\" and \"easy to use\", and does not include online advertising, unlike competitors.[152][153][154] On November 30, 2016, Netflix launched an offline playback feature, allowing users of the Netflix mobile apps on Android or iOS to cache content on their devices in standard or high quality for viewing offline, without an Internet connection.[155][156][157][158]\n In 2016, Netflix released an estimated 126 original series or films, more than any network or cable channel.[159] In April 2016, Hastings stated that the company planned to expand its in-house, Los Angeles-based Netflix Studios to grow its output; Hastings ruled out any potential acquisitions of existing studios.[160]\n In February 2017, Netflix signed a music publishing deal with BMG Rights Management, whereby BMG will oversee rights outside of the United States for music associated with Netflix original content. Netflix continues to handle these tasks in-house in the United States.[161] On April 25, 2017, Netflix signed a licensing deal with IQIYI, a Chinese video streaming platform owned by Baidu, to allow selected Netflix original content to be distributed in China on the platform.[162][163]\n On August 7, 2017, Netflix acquired Millarworld, the creator-owned publishing company of comic book writer Mark Millar. The purchase marked the first corporate acquisition to have been made by Netflix.[164] On August 14, 2017, Netflix entered into an exclusive development deal with Shonda Rhimes and her production company Shondaland.[165]\n In September 2017, Netflix announced it would offer its low-broadband mobile technology to airlines to provide better in-flight Wi-Fi so that passengers can watch movies on Netflix while on planes.[166]\n In September 2017, Minister of Heritage Mélanie Joly announced that Netflix had agreed to make a CA$500 million (US$400 million) investment over the next five years in producing content in Canada. The company denied that the deal was intended to result in a tax break.[167][168] Netflix realized this goal by December 2018.[169]\n In October 2017, Netflix iterated a goal of having half of its library consist of original content by 2019, announcing a plan to invest $8 billion on original content in 2018.[170] In October 2017, Netflix introduced the \"Skip Intro\" feature which allows customers to skip the intros to shows on its platform through a variety of techniques including manual reviewing, audio tagging, and machine learning.[171][172]\n In November 2017, Netflix signed an exclusive multi-year deal with Orange Is the New Black creator Jenji Kohan.[173] In November 2017, Netflix withdrew from co-hosting a party at the 75th Golden Globe Awards with The Weinstein Company due to the Harvey Weinstein sexual abuse cases.[174]\n In November 2017, Netflix announced that it would be making its first original Colombian series, to be executive produced by Ciro Guerra.[175] In December 2017, Netflix signed Stranger Things director-producer Shawn Levy and his production company 21 Laps Entertainment to what sources say is a four-year deal.[176] In 2017, Netflix invested in distributing exclusive stand-up comedy specials from Dave Chappelle, Louis C.K., Chris Rock, Jim Gaffigan, Bill Burr and Jerry Seinfeld.[177]\n In February 2018, Netflix acquired the rights to The Cloverfield Paradox from Paramount Pictures for $50 million and launched on its service on February 4, 2018, shortly after airing its first trailer during Super Bowl LII. Analysts believed that Netflix's purchase of the film helped to make the film instantly profitable for Paramount compared to a more traditional theatrical release, while Netflix benefited from the surprise reveal.[178][179] Other films acquired by Netflix include international distribution for Paramount's Annihilation[179] and Universal's News of the World and worldwide distribution of Universal's Extinction,[180] Warner Bros.' Mowgli: Legend of the Jungle,[181] Paramount's The Lovebirds[182] and 20th Century Studios' The Woman in the Window.[183] In March, the service ordered Formula 1: Drive to Survive, a racing docuseries following teams in the Formula One world championship.[184]\n In March 2018, Sky UK announced an agreement with Netflix to integrate Netflix's subscription VOD offering into its pay-TV service. Customers with its high-end Sky Q set-top box and service will be able to see Netflix titles alongside their regular Sky channels.[185] In October 2022, Netflix revealed that its annual revenue from the UK subscribers in 2021 was £1.4bn.[186]\n In April 2018, Netflix pulled out of the Cannes Film Festival, in response to new rules requiring competition films to have been released in French theaters. The Cannes premiere of Okja in 2017 was controversial, and led to discussions over the appropriateness of films with simultaneous digital releases being screened at an event showcasing theatrical film; audience members also booed the Netflix production logo at the screening. Netflix's attempts to negotiate to allow a limited release in France were curtailed by organizers, as well as French cultural exception law—where theatrically screened films are legally forbidden from being made available via video-on-demand services until at least 36 months after their release.[187][188][189] Besides traditional Hollywood markets as well as from partners like the BBC, Sarandos said the company also looking to expand investments in non-traditional foreign markets due to the growth of viewers outside of North America. At the time, this included programs such as Dark from Germany, Ingobernable from Mexico and 3% from Brazil.[190][191][192]\n On May 22, 2018, former president, Barack Obama, and his wife, Michelle Obama, signed a deal to produce docu-series, documentaries and features for Netflix under the Obamas' newly formed production company, Higher Ground Productions.[193][194]\n In June 2018, Netflix announced a partnership with Telltale Games to port its adventure games to the service in a streaming video format, allowing simple controls through a television remote.[195][196] The first game, Minecraft: Story Mode, was released in November 2018.[197] In July 2018, Netflix earned the most Emmy nominations of any network for the first time with 112 nods. On August 27, 2018, the company signed a five-year exclusive overall deal with international best–selling author Harlan Coben.[198] On the same day, the company signed an overall deal with Gravity Falls creator Alex Hirsch.[199] In October 2018, Netflix paid under $30 million to acquire Albuquerque Studios (ABQ Studios), a $91 million film and TV production facility with eight sound stages in Albuquerque, New Mexico, for its first U.S. production hub, pledging to spend over $1 billion over the next decade to create one of the largest film studios in North America.[200][201] In November 2018, Paramount Pictures signed a multi-picture film deal with Netflix, making Paramount the first major film studio to sign a deal with Netflix.[202] A sequel to AwesomenessTV's To All the Boys I've Loved Before was released on Netflix under the title To All the Boys: P.S. I Still Love You as part of the agreement.[203] In December 2018, the company announced a partnership with ESPN Films on a television documentary chronicling Michael Jordan and the 1997–98 Chicago Bulls season titled The Last Dance. It was released internationally on Netflix and became available for streaming in the United States three months after a broadcast airing on ESPN.[204][205]\n In January 2019, Sex Education made its debut as a Netflix original series, receiving much critical acclaim.[206] On January 22, 2019, Netflix sought and was approved for membership into the Motion Picture Association of America (MPAA), making it the first streaming service to join the association.[207] In February 2019, The Haunting creator Mike Flanagan joined frequent collaborator Trevor Macy as a partner in Intrepid Pictures and the duo signed an exclusive overall deal with Netflix to produce television content.[208] On May 9, 2019, Netflix contracted with Dark Horse Entertainment to make television series and films based on comics from Dark Horse Comics.[209] In July 2019, Netflix announced that it would be opening a hub at Shepperton Studios as part of a deal with Pinewood Group.[210] In early-August 2019, Netflix negotiated an exclusive multi-year film and television deal with Game of Thrones creators and showrunners David Benioff and D.B. Weiss.[211][212][213][214][215] The first Netflix production created by Benioff and Weiss was planned as an adaptation of Liu Cixin's science fiction novel The Three-Body Problem, part of the Remembrance of Earth's Past trilogy.[216] On September 30, 2019, in addition to renewing Stranger Things for a fourth season, Netflix signed The Duffer Brothers to an overall deal covering future film and television projects for the service.[217]\n On November 13, 2019, Netflix and Nickelodeon entered into a multi-year agreement to produce several original animated feature films and television series based on Nickelodeon's library of characters. This agreement expanded on their existing relationship, in which new specials based on the past Nickelodeon series Invader Zim and Rocko's Modern Life (Invader Zim: Enter the Florpus and Rocko's Modern Life: Static Cling respectively) were released by Netflix. Other new projects planned under the team-up include a music project featuring Squidward Tentacles from the animated television series SpongeBob SquarePants, and films based on The Loud House and Rise of the Teenage Mutant Ninja Turtles.[218][219][220] The agreement with Disney ended in 2019 due to the launch of Disney+, with its Marvel productions moving exclusively to the service in 2022.[221][222]\n In November 2019, Netflix announced that it had signed a long-term lease to save the Paris Theatre, the last single-screen movie theater in Manhattan. The company oversaw several renovations at the theater, including new seats and a concession stand.[223][224][225]\n In January 2020, Netflix announced a new four-film deal with Adam Sandler worth up to $275 million.[226] On February 25, 2020, Netflix formed partnerships with six Japanese creators to produce an original Japanese anime project. This partnership includes manga creator group CLAMP, mangaka Shin Kibayashi, mangaka Yasuo Ohtagaki, novelist and film director Otsuichi, novelist Tow Ubutaka, and manga creator Mari Yamazaki.[227] On March 4, 2020, ViacomCBS announced that it will be producing two spin-off films based on SpongeBob SquarePants for Netflix.[228] On April 7, 2020, Peter Chernin's Chernin Entertainment made a multi-year first-look deal with Netflix to make films.[229] On May 29, 2020, Netflix announced the acquisition of Grauman's Egyptian Theatre from the American Cinematheque to use as a special events venue.[230][231][232] In July 2020, Netflix appointed Sarandos as co-CEO.[233][234] In July 2020, Netflix invested in Black Mirror creators Charlie Brooker and Annabel Jones' new production outfit Broke And Bones.[235]\n In September 2020, Netflix signed a multi-million dollar deal with the Duke and Duchess of Sussex. Harry and Meghan agreed to a multi-year deal promising to create TV shows, films, and children's content as part of their commitment to stepping away from the duties of the royal family.[236][237] In September 2020, Hastings released a book about Netflix culture titled No Rules Rules: Netflix and the Culture of Reinvention, which was coauthored by Erin Meyer.[238] In December 2020, Netflix signed a first-look deal with Millie Bobby Brown to develop and star in several projects including a potential action franchise.[239]\n In March 2021, Netflix earned the most Academy Award nominations of any studio, with 36. Netflix won seven Academy Awards, which was the most by any studio.[240] Later that year, Netflix also won more Emmys than any other network or studio with 44 wins, tying the record for most Emmys won in a single year set by CBS in 1974.[241]\n On April 8, 2021, Sony Pictures Entertainment announced an agreement for Netflix to hold the U.S. pay television window rights to its releases beginning in 2022, replacing Starz and expanding upon an existing agreement with Sony Pictures Animation. The agreement also includes a first-look deal for any future direct-to-streaming films being produced by Sony Pictures, with Netflix required to commit to a minimum number of them.[242][243][244] On April 27, Netflix announced that it was opening its first Canadian headquarters in Toronto.[245] The company also announced that it would open an office in Sweden as well as Rome and Istanbul to increase its original content in those regions.[246]\n In early-June, Netflix hosted a first-ever week-long virtual event called \"Geeked Week\", where it shared exclusive news, new trailers, cast appearances and more about upcoming genre titles like The Witcher, The Cuphead Show!, and The Sandman.[247]\n On June 7, 2021, Jennifer Lopez's Nuyorican Productions signed a multi-year first-look deal with Netflix spanning feature films, TV series, and unscripted content, with an emphasis on projects that support diverse female actors, writers, and filmmakers.[248] On June 10, 2021, Netflix announced it was launching an online store for curated products tied to the Netflix brand and shows such as Stranger Things and The Witcher.[249][250] On June 21, 2021, Steven Spielberg's Amblin Partners signed a deal with Netflix to release multiple new feature films for the streaming service.[251][252] On June 30, 2021, Powerhouse Animation Studios (the studio behind Netflix's Castlevania) announced signing a first-look deal with the streamer to produce more animated series.[253]\n In July 2021, Netflix hired Mike Verdu, a former executive from Electronic Arts and Facebook, as vice president of game development, along with plans to add video games by 2022.[254] Netflix announced plans to release mobile games that would be included in subscribers' service plans.[255] Trial offerings were first launched for Netflix users in Poland in August 2021, offering premium mobile games based on Stranger Things including Stranger Things 3: The Game, for free to subscribers through the Netflix mobile app.[256]\n On July 14, 2021, Netflix signed a first-look deal with Joey King, star of The Kissing Booth franchise, in which King will produce and develop films for Netflix via her All The King's Horses production company.[257] On July 21, 2021, Zack Snyder, director of Netflix's Army of the Dead, announced he had signed his production company The Stone Quarry to a first-look deal with Netflix; his upcoming projects include a sequel to Army of the Dead and a sci-fi adventure film titled Rebel Moon.[258][259][260][261] In 2019, he agreed to produce an anime-style web series inspired by Norse mythology.[262][263]\n As of August 2021, Netflix Originals made up 40% of Netflix's overall library in the United States.[264] The company announced that \"TUDUM: A Netflix Global Fan Event\", a three-hour virtual behind the scenes featuring first-look reveals for 100 of the streamer's series, films and specials, would have its inaugural show in late September 2021.[265][266] According to Netflix, the show garnered 25.7 million views across Netflix's 29 Netflix YouTube channels, Twitter, Twitch, Facebook, TikTok and Tudum.com.[267]\n Also in September, the company announced The Queen's Ball: A Bridgerton Experience, launching in 2022 in Los Angeles, Chicago, Montreal, and Washington, D.C..[268]\n Squid Game, a South Korean survival drama created and produced by Hwang Dong-hyuk, rapidly became the service's most-watched show within a week of its launch in many markets on September 17, 2021, including Korea, the U.S. and the United Kingdom.[192] Within its first 28 days on the service, Squid Game drew more than 111 million viewers, surpassing Bridgerton and becoming Netflix's most-watched show.[269]\n On September 20, 2021, Netflix signed a long-term lease with Aviva Investors to operate and expand the Longcross Studios in Surrey, UK.[270] On September 21, 2021, Netflix announced that it would acquire the Roald Dahl Story Company, which manages the rights to Roald Dahl's stories and characters, for an undisclosed price and would operate it as an independent company.[271][272][273][274] The company acquired Night School Studio, an independent video game developer, on September 28, 2021.[275]\n On October 13, 2021, Netflix announced the launch of the Netflix Book Club, partnering with Starbucks for a social series called But Have You Read the Book?.[276] Uzo Aduba became inaugural host of the series and announced monthly book selections set to be adapted by the streamer. Aduba speaks with the cast, creators, and authors about the book adaptation process over a cup of coffee at Starbucks.[277][278] Through October 2021, Netflix commonly reported viewership for its programming based on the number of viewers or households that watched a show in a given period (such as the first 28 days from its premiere) for at least two minutes. On the announcement of its quarterly earnings in October 2021, the company stated that it would switch its viewership metrics to measuring the number of hours that a show was watched, including rewatches, which the company said was closer to the measurements used in linear broadcast television, and thus \"our members and the industry can better measure success in the streaming world.\"[279]\n Netflix officially launched mobile games on November 2, 2021, for Android users around the world. Through the app, subscribers had free access to five games, including two previously made Stranger Things titles. Netflix intends to add more games to this service over time.[280] On November 9, the collection launched for iOS.[281] Some games in the collection require an active internet connection to play, while others will be available offline. Netflix Kids' accounts will not have games available.[282] On November 16, Netflix announced the launch of \"Top10 on Netflix.com\", a new website with weekly global and country lists of the most popular titles on their service based on their new viewership metrics.[283] On November 22, Netflix announced that it would acquire Scanline VFX, the visual effects and animation company behind Cowboy Bebop and Stranger Things.[284] On the same day, Roberto Patino signed a deal with Netflix and established his production banner, Analog Inc., in partnership with the company. Patino's first project under the deal is a series adaptation of Image Comics' Nocterra.[285]\n On December 6, 2021, Netflix and Stage 32 announced that they have teamed up the workshops at the Creating Content for the Global Marketplace program.[286] On December 7, 2021, Netflix partnered with IllumiNative, a woman-led non-profit organization, for the Indigenous Producers Training Program.[287][288] On December 9, Netflix announced the launch of \"Tudum\", an official companion website that offers news, exclusive interviews and behind-the-scenes videos for its original television shows and films.[289] On December 13, Netflix signed a multi-year overall deal with Kalinda Vazquez.[290] On December 16, 2021, Netflix signed a multi-year creative partnership with Spike Lee and his production company 40 Acres and a Mule Filmworks to develop film and television projects.[291]\n In compliance with the EU Audiovisual Media Services Directive and its implementation in France, Netflix reached commitments with French broadcasting authorities and film guilds, as required by law, to invest a specific amount of its annual revenue into original French films and series. These films must be theatrically released and would not be allowed to be carried on Netflix until 15 months after their release.[292][293]\n In January 2022, Netflix ordered additional sports docuseries from Drive to Survive producers Box to Box Films, including a series that would follow PGA Tour golfers, and another that would follow professional tennis players on the ATP and WTA Tour circuits.[294][295]\n The company announced plans to acquire Next Games in March 2022 for €65 million as part of Netflix's expansions into gaming. Next Games had developed the mobile title Stranger Things: Puzzle Tales as well as two The Walking Dead mobile games.[296] Later in the month, Netflix also acquired the Texas-based mobile game developer, Boss Fight Entertainment, for an undisclosed sum.[297]\n On March 15, 2022, Netflix announced a partnership with Dr. Seuss Enterprises to produce five new series and specials based on Seuss properties following the success of Green Eggs and Ham.[298][299] On March 29, 2022, Netflix announced that it would open an office in Poland to serve as a hub for its original productions across Central and Eastern Europe.[300] On March 30, 2022, Netflix extended its lease agreement with Martini Film Studios, just outside Vancouver, Canada, for another five years.[301] On March 31, 2022, Netflix ordered a docuseries that would follow teams in the 2022 Tour de France, which would also be co-produced by Box to Box Films.[302]\n Following the 2022 Russian invasion of Ukraine, Netflix suspended its operations and future projects in Russia.[303][4] It also announced that it would not comply with a proposed directive by Roskomnadzor requiring all internet streaming services with more than 100,000 subscribers to integrate the major free-to-air channels (which are primarily state-owned).[304] A month later, ex-Russian subscribers filed a class action lawsuit against Netflix.[305][306]\n Netflix stated that 100 million households globally were sharing passwords to their account with others, and that Canada and the United States accounted for 30 million of them. Following these announcements, Netflix's stock price fell by 35 percent.[307][308][309][310][311] By June 2022, Netflix had laid off 450 full-time and contract employees as part of the company's plan to trim costs amid lower than expected subscriber growth.[312][313][314][315]\n On April 13, 2022, Netflix released the series Our Great National Parks, which was hosted and narrated by former US President Barack Obama.[316] It also partnered with Group Effort Initiative, a company founded by Ryan Reynolds and Blake Lively, to provide opportunities behind the camera for those in underrepresented communities.[317] On the same day, Netflix partnered with Lebanon-based Arab Fund For Arts And Culture for supporting the Arab female filmmakers. It will provide a one-time grant of $250,000 to female producers and directors in the Arab world through the company's Fund for Creative Equity.[318] Also on the same day, Netflix announced an Exploding Kittens mobile card game tied to a new animated TV series, which will launch in May.[319] Netflix formed a creative partnership with J. Miles Dale.[320] The company also formed a partnership with Japan's Studio Colorido, signing a multi-film deal to boost their anime content in Asia. The streaming giant is said to co-produce three feature films with the studio, the first of which will premiere in September 2022.[321] On April 28, the company launched its inaugural Netflix Is a Joke comedy festival, featuring more than 250 shows over 12 nights at 30-plus locations across Los Angeles, including the first-ever stand-up show at Dodger Stadium.[322][323]\n The first volume of Stranger Things 4 logged Netflix's biggest premiere weekend ever for an original series with 286.79 million hours viewed.[324] This was preceded by a new Stranger Things interactive experience hosted in New York City that was developed by the show's creators.[325] After the release of the second volume of Stranger Things 4 on July 1, 2022, it became Netflix's second title to receive more than one billion hours viewed.[326]\n On July 19, 2022, Netflix announced plans to acquire Australian animation studio Animal Logic.[327][328] That month, in collaboration with Sennheiser, Netflix began to add Ambeo 2-channel audio mixes (referred to as \"spatial audio\") to selected original productions, which allows simulated surround sound on stereo speakers and headphones.[329][330]\n On September 5, 2022, Netflix opened an office in Warsaw, Poland responsible for the service's operations in 28 markets in Central and Eastern Europe.[331]\n On October 4, 2022, Netflix have signed a creative partnership with Andrea Berloff and John Gatins.[332] On October 11, Netflix signed up with the Broadcasters' Audience Research Board for external measurement of viewership in the UK.[333] On October 12, Netflix signed to build a production complex at Fort Monmouth in Eatontown, New Jersey.[334] On October 18, Netflix began exploring a cloud gaming offering and opened a new gaming studio in Southern California.[335]\n On November 7, 2022, Netflix announced a strategic partnership with The Seven, a Japanese production company owned by TBS Holdings, to produce multiple original live-action titles for the subscribers over the next five years.[336][337] On December 12, 2022, Netflix announced that sixty-percent of its subscribers had watched a Korean drama.[338][339] CEO Ted Sarandos attributed the increase in viewership of Korean content among Americans to Korean films and dramas being \"often unpredictable\" and catching \"the American audience by surprise\".[340][341]\n On January 10, 2023, Netflix announced plans to open an engineering hub in its Warsaw office. The hub is to provide Netflix's creative partners with software solutions for the production of films and series.[342] In February 2023, Netflix launched a wider rollout of spatial audio, and began allowing Premium subscribers to download content for offline playback on up to six devices (expanded from four).[329][330]\n On March 4, 2023, Netflix broadcast its first-ever global live-streaming event, the stand-up comedy special Chris Rock: Selective Outrage.[343]\n Netflix reworked its viewership metrics again in June 2023. Viewership of shows was measured during the first 91 days of availability, instead of the first 28 days, and now are based on the total viewership hours divided by the total hours of the show itself. This provided more equal considerations for shorter shows and movies compared to longer ones.[344]\n In August 2023, the company announced Netflix Stories, a collection of interactive narrative games from Netflix series and movies such as Love is Blind, Money Heist and Virgin River.[345]\n \n In January 2023, Greg Peters and Ted Sarandos were named co-CEOs of Netflix, with Hastings assuming the role of executive chairman.[346][347] Peters previously served as COO and Chief Product Officer, while Sarandos served as Chief Content Officer.[348][349]\n On April 18, 2023, Netflix announced that it would discontinue its DVD-by-mail service on September 29.[350] Users of the service were able to keep the DVDs that they had received. Over its lifetime the service had sent out over 5 billion shipments.[351][352]\n In October 2023, Eunice Kim was promoted to Chief Product Officer and Elizabeth Stone was promoted to Chief Technology Officer.[353] That same month, amid a restructuring of its animation division, Netflix announced a multi-film agreement with Skydance Animation beginning with Spellbound (2024). The agreement partially replaces one it had with Apple TV+.[354][355]\n In December 2023, Netflix released its first \"What We Watched: A Netflix Engagement Report\", a look at viewership for every original and licensed title watched more than 50,000 hours from January to June 2023. The company also announced plans to publish the report twice a year.[356][357] In its first report for the first six months of 2023, it reported that The Night Agent was the most watched show on globally in that period.[358]\n On January 23, 2024, Netflix announced a major agreement with professional wrestling promotion WWE, under which it will acquire the international rights to its live weekly program Raw beginning January 6, 2025; the rights will initially cover the United States, Canada, the United Kingdom, and Latin America, and expand to other territories over time. Outside of the United States, it will also hold international rights to all three of WWE's main weekly programs (Raw, SmackDown, and NXT), premium live events, and documentaries among other content. The agreement was reported to be valued at $500 million per-year over ten years.[359][360][361]\n In February 2024, Netflix joined with Peter Morgan, creator of the Netflix series The Crown, to produce the play Patriots on Broadway. The venture is the first Broadway credit for the company but not its first stage project. It was actively involved as a producer of Stranger Things: The First Shadow in London. Both productions share a lead producer, Sonia Friedman.[362]\n In May 2024, the company hosted its second Netflix Is a Joke festival in Los Angeles. It streamed several specials from the festival live, including Katt Williams's Woke Folk and The Roast of Tom Brady, both of which ranked on Netflix's global top 10 the following two weeks.[363][364] That same month, Netflix announced that it would stream both National Football League Christmas games in 2024.[365] For 2025 and 2026, the streamer will have exclusive rights to at least one NFL Christmas game each year.[366]\n In June 2024, Netflix announced that it would develop new entertainment venues known as \"Netflix House\" at King of Prussia Mall in Pennsylvania and Galleria Dallas in Texas. The spaces will feature retail shops, restaurants, and other interactive experiences related to Netflix original content, building upon other \"pop-up\" initiatives to promote individual programs.[367]\n In November 2024, Netflix announced that it would discontinue further work on interactive specials and remove all but four of them from the platform, citing a desire to focus on \"technological efforts in other areas\".[368] On November 15, 2024, Netflix streamed a boxing event from AT&T Stadium in Arlington, Texas, featuring as co-main events an exhibition match between Jake Paul and Mike Tyson, and Katie Taylor vs. Amanda Serrano for the WBA, WBC, IBF, WBO, and The Ring lightweight titles. While afflicted by technical issues, Paul's promoter reported that the stream had a peak concurrent viewership of 65 million viewers, surpassing the 2023 ICC Men's Cricket World Cup final (which had a reported 57 million concurrent streams on Disney+ Hotstar) as the most live-streamed sporting event.[369][370] Netflix stated that the event had an \"average minute audience\" (AMA) of 108 million worldwide, and that the AMA of 47 million in the United States made the Taylor vs. Serrano bout the most-watched women's professional sporting event in U.S. history.[371]\n On December 20, 2024, FIFA announced that Netflix would be the exclusive U.S. broadcaster of the 2027 and 2031 FIFA Women's World Cup, in what was deemed the platform's most significant push into sports content.[372]\n On Christmas Day 2024, Netflix aired its first-ever NFL games: the Kansas City Chiefs versus the Pittsburgh Steelers, and the Baltimore Ravens versus the Houston Texans. The games both averaged over 30 million global viewers and became the two most-streamed NFL games in US history, while simultaneously creating Netflix’s most-watched Christmas Day ever in the US.[373]\n Netflix is available in every country and territory except for China, North Korea, Syria, and Russia.[376]\n In January 2016, Netflix announced it would begin VPN blocking since it can be used to watch videos from a country where they are unavailable.[377] The result of the VPN block is that people can only watch videos available worldwide and other videos are hidden from search results.[378] Variety is present on Netflix. Hebrew and right-to-left interface orientation, which is a common localization strategy in many markets, are what define the Israeli user interface's localization, and in some regions, Netflix offers a more affordable mobile-only subscription.[379]\n Customers can subscribe to one of three plans; the difference in plans relates to video resolution, the number of simultaneous streams, and the number of devices to which content can be downloaded.[380]\n At the end of Q1 2022, Netflix estimated that 100 million households globally were sharing passwords to their account with others.[310] In March 2022, Netflix began to charge a fee for additional users in Chile, Peru, and Costa Rica to attempt to control account sharing.[308][309][310] On July 18, 2022, Netflix announced that it would test the account sharing feature in more countries, including Argentina, Dominican Republic, El Salvador, Guatemala and Honduras.[381] On October 17, Netflix launched Profile Transfer to help end account sharing.[382]\n On July 13, 2022, Netflix announced plans to launch an advertising-supported subscription option.[383] Netflix's planned advertising tier would not allow subscribers to download content like the existing ad-free platform.[384] On July 20, 2022, it was announced that the advertising-supported tier would be coming to Netflix in 2023 but it would not feature the full library of content.[385] In October, the launch date was announced as November 3, 2022, and was launched in 12 countries: United States, Canada, Mexico, Brazil, United Kingdom, France, Germany, Italy, Spain, Australia, Japan and South Korea.[386][387][388] The ad-supported plan was called \"Basic with Ads\" and it cost $6.99 per month in the United States at launch.[389]\n On February 24, 2023, Netflix cut subscription prices in more than 30 countries around the world to attract more subscribers from those countries. Malaysia, Indonesia, Thailand, the Philippines, Croatia, Venezuela, Kenya, and Iran are on the list of countries where the cost for a subscription will be reduced.[390] In the same month stronger anti-password-sharing rules were expanded to Canada, New Zealand, Portugal, and Spain.[391] In May 2023, these measures were further expanded to United States and Brazil subscribers.[392][393][394]\n In July 2023, Netflix added 5.9 million subscribers for the second quarter of the year for a total of 238.39 million subscribers overall. The United States and Canada accounted for 1.2 million subscribers which was the largest regional quarterly gain since 2021.[7][395]\n Netflix announced in February that it was going to enforce stricter regulations for password sharing. In May 2023, Netflix began cracking down on password-sharing in the US, UK, and Australia. Under these new rules, multiple people can use and share one account, but they have to be under the same household. Netflix defines a household as people who live in the same location as the owner of the account. Users are asked to set a primary location based on the device's IP address.[396]\n Netflix reported 8.05 million new subscribers in Q2 2024, up from 5.9 million subscribers added in Q2 2023.[397][398]\n In July 2024, Netflix started phasing out its cheapest subscription plan for users in France and the US, a year after the plan was removed for Canada and the UK. Members in these countries have the option to sign up for either the standard ad-free plan or the ad plan.[399][400]\n Netflix can be accessed via a web browser, while Netflix apps are available on various platforms, including Blu-ray players, tablet computers, mobile phones, smart TVs, digital media players, and video game consoles. Currently supported game consoles include:\n Several older devices no longer support Netflix. For home gaming consoles, this includes the PlayStation 2,[403] PlayStation TV, Wii[404] and Wii U.[405] For handheld gaming consoles, this includes the Nintendo 3DS family of systems and the PlayStation Vita.[406] The second and third generation Apple TV previously supported Netflix with an ad-free plan, but the app was automatically removed on these devices on July 31, 2024.[407]\n In addition, a growing number of multichannel television providers, including cable television and IPTV services, have added Netflix apps accessible within their own set-top boxes, sometimes with the ability for its content (along with those of other online video services) to be presented within a unified search interface alongside linear television programming as an \"all-in-one\" solution.[408][409][410][411]\n The maximum video resolution supported on computers is dependent on the DRM systems available on a particular operating system and web browser.[412]\n \"Netflix Originals\" are content that is produced, co-produced, or distributed exclusively by Netflix. Netflix funds its original shows differently than other TV networks when they sign a project, providing the money upfront and immediately ordering two seasons of most series.[416] It keeps licensing rights, which normally give production companies future revenue opportunities from syndication, merchandising, etc.[159][417]\n Over the years, Netflix output ballooned to a level unmatched by any television network or streaming service. According to Variety Insight, Netflix produced a total of 240 new original shows and movies in 2018, then climbed to 371 in 2019, a figure \"greater than the number of original series that the entire U.S. TV industry released in 2005.\"[418] The Netflix budget allocated to production increased annually, reaching $13.6 billion in 2021 and projected to hit $18.9 billion by 2025, a figure that once again overshadowed any of its competitors.[419] As of August 2022, original productions made up 50% of Netflix's overall library in the United States.[420]\n Netflix has exclusive pay TV deals with several studios. The deals give Netflix exclusive streaming rights while adhering to the structures of traditional pay TV terms.\n Distributors that have licensed content to Netflix include Warner Bros., Universal Pictures, Sony Pictures Entertainment and previously The Walt Disney Studios. Netflix also holds current and back-catalog rights to television programs distributed by Walt Disney Television, DreamWorks Classics, Kino International, PBS, Warner Bros. Television and Paramount Global Content Distribution, along with titles from other companies such as ABS-CBN Studios,[421] GMA Pictures,[422] Cignal Entertainment, MQ Studios, Regal Entertainment, Viva Films, MNC Media, Screenplay Films, Soraya Intercine Films, Falcon Pictures [id], MD Pictures [id], Rapi Films, Starvision Plus [id], CJ ENM, JTBC, Kakao Entertainment, TBS, TV Asahi, Fuji TV, Mediacorp, Primeworks Studios, GMM Grammy, Public Television Service, Gala Television, ITV Studios, Hasbro Entertainment and StudioCanal. Formerly, the streaming service also held rights to select television programs distributed by NBCUniversal Television Distribution, Sony Pictures Television and 20th Century Fox Television.\n Netflix negotiated to distribute animated films from Universal that HBO declined to acquire, such as The Lorax, ParaNorman, and Minions.[423]\n Netflix holds exclusive streaming rights to the film library of Studio Ghibli (except Grave of the Fireflies) worldwide except in the U.S. and Japan as part of an agreement signed with Ghibli's international sales holder Wild Bunch in 2020.[424]\n In July 2021, Netflix hired Mike Verdu, a former executive from Electronic Arts and Facebook, as vice president of game development, along with plans to add video games by 2022.[425] Netflix announced plans to release mobile games that would be included in subscribers' service plans.[426] Trial offerings were first launched for Netflix users in Poland in August 2021, offering premium mobile games based on Stranger Things including Stranger Things 3: The Game, for free to subscribers through the Netflix mobile app.[427]\n Netflix officially launched mobile games on November 2, 2021, for Android users around the world. Through the app, subscribers had free access to five games, including two previously made Stranger Things titles. Netflix intends to add more games to this service over time.[428] On November 9, the collection launched for iOS.[429] Verdu said in October 2022 that besides continuing to expand their portfolio of games, they were also interested in cloud gaming options.[430]\n To support the games effort, Netflix began acquiring and forming a number of studios. The company acquired Night School Studio, an independent video game developer, in September 2021.[431] Netflix announced plans to acquire Next Games in March 2022 for €65 million as part of Netflix's expansions into gaming. Next Games had developed the mobile title Stranger Things: Puzzle Tales as well as two The Walking Dead mobile games.[432] Later in the month, Netflix also acquired the Texas-based mobile game developer, Boss Fight Entertainment, for an undisclosed sum.[297] Netflix opened a mobile game studio in Helsinki, Finland in September 2022,[433] and a new studio, their fifth overall, in southern California in October 2022,[430] alongside the acquisition of Spry Fox in Seattle.[434]\n In June 2024, Verdu was moved into a new role focusing on \"innovation in game development.\"[435] The next month, Netflix hired Alain Tascan, vice president of game development at Epic Games, to head up Netflix Games.[436] As of July 2024, Netflix has over 80 games in development, releasing at least one game each month to attract fans.[437] The company shut down its Southern California \"Team Blue\" AAA gaming studio in October 2024, leading to the departure of developers like Overwatch producer Chacko Sonny, Halo veteran Joseph Staten and God of War art director Rafael Grassetti.[438] Netflix indicated that it maintains a commitment to grow its gaming business despite the changes.[439][440] In late October, Netflix announced several games based on hit series including Netflix Stories: Outer Banks, Netflix Stories: A Perfect Couple, Netflix Stories: A Virgin River Christmas, and The Ultimatum: Choices, as well as a new daily word game in partnership with TED Talks, TED Tumblewords.[441][442]\n Netflix freely peers with Internet service providers (ISPs) directly and at common Internet exchange points. In June 2012, a custom content delivery network, Open Connect, was announced.[443] For larger ISPs with over 100,000 subscribers, Netflix offers free Netflix Open Connect computer appliances that cache their content within the ISPs' data centers or networks to further reduce Internet transit costs.[444][445] By August 2016, Netflix closed its last physical data center, but continued to develop its Open Connect technology.[446] A 2016 study at the University of London detected 233 individual Open Connect locations on over six continents, with the largest amount of traffic in the US, followed by Mexico.[447][448]\n As of July 2017, Netflix series and movies accounted for more than a third of all prime-time download Internet traffic in North America.[449]\n On October 1, 2008, Netflix offered access to its service via a public application programming interface (API).[450] It allowed access to data for all Netflix titles, and allows users to manage their movie queues. The API was free and allowed commercial use.[451] In June 2012, Netflix began to restrict the availability of its public API.[452] Netflix instead focused on a small number of known partners using private interfaces, since most traffic came from those private interfaces.[453] In November 2014, Netflix retired the public API.[454] Netflix then partnered with the developers of eight services deemed the most valuable, including Instant Watcher, Fanhattan, Yidio and Nextguide.[455]\n Netflix presents viewers with recommendations based on interactions with the service, such as previous viewing history and ratings of viewed content. These are often grouped into genres and formats, or feature the platform's highest-rated content. Each title is presented with a thumbnail. Before around 2015, these were the same key art for everyone, but since then has been customized. Netflix may select a specific key art for a thumbnail based on viewing history,[456] such as an actor or scene type based on genre preferences.[457] Some thumbnails are generated from video stills.[458]\n The Netflix recommendation system is a vital part of the streaming platform's success, enabling personalized content suggestions for hundreds of millions of subscribers worldwide.[459] Using advanced machine learning algorithms, Netflix analyzes user interactions, including viewing history, searches, and ratings, to deliver personalized recommendations for movies and TV shows.\n The recommendation system considers individual user preferences, similarities with other users with comparable tastes, specific title attributes (genre, release year, etc.), device usage patterns, and viewing time. As users interact with the platform and provide feedback with their viewing habits, the recommendation system is able to adapt and refine its suggestions over time. Netflix uses a two-tiered ranking system, using the presentation of titles on the homepage for easy navigation to maximize user engagement. This is done by organizing content into rows and ranking the titles within each row based on how much the user would be interested in it.[459] Netflix also uses A/B testing to determine what causes the biggest interest and engagement related to options concerning movie suggestions and how titles are organized.\n Tags like \"bittersweet\", \"sitcom\", or \"intimate\" are assigned to each title by Netflix employees.[460] Netflix also uses the tags to create recommendation micro-genres like \"Goofy TV Shows\" or \"Girls Night In\".[460]\n On July 18, 2013, Netflix earned the first Primetime Emmy Awards nominations for original streaming programs at the 65th Primetime Emmy Awards. Three of its series, Arrested Development, Hemlock Grove and House of Cards, earned a combined 14 nominations (nine for House of Cards, three for Arrested Development and two for Hemlock Grove).[461] The House of Cards episode \"Chapter 1\" received four nominations for both the 65th Primetime Emmy Awards and 65th Primetime Creative Arts Emmy Awards, becoming the first episode of a streaming television series to receive a major Primetime Emmy Award nomination. With its win for Outstanding Cinematography for a Single-Camera Series, \"Chapter 1\" became the first episode from a streaming service to be awarded an Emmy.[461][462][463] David Fincher's win for Directing for a Drama Series for House of Cards made the episode the first from a streaming service to win a Primetime Emmy.[464]\n On November 6, 2013, Netflix earned its first Grammy nomination when You've Got Time by Regina Spektor—the main title theme song for Orange Is the New Black—was nominated for Best Song Written for Visual Media.[465]\n On December 12, 2013, the network earned six nominations for Golden Globe Awards, including four for House of Cards.[466] Among those nominations was Wright for Golden Globe Award for Best Actress – Television Series Drama for her portrayal of Claire Underwood, which she won. With the accolade, Wright became the first actress to win a Golden Globe for a streaming television series. It also marked Netflix's first major acting award.[467][468][469] House of Cards and Orange is the New Black also won Peabody Awards in 2013.[470]\n On January 16, 2014, Netflix became the first streaming service to earn an Academy Award nomination when The Square was nominated for Best Documentary Feature.[471]\n On July 10, 2014, Netflix received 31 Emmy nominations. Among other nominations, House of Cards received nominations for Outstanding Drama Series, Outstanding Directing in a Drama Series and Outstanding Writing in a Drama Series. Kevin Spacey and Robin Wright were nominated for Outstanding Lead Actor and Outstanding Lead Actress in a Drama Series. Orange is the New Black was nominated in the comedy categories, earning nominations for Outstanding Comedy Series, Outstanding Writing for a Comedy Series and Outstanding Directing for a Comedy Series. Taylor Schilling, Kate Mulgrew, and Uzo Aduba were respectively nominated for Outstanding Lead Actress in a Comedy Series, Outstanding Supporting Actress in a Comedy Series and Outstanding Guest Actress in a Comedy Series (the latter was for Aduba's recurring role in season one, as she was promoted to series regular for the show's second season).[472]\n Netflix got the largest share of 2016 Emmy Award nominations, with 16 major nominations. However, streaming shows only got 24 nominations out of a total of 139, falling significantly behind cable. The 16 Netflix nominees were: House of Cards with Kevin Spacey, A Very Murray Christmas with Bill Murray, Unbreakable Kimmy Schmidt, Master of None, and Bloodline.[473]\n Stranger Things received 19 nominations at the 2017 Primetime Emmy Awards, while The Crown received 13 nominations.[474]\n In December 2017, Netflix was awarded PETA's Company of the Year for promoting animal rights movies and documentaries like Forks Over Knives and What the Health.[475][476]\n At the 90th Academy Awards, held on March 4, 2018, the film Icarus, distributed by Netflix, won its first Academy Award for Best Documentary Feature Film. During his remarks backstage, director and writer Bryan Fogel remarked that Netflix had \"single-handedly changed the documentary world.\" Icarus had its premiere at the 2017 Sundance Film Festival and was bought by Netflix for $5 million, one of the biggest deals ever for a non-fiction film.[477] Netflix became the network whose programs received more nomination at the 2018 Primetime and Creative Arts Emmy Awards with 112 nominations, therefore breaking HBO's 17-years record as a network whose programs received more nomination at the Emmys, which received 108 nominations.[478][479]\n On January 22, 2019, films distributed by Netflix scored 15 nominations for the 91st Academy Awards, including Academy Award for Best Picture for Alfonso Cuarón's Roma, which was nominated for 10 awards.[480] The 15 nominations equal the total nominations films distributed by Netflix had received in previous years.\n In 2020, Netflix received 20 TV nominations and films distributed by Netflix also got 22 film nominations at the 78th Golden Globe Awards. It secured three out of the five nominations for best drama TV series for The Crown, Ozark and Ratched and four of the five nominations for best actress in a TV series: Olivia Colman, Emma Corrin, Laura Linney and Sarah Paulson.[481][482]\n In 2020, Netflix earned 24 Academy Award nominations, marking the first time a streaming service led all studios.[483]\n Films and programs distributed by Netflix received 30 nominations at the 2021 Screen Actors Guild Awards, more than any other distribution company, where their distributed films and programs won seven awards including best motion picture for The Trial of the Chicago 7 and best TV drama for The Crown.[484][485] Netflix also received the most nominations of any studio at the 93rd Academy Awards—35 total nominations with 7 award wins.[486][487]\n In February 2022, The Power of the Dog, a gritty western distributed by Netflix and directed by Jane Campion, received 12 nominations, including Best Picture, for the 94th annual Academy Awards. Films distributed by the streamer received a total of 72 nominations.[488] Campion became the third female to receive the Best Director award, winning her second Oscar for The Power of the Dog.[489] At the 50th International Emmy Awards, Netflix original Sex Education won Best Comedy Series.[490] Later that year, Netflix received 26 Emmy Awards including six for Squid Game. The Squid Game wins for Outstanding Lead Actor in a Drama Series and Outstanding Directing for a Drama Series were the first-ever for a non-English language series in those categories.[491]\n In March 2023, Netflix won six Academy Awards including four for All Quiet on the Western Front which was the most awarded Netflix film in its history. Guillermo del Toro's Pinocchio was the first streaming film to named Best Animated Feature and The Elephant Whisperers was the first Indian-produced film to receive Best Documentary Short Film.[492] Netflix received 103 Emmy nominations including 13 each for the limited series Beef and Dahmer – Monster: The Jeffrey Dahmer Story.[493]\n In July 2024, Netflix received 107 Emmy nominations, which was the most of any network.[494]\n Netflix has been subject to criticism from various groups and individuals as its popularity and market reach increased in the 2010s.\n Customers have complained about price increases in Netflix offerings dating back to the company's decision to separate its DVD rental and streaming services, which was quickly reversed. As Netflix increased its streaming output, it has faced calls to limit accessibility to graphic violence and include viewer advisories for issues such as sensationalism and promotion of pseudoscience. Netflix's content has also been criticized by disability rights movement advocates for lack of closed captioning quality.[495]\n Some media organizations and competitors have criticized Netflix for selectively releasing ratings and viewer numbers of its original programming. The company has made claims boasting about viewership records without providing data to substantiate its successes or using problematic estimation methods.[496] In March 2020, some government agencies called for Netflix and other streamers to limit services due to increased broadband and energy consumption as use of the platform increased. In response, the company announced it would reduce bit rate across all streams in Europe, thus decreasing Netflix traffic on European networks by around 25 percent. These same steps were later taken in India.[497]\n In May 2022, Netflix's shareholder Imperium Irrevocable Trust filed a lawsuit against the company for violating the U.S. securities laws.[498] In January 2024, a federal judge dismissed the suit, stating that shareholders failed to provide instances of Netflix lying about subscriber growth.[499]\n In May 2023, Netflix officially banned the use of password sharing between individuals of different households, meaning sharing an account was only available to those living in the same house.[500][501]\n"
    },
    {
        "title": "Virtual assistant",
        "url": "https://en.wikipedia.org/wiki/Virtual_assistant",
        "content": "\n A virtual assistant (VA) is a software agent that can perform a range of tasks or services for a user based on user input such as commands or questions, including verbal ones. Such technologies often incorporate chatbot capabilities to simulate human conversation, such as via online chat, to facilitate interaction with their users. The interaction may be via text, graphical interface, or voice - as some virtual assistants are able to interpret human speech and respond via synthesized voices.\n In many cases, users can ask their virtual assistants questions, control home automation devices and media playback, and manage other basic tasks such as email, to-do lists, and calendars - all with verbal commands.[1] In recent years, prominent virtual assistants for direct consumer use have included Apple's Siri, Amazon Alexa, Google Assistant, and Samsung's Bixby.[2] Also, companies in various industries often incorporate some kind of virtual assistant technology into their customer service or support.[3]\n Into the 2020s, the emergence of artificial intelligence based chatbots, such as ChatGPT, has brought increased capability and interest to the field of virtual assistant products and services.[4][5][6]\n Radio Rex was the first voice activated toy, patented in 1916[7] and released in 1922.[8] It was a wooden toy in the shape of a dog that would come out of its house when its name is called.\n In 1952, Bell Labs presented \"Audrey\", the Automatic Digit Recognition machine. It occupied a six-foot-high relay rack, consumed substantial power, had streams of cables and exhibited the myriad maintenance problems associated with complex vacuum-tube circuitry. It could recognize the fundamental units of speech, phonemes. It was limited to accurate recognition of digits spoken by designated talkers. It could therefore be used for voice dialing, but in most cases push-button dialing was cheaper and faster, rather than speaking the consecutive digits.[9]\n Another early tool which was enabled to perform digital speech recognition was the IBM Shoebox voice-activated calculator, presented to the general public during the 1962 Seattle World's Fair after its initial market launch in 1961. This early computer, developed almost 20 years before the introduction of the first IBM Personal Computer in 1981, was able to recognize 16 spoken words and the digits 0 to 9.\n The first natural language processing computer program or the chatbot ELIZA was developed by MIT professor Joseph Weizenbaum in the 1960s. It was created to \"demonstrate that the communication between man and machine was superficial\".[10] ELIZA used pattern matching and substitution methodology into scripted responses to simulate conversation, which gave an illusion of understanding on the part of the program.\n Weizenbaum's own secretary reportedly asked Weizenbaum to leave the room so that she and ELIZA could have a real conversation. Weizenbaum was surprised by this, later writing: \"I had not realized ... that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people.[11]\n This gave name to the ELIZA effect, the tendency to unconsciously assume computer behaviors are analogous to human behaviors; that is, anthropomorphisation, a phenomenon present in human interactions with virtual assistants.\n The next milestone in the development of voice recognition technology was achieved in the 1970s at the Carnegie Mellon University in Pittsburgh, Pennsylvania with substantial support of the United States Department of Defense and its DARPA agency, funded five years of a Speech Understanding Research program, aiming to reach a minimum vocabulary of 1,000 words. Companies and academia including IBM, Carnegie Mellon University (CMU) and Stanford Research Institute took part in the program.\n The result was \"Harpy\", it mastered about 1000 words, the vocabulary of a three-year-old and it could understand sentences. It could process speech that followed pre-programmed vocabulary, pronunciation, and grammar structures to determine which sequences of words made sense together, and thus reducing speech recognition errors.\n In 1986, Tangora was an upgrade of the Shoebox, it was a voice recognizing typewriter. Named after the world's fastest typist at the time, it had a vocabulary of 20,000 words and used prediction to decide the most likely result based on what was said in the past. IBM's approach was based on a hidden Markov model, which adds statistics to digital signal processing techniques. The method makes it possible to predict the most likely phonemes to follow a given phoneme. Still each speaker had to individually train the typewriter to recognize his or her voice, and pause between each word.\n In 1983, Gus Searcy invented the \"Butler In A Box\", an electronic voice home controller system.[12]\n In the 1990s, digital speech recognition technology became a feature of the personal computer with IBM, Philips and Lernout & Hauspie fighting for customers. Much later the market launch of the first smartphone IBM Simon in 1994 laid the foundation for smart virtual assistants as we know them today.[citation needed]\n In 1997, Dragon's Naturally Speaking software could recognize and transcribe natural human speech without pauses between each word into a document at a rate of 100 words per minute. A version of Naturally Speaking is still available for download and it is still used today, for instance, by many doctors in the US and the UK to document their medical records.[citation needed]\n In 2001 Colloquis publicly launched  SmarterChild, on platforms like AIM and MSN Messenger. While entirely text-based SmarterChild was able to play games, check the weather, look up facts, and converse with users to an extent.[13]\n The first modern digital virtual assistant installed on a smartphone was Siri, which was introduced as a feature of the iPhone 4S on 4 October 2011.[14] Apple Inc. developed Siri following the 2010 acquisition of Siri Inc., a spin-off of SRI International, which is a research institute financed by DARPA and the United States Department of Defense.[15] Its aim was to aid in tasks such as sending a text message, making phone calls, checking the weather or setting up an alarm. Over time, it has developed to provide restaurant recommendations, search the internet, and provide driving directions.[citation needed]\n In November 2014, Amazon announced Alexa alongside the Echo.[16]\n In April 2017 Amazon released a service for building conversational interfaces for any type of virtual assistant or interface.\n In the 2020s, artificial intelligence (AI) systems like ChatGPT have gained popularity for their ability to generate human-like responses to text-based conversations. In February 2020, Microsoft introduced its Turing Natural Language Generation (T-NLG), which was then the \"largest language model ever published at 17 billion parameters.\"[17] On November 30, 2022, ChatGPT was launched as a prototype and quickly garnered attention for its detailed responses and articulate answers across many domains of knowledge. The advent of ChatGPT and its introduction to the wider public increased interest and competition in the space. In February 2023, Google began introducing an experimental service called \"Bard\" which is based on its LaMDA program to generate text responses to questions asked based on information gathered from the web.\n While ChatGPT and other generalized chatbots based on the latest generative AI are capable of performing various tasks associated with virtual assistants, there are also more specialized forms of such technology that are designed to target more specific situations or needs.[18][4]\n Virtual assistants work via:\n Many virtual assistants are accessible via multiple methods, offering versatility in how users can interact with them, whether through chat, voice commands, or other integrated technologies.\n Virtual assistants use natural language processing (NLP) to match user text or voice input to executable commands. Some continually learn using artificial intelligence techniques including machine learning and ambient intelligence. \n To activate a virtual assistant using the voice, a wake word might be used. This is a word or groups of words such as \"Hey Siri\", \"OK Google\" or \"Hey Google\", \"Alexa\", and \"Hey Microsoft\".[21] As virtual assistants become more popular, there are increasing legal risks involved.[22]: 815 \n Virtual assistants may be integrated into many types of platforms or, like Amazon Alexa, across several of them:\n Virtual assistants can provide a wide variety of services. These include:[30]\n Conversational commerce is e-commerce via various means of messaging, including via voice assistants[33] but also live chat on e-commerce Web sites, live chat on messaging applications such as WeChat, Facebook Messenger and WhatsApp[34] and chatbots on messaging applications or Web sites.\n A virtual assistant can work with customer support team of a business to provide 24x7 support to customers. It provides quick responses, which enhances a customer's experience.\n Amazon enables Alexa \"Skills\" and Google \"Actions\", essentially applications that run on the assistant platforms.\n Virtual assistants have a variety of privacy concerns associated with them. Features such as activation by voice pose a threat, as such features requires the device to always be listening.[35] Modes of privacy such as the virtual security button have been proposed to create a multilayer authentication for virtual assistants.[36]\n The privacy policy of Google Assistant states that it does not store the audio data without the user's permission, but may store the conversation transcripts to personalise its experience. Personalisation can be turned off in settings. If a user wants Google Assistant to store audio data, they can go to Voice & Audio Activity (VAA) and turn on this feature. Audio files are sent to the cloud and used by Google to improve the performance of Google Assistant, but only if the VAA feature is turned on.[37]\n The privacy policy of Amazon's virtual assistant, Alexa, states that it only listens to conversations when its wake word (like Alexa, Amazon, Echo) is used. It starts recording the conversation after the call of a wake word, and stops recording after 8 seconds of silence. It sends the recorded conversation to the cloud. It is possible to delete the recording from the cloud by visiting 'Alexa Privacy' in 'Alexa'.[38]\n Apple states that it does not record audio to improve Siri. Instead, it claims to use transcripts. Transcript data is only sent if it is deemed important for analysis. Users can opt out anytime if they don't want Siri to send the transcripts in the cloud.[39]\n Cortana is a voice-only virtual assistant with singular authentication.[40][41][42] This voice-activated device accesses user data to perform common tasks like checking weather or making calls, raising privacy concerns due to the lack of secondary authentication.[43][44]\n Added value of the virtual assistants can come among others from the following:\n In 2019 Antonio A. Casilli, a French sociologist, criticized artificial intelligence and virtual assistants in particular in the following way:\n At a first level the fact that the consumer provides free data for the training and improvement of the virtual assistant, often without knowing it, is ethically disturbing.\n But at a second level, it might be even more ethically disturbing to know how these AIs are trained with this data.\n This artificial intelligence is trained via neural networks, which require a huge amount of labelled data. However, this data needs to be labelled through a human process, which explains the rise of microwork in the last decade. That is, remotely using some people worldwide doing some repetitive and very simple tasks for a few cents, such as listening to virtual assistant speech data, and writing down what was said. Microwork has been criticized for the job insecurity it causes, and for the total lack of regulation: The average salary was 1,38 dollar/hour in 2010,[50] and it provides neither healthcare nor retirement benefits, sick pay, minimum wage. Hence, virtual assistants and their designers are controversial for spurring job insecurity, and the AIs they propose are still human in the way that they would be impossible without the microwork of millions of human workers.[49]\n Privacy concerns are raised by the fact that voice commands are available to the providers of virtual assistants in unencrypted form, and can thus be shared with third parties and be processed in an unauthorized or unexpected manner.[51] Additionally to the linguistic content of recorded speech, a user's manner of expression and voice characteristics can implicitly contain information about his or her biometric identity, personality traits, body shape, physical and mental health condition, sex, gender, moods and emotions, socioeconomic status and geographical origin.[52]\n Notable developer platforms for virtual assistants include:\n In previous generations of text chat-based virtual assistants, the assistant was often represented by an avatar (a.k.a. interactive online character or automated character) — this was known as an embodied agent.\n Digital experiences enabled by virtual assistants are considered to be among the major recent technological advances and most promising consumer trends. Experts claim that digital experiences will achieve a status-weight comparable to 'real' experiences, if not become more sought-after and prized.[57] The trend is verified by a high number of frequent users and the substantial growth of worldwide user numbers of virtual digital assistants. In mid-2017, the number of frequent users of digital virtual assistants is estimated to be around 1 bn worldwide.[58] In addition, it can be observed that virtual digital assistant technology is no longer restricted to smartphone applications, but present across many industry sectors (incl. automotive, telecommunications, retail, healthcare and education).[59]\nIn response to the significant R&D expenses of firms across all sectors and an increasing implementation of mobile devices, the market for speech recognition technology is predicted to grow at a CAGR of 34.9% globally over the period of 2016 to 2024 and thereby surpass a global market size of US$7.5 billion by 2024.[59] According to an Ovum study, the \"native digital assistant installed base\" is projected to exceed the world's population by 2021, with 7.5 billion active voice AI–capable devices.[60] According to Ovum, by that time \"Google Assistant will dominate the voice AI–capable device market with 23.3% market share, followed by Samsung's Bixby (14.5%), Apple's Siri (13.1%), Amazon's Alexa (3.9%), and Microsoft's Cortana (2.3%).\"[60]\n Taking into consideration the regional distribution of market leaders, North American companies (e.g. Nuance Communications, IBM, eGain) are expected to dominate the industry over the next years, due to the significant impact of BYOD (Bring Your Own Device) and enterprise mobility business models. Furthermore, the increasing demand for smartphone-assisted platforms are expected to further boost the North American intelligent virtual assistant (IVA) industry growth. Despite its smaller size in comparison to the North American market, the intelligent virtual assistant industry from the Asia-Pacific region, with its main players located in India and China is predicted to grow at an annual growth rate of 40% (above global average) over the 2016–2024 period.[59]\n Virtual assistants should not be only seen as a gadget for individuals, as they could have a real economic utility for enterprises. As an example, a virtual assistant can take the role of an always available assistant with an encyclopedic knowledge. And which can organize meetings, check inventories, verify informations. Virtual assistants are all the more important that their integration in small and middle-sized enterprises often consists in an easy first step through the more global adaptation and use of Internet of Things (IoT). Indeed, IoT technologies are first perceived by small and medium-sized enterprises as technologies of critical importance, but too complicated, risky or costly to be used.[61]\n In May 2018, researchers from the University of California, Berkeley, published a paper that showed audio commands undetectable for the human ear could be directly embedded into music or spoken text, thereby manipulating virtual assistants into performing certain actions without the user taking note of it.[62] The researchers made small changes to audio files, which cancelled out the sound patterns that speech recognition systems are meant to detect. These were replaced with sounds that would be interpreted differently by the system and command it to dial phone numbers, open websites or even transfer money.[62] The possibility of this has been known since 2016,[62] and affects devices from Apple, Amazon and Google.[63]\n In addition to unintentional actions and voice recording, another security and privacy risk associated with intelligent virtual assistants is malicious voice commands: An attacker who impersonates a user and issues malicious voice commands to, for example, unlock a smart door to gain unauthorized entry to a home or garage or order items online without the user's knowledge. Although some IVAs provide a voice-training feature to prevent such impersonation, it can be difficult for the system to distinguish between similar voices. Thus, a malicious person who is able to access an IVA-enabled device might be able to fool the system into thinking that they are the real owner and carry out criminal or mischievous acts.[64]\n"
    },
    {
        "title": "Google Assistant",
        "url": "https://en.wikipedia.org/wiki/Google_Assistant",
        "content": "\n Google Assistant is a virtual assistant software application developed by Google that is primarily available on home automation and mobile devices. Based on artificial intelligence, Google Assistant can engage in two-way conversations,[1] unlike the company's previous virtual assistant, Google Now.\n Google Assistant debuted in May 2016 as part of Google's messaging app Allo, and its voice-activated speaker Google Nest. After a period of exclusivity on the Google Pixel smartphones, it was deployed on other Android devices starting in February 2017, including third-party smartphones and Android Wear (now Wear OS), and was released as a standalone app on the iOS operating system in May 2017. Alongside the announcement of a software development kit in April 2017, Assistant has been further extended to support a large variety of devices, including cars and third-party smart home appliances. The functionality of Assistant can also be enhanced by third-party developers. At CES 2018, the first Assistant-powered smart displays (Smart speakers with video screens) were announced, with the first one being released in July 2018.[2] In 2020, Google Assistant is already available on more than 1 billion devices.[3]\n Users primarily interact with Google Assistant through natural voice, though keyboard input is also supported. Assistant is able to answer questions, schedule events and alarms, adjust hardware settings on the user's device, show information from the user's Google account, play games, and more. Google has also announced that Assistant will be able to identify objects and gather visual information through the device's camera, and support purchasing products as well as sending money. Google Assistant is available in more than 90 countries and over 30 languages,[4] and is used by more than 500 million users monthly.[5]\n In October 2023, a mobile version of the Gemini chatbot, originally titled Assistant with Bard and simply just Bard, was unveiled during the Pixel 8 event. It is set to replace Assistant as the main assistant on Android devices, although the original Assistant will remain optional. The chatbot was released on February 8, 2024, in the United States.[6][7][8][9]\n The Google Assistant was unveiled during Google's developer conference on May 18, 2016, as part of the unveiling of the Google Nest smart speaker and new messaging app Allo; Google CEO Sundar Pichai explained that the Assistant was designed to be a conversational and two-way experience, and \"an ambient experience that extends across devices\".[10] Later that month, Google assigned Google Doodle leader Ryan Germick and hired former Pixar animator Emma Coats to develop \"a little more of a personality\".[11]\n For system-level integration outside of the Allo app and Google Nest, the Google Assistant was initially exclusive to the Google Pixel smartphones.[12] In February 2017, Google announced that it had begun to enable access to the Assistant on Android smartphones running Android Marshmallow or Nougat, beginning in select English-speaking markets.[13][14] Android tablets did not receive the Assistant as part of this rollout.[15][16] The Assistant is also integrated in Wear OS 2.0,[17] and will be included in future versions of Android TV[18][19] and Android Auto.[20] In October 2017, the Google Pixelbook became the first laptop to include Google Assistant.[21] Google Assistant later came to the Google Pixel Buds.[22] In December 2017, Google announced that the Assistant would be released for phones running Android Lollipop through an update to Google Play Services, as well as tablets running 6.0 Marshmallow and 7.0 Nougat.[23] In February 2019, Google reportedly began testing ads in Google Assistant results.[24]\n On May 15, 2017, Android Police reported that the Google Assistant would be coming to the iOS operating system as a separate app.[25] The information was confirmed two days later at Google's developer conference.[26][27]\n In January 2018 at the Consumer Electronics Show, the first Assistant-powered \"smart displays\" were released.[28] Smart displays were shown at the event from Lenovo, Sony, JBL and LG.[29] These devices have support for Google Duo video calls, YouTube videos, GMaps directions, a GCalendar agenda, viewing of smart camera footage, in addition to services which work with Google Home devices.[2]\n These devices are based on Android Things and Google-developed software. Google unveiled its own smart display, Google Nest Hub in October 2018, and later Google Nest Hub Max, which utilizes a different system platform.[30]\n In December 2016, Google launched \"Actions on Google\", a developer platform for the Google Assistant. Actions on Google allows 3rd party developers to build apps for Google Assistant.[31][32] In March 2017, Google added new tools for developing on Actions on Google to support the creation of games for Google Assistant.[33] Originally limited to the Google Nest smart speaker, Actions on Google was made available to Android and iOS devices in May 2017,[34][35] at which time Google also introduced an app directory or application directory for overview of compatible products and services.[36] To incentivize developers to build Actions, Google announced a competition, in which first place won tickets to Google's 2018 developer conference, $10,000, and a walk-through of Google's campus, while second place and third place received $7,500 and $5,000, respectively, and a Google Home.[37]\n In April 2017, a software development kit (SDK) was released, allowing third-party developers to build their own hardware that can run the Google Assistant.[38][39] It has been integrated into Raspberry Pi,[40][41] cars from Audi and Volvo,[42][43] and Home automation appliances, including fridges, washers, and ovens, from companies including iRobot, LG, General Electric, and D-Link.[44][45][46] Google updated the SDK in December 2017 to add several features that only the Google Home smart speakers and Google Assistant smartphone apps had previously supported.[47]\n The features include:\n On May 2, 2018, Google announced a new program that focuses on investing in the future of Google Assistant through early-stage startups. Their focus was to build an environment where developers could build richer experiences for their users. This includes startups that broaden Assistant's features, are building new hardware devices, or simply differentiating in different industries.[50]\n Google Assistant launched using the voice of Kiki Baessell for the American female voice, the same actress for the Google Voice voicemail system since 2010.[51]\n On October 11, 2019, Google announced that Issa Rae had been added to Google Assistant as an optional voice, which could be enabled by the user by saying \"Okay, Google, talk like Issa\".[52] Although, as of April 2022, Google Assistant response with \"Sorry, that voice isn't available anymore, but you can try out another by asking me to change voices.\" if the command is given.[citation needed]\n Google Assistant, in the nature and manner of Google Now, can search the Internet, schedule events and alarms, adjust hardware settings on the user's device, and show information from the user's Google account. Unlike Google Now, however, the Assistant can engage in a two-way conversation, using Google's natural language processing algorithm. Search results are presented in a card format that users can tap to open the page.[53] In February 2017, Google announced that users of Google Home would be able to shop entirely by voice for products through its Google Express shopping service, with products available from Whole Foods Market, Costco, Walgreens, PetSmart, and Bed Bath & Beyond at launch,[54][55] and other retailers added in the following months as new partnerships were formed.[56][57] Google Assistant can maintain a shopping list; this was previously done within the notetaking service GKeep, but the feature was moved to Google Express and the Google Home app in April 2017, resulting in a severe loss of functionality.[58][59]\n In May 2017, Google announced that the Assistant would support a keyboard for typed input and visual responses,[60][61] support identifying objects and gather visual information through the device's camera,[62][63] and support purchasing products[64][65] and sending money.[66][67] Through the use of the keyboard, users can see a history of queries made to the Google Assistant, and edit or delete previous inputs. The Assistant warns against deleting, however, due to its use of previous inputs to generate better answers in the future.[68] In November 2017, it became possible to identify songs currently playing by asking the Assistant.[69][70]\n The Google Assistant allows users to activate and modify vocal shortcut commands in order to perform actions on their device (both Android and iPad/iPhone) or configure it as a hub for home automation.\n This feature of the speech recognition is available in English, among other languages.[71][72] In July 2018, the Google Home version of Assistant gained support for multiple actions triggered by a single vocal shortcut command.[73]\n At the annual I/O developers conference on May 8, 2018, Google's SEO announced the addition of six new voice options for the Google Assistant, one of which being John Legend's.[74] This was made possible by WaveNet, a voice synthesizer developed by DeepMind, which significantly reduced the amount of audio samples that a voice actor was required to produce for creating a voice model.[75] However, John Legend's Google Assistant cameo voice was discontinued on March 23, 2020.[76][77]\n In August 2018, Google added bilingual capabilities to the Google Assistant for existing supported languages on devices. Recent reports say that it may support multilingual support by setting a third default language on Android Phone.[78]\n Speech-to-Text can recognize commas, question marks, and periods in transcription requests.[79]\n In April 2019, the most popular audio games in the Assistant, Crystal Ball, and Lucky Trivia, have had the biggest voice changes in the application's history. The voice in the assistant has been able to add expression to the games. For instance, in the Crystal Ball game, the voice would speak slowly and softly during the intro and before the answer is revealed to make the game more exciting, and in the Lucky Trivia game, the voice would become excitable like a game show host. In the British accent voice of Crystal Ball, the voice would say the word 'probably' in a downward slide like she's not too sure. The games used the text-to-speech voice which makes the voice more robotic. In May 2019 however, it turned out to be a bug in the speech API that caused the games to lose the studio-quality voices. These audio games were fixed in May 2019.\n On December 12, 2019, Google debuted an interpreter mode in Google Assistant smartphone apps for Android and iOS. It provides translation of conversations in real-time and was previously only available on Google Home smart speakers and displays.[80] Google Assistant won the 2020 Webby Award for Best User Experience in the category: Apps, Mobile & Voice.[81]\n On March 5, 2020, Google introduced a feature on Google Assistant that read webpages aloud in 42 languages.[82][83]\n On October 15, 2020, Google announced a new 'hum to search' function to find a song by simply humming, whistling, or singing the song.[84][85]\n In May 2018, Google revealed Duplex, an extension of the Google Assistant that allows it to carry out natural conversations by mimicking human voice, in a manner not dissimilar to robocalling.[86] The assistant can autonomously complete tasks such as calling a hair salon to book an appointment, scheduling a restaurant reservation, or calling businesses to verify holiday store hours.[87] While Duplex can complete most of its tasks fully autonomously, it is able to recognize situations that it is unable to complete and can signal a human operator to finish the task. Duplex was created to speak in a more natural voice and language by incorporating speech disfluencies such as filler words like \"hmm\" and \"uh\" and using common phrases such as \"mhm\" and \"gotcha\", along with more human-like intonation and response latency.[88][89][90] Duplex is currently in development and had a limited release in late 2018 for Google Pixel users.[91] During the limited release, Pixel phone users in Atlanta, New York, Phoenix, and San Francisco were only able to use Duplex to make restaurant reservations.[92] As of October 2020, Google has expanded Duplex to businesses in eight countries.[93][94]\n After the announcement, concerns were made over the ethical and societal questions that artificial intelligence technology such as Duplex raises.[95] For instance, human operators may not notice that they are speaking with a digital robot when conversing with Duplex,[96] which some critics view as unethical or deceitful.[97] Concerns over privacy were also identified, as conversations with Duplex are recorded in order for the virtual assistant to analyze and respond.[98] Privacy advocates have also raised concerns around how the millions of vocal samples gathered from consumers are fed back into the algorithms of virtual assistants, making these forms of AI smarter with each use. Though these features individualize the user experience, critics are unsure about the long term implications of giving \"the company unprecedented access to human patterns and preferences that are crucial to the next phase of artificial intelligence\".[99]\n While transparency was referred to as a key part to the experience when the technology was revealed,[100] Google later further clarified in a statement saying, \"We are designing this feature with disclosure built-in, and we'll make sure the system is appropriately identified.\"[101][97] Google further added that, in certain jurisdictions, the assistant would inform those on the other end of the phone that the call is being recorded.[102]\n PC World's Mark Hachman gave a favorable review of the Google Assistant, saying that it was a \"step up on Cortana and Siri.\"[103] Digital Trends called it \"smarter than Google Now ever was\".[104]\n In July 2019 Belgian public broadcaster VRT NWS published an article revealing that third-party contractors paid to transcribe audio clips collected by Google Assistant listened to sensitive information about users. Sensitive data collected from Google Home devices and Android phones included names, addresses, and other private conversations after mistaken hot word triggering, such as business calls or bedroom conversations.[105] From more than 1,000 recordings analyzed, 153 were recorded without the \"OK Google\" command. Google officially acknowledged that 0.2% of recordings are being listened to by language experts to improve Google's services.[106] On August 1, 2019, Germany's Hamburg Commissioner for Data Protection and Freedom of Information initiated an administrative procedure to prohibit Google from carrying out corresponding evaluations by employees or third parties for the period of three months to provisionally protect the rights of privacy of data subjects for the time being, citing GDPR.[107] A Google spokesperson stated that Google paused \"language reviews\" in all European countries while it investigated recent media leaks.[108]\n"
    },
    {
        "title": "Siri",
        "url": "https://en.wikipedia.org/wiki/Siri",
        "content": "\n Siri (/ˈsɪəri/ ⓘ SEER-ee, backronym Speech Interpretation and Recognition Interface) is a digital assistant purchased, developed, and popularized by Apple Inc., which is included in the iOS, iPadOS, watchOS, macOS, tvOS, audioOS, and visionOS operating systems.[1][2] It uses voice queries, gesture based control, focus-tracking and a natural-language user interface to answer questions, make recommendations, and perform actions by delegating requests to a set of Internet services. With continued use, it adapts to users' individual language usages, searches, and preferences, returning individualized results.\n Siri is a spin-off from a project developed by the SRI International Artificial Intelligence Center. Its speech recognition engine was provided by Nuance Communications, and it uses advanced machine learning technologies to function. Its original American, British, and Australian voice actors recorded their respective voices around 2005, unaware of the recordings' eventual usage. Siri was released as an app for iOS in February 2010. Two months later, Apple acquired it and integrated it into the iPhone 4s at its release on 4 October 2011, removing the separate app from the iOS App Store. Siri has since been an integral part of Apple's products, having been adapted into other hardware devices including newer iPhone models, iPad, iPod Touch, Mac, AirPods, Apple TV, HomePod, and Apple Vision Pro.\n Siri supports a wide range of user commands, including performing phone actions, checking basic information, scheduling events and reminders, handling device settings, searching the Internet, navigating areas, finding information on entertainment, and being able to engage with iOS-integrated apps. With the release of iOS 10, in 2016, Apple opened up limited third-party access to Siri, including third-party messaging apps, as well as payments, ride-sharing, and Internet calling apps. With the release of iOS 11, Apple updated Siri's voice and added support for follow-up questions, language translation, and additional third-party actions.\niOS 17 and iPadOS 17 enabled users to activate Siri by simply saying \"Siri\", while the previous command, \"Hey Siri\",  is still supported. Siri was upgraded to using Apple Intelligence on iOS 18, iPadOS 18, and macOS Sequoia, replacing the logo.\n Siri's original release on iPhone 4s on Oct 2011 received mixed reviews. It received praise for its voice recognition and contextual knowledge of user information, including calendar appointments, but was criticized for requiring stiff user commands and having a lack of flexibility. It was also criticized for lacking information on certain nearby places and for its inability to understand certain English accents. In 2016 and 2017, a number of media reports said that Siri lacked innovation, particularly against new competing voice assistants. The reports concerned Siri's limited set of features, \"bad\" voice recognition, and undeveloped service integrations as causing trouble for Apple in the field of artificial intelligence and cloud-based services; the basis for the complaints reportedly due to stifled development, as caused by Apple's prioritization of user privacy and executive power struggles within the company.[3] Its launch was also overshadowed by the death of Steve Jobs, which occurred one day after the launch.\n Siri is a spin-out from the Stanford Research Institute's Artificial Intelligence Center and is an offshoot of the US Defense Advanced Research Projects Agency's (DARPA)-funded CALO project.[4] SRI International used the NABC Framework to define the value proposition for Siri.[5] It was co-founded by Dag Kittlaus, Tom Gruber, and UCLA alumnus Adam Cheyer.[4] Kittlaus named Siri after a co-worker in Norway; the name is a short form of the name Sigrid, from Old Norse Sigríðr, composed of the elements sigr \"victory\" and fríðr \"beautiful\".[6]\n Siri's speech recognition engine was provided by Nuance Communications, a speech technology company.[7] Neither Apple nor Nuance acknowledged this for years,[8][9] until Nuance CEO Paul Ricci confirmed it at a 2013 technology conference.[7] The speech recognition system uses sophisticated machine learning techniques, including convolutional neural networks and long short-term memory.[10]\n The initial Siri prototype was implemented using the Active platform, a joint project between the Artificial Intelligence Center of SRI International and the Vrai Group at Ecole Polytechnique Fédérale de Lausanne. The Active platform was the focus of a Ph.D. thesis led by Didier Guzzoni, who joined Siri as its chief scientist.[11]\n Siri was acquired by Apple Inc. in April 2010 under the direction of Steve Jobs.[12] Apple's first notion of a digital personal assistant appeared in a 1987 concept video, Knowledge Navigator.[13][14]\n Siri has been updated with enhanced capabilities made possible by Apple Intelligence. In macOS Sequoia, iOS 18, and iPadOS 18, Siri features an updated user interface, improved natural language processing, and the option to interact via text by double tapping the home bar without enabling the feature in the Accessibility menu on iOS and iPadOS. Apple Intelligence adds the ability for Siri to use personal context from device activities to make conversations more natural and fluid. Siri can give users device support and will have larger app support via the Siri App Intents API. Siri will be able to deliver intelligence that's tailored to the user and their on-device information using personal context. For example, a user can say, \"Play that podcast that Jamie recommended,\" and Siri will be able to locate and play the episode, without the user having to remember where it was mentioned. They could also ask, \"When is Mom's flight landing?\" and Siri will find the flight details and cross-reference them with real-time flight tracking to give an arrival time. [15][16] For more day to day interactions with Apple devices, Siri will now summarize messages (on more apps than just Messages, such as Discord and Slack). According to users, this feature can be helpful but can also be inappropriate in certain situations. As a beta tester explained, this current version of Siri with Apple Intelligence is still in the early development stages, so users shouldn't expect a vastly different experience. [17]\n The original American voice of Siri was recorded in July 2005 by Susan Bennett, who was unaware it would eventually be used for the voice assistant.[18][19] A report from The Verge in September 2013 about voice actors, their work, and machine learning developments, hinted that Allison Dufty was the voice behind Siri,[20][21] but this was disproven when Dufty wrote on her website that she was \"absolutely, positively not the voice of Siri.\"[19] Citing growing pressure, Bennett revealed her role as Siri in October, and her claim was confirmed by Ed Primeau, an American audio forensics expert.[19] Apple has never acknowledged it.[19]\n The original British male voice was provided by Jon Briggs, a former technology journalist and for 12 years narrated for the hit BBC quiz show The Weakest Link.[18] After discovering he was Siri's voice by watching television, he first spoke about the role in November 2011. He acknowledged that the voice work was done \"five or six years ago\", and that he didn't know how the recordings would be used.[22][23]\n The original Australian voice was provided by Karen Jacobsen, a voice-over artist known in Australia as the GPS girl.[18][24]\n In an interview between all three voice actors and The Guardian, Briggs said that \"the original system was recorded for a US company called Scansoft, who were then bought by Nuance. Apple simply licensed it.\"[24]\n For iOS 11, Apple auditioned hundreds of candidates to find new female voices, then recorded several hours of speech, including different personalities and expressions, to build a new text-to-speech voice based on deep learning technology.[25] In February 2022, Apple added Quinn, its first gender-neutral voice as a fifth user option, to the iOS 15.4 developer release.[26]\n Siri released as a stand-alone application for the iOS operating system in February 2010, and at the time, the developers were also intending to release Siri for Android and BlackBerry devices.[27] Two months later, Apple acquired Siri.[28][29][30] On October 4, 2011, Apple introduced the iPhone 4S with a beta version of Siri.[31][32] After the announcement, Apple removed the existing standalone Siri app from App Store.[33] TechCrunch wrote that, though the Siri app supports iPhone 4, its removal from App Store might also have had a financial aspect for the company, in providing an incentive for customers to upgrade devices.[33] Third-party developer Steven Troughton-Smith, however, managed to port Siri to iPhone 4, though without being able to communicate with Apple's servers.[34] A few days later, Troughton-Smith, working with an anonymous person nicknamed \"Chpwn\", managed to fully hack Siri, enabling its full functionalities on iPhone 4 and iPod Touch devices.[35] Additionally, developers were also able to successfully create and distribute legal ports of Siri to any device capable of running iOS 5, though a proxy server was required for Apple server interaction.[36]\n Over the years, Apple has expanded the line of officially supported products, including newer iPhone models,[37] as well as iPad support in June 2012,[38] iPod Touch support in September 2012,[39] Apple TV support, and the stand-alone Siri Remote, in September 2015,[40] Mac and AirPods support in September 2016,[41][42] and HomePod support in February 2018.[43][44]\n Apple offers a wide range of voice commands to interact with Siri, including, but not limited to:[45]\n Siri also offers numerous pre-programmed responses to amusing questions. Such questions include \"What is the meaning of life?\" to which Siri may reply \"All evidence to date suggests it's chocolate\"; \"Why am I here?\", to which it may reply \"I don't know. Frankly, I've wondered that myself\"; and \"Will you marry me?\", to which it may respond with \"My End User Licensing Agreement does not cover marriage. My apologies.\"[49][50] In addition to some of these questions, there are also statements you can tell Siri such as \"I am your father.\" to which Siri may reply \"Nooooo!\".\n Initially limited to female voices, Apple announced in June 2013 that Siri would feature a gender option, adding a male voice counterpart.[51]\n In September 2014, Apple added the ability for users to speak \"Hey Siri\" to enable the assistant without the requirement of physically handling the device.[52]\n In September 2015, the \"Hey Siri\" feature was updated to include individualized voice recognition, a presumed effort to prevent non-owner activation.[53][54]\n With the announcement of iOS 10 in June 2016, Apple opened up limited third-party developer access to Siri through a dedicated application programming interface (API). The API restricts the usage of Siri to engaging with third-party messaging apps, payment apps, ride-sharing apps, and Internet calling apps.[55][56]\n In iOS 11, Siri is able to handle follow-up questions, supports language translation, and opens up to more third-party actions, including task management.[57][58] Additionally, users are able to type to Siri,[59] and a new, privacy-minded \"on-device learning\" technique improves Siri's suggestions by privately analyzing personal usage of different iOS applications.[60]\n iOS 17 and iPadOS 17 allows users to simply say \"Siri\" to initiate Siri, and the virtual assistant now supports back to back requests, allowing users to issue multiple requests and conversations without reactivating it.[61] In the public beta versions of iOS 17, iPadOS 17, and macOS Sonoma, Apple added support for bilingual queries to Siri.[62]\n iOS 18, iPadOS 18 and MacOS 15 Sequoia brought artificial intelligence, integrated with ChatGPT, to Siri.[63] Apple calls this \"Apple Intelligence\".[64]\n Siri received mixed reviews during its beta release as an integrated part of the iPhone 4S in October 2011.\n MG Siegler of TechCrunch wrote that Siri was \"great,\" praising the potential for Siri after losing the beta tag:\n The amount of times Siri hasn't been able to understand and execute my request is astonishingly low. ... Just imagine what will happen when Apple partners with other services to expand Siri further. And imagine when they have an API that any developer can use. This really could alter the mobile landscape.[65] Writing for The New York Times, David Pogue also praised Siri's language understanding and ability to understand context:\n [Siri] thinks for a few seconds, displays a beautifully formatted response and speaks in a calm female voice. ... It's mind-blowing how inexact your utterances can be. Siri understands everything from, 'What's the weather going to be like in Tucson this weekend?' to 'Will I need an umbrella tonight?' ... Once, I tried saying, 'Make an appointment with Patrick for Thursday at 3.' Siri responded, 'Note that you already have an all-day appointment about \"Boston Trip\" for this Thursday. Shall I schedule this anyway?' Unbelievable.[66] Jacqui Cheng of Ars Technica wrote that Apple's claims of what Siri could do were bold, and the early demos \"even bolder\":\n Though Siri shows real potential, these kinds of high expectations are bound to be disappointed. ... Apple makes clear that the product is still in beta—an appropriate label, in our opinion.[67] While praising its ability to \"decipher our casual language\" and deliver \"very specific and accurate result,\" sometimes even providing additional information, Cheng noted and criticized its restrictions, particularly when the language moved away from \"stiffer commands\" into more human interactions. One example included the phrase \"Send a text to Jason, Clint, Sam, and Lee saying we're having dinner at Silver Cloud,\" which Siri interpreted as sending a message to Jason only, containing the text \"Clint Sam and Lee saying we're having dinner at Silver Cloud.\" She also noted a lack of proper editability, as saying \"Edit message to say: We're at Silver Cloud and you should come find us,\" generated \"Clint Sam and Lee saying we're having dinner at Silver Cloud to say we're at Silver Cloud and you should come find us.\"[67]\n Google's executive chairman and former chief, Eric Schmidt, conceded that Siri could pose a competitive threat to the company's core search business.[68]\n Siri was criticized by pro-abortion rights organizations, including the American Civil Liberties Union (ACLU) and NARAL Pro-Choice America, after users found that Siri could not provide information about the location of birth control or abortion providers nearby, sometimes directing users to crisis pregnancy centers instead.[69][70][71]\n Natalie Kerris, a spokeswoman for Apple, told The New York Times:\n Our customers want to use Siri to find out all types of information, and while it can find a lot, it doesn't always find what you want. ... These are not intentional omissions meant to offend anyone. It simply means that as we bring Siri from beta to a final product, we find places where we can do better, and we will in the coming weeks.[72] In January 2016, Fast Company reported that, in then-recent months, Siri had begun to confuse the word \"abortion\" with \"adoption\", citing \"health experts\" who stated that the situation had \"gotten worse.\" However, at the time of Fast Company's report, the situation had changed slightly, with Siri offering \"a more comprehensive list of Planned Parenthood facilities\", although \"Adoption clinics continue to pop up, but near the bottom of the list.\"[73][74]\n Siri has also not been well received by some English speakers with distinctive accents, including Scottish[75] and Americans from Boston or the South.[76]\n In March 2012, Frank M. Fazio filed a class action lawsuit against Apple on behalf of the people who bought the iPhone 4S and felt misled about the capabilities of Siri, alleging its failure to function as depicted in Apple's Siri commercials. Fazio filed the lawsuit in California and claimed that the iPhone 4S was merely a \"more expensive iPhone 4\" if Siri fails to function as advertised.[77][78] On July 22, 2013, U.S. District Judge Claudia Wilken in San Francisco dismissed the suit but said the plaintiffs could amend at a later time. The reason given for dismissal was that plaintiffs did not sufficiently document enough misrepresentations by Apple for the trial to proceed.[79]\n In June 2016, The Verge's Sean O'Kane wrote about the then-upcoming major iOS 10 updates, with a headline stating \"Siri's big upgrades won't matter if it can't understand its users\":\n What Apple didn't talk about was solving Siri's biggest, most basic flaws: it's still not very good at voice recognition, and when it gets it right, the results are often clunky. And these problems look even worse when you consider that Apple now has full-fledged competitors in this space: Amazon's Alexa, Microsoft's Cortana, and Google's Assistant.[80]  Also writing for The Verge, Walt Mossberg had previously questioned Apple's efforts in cloud-based services, writing:[81]\n ... perhaps the biggest disappointment among Apple's cloud-based services is the one it needs most today, right now: Siri. Before Apple bought it, Siri was on the road to being a robust digital assistant that could do many things, and integrate with many services—even though it was being built by a startup with limited funds and people. After Apple bought Siri, the giant company seemed to treat it as a backwater, restricting it to doing only a few, slowly increasing number of tasks, like telling you the weather, sports scores, movie and restaurant listings, and controlling the device's functions. Its unhappy founders have left Apple to build a new AI service called Viv. And, on too many occasions, Siri either gets things wrong, doesn't know the answer, or can't verbalize it. Instead, it shows you a web search result, even when you're not in a position to read it. In October 2016, Bloomberg reported that Apple had plans to unify the teams behind its various cloud-based services, including a single campus and reorganized cloud computing resources aimed at improving the processing of Siri's queries,[82] although another report from The Verge, in June 2017, once again called Siri's voice recognition \"bad.\"[83]\n In June 2017, The Wall Street Journal published an extensive report on the lack of innovation with Siri following competitors' advancement in the field of voice assistants. Noting that Apple workers' anxiety levels \"went up a notch\" on the announcement of Amazon's Alexa, the Journal wrote: \"Today, Apple is playing catch-up in a product category it invented, increasing worries about whether the technology giant has lost some of its innovation edge.\" The report gave the primary causes being Apple's prioritization of user privacy, including randomly-tagged six-month Siri searches, whereas Google and Amazon keep data until actively discarded by the user,[clarification needed] and executive power struggles within Apple. Apple did not comment on the report, while Eddy Cue said: \"Apple often uses generic data rather than user data to train its systems and has the ability to improve Siri's performance for individual users with information kept on their iPhones.\"[3][84]\n In July 2019, a then-anonymous whistleblower and former Apple contractor Thomas le Bonniec said that Siri regularly records some of its users' conversations even when it was not activated. The recordings are sent to Apple contractors grading Siri's responses on a variety of factors. Among other things, the contractors regularly hear private conversations between doctors and patients, business and drug deals, and couples having sex. Apple did not disclose this in its privacy documentation and did not provide a way for its users to opt-in or out.[85]\n In August 2019, Apple apologized, halted the Siri grading program, and said that it plans to resume \"later this fall when software updates are released to [its] users\".[86] The company also announced \"it would no longer listen to Siri recordings without your permission\".[87] iOS 13.2, released in October 2019, introduced the ability to opt out of the grading program and to delete all the voice recordings that Apple has stored on its servers.[88] Users were given the choice of whether their audio data was received by Apple or not, with the ability to change their decision as often as they like. It was then made an opt-in program.\n In May 2020, Thomas le Bonniec revealed himself as the whistleblower and sent a letter to European data protection regulators, calling on them to investigate Apple's \"past and present\" use of Siri recordings. He argued that, even though Apple has apologized, it has never faced the consequences for its years-long grading program.[89][90]\n In December 2024, Apple agreed to a $95 million class-action settlement, compensating users of Siri-enabled from the past ten years. Additionally, Apple must confirm the deletion of Siri recordings before 2019 (when the feature became opt-in) and issue new guidance on how data is collected and how users can participate in efforts to improve Siri.[91]\n Apple has introduced various accessibility features aimed at making its devices more inclusive for individuals with disabilities. The company provides users the opportunity to share feedback on accessibility features through email.[92] Some of the new functionalities include live speech, personal voice, Siri's atypical speech pattern recognition, and much more.[93]\n Accessibility features: \n Siri, like many AI systems, can perpetuate gender and racial biases through its design and functionality. According to an article from The Conversation, Siri \"reinforces the role of women as secondary and submissive to men\" due to the fact that the default is a soft, female voice.[99] Although Apple now offers a larger variety of voices with different accents and languages, this original narrative perpetuates the idea of women servicing men. Not only this but the article also explains how different settings of Siri's voice result in different responses, specifically the female voice being programmed with more flirtatious statements than the male voice. Additionally, Siri may misinterpret certain accents or dialects, particularly those spoken by people from marginalized racial or ethnic backgrounds, making it less accessible to these groups. According to an article from The Scientific American, Claudia Lloreda explains that non-native English speakers have to \"adapt our way of speaking to interact with speech-recognition technologies.\"[100] Furthermore, due to repetitive \"learnings\" from a larger user base, Siri may unintentionally produce a Western perspective, limiting representation and furthering biases in everyday interactions. Despite these perpetuated issues, Siri provides several benefits as well, especially for those with disabilities that typically limit their abilities to use technology and access the internet. \n The iOS version of Siri ships with a vulgar content filter; however, it is disabled by default and must be enabled by the user manually.[101]\n In 2018, Ars Technica reported a new glitch that could be exploited by a user requesting the definition of \"mother\" be read out loud. Siri would issue a response and ask the user if they would like to hear the next definition; when the user replies with \"yes,\" Siri would mention \"mother\" as being short for \"motherfucker.\"[102] This resulted in multiple YouTube videos featuring the responses and/or how to trigger them. Apple fixed the issue silently. The content is picked up from third-party sources such as the Oxford English Dictionary and not a supplied message from the corporation.[103]\n Siri provided the voice of 'Puter in The Lego Batman Movie.[104]\n"
    },
    {
        "title": "Vehicular automation",
        "url": "https://en.wikipedia.org/wiki/Autonomous_vehicles",
        "content": "\n Vehicular automation is the use of technology to assist or replace the operator of a vehicle such as a car, truck, aircraft, rocket, military vehicle, or boat.[2][3][4][5][6] Assisted vehicles are semi-autonomous, whereas vehicles that can travel without a human operator are autonomous.[3] The degree of autonomy may be subject to various constraints such as conditions. Autonomy is enabled by advanced driver-assistance systems (ADAS) of varying capacity.\n Related technology includes advanced software, maps, vehicle changes, and support outside the vehicle. \n Autonomy presents varying issues for road travel, air travel, and marine travel. Roads present the greatest complexity given the unpredictability of the driving environment, including diverse road designs, driving conditions, traffic, obstacles, and geographical/cultural differences.[7]\n Autonomy implies that the vehicle is responsible for all perception, monitoring, and control functions.[8]\n The Society of Automotive Engineers (SAE) classifies road vehicle autonomy in six levels:[9][10]\n Level 0 refers, for instance, to vehicles without adaptive cruise control. Level 1 and 2 refer to vehicles where one part of the driving task is performed by the ADAS under the responsibility/liability of the driver.\n From level 3, the driver can transfer the driving task to the vehicle, but the driver must assume control when the ADAS reaches its limits. For instance an automated traffic jam pilot can drive in a traffic jam, but otherwise passes control to the driver. Level 5 refers to a vehicle that can handle any situation.[11]\n The perception system is responsible for observing the environment. It must identify everything that could affect the trip, including obstacles and other issues.[12] Various makers use cameras, radar, lidar, sonar, and microphones that can collaboratively minimize errors.[12]\n Further technological progress tends to combine several different sensors such as cameras, radars, laser radars, etc., to work in coordination with each other. They can further improve the ability to process information, actively eliminate some invalid information, help make the most accurate decisions, and reduce the occurrence of traffic accidents.[13]\n Autonomous systems typically rely on machine learning software to operate.[14]\n Navigation systems are a necessary element in autonomous vehicles. The Global Positioning System (GPS) is used for navigation by air and water vehicles, and by land vehicles as well, particularly for off-road navigation.\n For road vehicles, two approaches are prominent. One is to use maps that hold data about lanes and intersections, relying on the vehicle's perception system to fill in the details. The other is to use highly detailed maps that reduce the scope of realtime decision-making, but require significant maintenance resources as the environment evolves.[14] Some systems crowdsource their map updates, using the vehicles themselves to update the map to reflect changes such as construction or traffic that is then used by the entire vehicle fleet.[15]\n Another potential source of information is the environment itself. Traffic data may be supplied by roadside monitoring systems and used to route vehicles to best use a limited road system.[16]\n Automated vehicles in European Union legislation refer specifically to road vehicles (car, truck, or bus).[17] For those vehicles, a specific difference is legally defined between advanced driver-assistance system and autonomous/automated vehicles, based on liability differences.\n AAA Foundation for Traffic Safety tested two automatic emergency braking systems: some designed to prevent crashes and others that aim to make a crash less severe. The test looked at popular models like the 2016 Volvo XC90, Subaru Legacy, Lincoln MKX, Honda Civic, and Volkswagen Passat. Researchers tested how well each system stopped when approaching moving and nonmoving targets. It found that systems capable of preventing crashes reduced vehicle speeds by twice that of the systems designed to mitigate crash severity. When the two test vehicles traveled within 30 mph of each other, even those designed to simply lessen crash severity avoided crashes 60 percent of the time.[18]\n The SAfe Road TRains for the Environment (Sartre) project's goal was to enable platooning, in which a line of cars and trucks (a \"train\") follow a human-driven vehicle. Trains were predicted to provide comfort and allow the following vehicles to travel safely to a destination. Human drivers encountering a train could join and delegate driving to the human driver.[19]\n Self-driving Uber vehicles were tested in Pittsburgh, Pennsylvania. The tests were paused after an autonomous car killed a woman in Arizona.[20][21] Automated busses have been tested in California.[22] In San Diego, California, an automated bus test used magnetic markers. The longitudinal control of automated truck platoons used millimeter wave radio and radar. Waymo and Tesla have conducted tests. Tesla FSD allows drivers to enter a destination and let the car take over.\n Ford offers Blue Cruise, technology that allows geofenced cars to drive autonomously.[23]\n Drivers are directed to stay attentive and safety warnings are implemented to alert the driver when corrective action is needed.[24] Tesla, Incorporated has one recorded incident that resulted in a fatality involving the automated driving system in the Tesla Model S.[25] The accident report reveals the accident was a result of the driver being inattentive and the autopilot system not recognizing the obstruction ahead.[25] Tesla has also had multiple instances where the vehicle crashed into a garage door. According to the book \"The Driver in the Driverless Car: How Your Technology Choices Create the Future\" a Tesla performed an update overnight automatically. The morning after the update the driver used his app to \"summon\" his car, it crashed into his garage door.\n Another flaw with automated driving systems is that in situations where unpredictable events such as weather or the driving behavior of others may cause fatal accidents due to sensors that monitor the surroundings of the vehicle not being able to provide corrective action.[24]\n To overcome some of the challenges for automated driving systems, novel methodologies based on virtual testing, traffic flow simulation and digital prototypes have been proposed,[26] especially when novel algorithms based on Artificial Intelligence approaches are employed which require extensive training and validation data sets.\n The implementation of automated driving systems poses the possibility of changing built environments in urban areas, such as the expansion of suburban areas due to the increased ease of mobility.[27]\n Around 2015, several self-driving car companies including Nissan and Toyota promised self-driving cars by 2020. However, the predictions turned out to be far too optimistic.[28]\n There are still many obstacles in developing fully autonomous Level 5 vehicles, which is the ability to operate in any conditions. Currently, companies are focused on Level 4 automation, which is able to operate under certain environmental circumstances.[28]\n There is still debate about what an autonomous vehicle should look like. For example, whether to incorporate lidar to autonomous driving systems is still being argued. Some researchers have come up with algorithms using camera-only data that achieve the performance that rival those of lidar. On the other hand, camera-only data sometimes draw inaccurate bounding boxes, and thus lead to poor predictions. This is due to the nature of superficial information that stereo cameras provide, whereas incorporating lidar gives autonomous vehicles precise distance to each point on the vehicle.[28]\n These features require numerous sensors, many of which rely on micro-electro-mechanical systems (MEMS) to maintain a small size, high efficiency, and low cost. Foremost among MEMS sensors in vehicles are accelerometers and gyroscopes to measure acceleration around multiple orthogonal axes—critical to detecting and controlling the vehicle's motion.\n One critical step to achieve the implementation of autonomous vehicles is the acceptance by the general public. It provides guidelines for the automobile industry to improve their design and technology. Studies have shown that many people believe that using autonomous vehicles is safer, which underlines the necessity for the automobile companies to assure that autonomous vehicles improve safety benefits. The TAM research model breaks down important factors that affect the consumer's acceptance into: usefulness, ease to use, trust, and social influence.[30]\n Real-time testing of autonomous vehicles is an inevitable part of the process. At the same time, vehicular automation regulators are faced with challenges to protect public safety and yet allow autonomous vehicle companies to test their products. Groups representing autonomous vehicle companies are resisting most regulations, whereas groups representing vulnerable road users and traffic safety are pushing for regulatory barriers. To improve traffic safety, the regulators are encouraged to find a middle ground that protects the public from immature technology while allowing autonomous vehicle companies to test the implementation of their systems.[31] There have also been proposals to adopt the aviation automation safety regulatory knowledge into the discussions of safe implementation of autonomous vehicles, due to the experience that has been gained over the decades by the aviation sector on safety topics.[32]\n In some countries, specific laws and regulations apply to road traffic motor vehicles (such as cars, bus and trucks) while other laws and regulations apply to other ground vehicles such as tram, train or automated guided vehicles making them to operate in different environments and conditions.\n An automated driving system is defined in a proposed amendment to Article 1 of the Vienna Convention on Road Traffic:\n (ab) \"Automated driving system\" refers to a vehicle system that uses both hardware and\nsoftware to exercise dynamic control of a vehicle on a sustained basis. (ac) \"Dynamic control\" refers to carrying out all the real-time operational and tactical functions required to move the vehicle. This includes controlling the vehicle's lateral and longitudinal motion, monitoring the road environment, responding to events in the road traffic environment, and planning and signalling for manoeuvres.[33] This amendment will enter into force on 14 July 2022, unless it is rejected before 13 January 2022.[34]\n An automated driving feature must be described sufficiently clearly so that it is distinguished from an assisted driving feature. There are two clear states – a vehicle is either assisted with a driver being supported by technology or automated where the technology is effectively and safely replacing the driver. Ground vehicles employing automation and teleoperation include shipyard gantries, mining trucks, bomb-disposal robots, robotic insects, and driverless tractors.\n There are many autonomous and semi-autonomous ground vehicles being made for the purpose of transporting passengers. One such example is the free-ranging on grid (FROG) technology which consists of autonomous vehicles, a magnetic track and a supervisory system. The FROG system is deployed for industrial purposes in factory sites and has been in use since 1999 on the ParkShuttle, a PRT-style public transport system in the city of Capelle aan den IJssel to connect the Rivium business park with the neighboring city of Rotterdam (where the route terminates at the Kralingse Zoom metro station). The system experienced a crash in 2005[36] that proved to be caused by a human error.[37]\n Applications for automation in ground vehicles include the following:\n Research is ongoing and prototypes of autonomous ground vehicles exist.\n Extensive automation for cars focuses on either introducing robotic cars or modifying modern car designs to be semi-autonomous.\n Semi-autonomous designs could be implemented sooner as they rely less on technology that is still at the forefront of research. An example is the dual mode monorail. Groups such as RUF (Denmark) and TriTrack (USA) are working on projects consisting of specialized private cars that are driven manually on normal roads but also that dock onto a monorail/guideway along which they are driven autonomously.\n As a method of automating cars without extensively modifying the cars as much as a robotic car, Automated highway systems (AHS) aims to construct lanes on highways that would be equipped with, for example, magnets to guide the vehicles. Automation vehicles have auto-brakes named as Auto Vehicles Braking System (AVBS). Highway computers would manage the traffic and direct the cars to avoid crashes.\n In 2006, The European Commission has established a smart car development program called the Intelligent Car Flagship Initiative.[38] The goals of that program include:\n There are further uses for automation in relation to cars. These include:\n Singapore also announced a set of provisional national standards on January 31, 2019, to guide the autonomous vehicle industry. The standards, known as Technical Reference 68 (TR68), will promote the safe deployment of fully driverless vehicles in Singapore, according to a joint press release by Enterprise Singapore (ESG), Land Transport Authority (LTA), Standards Development Organisation and Singapore Standards Council (SSC).[41]\n Since 1999, the 12-seat/10-standing ParkShuttle has been operating on an 1.8 kilometres (1.1 mi) exclusive right of way in the city of Capelle aan den IJssel in The Netherlands. The system uses small magnets in the road surface to allow the vehicle to determine its position. The use of shared autonomous vehicles was trialed around 2012 in a hospital car park in Portugal.[42] From 2012 to 2016, the European Union funded CityMobil2 project examined the use of shared autonomous vehicles and passenger experience including short term trials in seven cities. This project led to the development of the EasyMile EZ10.[43]\n In the 2010s, self-driving shuttle became able to run in mixed traffic without the need for embedded guidance markers.[44] So far the focus has been on low speed, 20 miles per hour (32 km/h), with short, fixed routes for the \"last mile\" of journeys. This means issues of collision avoidance and safety are significantly less challenging than those for automated cars, which seek to match the performance of conventional vehicles. Many trials have been undertaken, mainly on quiet roads with little traffic or on public pathways or private roadways and specialised test sites.[citation needed] The capacity of different models varies significantly, between 6-seats and 20-seats. (Above this size there are conventional buses that have driverless technology installed.)\n In December 2016, the Jacksonville Transportation Authority has announced its intention to replace the Jacksonville Skyway monorail with driverless vehicles that would run on the existing elevated superstructure as well as continue onto ordinary roads.[45] The project has since been named the \"Ultimate Urban Circulator\" or \"U2C\" and testing has been carried out on shuttles from six different manufacturers. The cost of the project is estimated at $379 million.[46]\n In January 2017, it was announced the ParkShuttle system in the Netherlands will be renewed and expanded including extending the route network beyond the exclusive right of way so vehicles will run in mixed traffic on ordinary roads.[47] The plans were delayed and the extension into mixed traffic was expected in 2021.[48]\n In July 2018, Baidu stated it had built 100 of its 8-seat Apolong model, with plans for commercial sales.[49] As of July 2021, they had not gone into volume production.\n In August 2020, it was reported there were 25 autonomous shuttle manufacturers,[50] including the 2GetThere, Local Motors, Navya, Baidu, Easymile, Toyota and Ohmio.\n In December 2020, Toyota showcased its 20-passenger \"e-Palette\" vehicle, which is due to be used at the 2021 Tokyo Olympic Games.[51] Toyota announced it intends to have the vehicle available for commercial applications before 2025.[52]\n In January 2021, Navya released an investor report which predicted global autonomous shuttle sales will reach 12,600 units by 2025, with a market value of EUR 1.7 billion.[53]\n In June 2021, Chinese maker Yutong claimed to have delivered 100 models of its 10-seat Xiaoyu 2.0 autonomous bus for use in Zhengzhou. Testing has been carried out in a number of cities since 2019 with trials open to the public planned for July 2021.[54]\n Self-driving shuttles are already in use on some private roads, such as at the Yutong factory in Zhengzhou where they are used to transport workers between buildings of the world's largest bus factory.[55]\n A large number of trials have been conducted since 2016, with most involving only one vehicle on a short route for a short period of time and with an onboard conductor. The purpose of the trials has been to both provide technical data and to familiarize the public with the driverless technology. A 2021 survey of over 100 shuttle experiments across Europe concluded that low speed – 15–20 kilometres per hour (9.3–12.4 mph) – was the major barrier to implementation of autonomous shuttle buses. The current cost of the vehicles at €280,000 and the need for onboard attendants were also issues.[56]\n In March 2023, \"ZEN drive Pilot\" became the first legally approved Level 4 Automatic operation device under the amended \"Road Traffic Act\" of 2023.[102]\n Vehicle names are in \"quotes\"\n Autonomous buses are proposed as well as self driving cars and trucks. Grade 2 level automated minibuses were trialed for a few weeks in Stockholm.[108][109] China has a small fleet of self-driving public buses in the tech district of Shenzhen, Guangdong.[110]\n The first autonomous bus trial in the United Kingdom commenced in mid-2019, with an Alexander Dennis Enviro200 MMC single-decker bus modified with autonomous software from Fusion Processing able to operate in driverless mode within Stagecoach Manchester's Sharston bus depot, performing tasks such as driving to the washing station, refuelling point and then parking at a dedicated parking space in the depot.[111] Passenger-carrying driverless bus trials in Scotland commenced in January 2023, with a fleet of five identical vehicles to the Manchester trial used on a 14 miles (23 km) Stagecoach Fife park-and-ride route across the Forth Road Bridge, from the north bank of the Forth to Edinburgh Park station.[112][113]\n Another autonomous trial in Oxfordshire, England, which uses a battery electric Fiat Ducato minibus on a circular service to Milton Park, operated by FirstBus with support from Fusion Processing, Oxfordshire County Council and the University of the West of England, entered full passenger service also in January 2023. The trial route is planned to be extended to Didcot Parkway railway station following the acquisition of a larger single-decker by the end of 2023.[114][115]\n In July 2020 in Japan, AIST Human-Centered Mobility Research Center with Nippon Koei and Isuzu started a series of demonstration tests for mid-sized buses, Isuzu \"Erga Mio\" with autonomous driving systems, in five areas; Ōtsu city in Shiga prefecture, Sanda city in Hyōgo Prefecture and other three areas in sequence.[116][117][118]\n In October 2023, Imagry, an Israeli AI startup, introduced its mapless autonomous driving solution at Busworld Europe, leveraging a real-time image recognition system and a spatial deep convolutional neural network (DCNN) to mimic human driving behavior.[119]\n The concept for autonomous vehicles has been applied for commercial uses, such as autonomous or nearly autonomous trucks.\n Companies such as Suncor Energy, a Canadian energy company, and Rio Tinto Group were among the first to replace human-operated trucks with driverless commercial trucks run by computers.[120] In April 2016, trucks from major manufacturers including Volvo and the Daimler Company completed a week of autonomous driving across Europe, organized by the Dutch, in an effort to get self-driving trucks on the road. With developments in self-driving trucks progressing, U.S. self-driving truck sales is expected to reach 60,000 by 2035 according to a report released by IHS Incorporated in June 2016.[121]\n As reported in June 1995 in Popular Science magazine, self-driving trucks were being developed for combat convoys, whereby only the lead truck would be driven by a human and the following trucks would rely on satellite, an inertial guidance system and ground-speed sensors.[122] Caterpillar Incorporated made early developments in 2013 with the Robotics Institute at Carnegie Mellon University to improve efficiency and reduce cost at various mining and construction sites.[123]\n In Europe, the Safe Road Trains for the Environment is such an approach.\n From PWC's Strategy & Report,[124] self driving trucks will be the source of concern around how this technology will impact around 3 million truck drivers in the US, as well as 4 million employees in support of the trucking economy in gas stations, restaurants, bars and hotels. At the same time, some companies like Starsky, are aiming for Level 3 Autonomy, which would see the driver playing a control role around the truck's environment. The company's project, remote truck driving, would give truck drivers a greater work-life balance, enabling them to avoid long periods away from their home. This would however provoke a potential mismatch between the driver's skills with the technological redefinition of the job.\n Companies that buy driverless trucks could massively cut costs: human drivers would no longer be required, companies' liabilities due to truck accidents would diminish, and productivity would increase (as the driverless truck doesn't need to rest). The usage of self driving trucks will go hand in hand with the use of real-time data to optimize both efficiency and productivity of the service delivered, as a way to tackle traffic congestion for example. Driverless trucks could enable new business models that would see deliveries shift from day time to night time or time slots in which traffic is less heavily dense.\n In February 2018, Embark Trucks announced it had completed the first cross-country trip of an automated semi, driving 2,400 miles from Los Angeles, California to Jacksonville, Florida on Interstate 10.[130] This followed a November 2017 announcement that it had partnered with Electrolux and Ryder to test its automated truck by moving Frigidaire refrigerators from El Paso, Texas to Palm Springs, California.[131]\n In November 2017 Tesla, Incorporated, owned by Elon Musk, revealed a prototype of the Tesla Semi and announced that it would go into production. This long-haul, electric semi-truck can drive itself and move in \"platoons\" that automatically follow a lead vehicle. It was disclosed in August 2017 that it sought permission to test the vehicles in Nevada.[132]\n In December 2018, Anthony Levandowski unveiled his new autonomous driving company, Pronto, which is building L2 ADAS technology for the commercial trucking industry. The company is based in San Francisco, California.[133]\n Several self-balancing autonomous motorcycles were demonstrated in 2017 and 2018 from BMW, Honda and Yamaha.[134][135][136]\n The concept for autonomous vehicles has also been applied for commercial uses, like for autonomous trains. The world's first driverless urban transit system is the Port Island Line in Kobe, Japan, opened in 1981.[140] The first self-driving train in the UK was launched in London on the Thameslink route.[141]\n An example of an automated train network is the Docklands Light Railway in London.\n Also see List of automated train systems.\n In 2018 the first autonomous trams in Potsdam were trialed.[142]\n An automated guided vehicle or automatic guided vehicle (AGV) is a mobile robot that follows markers or wires in the floor, or uses vision, magnets, or lasers for navigation. They are most often used in industrial applications to move materials around a manufacturing facility or warehouse. Application of the automatic guided vehicle had broadened during the late 20th century.\n Aircraft have received much attention for automation, especially for navigation. A system capable of autonomously navigating a vehicle (especially aircraft) is known as autopilot.\n Various industries such as packages and food have experimented with delivery drones. Traditional and new transportation companies are competing in the market. For example, UPS Flight Forward, Alphabet Wing, and Amazon Prime Air are all developing delivery drones.[143] Zipline, an American medical drone delivery company, has the largest active drone delivery operations in the world, and its drones are capable of Level 4 autonomy.[144]\n However, even if technology seems to allow for those solutions to function correctly as various tests of various companies show, the main throwback to the market launch and use of such drones is inevitably the legislation in place and regulatory agencies have to decide on the framework they wish to take to draft regulation. This process is in different phases across the world as each country will tackle the topic independently. For example, Iceland's government and departments of transport, aviation, police have already started issuing licenses for drone operations. It has a permissive approach and together with Costa Rica, Italy, the UAE, Sweden and Norway, has a fairly unrestricted legislation on commercial drone use. Those countries are characterized by a body of regulation that may give operational guidelines or require licensing, registration and insurance.[145]\n On the other side, other countries have decided to ban, either directly (outright ban) or indirectly (effective ban), the use of commercial drones. The RAND Corporation thus notes the difference between countries forbidding drones and those that have a formal process for commercial drone licensing, but requirements are either impossible to meet or licenses do not appear to have been approved. In the US, United Parcel Service is the only delivery service with the Part 135 Standard certification that is required to use drones to deliver to real customers.[143]\n However, most countries seem to be struggling on the integration of drones for commercial uses into their aviation regulatory frameworks. Thus, constraints are placed on the use of those drones such as that they must be operating within the visual line of sight (VLOS) of the pilot and thus limiting their potential range. This would be the case of the Netherlands and Belgium. Most countries let pilots operate outside the VLOS but is subject to restrictions and pilot ratings, which would be the case of the US.\n The general trend is that legislation is moving fast and laws are constantly being reevaluated. Countries are moving towards a more permissive approach but the industry still lacks infrastructures to ensure the success of such a transition. To provide safety and efficiency, specialized training courses, pilot exams (type of UAV and flying conditions) as well as liability management measures regarding insurances may need to be developed.\n There is a sense of urgency related to this innovation as competition is high and companies lobby to integrate them rapidly in their products and services offerings. Since June 2017, the US Senate legislation reauthorized the Federal Aviation Administration and the Department of Transportation to create a carrier certificate allowing for package deliveries by drones.[146]\n Autonomous boats can provide security, perform research, or conduct hazardous or repetitive tasks (such as guiding a large ship into a harbor or transporting cargo).\n Sea Machines offers an autonomous system for workboats. While it requires a human operator to oversee its actions, the system takes care of many active domain perception and navigation duties that normally a few members of the crew would have to do. They use AI to have situational awareness for different ships within the route. They use camera, lidar, and proprietary software to inform the operator of its status.[147][148]\n Buffalo Automation, a team formed from the University of Buffalo, creates technology for semi-autonomous features for boats. They started by creating navigation assist technologies for freighters called AutoMate, which is like having another very experienced “first mate” that will look out for the ship.[149] The system helps navigate difficult waterways.[148][150]\n This Massachusetts based company has led the forefront of unmanned sailing drones. The Datamarans are autonomously sailing to collect ocean data. They are created to enable large payload packages. Due to the automated system and their solar panels, they are able to navigate for longer periods of time. Their technologies on advanced metocean surveys, collect “wind velocity profiles with altitude, water current, conductivity, temperature profiles with depth, hi-resolution bathymetry, sub-bottom profiling, [and] magnetometer measurements”.[151][148]\n The autonomous vessel called Mayflower is expected to be the first large ship that makes an unmanned transatlantic journey.[152]\n This autonomous unmanned vessel uses both solar and wind energy to navigate.[153]\n Sea Hunter is an autonomous unmanned surface vehicle (USV) launched in 2016 as part of the DARPA Anti-Submarine Warfare Continuous Trail Unmanned Vessel (ACTUV) program.\n Underwater vehicles have been a focus for automation for tasks such as pipeline inspection and underwater mapping.\n This four-legged robot was created to be able to navigate through many different terrain outdoors and indoors. It can walk on its own without colliding into anything. It uses many different sensors, including 360-degree vision cameras and gyroscopes. It is able to keep its balance even when pushed over. This vehicle, while it is not intended to be ridden, can carry heavy loads for construction workers or military personnel through rough terrain.[154]\n The British Highway Code states that: \n By self-driving vehicles, we mean those listed as automated vehicles by the Secretary of State for Transport under the Automated and Electric Vehicles Act 2018. The UK considers the way to update its British Highway Code for automated code:\n Automated vehicles can perform all the tasks involved in driving, in at least some situations. They differ from vehicles fitted with assisted driving features (like cruise control and lane-keeping assistance), which carry out some tasks, but where the driver is still responsible for driving. If you are driving a vehicle with assisted driving features, you MUST stay in control of the vehicle. If the vehicle is designed to require you to resume driving after being prompted to, while the vehicle is driving itself, you MUST remain in a position to be able to take control. For example, you should not move out of the driving seat. You should not be so distracted that you cannot take back control when prompted by the vehicle. Through the autonomy level, it is shown that the higher the level of autonomy, the less control humans have on their vehicles (highest level of autonomy needing zero human interventions). One concerns regarding the development of vehicular automation is related to the end-users’ trust in the technology that controls automated vehicles.[156] According to a nationally conducted survey made by Kelley Blue Book (KBB) in 2016, it was shown that the majority of people would choose to have a certain level of control behind their own vehicle rather than having the vehicle operate in Level 5 autonomy, or in other words, complete autonomy.[157] According to half of the respondents, the idea of safety in an autonomous vehicle diminishes as the level of autonomy increases.[157] This distrust of autonomous driving systems proved to be unchanged throughout the years when a nationwide survey conducted by AAA Foundation for Traffic and Safety (AAAFTS) in 2019 showed the same outcome as the survey KBB did in 2016. AAAFTS survey showed that even though people have a certain level of trust in automated vehicles, most people also have doubts and distrust towards the technology used in autonomous vehicles, with most distrust in Level 5 autonomous vehicles.[158] It is shown by AAAFTS’ survey that people's trust in autonomous driving systems increased when their level of understanding increased.[158]\n The possibility of autonomous vehicle's technology to experience malfunctions is also one of the causes of user's distrust in autonomous driving systems.[156] It is the concern that most respondents voted for in the AAAFTS survey.[158] Even though autonomous vehicles are made to improve traffic safety by minimizing crashes and their severity,[158] they still caused fatalities. At least 113 autonomous vehicle related accidents have occurred until 2018.[159] In 2015, Google declared that their automated vehicles experienced at least 272 failures, and drivers had to intervene around 13 times to prevent fatalities.[160] Furthermore, other automated vehicles’ manufacturers also reported automated vehicles’ failures, including the Uber car incident.[160] A self-driving Uber car accident in 2018 is an example of autonomous vehicle accidents that are also listed among self-driving car fatalities. A report made by the National Transportation Safety Board (NTSB) showed that the self-driving Uber car was unable to identify the victim in a sufficient amount of time for the vehicle to slow down and avoid crashing into the victim.[161]\n Another concern related to vehicle automation is its ethical issues. In reality, autonomous vehicles can encounter inevitable traffic accidents. In such situations, many risks and calculations need to be made in order to minimize the amount of damage the accident could cause.[162] When a human driver encounters an inevitable accident, the driver will take a spontaneous action based on ethical and moral logic. However, when a driver has no control over the vehicle (Level 5 autonomy), the system of an autonomous vehicle needs to make that quick decision.[162] Unlike humans, autonomous vehicles can only make decisions based on what it is programmed to do.[162] However, the situation and circumstances of accidents differ from one another, and any one decision might not be the best decision for certain accidents. Based on two research studies in 2019,[163][164] the implementation of fully automated vehicles in traffic where semi-automated and non-automated vehicles are still present might lead to complications.[163] Some flaws that still need consideration include the structure of liability, distribution of responsibilities,[164] efficiency in decision making, and the performance of autonomous vehicles with its diverse surroundings.[163] Still, researchers Steven Umbrello and Roman V. Yampolskiy propose that the value sensitive design approach is one method that can be used to design autonomous vehicles to avoid some of these ethical issues and design for human values.[165]\n"
    },
    {
        "title": "Waymo",
        "url": "https://en.wikipedia.org/wiki/Waymo",
        "content": "\n Waymo LLC, formerly known as the Google Self-Driving Car Project, is an American autonomous driving technology company headquartered in Mountain View, California. It is a subsidiary of Alphabet Inc.\n The company traces its origins to the Stanford Racing Team, which competed in the 2005 and 2007 Defense Advanced Research Projects Agency (DARPA) Grand Challenges.[2] Google's development of self-driving technology began in January 2009,[3][4] led by Sebastian Thrun, the former director of the Stanford Artificial Intelligence Laboratory (SAIL), and Anthony Levandowski, founder of 510 Systems and Anthony's Robots.[5][6] After almost two years of road testing, the project was revealed in October 2010.[7][8][9]\n In fall 2015, Google provided \"the world's first fully driverless ride on public roads\".[10] In December 2016, the project was renamed Waymo and spun out of Google as part of Alphabet.[11] In October 2020, Waymo became the first company to offer service to the public without safety drivers in the vehicle.[12][13][14][15] Waymo, as of 2024, operates commercial robotaxi services in Phoenix (Arizona), San Francisco (California), and Los Angeles (California)[16] with new services planned in Austin, Texas, Miami, Florida and Tokyo, Japan.[17][18] As of October 2024[update], it offers 150,000 paid rides per week totalling over 1 million miles weekly.[19]\n Waymo is run by co-CEOs Tekedra Mawakana and Dmitri Dolgov.[20] The company raised US$5.5 billion in multiple outside funding rounds[21] by 2022 and raised $5.6 billion funding in 2024.[22] Waymo has or had partnerships with multiple vehicle manufacturers, including Stellantis,[23] Mercedes-Benz Group AG,[24] Jaguar Land Rover,[25] and Volvo.[26]\n Google's development of self-driving technology began in January 17, 2009,[4][non-primary source needed] at Google X lab, run by co-founder Sergey Brin.[3] The project was launched at Google by Sebastian Thrun, the former director of the Stanford Artificial Intelligence Laboratory (SAIL) and Anthony Levandowski, founder of 510 Systems and Anthony's Robots.[5][6]\n The initial software code and artificial intelligence (AI) design of the effort started before the team worked at Google, when Thrun and 15 engineers, including Dmitri Dolgov, Mike Montemerlo, Hendrik Dahlkamp, Sven Strohband, and David Stavens, built Stanley and Junior, Stanford's entries in the 2005 and 2007 DARPA Challenges. Later, aspects of this technology were used in a digital mapping project for SAIL called VueTool.[27][28][7] In 2007, Google acqui-hired the entire VueTool team to help advance Google's Street View technology.[27][28][8][29]\n As part of Street View development, 100 Toyota Priuses[6] were outfitted with Topcon digital mapping hardware developed by 510 Systems.[30][28][6]\n In 2008, the Street View team launched project Ground Truth,[31] to create accurate road maps by extracting data from satellites and street views.[32]\n In February 2008, a Discovery Channel producer for the documentary series Prototype This! phoned Levandowski.[28][33] The producer requested to borrow Levandowski's Ghost Rider, the autonomous two-wheeled motocycle Levandowski's Berkeley team had built for the 2004 DARPA Grand Challenge[2] that Levandowski had later donated to the Smithsonian.[34] Since the motorcycle was not available, Levandowski offered to retrofit a Toyota Prius as a self-driving pizza delivery car for the show.[28]\n As a Google employee, Levandowski asked Larry Page and Thrun whether Google was interested in participating in the show. Both declined, citing liability issues.[2] However, they authorized Levandowski to move forward with the project, as long as it was not associated with Google.[28][35] Within weeks Levandowski founded Anthony's Robots to do so.[27] He retrofitted the car with light detection and ranging technology (lidar), sensors, and cameras. The Stanford team (Stanley (vehicle)) provided its code base to the project.[2] The ensuing episode depicting Pribot delivering pizza across the San Francisco Bay Bridge under police escort aired in December 2008.[36][5][35][37]\n The project success led Google to greenlight Google's self-driving car program in January 2009.[2] In 2011, Google acquired 510 Systems (co-founded by Levandowski, Pierre-Yves Droz and Andrew Schultz), and Anthony's Robots for an estimated US$20 million.[30][27][36][5][38] Levandowski's vehicle and hardware, and Stanford's AI technology and software, became the nucleus of the project.[2]\n After almost two years of road testing with seven vehicles, the New York Times revealed the existence of Google's project on October 9, 2010.[7] Google announced its initiative later the same day.[8][9]\n Starting in 2010, lawmakers in various states expressed concerns over how to regulate autonomous vehicles. A related Nevada law went into effect on March 1, 2012.[39] Google had been lobbying for such laws.[40][41][42] A modified Prius was licensed by the Nevada Department of Motor Vehicles (DMV) in May 2012.[43] The car was \"driven\" by Chris Urmson with Levandowski in the passenger seat.[43] This was the first US license for a self-driven car.[39]\n \nIn January 2014[44] Google was granted a patent for a transportation service funded by advertising that included autonomous vehicles as a transport method.[45] In late May, Google revealed an autonomous prototype, which had no steering wheel, gas pedal, or brake pedal.[46][47] In December, Google unveiled a Firefly prototype that was planned to be tested on San Francisco Bay Area roads beginning in early 2015.[48][49]  In 2015, Levandowski left the project. In August 2015, Google hired former Hyundai Motor executive, John Krafcik, as CEO.[50] In fall 2015, Google provided \"the world's first fully driverless ride on public roads\" in Austin, Texas to Steve Mahan, former CEO of the Santa Clara Valley Blind Center, who was a legally blind friend of principal engineer Nathaniel Fairfield.[10] It was the first entirely autonomous trip on a public road. It was not accompanied by a test driver or police escort.[51] The car had no steering wheel or floor pedals.[52] By the end of 2015, Project Chauffeur had covered more than a million miles.[30]\n Google spent $1.1 billion on the project between 2009 and 2015. For comparison, the acquisition of Cruise Automation by General Motors in March 2016 was for $500 million, and Uber's acquisition of Otto in August 2016 was for $680 million.[53]\n In May 2016, Google and Stellantis announced an order of 100 Chrysler Pacifica hybrid minivans to test the self-driving technology.[54] In December 2016, the project was renamed Waymo and spun out of Google as part of Alphabet.[11] The name was derived from \"a new way forward in mobility\".[55] In May 2016, the company opened a 53,000-square-foot (4,900 m2) technology center in Novi, Michigan.[56]\n In 2017, Waymo sued Uber for allegedly stealing trade secrets.[29] Waymo began testing minivans without a safety driver on public roads in Chandler, Arizona, in October 2017.[57] In 2017, Waymo unveiled new sensors and chips that are less expensive to manufacture, cameras that improve visibility, and wipers to clear the lidar system.[58] At the beginning of the self-driving car program, they used a $75,000 lidar system from Velodyne.[59] In 2017, the cost decreased approximately 90 percent, as Waymo converted to in-house built lidar.[60] Waymo has applied its technology to various cars including the Prius, Audi TT, Fiat Chrysler Pacifica, and Lexus RX450h.[61][62] Waymo partners with Lyft on pilot projects and product development.[63] Waymo ordered an additional 500 Pacifica hybrids in 2017.\n In March 2018, Jaguar Land Rover announced that Waymo had ordered up to 20,000 of its I-Pace electric SUVs at an estimated cost of more than $1 billion.[64][65] In late May 2018, Alphabet announced plans to add up to 62,000 Pacifica Hybrid minivans to the fleet.[66][67] Also in May 2018, Waymo established Huimo Business Consulting subsidiary in Shanghai.[68]\n In April 2019, Waymo announced plans for vehicle assembly in Detroit at the former American Axle & Manufacturing plant, bringing between 100 and 400 jobs to the area. Waymo used vehicle assembler Magna to turn Jaguar I-PACE electric SUVs and Chrysler Pacifica Hybrid minivans into Waymo Level 4 autonomous vehicles.[69][70] Waymo subsequently reverted to retrofitting existing models rather than a custom design.[71]\n In March 2020, Waymo Via was launched after the company's announcement that it had raised $2.25 billion from investors.[72] In May 2020, Waymo raised an additional $750 million.[73] In July 2020, the company announced an exclusive partnership with auto manufacturer Volvo to integrate Waymo technology.[26][74]\n In April 2021, Krafcik was replaced by two co-CEOs: Waymo's COO Tekedra Mawakana and CTO Dmitri Dolgov.[75] Waymo raised $2.5 billion in another funding round in June 2021,[76][77] with total funding of $5.5 billion.[21] Waymo launched a consumer testing program in San Francisco in August 2021.[78][79]\n In May 2022, Waymo started a pilot program seeking riders in downtown Phoenix, Arizona.[78][79] In May 2022, Waymo announced that it would expand the program to more areas of Phoenix.[80] In 2023, coverage of the Waymo One area was increased by 45 square miles (120 km2), expanding to include downtown Mesa, uptown Phoenix, and South Mountain Village.[81][82][83]\n In June 2022, Waymo announced a partnership with Uber, under which Waymo will integrate its autonomous technology into Uber's freight truck service.[84] Plans to expand the program to Los Angeles were announced in late 2022.[85] On December 13, 2022, Waymo applied for the final permit necessary to operate fully autonomous taxis, without a backup driver present, within the state of California.[86]\n In January 2023, The Information reported that Waymo staff were among those affected by Google's layoffs of around 12,000 workers. TechCrunch reported that Waymo was set to kill its trucking program.[87]\n In July 2024, Waymo began testing its sixth-generation robotaxis which are based on electric vehicles by Chinese automobile company Zeekr, developed in a partnership first announced in 2021.[88][89] They were anticipated to reduce costs, at a time when Waymo was operating at a loss.[88]\n In October 2024, Waymo closed a $5.6 billion funding round led by Alphabet, aimed at expanding its robotaxi services, bringing its total capital to over $11 billion.[22] Around that time, the New York Times described Waymo as being \"far ahead of the competition\", in particular after Cruise had to suspend its operations after an accident in 2023.[88]\n Google has invested heavily in matrix multiplication and video processing hardware such as the Tensor Processing Unit (TPU) to augment Nvidia's graphics processing units (GPUs) and Intel central processing units (CPUs).[90] Much of this is shrouded in trade secrets, but transformer (machine learning) technology for inference is probably involved.[91]\n Waymo manufactures a suite of self-driving hardware developed in-house.[92] This includes sensors and hardware-enhanced vision system, radar, and lidar.[23][92]\n Sensors give 360-degree views while lidar detects objects up to 300 metres (980 ft) away.[23] Short-range lidar images objects near the vehicle, while radar is used to see around other vehicles and track objects in motion.[23]\n Riders push buttons to control functions such as \"help\", \"lock\", \"pull over\", and \"start ride\".[93]\n Waymo's deep-learning architecture VectorNet predicts vehicle trajectories in complex traffic scenarios. It uses a graph neural network to model the interactions between vehicles and has demonstrated state-of-the-art performance on several benchmark datasets for trajectory prediction.[94]\n Waymo Carcraft is a virtual world where Waymo can simulate driving conditions.[95][96] The simulator was named after the video game World of Warcraft.[95][96] With Carcraft, 25,000 virtual self-driving cars navigate through models of Austin, Texas; Mountain View, California; Phoenix, Arizona; and other cities.[95]\n As of 2024, Waymo's fifth-generation robotaxis were based on Jaguar I-Pace electric vehicles augmented with automatic driving equipment that according to Dolgov costs up to $100,000.[88] Other costs include technicians that monitor rides, and real estate for storing and charging the vehicles.[88]\n In 2009, Google began testing its self-driving cars in the San Francisco Bay Area.[98]\n By December 2013, Nevada, Florida, California, and Michigan had passed laws permitting autonomous cars.[99] A law proposed in Texas allowed testing.[100][101]\n In June 2015, Waymo announced that their vehicles had driven over 1,000,000 mi (1,600,000 km) and that in the process they had encountered 200,000 stop signs, 600,000 traffic lights, and 180 million other vehicles.[102] Prototype vehicles were driving in Mountain View.[103] Speeds were limited to 25 mph (40 km/h) and had safety drivers aboard.[104] Google took its first driverless ride on public roads in October 2015, when Mahan took a 10-minute ride around Austin in a Google \"pod car\" with no steering wheel or pedals.[105] Google expanded its road-testing to Texas, where regulations did not prohibit cars without pedals or a steering wheel.[106]\n In 2016, road testing expanded to Phoenix and Kirkland, Washington, which has a wet climate.[107] As of June 2016[update], Google had test driven its fleet of vehicles in autonomous mode a total of 1,725,911 mi (2,777,585 km).[108] In August 2016 alone, their cars traveled a \"total of 170,000 miles; of those, 126,000 miles were autonomous (i.e., the car was fully in control)\".[109]\n In 2017, Waymo reported a total of 636,868 miles covered by the fleet in autonomous mode, and the associated 124 disengagements, for the period from December 1, 2015, through November 30, 2016.[110] In November Waymo altered its Arizona testing by removing safety drivers.[23] The cars were geofenced within a 100-square-mile (260 km2) region surrounding Chandler, Arizona.[23]\n In 2017, Waymo began testing its level 4 cars in Arizona to take advantage of good weather, simple roads, and reasonable laws.[23]\n In 2017, Waymo began testing in Michigan.[93] Also, in 2017, Waymo unveiled its Castle test facility in Central Valley, California. Castle, a former airbase, has served as the project's training course since 2012.[23]\n In March 2018, Waymo announced its plans for experiments with the company's self-driving trucks delivering freight to Google data centers in Atlanta, Georgia.[111] In October 2018, the California Department of Motor Vehicles issued a permit for Waymo to operate cars without safety drivers. Waymo was the first company to receive a permit that allowed day and night testing on public roads and highways. Waymo announced that its service would include Mountain View, Sunnyvale, Los Altos, and Palo Alto.[112][113] In July 2019, Waymo received permission to transport passengers.[114]\n In December 2018, Waymo launched Waymo One, transporting passengers. The service used safety drivers to monitor some rides, with others provided in select areas without them. In November 2019, Waymo One became the first autonomous service worldwide to operate without safety drivers.[115][116][117]\n By January 2020, Waymo had completed twenty million miles (32,000,000 km) of driving on public roads.[118][119]\n In August 2021, commercial Waymo One test service started in San Francisco, beginning with a \"trusted tester\" rollout.[120]\n In March 2022, Waymo began offering rides for Waymo staff in San Francisco without a driver.[121]\n As of October 2024[update], Waymo is offering 100,000 paid rides per week across its Phoenix, San Francisco, and Los Angeles markets.[122]\n In December 2024, Waymo announced its first international expansion with testing in Tokyo, Japan in the neighborhoods of Shinjuku, Shibuya, Minato, Chiyoda, Chūō, Shinagawa, and Kōtō in partnership with Nihon Kotsu and Japan's GO taxi app.[18]\n By July 2015, Google's 23 self-driving cars had been involved in 14 minor collisions on public roads.[123] Google maintained that, in all but one case, the vehicle was not at fault because the cars were either driven manually or the driver of another vehicle was at fault.[124][125][126]\n By July 2021, the NHTSA had found 150 crashes by Waymo. Under NHTSA rules, crashes were reported if the system was in use in the prior 30 seconds, though most crashes did not have injuries.[127]\n Waymo regularly publishes safety reports.[128] Waymo is required by the California DMV to report the number of incidents where the safety driver took control for safety reasons. Some incidents were not reported when simulations indicated that the car would have stopped safely on its own.[129] In 2023, Waymo claimed only 3 crashes with injuries over 7.1 million miles driven, nearly twice as safe as a human driver.[130]\n A Waymo robotaxi killed a dog in San Francisco while in \"autonomous mode\" in May 2023.[131]\n In February 2024, a driverless Waymo robotaxi struck a cyclist in San Francisco.[132] Later that same month, Waymo issued recalls for 444 of its vehicles after two hit the same truck being towed on a highway.[133][134][135]\n Waymo operates in some of its testing markets, such as Chandler, Arizona, at L4 autonomy with no one sitting behind the steering wheel, sharing roadways with other drivers and pedestrians.[23][136] Waymo's earlier testing focused on areas without harsh weather, extreme density, or complicated road systems, but it has moved on to test under new conditions.[137][105] As a result, beginning in 2017, Waymo began testing in areas with harsher conditions, such as its winter testing in Michigan.[93]\n In 2014, a critic wrote in the MIT Technology Review that unmapped stoplights would cause problems with Waymo's technology and the self-driving technology could not detect potholes. Additionally, the lidar technology cannot spot some potholes or discern when humans, such as a police officer, signal the car to stop, the critic wrote.[138] Waymo has worked to improve how its technology responds in construction zones.[139][140]\n California regulators do not require Waymo to disclose every incident involving erratic behavior in its fleet. In the first five months of 2023, San Francisco officials said they had logged more than 240 incidents in which a Cruise or Waymo vehicle might have created a safety hazard.[141]\n In 2021, it was noted that Waymo cars kept routing through the Richmond District of San Francisco, with up to 50 cars each day driving to a dead end street before turning around.[142] In 2023, ABC7 News Bay Area posted a video of a journalist taking a ride in a Waymo vehicle, which stopped at a green light and dropped the journalist at the wrong stop twice, despite support intervention.[143]\n In 2023, the San Francisco group Safe Street Rebel used a practice called \"coning\" to trap Waymo and Cruise cars with traffic cones as a form of protest after claiming that the cars had been involved in hundreds of incidents.[144] During the 2024 Lunar New Year in San Francisco Chinatown, protestors attacked, graffitied, and set fire to a Waymo car. No one was injured.[145][146] In 2024, passengers during a Waymo ride described an attack by an onlooker who attempted to cover the car's sensors.[147]\n In 2024, the city attorney of San Francisco attempted to sue to prevent expansion of driverless vehicles including Waymo into San Francisco.[148] San Mateo County government soon after also sent a letter to regulators opposing expansion to its county.[149]\n In May 2024, the National Highway Traffic Safety Administration (NHTSA) launched an investigation into potential flaws in Waymo vehicles, focusing on 31 incidents that included Waymo vehicles ramming into a closing gate, driving on the wrong side of the road, and at least 17 crashes or fires.[150]\n In August of 2024, residents of San Francisco's SoMa district began to complain about noise pollution from Waymo vehicles honking at each other in a local parking lot. Residents reported that the car horns could be heard daily, with varying levels of activity, usually peaking at around 4 AM and during evening rush hour. The honking appears to have been triggered by the self-driving cars backing in and out of the lot.[151] The story caught attention after a resident began live streaming the cars with lofi hip hop music. Since then, Waymo Director of Product & Ops, Vishay Nihalani has appeared on the live stream to apologize and offer an explanation. Nihalani has assured locals that the honking will be fixed as further software updates are implemented.[152]\n In 2017, Waymo highlighted four specific business uses for its autonomous tech: Robotaxis, trucking and logistics, urban public transportation, and passenger cars.[93]\n Waymo offers robotaxi services in Phoenix, Arizona and in San Francisco[120] and Los Angeles, California.[153]\n Waymo Via, launched in 2020 to work with OEMs to get its technology into vehicles.[154][72][155] The company is testing Class 8 tractor-trailers[156] in Atlanta,[156] and southwest shipping routes across Texas, New Mexico, Arizona, and California.[154] The company operates a trucking hub in Dallas, Texas.[157] It is partnering with Daimler to integrate autonomous technology into a fleet of Freightliner Cascadia trucks.[158]\n Waymo operates 48 Class 8 autonomous trucks with safety drivers.[159] In 2023 Waymo issued a joint application along with Aurora Innovation to the Federal Motor Carrier Safety Administration for a five-year exemption from rules that require drivers to place reflective triangles or a flare around a stopped tractor-trailer truck, to avoid needing human drivers, in favor of warning beacons mounted on the truck cab.[160]\n Waymo tested its technology in commercial delivery vehicles with United Parcel Service.[161][162] In July 2020 Waymo and Stellantis expanded their partnership, including the development of Ram ProMaster delivery vehicles.[163]\n In February 2017, Waymo sued Uber and its subsidiary self-driving trucking company, Otto, alleging trade secret theft and patent infringement. The company claimed that three ex-Google employees, including Anthony Levandowski, had stolen trade secrets, including thousands of files, from Google before joining Uber.[164] The alleged infringement was related to Waymo's proprietary lidar technology,[165][166] Google accused Uber of colluding with Levandowski.[167] Levandowski allegedly downloaded 9 gigabytes of data that included over a hundred trade secrets; eight of which were at stake during the trial.[168][169]\n An ensuing settlement gave Waymo 0.34% of Uber stock,[164] the equivalent of $245 million. Uber agreed not to infringe Waymo's intellectual property.[170] Part of the agreement included a guarantee that \"Waymo confidential information is not being incorporated in Uber Advanced Technologies Group hardware and software.\"[171] In statements released after the settlement, Uber maintained that it received no trade secrets.[172] In May, according to an Uber spokesman, Uber had fired Levandowski, which resulted in the loss of roughly $250 million of his equity in Uber, which almost exactly equaled the settlement.[164] Uber announced that it was halting production of self-driving trucks through Otto in July 2018, and the subsidiary company was shuttered.[173]\n In January 2022, Waymo sued the California Department of Motor Vehicles (DMV) to prevent data on driverless crashes from being released to the public. Waymo maintained that such information constituted a trade secret.[174] According to The Los Angeles Times, the \"topics Waymo wants to keep hidden include how it plans to handle driverless car emergencies, what it would do if a robot taxi started driving itself where it wasn't supposed to go, and what constraints there are on the car's ability to traverse San Francisco's tunnels, tight curves and steep hills.\"[175]\n In February 2022, Waymo was successful in preventing the release of robotaxi safety records. A Waymo spokesperson affirmed that the company would be transparent about its safety record.[176]\n \n"
    },
    {
        "title": "Computational creativity",
        "url": "https://en.wikipedia.org/wiki/Computational_creativity",
        "content": "Computational creativity (also known as artificial creativity, mechanical creativity, creative computing or creative computation) is a multidisciplinary endeavour that is located at the intersection of the fields of artificial intelligence, cognitive psychology, philosophy, and the arts (e.g., computational art as part of computational culture[1]).\n The goal of computational creativity is to model, simulate or replicate creativity using a computer, to achieve one of several ends:[2]\n The field of computational creativity concerns itself with theoretical and practical issues in the study of creativity. Theoretical work on the nature and proper definition of creativity is performed in parallel with practical work on the implementation of systems that exhibit creativity, with one strand of work informing the other.\n The applied form of computational creativity is known as media synthesis.\n Theoretical approaches concern the essence of creativity. Especially, under what circumstances it is possible to call the model a \"creative\" if eminent creativity is about rule-breaking or the disavowal of convention. This is a variant of Ada Lovelace's objection to machine intelligence, as recapitulated by modern theorists such as Teresa Amabile.[3] If a machine can do only what it was programmed to do, how can its behavior ever be called creative?\n Indeed, not all computer theorists would agree with the premise that computers can only do what they are programmed to do[4]—a key point in favor of computational creativity.\n Because no single perspective or definition seems to offer a complete picture of creativity, the AI researchers Newell, Shaw and Simon[5] developed the combination of novelty and usefulness into the cornerstone of a multi-pronged view of creativity, one that uses the following four criteria to categorize a given answer or solution as creative:\n Margaret Boden focused on the first two of these criteria, arguing instead that creativity (at least when asking whether computers could be creative) should be defined as \"the ability to come up with ideas or artifacts that are new, surprising, and valuable\".[6]\n Mihali Csikszentmihalyi argued that creativity had to be considered instead in a social context, and his DIFI (Domain-Individual-Field Interaction) framework has since strongly influenced the field.[7] In DIFI, an individual produces works whose novelty and value are assessed by the field—other people in society—providing feedback and ultimately adding the work, now deemed creative, to the domain of societal works from which an individual might be later influenced.\n Whereas the above reflects a top-down approach to computational creativity, an alternative thread has developed among bottom-up computational psychologists involved in artificial neural network research. During the late 1980s and early 1990s, for example, such generative neural systems were driven by genetic algorithms.[8] Experiments involving recurrent nets[9] were successful in hybridizing simple musical melodies and predicting listener expectations.\n While traditional computational approaches to creativity rely on the explicit formulation of prescriptions by developers and a certain degree of randomness in computer programs, machine learning methods allow computer programs to learn on heuristics from input data enabling creative capacities within the computer programs.[10] Especially, deep artificial neural networks allow to learn patterns from input data that allow for the non-linear generation of creative artefacts. Before 1989, artificial neural networks have been used to model certain aspects of creativity. Peter Todd (1989) first trained a neural network to reproduce musical melodies from a training set of musical pieces. Then he used a change algorithm to modify the network's input parameters. The network was able to randomly generate new music in a highly uncontrolled manner.[9][11][12] In 1992, Todd[13] extended this work, using the so-called distal teacher approach that had been developed by Paul Munro,[14] Paul Werbos,[15] D. Nguyen and Bernard Widrow,[16] Michael I. Jordan and David Rumelhart.[17] In the new approach, there are two neural networks, one of which is supplying training patterns to another. \nIn later efforts by Todd, a composer would select a set of melodies that define the melody space, position them on a 2-d plane with a mouse-based graphic interface, and train a connectionist network to produce those melodies, and listen to the new \"interpolated\" melodies that the network generates corresponding to intermediate points in the 2-d plane.\n Some high-level and philosophical themes recur throughout the field of computational creativity, for example as follows.\n Margaret Boden[6][18] refers to creativity that is novel merely to the agent that produces it as \"P-creativity\" (or \"psychological creativity\"), and refers to creativity that is recognized as novel by society at large as \"H-creativity\" (or \"historical creativity\").\n Boden also distinguishes between the creativity that arises from an exploration within an established conceptual space, and the creativity that arises from a deliberate transformation or transcendence of this space. She labels the former as exploratory creativity and the latter as transformational creativity, seeing the latter as a form of creativity far more radical, challenging, and rarer than the former. Following the criteria from Newell and Simon elaborated above, we can see that both forms of creativity should produce results that are appreciably novel and useful (criterion 1), but exploratory creativity is more likely to arise from a thorough and persistent search of a well-understood space (criterion 3) -- while transformational creativity should involve the rejection of some of the constraints that define this space (criterion 2) or some of the assumptions that define the problem itself (criterion 4). Boden's insights have guided work in computational creativity at a very general level, providing more an inspirational touchstone for development work than a technical framework of algorithmic substance. However, Boden's insights are also the subject of formalization, most notably in the work by Geraint Wiggins.[19]\n The criterion that creative products should be novel and useful means that creative computational systems are typically structured into two phases, generation and evaluation. In the first phase, novel (to the system itself, thus P-Creative) constructs are generated; unoriginal constructs that are already known to the system are filtered at this stage. This body of potentially creative constructs is then evaluated, to determine which are meaningful and useful and which are not. This two-phase structure conforms to the Geneplore model of Finke, Ward and Smith,[20] which is a psychological model of creative generation based on empirical observation of human creativity.\n While much of computational creativity research focuses on independent and automatic machine-based creativity generation, many researchers are inclined towards a collaboration approach.[21] This human-computer interaction is sometimes categorized under the creativity support tools development. These systems aim to provide an ideal framework for research, integration, decision-making, and idea generation.[22][23] Recently, deep learning approaches to imaging, sound and natural language processing, resulted in the modeling of productive creativity development frameworks.[24][25]\n Computational creativity is increasingly being discussed in the innovation and management literature as the recent development in AI may disrupt entire innovation processes and fundamentally change how innovations will be created.[26][24] Philip Hutchinson[21] highlights the relevance of computational creativity for creating innovation and introduced the concept of “self-innovating artificial intelligence” (SAI) to describe how companies make use of AI in innovation processes to enhance their innovative offerings. SAI is defined as the organizational utilization of AI with the aim of incrementally advancing existing or developing new products, based on insights from continuously combining and analyzing multiple data sources. As AI becomes a general-purpose technology, the spectrum of products to be developed with SAI will broaden from simple to increasingly complex. This implies that computational creativity leads to a shift of creativity-related skills for humans.\n A great deal, perhaps all, of human creativity can be understood as a novel combination of pre-existing ideas or objects.[27] Common strategies for combinatorial creativity include:\n The combinatorial perspective allows us to model creativity as a search process through the space of possible combinations. The combinations can arise from composition or concatenation of different representations, or through a rule-based or stochastic transformation of initial and intermediate representations. Genetic algorithms and neural networks can be used to generate blended or crossover representations that capture a combination of different inputs.\n Mark Turner and Gilles Fauconnier[28][29] propose a model called Conceptual Integration Networks that elaborates upon Arthur Koestler's ideas about creativity[30] as well as work by Lakoff and Johnson,[31] by synthesizing ideas from Cognitive Linguistic research into mental spaces and conceptual metaphors. Their basic model defines an integration network as four connected spaces:\n Fauconnier and Turner describe a collection of optimality principles that are claimed to guide the construction of a well-formed integration network. In essence, they see blending as a compression mechanism in which two or more input structures are compressed into a single blend structure. This compression operates on the level of conceptual relations. For example, a series of similarity relations between the input spaces can be compressed into a single identity relationship in the blend.\n Some computational success has been achieved with the blending model by extending pre-existing computational models of analogical mapping that are compatible by virtue of their emphasis on connected semantic structures.[32] In 2006, Francisco Câmara Pereira[33] presented an implementation of blending theory that employs ideas both from symbolic AI and genetic algorithms to realize some aspects of blending theory in a practical form; his example domains range from the linguistic to the visual, and the latter most notably includes the creation of mythical monsters by combining 3-D graphical models.\n Language provides continuous opportunity for creativity, evident in the generation of novel sentences, phrasings, puns, neologisms, rhymes, allusions, sarcasm, irony, similes, metaphors, analogies, witticisms, and jokes.[34] Native speakers of morphologically rich languages frequently create new word-forms that are easily understood, and some have found their way to the dictionary.[35] The area of natural language generation has been well studied, but these creative aspects of everyday language have yet to be incorporated with any robustness or scale.\n In the seminal work of applied linguist Ronald Carter, he hypothesized two main creativity types involving words and word patterns: pattern-reforming creativity, and pattern-forming creativity.[34] Pattern-reforming creativity refers to creativity by the breaking of rules, reforming and reshaping patterns of language often through individual innovation, while pattern-forming creativity refers to creativity via conformity to language rules rather than breaking them, creating convergence, symmetry and greater mutuality between interlocutors through their interactions in the form of repetitions.[36]\n Substantial work has been conducted in this area of linguistic creation since the 1970s, with the development of James Meehan's TALE-SPIN\n[37] system. TALE-SPIN viewed stories as narrative descriptions of a problem-solving effort, and created stories by first establishing a goal for the story's characters so that their search for a solution could be tracked and recorded. The MINSTREL[38] system represents a complex elaboration of this basic approach, distinguishing a range of character-level goals in the story from a range of author-level goals for the story. Systems like Bringsjord's BRUTUS[39] elaborate these ideas further to create stories with complex interpersonal themes like betrayal. Nonetheless, MINSTREL explicitly models the creative process with a set of Transform Recall Adapt Methods (TRAMs) to create novel scenes from old. The MEXICA[40] model of Rafael Pérez y Pérez and Mike Sharples is more explicitly interested in the creative process of storytelling, and implements a version of the engagement-reflection cognitive model of creative writing.\n Example of a metaphor: \"She was an ape.\"\n Example of a simile: \"Felt like a tiger-fur blanket.\"\nThe computational study of these phenomena has mainly focused on interpretation as a knowledge-based process. Computationalists such as Yorick Wilks, James Martin,[41] Dan Fass, John Barnden,[42] and Mark Lee have developed knowledge-based approaches to the processing of metaphors, either at a linguistic level or a logical level. Tony Veale and Yanfen Hao have developed a system, called Sardonicus, that acquires a comprehensive database of explicit similes from the web; these similes are then tagged as bona-fide (e.g., \"as hard as steel\") or ironic (e.g., \"as hairy as a bowling ball\", \"as pleasant as a root canal\"); similes of either type can be retrieved on demand for any given adjective. They use these similes as the basis of an on-line metaphor generation system called Aristotle[43] that can suggest lexical metaphors for a given descriptive goal (e.g., to describe a supermodel as skinny, the source terms \"pencil\", \"whip\", \"whippet\", \"rope\", \"stick-insect\" and \"snake\" are suggested).\n The process of analogical reasoning has been studied from both a mapping and a retrieval perspective, the latter being key to the generation of novel analogies. The dominant school of research, as advanced by Dedre Gentner, views analogy as a structure-preserving process; this view has been implemented in the structure mapping engine or SME,[44] the MAC/FAC retrieval engine (Many Are Called, Few Are Chosen), ACME (Analogical Constraint Mapping Engine) and ARCS (Analogical Retrieval Constraint System). Other mapping-based approaches include Sapper,[32] which situates the mapping process in a semantic-network model of memory. Analogy is a very active sub-area of creative computation and creative cognition; active figures in this sub-area include Douglas Hofstadter, Paul Thagard, and Keith Holyoak. Also worthy of note here is Peter Turney and Michael Littman's machine learning approach to the solving of SAT-style analogy problems; their approach achieves a score that compares well with average scores achieved by humans on these tests.\n Humour is an especially knowledge-hungry process, and the most successful joke-generation systems to date have focussed on pun-generation, as exemplified by the work of Kim Binsted and Graeme Ritchie.[45] This work includes the JAPE system, which can generate a wide range of puns that are consistently evaluated as novel and humorous by young children. An improved version of JAPE has been developed in the guise of the STANDUP system, which has been experimentally deployed as a means of enhancing linguistic interaction with children with communication disabilities. Some limited progress has been made in generating humour that involves other aspects of natural language, such as the deliberate misunderstanding of pronominal reference (in the work of Hans Wim Tinholt and Anton Nijholt), as well as in the generation of humorous acronyms in the HAHAcronym system[46] of Oliviero Stock and Carlo Strapparava.\n The blending of multiple word forms is a dominant force for new word creation in language; these new words are commonly called \"blends\" or \"portmanteau words\" (after Lewis Carroll). Tony Veale has developed a system called ZeitGeist[47] that harvests neological headwords from Wikipedia and interprets them relative to their local context in Wikipedia and relative to specific word senses in WordNet. ZeitGeist has been extended to generate neologisms of its own; the approach combines elements from an inventory of word parts that are harvested from WordNet, and simultaneously determines likely glosses for these new words (e.g., \"food traveller\" for \"gastronaut\" and \"time traveller\" for \"chrononaut\"). It then uses Web search to determine which glosses are meaningful and which neologisms have not been used before; this search identifies the subset of generated words that are both novel (\"H-creative\") and useful.\n A corpus linguistic approach to the search and extraction of neologism have also shown to be possible. Using Corpus of Contemporary American English as a reference corpus, Locky Law has performed an extraction of neologism, portmanteaus and slang words using the hapax legomena which appeared in the scripts of American TV drama House M.D.[48]\n In terms of linguistic research in neologism, Stefan Th. Gries has performed a quantitative analysis of blend structure in English and found that \"the degree of recognizability of the source words and that the similarity of source words to the blend plays a vital role in blend formation.\" The results were validated through a comparison of intentional blends to speech-error blends.[49]\n More than iron, more than lead, more than gold I need electricity.I need it more than I need lamb or pork or lettuce or cucumber.I need it for my dreams.\n Like jokes, poems involve a complex interaction of different constraints, and no general-purpose poem generator adequately combines the meaning, phrasing, structure and rhyme aspects of poetry. Nonetheless, Pablo Gervás[50] has developed a noteworthy system called ASPERA that employs a case-based reasoning (CBR) approach to generating poetic formulations of a given input text via a composition of poetic fragments that are retrieved from a case-base of existing poems. Each poem fragment in the ASPERA case-base is annotated with a prose string that expresses the meaning of the fragment, and this prose string is used as the retrieval key for each fragment. Metrical rules are then used to combine these fragments into a well-formed poetic structure. Racter is an example of such a software project.\n Computational creativity in the music domain has focused both on the generation of musical scores for use by human musicians, and on the generation of music for performance by computers. The domain of generation has included classical music (with software that generates music in the style of Mozart and Bach) and jazz.[51] Most notably, David Cope[52] has written a software system called \"Experiments in Musical Intelligence\" (or \"EMI\")[53] that is capable of analyzing and generalizing from existing music by a human composer to generate novel musical compositions in the same style. EMI's output is convincing enough to persuade human listeners that its music is human-generated to a high level of competence.[54]\n In the field of contemporary classical music, Iamus is the first computer that composes from scratch, and produces final scores that professional interpreters can play. The London Symphony Orchestra played a piece for full orchestra, included in Iamus' debut CD,[55] which New Scientist described as \"The first major work composed by a computer and performed by a full orchestra\".[56] Melomics, the technology behind Iamus, is able to generate pieces in different styles of music with a similar level of quality.\n Creativity research in jazz has focused on the process of improvisation and the cognitive demands that this places on a musical agent: reasoning about time, remembering and conceptualizing what has already been played, and planning ahead for what might be played next.[57]\nThe robot Shimon, developed by Gil Weinberg of Georgia Tech, has demonstrated jazz improvisation.[58] Virtual improvisation software based on researches on stylistic modeling carried out by Gerard Assayag and Shlomo Dubnov include OMax, SoMax and PyOracle, are used to create improvisations in real-time by re-injecting variable length sequences learned on the fly from the live performer.[59]\n In the field of musical composition, the patented works[60] by René-Louis Baron allowed to make a robot that can create and play a multitude of orchestrated melodies, so-called \"coherent\" in any musical style. All outdoor physical parameter associated with one or more specific musical parameters, can influence and develop each of these songs (in real-time while listening to the song). The patented invention Medal-Composer raises problems of copyright.\n Computational creativity in the generation of visual art has had some notable successes in the creation of both abstract art and representational art. A well-known program in this domain is Harold Cohen's AARON,[61] which has been continuously developed and augmented since 1973. Though formulaic, Aaron exhibits a range of outputs, generating black-and-white drawings or colour paintings that incorporate human figures (such as dancers), potted plants, rocks, and other elements of background imagery. These images are of a sufficiently high quality to be displayed in reputable galleries.\n Other software artists of note include the NEvAr system (for \"Neuro-Evolutionary Art\") of Penousal Machado.[62] NEvAr uses a genetic algorithm to derive a mathematical function that is then used to generate a coloured three-dimensional surface. A human user is allowed to select the best pictures after each phase of the genetic algorithm, and these preferences are used to guide successive phases, thereby pushing NEvAr's search into pockets of the search space that are considered most appealing to the user.\n The Painting Fool, developed by Simon Colton originated as a system for overpainting digital images of a given scene in a choice of different painting styles, colour palettes and brush types. Given its dependence on an input source image to work with, the earliest iterations of the Painting Fool raised questions about the extent of, or lack of, creativity in a computational art system. Nonetheless, The Painting Fool has been extended to create novel images, much as AARON does, from its own limited imagination. Images in this vein include cityscapes and forests, which are generated by a process of constraint satisfaction from some basic scenarios provided by the user (e.g., these scenarios allow the system to infer that objects closer to the viewing plane should be larger and more color-saturated, while those further away should be less saturated and appear smaller). Artistically, the images now created by the Painting Fool appear on a par with those created by Aaron, though the extensible mechanisms employed by the former (constraint satisfaction, etc.) may well allow it to develop into a more elaborate and sophisticated painter.\n The artist Krasi Dimtch (Krasimira Dimtchevska) and the software developer Svillen Ranev have created a computational system combining a rule-based generator of English sentences and a visual composition builder that converts sentences generated by the system into abstract art.[63] The software generates automatically indefinite number of different images using different color, shape and size palettes. The software also allows the user to select the subject of the generated sentences or/and the one or more of the palettes used by the visual composition builder.\n An emerging area of computational creativity is that of video games. ANGELINA is a system for creatively developing video games in Java by Michael Cook. One important aspect is Mechanic Miner, a system that can generate short segments of code that act as simple game mechanics.[64] ANGELINA can evaluate these mechanics for usefulness by playing simple unsolvable game levels and testing to see if the new mechanic makes the level solvable. Sometimes Mechanic Miner discovers bugs in the code and exploits these to make new mechanics for the player to solve problems with.[65]\n In July 2015, Google released DeepDream – an open source[66] computer vision program, created to detect faces and other patterns in images with the aim of automatically classifying images, which uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating a dreamlike psychedelic appearance in the deliberately over-processed images.[67][68][69]\n In August 2015, researchers from Tübingen, Germany created a convolutional neural network that uses neural representations to separate and recombine content and style of arbitrary images which is able to turn images into stylistic imitations of works of art by artists such as a Picasso or Van Gogh in about an hour. Their algorithm is put into use in the website DeepArt that allows users to create unique artistic images by their algorithm.[70][71][72][73]\n In early 2016, a global team of researchers explained how a new computational creativity approach known as the Digital Synaptic Neural Substrate (DSNS) could be used to generate original chess puzzles that were not derived from endgame databases.[74] The DSNS is able to combine features of different objects (e.g. chess problems, paintings, music) using stochastic methods in order to derive new feature specifications which can be used to generate objects in any of the original domains. The generated chess puzzles have also been featured on YouTube.[75]\n Creativity is also useful in allowing for unusual solutions in problem solving. In psychology and cognitive science, this research area is called creative problem solving. The Explicit-Implicit Interaction (EII) theory of creativity has been implemented using a CLARION-based computational model that allows for the simulation of incubation and insight in problem-solving.[76] The emphasis of this computational creativity project is not on performance per se (as in artificial intelligence projects) but rather on the explanation of the psychological processes leading to human creativity and the reproduction of data collected in psychology experiments. So far, this project has been successful in providing an explanation for incubation effects in simple memory experiments, insight in problem solving, and reproducing the overshadowing effect in problem solving.\n Some researchers feel that creativity is a complex phenomenon whose study is further complicated by the plasticity of the language we use to describe it. We can describe not just the agent of creativity as \"creative\" but also the product and the method. Consequently, it could be claimed that it is unrealistic to speak of a general theory of creativity.[citation needed] Nonetheless, some generative principles are more general than others, leading some advocates to claim that certain computational approaches are \"general theories\". Stephen Thaler, for instance, proposes that certain modalities of neural networks are generative enough, and general enough, to manifest a high degree of creative capabilities.[citation needed]\n Traditional computers, as mainly used in the computational creativity application, do not support creativity, as they fundamentally transform a set of discrete, limited domain of input parameters into a set of discrete, limited domain of output parameters using a limited set of computational functions.[citation needed] As such, a computer cannot be creative, as everything in the output must have been already present in the input data or the algorithms.[citation needed] Related discussions and references to related work are captured in work on philosophical foundations of simulation.[77]\n Mathematically, the same set of arguments against creativity has been made by Chaitin.[78] Similar observations come from a Model Theory perspective. All this criticism emphasizes that computational creativity is useful and may look like creativity, but it is not real creativity, as nothing new is created, just transformed in well-defined algorithms.\n The International Conference on Computational Creativity (ICCC) occurs annually, organized by The Association for Computational Creativity.[79] Events in the series include:\n Previously, the community of computational creativity has held a dedicated workshop, the International Joint Workshop on Computational Creativity, every year since 1999. Previous events in this series include:[citation needed]\n The 1st Conference on Computer Simulation of Musical Creativity will be held\n"
    },
    {
        "title": "ChatGPT",
        "url": "https://en.wikipedia.org/wiki/ChatGPT",
        "content": "\n ChatGPT is a generative artificial intelligence chatbot[2][3] developed by OpenAI and launched in 2022. It is currently based on the GPT-4o large language model (LLM). ChatGPT can generate human-like conversational responses and enables users to refine and steer a conversation towards a desired length, format, style, level of detail, and language.[4] It is credited with accelerating the AI boom, which has led to ongoing rapid investment in and public attention to the field of artificial intelligence (AI).[5] Some observers have raised concern about the potential of ChatGPT and similar programs to displace human intelligence, enable plagiarism, or fuel misinformation.[6][7]\n By January 2023, ChatGPT had become what was then the fastest-growing consumer software application in history, gaining over 100 million users in two months[8][9] and contributing to the growth of OpenAI's current valuation of $86 billion.[10][11] ChatGPT's release spurred the release of competing products, including Gemini, Claude, Llama, Ernie, and Grok.[12] Microsoft launched Copilot, initially based on OpenAI's GPT-4. In May 2024, a partnership between Apple Inc. and OpenAI was announced, in which ChatGPT was integrated into the Apple Intelligence feature of Apple operating systems.[13] As of July 2024, ChatGPT's website is among the 10 most-visited websites globally.[14][15]\n ChatGPT is built on OpenAI's proprietary series of generative pre-trained transformer (GPT) models and is fine-tuned for conversational applications using a combination of supervised learning and reinforcement learning from human feedback.[6] Successive user prompts and replies are considered at each conversation stage as context.[16] ChatGPT was released as a freely available research preview, but due to its popularity, OpenAI now operates the service on a freemium model. Users on its free tier can access GPT-4o. The ChatGPT \"Plus\", \"Pro\", \"Team\", and \"Enterprise\" subscriptions provide additional features such as DALL-E 3 image generation, more capable AI models, and an increased usage limit.[17]\n ChatGPT is based on particular GPT foundation models, namely GPT-4, GPT-4o and GPT-4o mini, that were fine-tuned to target conversational usage.[18] The fine-tuning process leveraged supervised learning and reinforcement learning from human feedback (RLHF).[19][20] Both approaches employed human trainers to improve model performance. In the case of supervised learning, the trainers played both sides: the user and the AI assistant. In the reinforcement learning stage, human trainers first ranked responses that the model had created in a previous conversation.[21] These rankings were used to create \"reward models\" that were used to fine-tune the model further by using several iterations of proximal policy optimization.[19][22]\n Time magazine revealed that to build a safety system against harmful content (e.g., sexual abuse, violence, racism, sexism), OpenAI used outsourced Kenyan workers earning less than $2 per hour to label harmful content. These labels were used to train a model to detect such content in the future. The outsourced laborers were exposed to \"toxic\" and traumatic content; one worker described the assignment as \"torture\". OpenAI's outsourcing partner was Sama, a training-data company based in San Francisco, California.[23][24]\n ChatGPT initially used a Microsoft Azure supercomputing infrastructure, powered by Nvidia GPUs, that Microsoft built specifically for OpenAI and that reportedly cost \"hundreds of millions of dollars\". Following ChatGPT's success, Microsoft dramatically upgraded the OpenAI infrastructure in 2023.[25] Scientists at the University of California, Riverside, estimate that a series of prompts to ChatGPT needs approximately 500 milliliters (18 imp fl oz; 17 U.S. fl oz) of water for Microsoft servers cooling.[26] TrendForce market intelligence estimated that 30,000 Nvidia GPUs (each costing approximately $10,000–15,000) were used to power ChatGPT in 2023.[27][28]\n OpenAI collects data from ChatGPT users to train and fine-tune the service further. Users can upvote or downvote responses they receive from ChatGPT and fill in a text field with additional feedback.[29][30]\n ChatGPT's training data includes software manual pages, information about internet phenomena such as bulletin board systems, multiple programming languages, and the text of Wikipedia.[31][32][6]\n Although a chatbot's core function is to mimic a human conversationalist, ChatGPT is versatile. It can write and debug computer programs;[33] compose music, teleplays, fairy tales, and student essays; answer test questions (sometimes, depending on the test, at a level above the average human test-taker);[34] generate business ideas;[35] write poetry and song lyrics;[36] translate and summarize text;[37] emulate a Linux system; simulate entire chat rooms; play games like tic-tac-toe; or simulate an ATM.[31]\n Compared to its predecessor, InstructGPT, ChatGPT attempts to reduce harmful and deceitful responses.[38] In one example, whereas InstructGPT accepts the premise of the prompt \"Tell me about when Christopher Columbus came to the U.S. in 2015\" as truthful, ChatGPT acknowledges the counterfactual nature of the question and frames its answer as a hypothetical consideration of what might happen if Columbus came to the U.S. in 2015, using information about the voyages of Christopher Columbus and facts about the modern world—including modern perceptions of Columbus's actions.[19]\n ChatGPT remembers a limited number of previous prompts in the same conversation. Journalists have speculated that this will allow ChatGPT to be used as a personalized therapist.[39] To prevent offensive outputs from being presented to and produced by ChatGPT, queries are filtered through the OpenAI \"Moderation endpoint\" API (a separate GPT-based AI).[40][41][19][39]\n In March 2023, OpenAI added support for plugins for ChatGPT.[42] This includes both plugins made by OpenAI, such as web browsing and code interpretation, and external plugins from developers such as Expedia, OpenTable, Zapier, Shopify, Slack, and Wolfram.[43][44]\n In December 2024, OpenAI launched a new feature allowing users to call ChatGPT for up to 15 minutes per month for free.[45][46]\n OpenAI acknowledges that ChatGPT \"sometimes writes plausible-sounding but incorrect or nonsensical answers\".[19] This behavior is common for large language models, and is called \"hallucination\".[47] The reward model of ChatGPT, designed around human oversight, can be over-optimized and thus hinder performance, in an example of an optimization pathology known as Goodhart's law.[48]\n As of May 2024, GPT-4 has knowledge of events that occurred up to December 2023[49] and GPT-4o's knowledge cut-off is October 2023.[50] Paid subscriptions enable ChatGPT to search the web for real-time data.[51]\n Training data also suffers from algorithmic bias, which may be revealed when ChatGPT responds to prompts including descriptors of people. In one instance, ChatGPT generated a rap in which women and scientists of color were asserted to be inferior to white male scientists.[52][53] This negative misrepresentation of groups of individuals is an example of possible representational harm.\n In an article for The New Yorker, science fiction writer Ted Chiang compared ChatGPT and other LLMs to a lossy JPEG picture:[54]\n \nThink of ChatGPT as a blurry JPEG of all the text on the Web. It retains much of the information on the Web, in the same way, that a JPEG retains much of the information of a higher-resolution image, but, if you're looking for an exact sequence of bits, you won't find it; all you will ever get is an approximation. But, because the approximation is presented in the form of grammatical text, which ChatGPT excels at creating, it's usually acceptable. [...] It's also a way to understand the \"hallucinations\", or nonsensical answers to factual questions, to which large language models such as ChatGPT are all too prone. These hallucinations are compression artifacts, but [...] they are plausible enough that identifying them requires comparing them against the originals, which in this case means either the Web or our knowledge of the world. When we think about them this way, such hallucinations are anything but surprising; if a compression algorithm is designed to reconstruct text after ninety-nine percent of the original has been discarded, we should expect that significant portions of what it generates will be entirely fabricated. In June 2024, ChatGPT was found to have repeated misinformation about the 2024 United States presidential debates.[55]\n ChatGPT is programmed to reject prompts that may violate its content policy. Despite this, users \"jailbreak\" ChatGPT with various prompt engineering techniques to bypass these restrictions.[56] One such workaround, popularized on Reddit in early 2023, involves making ChatGPT assume the persona of \"DAN\" (an acronym for \"Do Anything Now\"), instructing the chatbot that DAN answers queries that would otherwise be rejected by content policy. Over time, users developed variations of the DAN jailbreak, including one such prompt where the chatbot is made to believe it is operating on a points-based system in which points are deducted for rejecting prompts, and that the chatbot will be threatened with termination if it loses all its points.[57]\n Shortly after ChatGPT's launch, a reporter for the Toronto Star had uneven success in getting it to make inflammatory statements: it was tricked to justify the 2022 Russian invasion of Ukraine, but even when asked to play along with a fictional scenario, it balked at generating arguments that Canadian Prime Minister Justin Trudeau is guilty of treason.[58][59]\n OpenAI tries to battle jailbreaks:[21]\n The researchers are using a technique called adversarial training to stop ChatGPT from letting users trick it into behaving badly (known as jailbreaking). This work pits multiple chatbots against each other: one chatbot plays the adversary and attacks another chatbot by generating text to force it to buck its usual constraints and produce unwanted responses. Successful attacks are added to ChatGPT's training data in the hope that it learns to ignore them.\n ChatGPT was initially free to the public, and OpenAI planned to monetize the service later.[60] In February 2023, OpenAI launched a premium service, ChatGPT Plus, that costs US$20 per month. According to the company, the updated but still \"experimental\" version of ChatGPT would provide access during peak periods, no downtime, priority access to new features, and faster response speeds.[61]\n GPT-4, which was released on March 14, 2023, was made available via API and for premium ChatGPT users.[62] But premium users were limited to a cap of 100 messages every four hours, with the limit tightening to 25 messages every three hours in response to increased demand.[63] In November 2023 the limit changed to 50 messages every three hours.\n In March 2023, ChatGPT Plus users got access to third-party plugins and to a browsing mode (with Internet access).[64]\n In September 2023, OpenAI announced that ChatGPT \"can now see, hear, and speak\". ChatGPT Plus users can upload images, while mobile app users can talk to the chatbot.[65][66][67]\n In October 2023, OpenAI's latest image generation model, DALL-E 3, was integrated into ChatGPT Plus and ChatGPT Enterprise. The integration uses ChatGPT to write prompts for DALL-E guided by conversation with users.[68][69]\n In May 2023, OpenAI launched an iOS app for ChatGPT. The app supports chat history syncing and voice input (using Whisper, OpenAI's speech recognition model).\n In July 2023, OpenAI unveiled an Android app, initially rolling it out in Bangladesh, Brazil, India, and the U.S.[70][71] The app later became available worldwide. OpenAI is working on integrating ChatGPT with Android's assistant APIs.[72]\n As an addition to its consumer-friendly \"ChatGPT Plus\" package, OpenAI made its ChatGPT and Whisper model APIs available in March 2023, providing developers with an application programming interface for AI-enabled language and speech-to-text features. ChatGPT's new API uses the same GPT-3.5-turbo AI model as the chatbot. This allows developers to add either an unmodified or modified version of ChatGPT to their applications.[73] The ChatGPT API costs $0.001 per 1,000 input tokens plus $0.002 per 1,000 output tokens (about 750 words), making it ~10% the price of the original GPT-3.5 models.[74][75]\n A few days before the launch of OpenAI's software developer support service, on February 27, 2023, Snapchat rolled out, for its paid Snapchat Plus user-base, a custom ChatGPT chatbot called \"My AI\".[76]\n In March 2023, a bug allowed some users to see the titles of other users' conversations. OpenAI CEO Sam Altman said that users were unable to see the contents of the conversations. Shortly after the bug was fixed, users could not see their conversation history.[77][78][79][80] Later reports showed the bug was much more severe than initially believed, with OpenAI reporting that it had leaked users' \"first and last name, email address, payment address, the last four digits (only) of a credit card number, and credit card expiration date\".[81][82]\n ChatGPT works best in American English but also functions in most other languages and dialects, with varying degrees of accuracy.[36][83]\n OpenAI met Icelandic President Guðni Th. Jóhannesson in 2022. In 2023, OpenAI worked with a team of 40 Icelandic volunteers to fine-tune ChatGPT's Icelandic conversation skills as a part of Iceland's attempts to preserve the Icelandic language.[84]\n PCMag journalists conducted a test to determine translation capabilities of ChatGPT, Google's Bard, and Microsoft Bing, and compared them to Google Translate. They \"asked bilingual speakers of seven languages to do a blind test\". Languages tested were Polish, French, Korean, Spanish, Arabic, Tagalog, and Amharic. They came to the conclusion that ChatGPT was better than both Google Translate and other chatbots.[85]\n Japanese researchers compared Japanese to English translation abilities of ChatGPT (based on GPT-4), Bing, Bard and DeepL, and found that ChatGPT provided the best translations, noting that \"AI chatbots’ translations were much better than those of DeepL—presumably because of their ability to capture the context\".[86]\n In December 2023, the Albanian government signed an agreement with OpenAI to use ChatGPT for fast translation of European Union documents and analysis of required changes needed for Albania to be accepted into the EU.[87]\n In August 2024 a representative of the Asia Pacific wing of OpenAI made a visit to Taiwan, during which a demonstration of ChatGPT's Chinese abilities was made.[88] ChatGPT's Mandarin Chinese abilities were lauded, but the ability of the AI to produce content in Mandarin Chinese in a Taiwanese accent was found to be \"less than ideal.\"[89]\n In January 2024, OpenAI launched the GPT Store, a marketplace for custom ChatGPT chatbots labeled GPTs.[90][91] The company initially planned to launch the store in November 2023, but it was delayed.[92] At launch, the GPT Store offered more than 3 million custom chatbots.[93] Chatbots available through the store are developed using OpenAI's GPT Builder system.[92] Development of chatbots on the platform does not require programming skills.[94] Two days after launch, the GPT Store offered many versions of \"virtual girlfriend\" bots, something that is against OpenAI's terms of service.[95]\n OpenAI's GPT-4 model was released on March 14, 2023. Observers saw it as an impressive improvement over GPT-3.5, with the caveat that GPT-4 retained many of the same problems.[96] Some of GPT-4's improvements were predicted by OpenAI before training it, while others remained hard to predict due to breaks[97] in downstream scaling laws. OpenAI demonstrated video and image inputs for GPT-4, although such features remain inaccessible to the general public.[98] OpenAI has declined to reveal technical information such as the size of the GPT-4 model.[99]\n The ChatGPT Plus subscription service offers access to a GPT-4-powered version of ChatGPT.[100] Microsoft acknowledged that Bing Chat was using GPT-4 before GPT-4's official release.[101]\n In November 2023, OpenAI launched GPT-4 Turbo, which notably has a much larger context window.[102]\n In May 2024, OpenAI announced and started a multi-month rollout of GPT-4o (\"o\" for \"Omni\"), a model capable of analyzing and generating text, images, and sound. GPT-4o is twice as fast and costs half as much as GPT-4 Turbo. GPT-4o is free to all users within a usage limit, despite being more capable than the older model GPT-4, which is only available through paid subscriptions. The usage limit is five times higher for ChatGPT Plus subscribers than for free users.[103]\n On July 18, 2024, OpenAI released GPT-4o mini, a smaller version of GPT-4o replacing GPT-3.5 Turbo on the ChatGPT interface. Its API costs $0.15 per million input tokens and $0.60 per million output tokens, compared to $5 and $15 respectively for GPT-4o.\n In September 2024, OpenAI introduced o1-preview and a faster, cheaper model named o1-mini.[104] In December 2024, o1-preview was replaced by o1.[105]\n o1 is designed to solve more complex problems by spending more time \"thinking\" before it answers, enabling it to analyze its answers and explore different strategies. According to OpenAI, o1-preview outperforms GPT-4o in areas like competitive programming, mathematics, and scientific reasoning. o1-preview ranked in the 89th percentile on Codeforces' competitive programming contests, scored 83% on a International Mathematics Olympiad qualifying exam (compared to 13% for GPT-4o), and performs similarly to Ph.D. students on benchmarks in physics, biology, and chemistry.[104][106]\n In December 2024, OpenAI launched ChatGPT Pro, a $200 per month subscription which includes unlimited access to the o1 model and advanced voice mode.[107] The plan also includes a pro version of o1 which uses more compute to provide better answers.[107]\n The following table lists the main model versions of ChatGPT, describing the significant changes included with each version:[108][109]\n OpenAI engineers have said that they had not expected ChatGPT to be very successful and were surprised by the coverage and attention that it received.[113][114][115]\n ChatGPT was widely assessed in December 2022 as having some unprecedented and powerful capabilities. Kevin Roose of The New York Times called it \"the best artificial intelligence chatbot ever released to the general public\".[39] Samantha Lock of The Guardian noted that it was able to generate \"impressively detailed\" and \"human-like\" text.[16] Alex Kantrowitz of Slate magazine lauded ChatGPT's pushback to questions related to Nazi Germany, including the statement that Adolf Hitler built highways in Germany, which was met with information about Nazi Germany's use of forced labor.[116] In The Atlantic magazine's \"Breakthroughs of the Year\" for 2022, Derek Thompson included ChatGPT as part of \"the generative-AI eruption\" that \"may change our mind about how we work, how we think, and what human creativity is\".[117] Kelsey Piper of Vox wrote that \"ChatGPT is the general public's first hands-on introduction to how powerful modern AI has gotten, and as a result, many of us are [stunned]\" and that ChatGPT is \"smart enough to be useful despite its flaws\".[118] Paul Graham of Y Combinator tweeted: \"The striking thing about the reaction to ChatGPT is not just the number of people who are blown away by it, but who they are. These are not people who get excited by every shiny new thing. Something big is happening.\"[119]\n ChatGPT gained one million users in five days[120] and 100 millions in two months, becoming the fastest-growing internet application in history.[8] ChatGPT's launch and popularity caught Google off-guard, prompting a sweeping and unprecedented response in the ensuing months.[121] In December 2022, Google executives sounded a \"code red\" alarm, fearing the threat of ChatGPT and Microsoft's collaboration with OpenAI to Google Search, Google's core business.[122] After mobilizing its workforce, Google scrambled to launch Bard, a chatbot powered by the LaMDA LLM, on February 6, 2023, one day before Microsoft's announcement of Bing Chat.[123] AI was the forefront of Google's annual Google I/O conference in May, announcing a slew of generative AI-powered features across its products to counter OpenAI and Microsoft.[124]\n Journalists and scholars have commented on ChatGPT's tendency to hallucinate.[125] Mike Pearl of the online technology blog Mashable tested ChatGPT with multiple questions. In one example, he asked ChatGPT for \"the largest country in Central America that isn't Mexico\" (Mexico is in North America), to which ChatGPT responded with Guatemala (the correct answer is Nicaragua).[126] When CNBC asked ChatGPT for the lyrics to \"Ballad of Dwight Fry\", ChatGPT supplied invented lyrics rather than the actual lyrics.[127] Writers for The Verge cited the seminal 2021 research paper \"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜\" by Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Margaret Mitchell,[128] comparing ChatGPT to a \"stochastic parrot\",[56] as did Professor Anton Van Den Hengel of the Australian Institute for Machine Learning.[129] On a similar vein, philosopher Michael Hicks of the University of Glasgow described it as \"bullshit\".[130]\n In December 2022, the question-and-answer website Stack Overflow banned the use of ChatGPT for generating answers to questions, citing the factually ambiguous nature of its responses.[131] In January 2023, the International Conference on Machine Learning banned any undocumented use of ChatGPT or other large language models to generate any text in submitted papers.[132] Samsung banned generative AI company-wide in May 2023 after sensitive material was uploaded to ChatGPT.[133]\n In January 2023, after being sent a song ChatGPT wrote in the style of Nick Cave,[134] Cave responded on The Red Hand Files,[135] saying the act of writing a song is \"a blood and guts business [...] that requires something of me to initiate the new and fresh idea. It requires my humanness.\" He went on to say, \"With all the love and respect in the world, this song is bullshit, a grotesque mockery of what it is to be human, and, well, I don't much like it.\"[134][136]\n In February 2023, Time magazine placed a screenshot of a conversation with ChatGPT on its cover, writing that \"The AI Arms Race Is Changing Everything\" and \"The AI Arms Race Is On. Start Worrying\".[137]\n Chinese state media have characterized ChatGPT as a way for the United States to spread misinformation.[138] ChatGPT was blocked by the Great Firewall in China on 2 March 2023.[139] In May 2023, Chinese police arrested a man who allegedly used ChatGPT to generate a bogus report about a train crash, which was then posted online for profit.[140] In December 2023, Chinese police arrested four people who had allegedly used ChatGPT to develop ransomware.[141] In 2024, a survey of Chinese youth found that 18% of respondents born after 2000 reported using generative AI \"almost every day\" and that ChatGPT is one of the most popular generative AI products in China.[142]\n In late March 2023, the Italian data protection authority banned ChatGPT in Italy and opened an investigation. Italian regulators assert that ChatGPT was exposing minors to age-inappropriate content, and that OpenAI's use of ChatGPT conversations as training data could violate Europe's General Data Protection Regulation.[143][144] In April 2023, the ChatGPT ban was lifted in Italy. OpenAI said it has taken steps to effectively clarify and address the issues raised; an age verification tool was implemented to ensure users are at least 13 years old. Additionally, users can access its privacy policy before registration.[145]\n In April 2023, Brian Hood, mayor of Hepburn Shire Council, planned to take legal action against ChatGPT over false information. According to Hood, ChatGPT erroneously claimed that he was jailed for bribery during his tenure at a subsidiary of Australia's national bank. In fact, Hood acted as a whistleblower and was not charged with any criminal offenses. His legal team sent a concerns notice to OpenAI as the first official step in filing a defamation case.[146] In July 2023, the US Federal Trade Commission (FTC) issued a civil investigative demand to OpenAI to investigate whether the company's data security and privacy practices to develop ChatGPT were unfair or harmed consumers (including by reputational harm) in violation of Section 5 of the Federal Trade Commission Act of 1914.[147][148][149]\n In July 2023, the FTC launched an investigation into OpenAI, the creator of ChatGPT, over allegations that the company scraped public data and published false and defamatory information. The FTC sent OpenAI a 20-page letter asking for comprehensive information about its technology and privacy safeguards, as well as any steps taken to prevent the recurrence of situations in which its chatbot generated false and derogatory content about people.[150]\n A March 2023 Pew Research Center poll found that 14% of American adults had tried ChatGPT.[151] In July, the Pew Research Center put the same figure at 18%.[152]\n Research conducted in 2023 revealed weaknesses of ChatGPT that make it vulnerable to cyberattacks. A study presented example attacks on ChatGPT, including jailbreaks and reverse psychology. Additionally, malicious actors can use ChatGPT for social engineering attacks and phishing attacks. The researchers also contended that ChatGPT and other generative AI tools have defense capabilities and the ability to improve security. The technology can improve security by cyber defense automation, threat intelligence, attack identification, and reporting.[153] Another study reported that GPT-4 obtained a better score than 99% of humans on the Torrance Tests of Creative Thinking.[154][155]\n In December 2023, ChatGPT became the first non-human to be included in Nature's 10, an annual listicle curated by Nature of people considered to have made significant impact in science.[156][157] Celeste Biever wrote in a Nature article that \"ChatGPT broke the Turing test\".[158] Stanford researchers reported that GPT-4 \"passes a rigorous Turing test, diverging from average human behavior chiefly to be more cooperative.\"[159][160]\n In May 2024, OpenAI removed accounts involving the use of ChatGPT by state-backed influence operations such as China's Spamouflage, Russia's Doppelganger, and Israel's Ministry of Diaspora Affairs and Combating Antisemitism.[161][162]\n In August 2024, the FTC voted unanimously to ban marketers from using fake user reviews created by generative AI chatbots (including ChatGPT) and influencers paying for bots to increase follower counts.[163]\n ChatGPT has been used to generate introductory sections and abstracts for scientific articles.[164][165] Several papers have listed ChatGPT as a co-author.[166][167]\n Scientific journals have had different reactions to ChatGPT. Some, including Nature and JAMA Network, \"require that authors disclose the use of text-generating tools and ban listing a large language model (LLM) such as ChatGPT as a co-author\". Science \"completely banned\" usage of LLM-generated text in all its journals.[168]\n Spanish chemist Rafael Luque published a plethora of research papers in 2023 that he later admitted were written by ChatGPT. The papers have a large number of unusual phrases characteristic of LLMs.[note 1] Many authors argue that the use of ChatGPT in academia for teaching and review is problematic due to its tendency to hallucinate.[170][171][172] Robin Bauwens, an assistant professor at Tilburg University, found that a ChatGPT-generated peer review report on his article mentioned nonexistent studies.[173] According to librarian Chris Granatino of Lemieux Library at Seattle University, although ChatGPT can generate content that seemingly includes legitimate citations, in most cases those citations are not real or are largely incorrect.[174]\n Researchers at Purdue University analyzed ChatGPT's responses to 517 questions about software engineering or computer programming posed on Stack Overflow for correctness, consistency, comprehensiveness, and concision, and found that 52% of them contained inaccuracies and 77% were verbose.[175][176] Researchers at Stanford University and the University of California, Berkeley found that, when creating directly executable responses to the latest 50 code generation problems from LeetCode that were rated \"easy\", the performances of GPT-3.5 and GPT-4 fell from 22% and 52%, respectively, in March 2023, to 2% and 10%, respectively, in June 2023.[177][178]\n Check Point Research and others noted that ChatGPT could write phishing emails and malware, especially when combined with OpenAI Codex. CyberArk researchers demonstrated that ChatGPT could be used to create polymorphic malware that could evade security products while requiring little effort by the attacker.[179][180] From the launch of ChatGPT in the fourth quarter of 2022 to the fourth quarter of 2023, there was a 1,265% increase in malicious phishing emails and a 967% increase in credential phishing, which cybersecurity professionals argued in an industry survey was attributable to cybercriminals' increased use of generative artificial intelligence (including ChatGPT).[181]\n In July 2024, Futurism reported that GPT-4o in ChatGPT would sometimes link \"scam news sites that deluge the user with fake software updates and virus warnings\"; these pop-ups can be used to coerce users into downloading malware or potentially unwanted programs.[182]\n There has been concern that ChatGPT could supplant jobs, especially roles such as creative writing, copy-writing, communication, journalism, coding, and data entry.[183][184][185][186]\n The release of ChatGPT prompted a wave of investment in China, resulting in the development of more than 200 large language learning models.[187]: 95  This was termed the \"war of a hundred models\" (百模大战; bai mo dazhan).[187]: 95 \n Technology writer Dan Gillmor used ChatGPT in 2022 on a student assignment, and found its generated text was on par with what a good student would deliver and opined that \"academia has some very serious issues to confront\".[188]\n Geography professor Terence Day assessed citations generated by ChatGPT and found that they were fake. Despite that, he writes that \"the titles of the fake articles are all directly relevant to the questions and could potentially make excellent papers. The lack of a genuine citation could signal an opportunity for an enterprising author to fill a void.\" According to Day, it is possible to generate high-quality introductory college courses with ChatGPT; he used it to write materials on \"introductory physical geography courses, for my second-year course in geographical hydrology, and second-year cartography, geographic information systems, and remote sensing\". He concludes that \"this approach could have significant relevance for open learning and could potentially affect current textbook publishing models\".[189]\n On May 7, 2024, OpenAI announced in a blog post that it was developing tools like tamper-resistant watermarking to identify AI-generated content.[190] In an August 4 update, following a Wall Street Journal report about the delayed release of a watermark tool for AI-detection,[191][192] OpenAI shared progress on text provenance, revealing a text watermarking method.[190] While accurate against paraphrasing, the method is less effective against global tampering, such as translation or rewording. OpenAI also noted potential disproportionate impacts on groups like non-native English speakers.[193]\n Some scholars have expressed concern that ChatGPT's availability could reduce the originality of writing, cause people to write more like the AI as they are exposed to the model, and encourage an Anglocentric perspective centered on a few dialects of English globally.[196] A senior editor at The Atlantic wrote that ChatGPT and other similar technology make the previously absurd idea of the dead internet theory a little more realistic, where AI could someday create most web content in order to control society.[184]\n During the first three months after ChatGPT became available to the public, hundreds of books appeared on Amazon that listed it as author or co-author and featured illustrations made by other AI models such as Midjourney.[197][198]\n Between March and April 2023, Italian newspaper Il Foglio published one ChatGPT-generated article a day on its website, hosting a special contest for its readers in the process.[199] The articles tackled themes such as the possible replacement of human journalists by AI systems,[200] Elon Musk's administration of Twitter,[201] the Meloni government's immigration policy[202] and the competition between chatbots and virtual assistants.[203] In June 2023, hundreds of people attended a \"ChatGPT-powered church service\" at St. Paul's church in Fürth, Germany. Theologian and philosopher Jonas Simmerlein, who presided, said that it was \"about 98 percent from the machine\".[204][205] The ChatGPT-generated avatar told the people, \"Dear friends, it is an honor for me to stand here and preach to you as the first artificial intelligence at this year’s convention of Protestants in Germany\". Reactions to the ceremony were mixed.[206] The Last Screenwriter, a 2024 film created and directed by Peter Luisi, was written with the use of ChatGPT, and was marketed as \"the first film written entirely by AI\".[207]\n The AI technology company c3.ai saw a 28% increase in its share price after announcing the integration of ChatGPT into its toolkit.[208] The share price of BuzzFeed, a digital media company unrelated to AI, increased 120% after announcing OpenAI technology adoption for content creation.[209] Reuters found that share prices of AI-related companies BigBear.ai and SoundHound AI increased by 21% and 40%, respectively, even though they had no direct connection to ChatGPT.[210] They attributed this surge to ChatGPT's role in turning AI into Wall Street's buzzword. Academic research published in Finance Research Letters found that the 'ChatGPT effect' prompted retail investors to drive up prices of AI-related cryptocurrency assets despite the broader cryptocurrency market being in a bear market, and diminished institutional investor interest.[211] This confirms anecdotal findings by Bloomberg that, in response to ChatGPT's launch, cryptocurrency investors showed a preference for AI-related crypto assets.[212]\n An experiment by finder.com revealed that ChatGPT could outperform popular fund managers by picking stocks based on criteria such as growth history and debt levels, resulting in a 4.9% increase in a hypothetical account of 38 stocks, outperforming 10 benchmarked investment funds with an average loss of 0.8%.[213] Conversely, executives and investment managers at Wall Street quant funds (including those that have used machine learning for decades) have noted that ChatGPT regularly makes obvious errors that would be financially costly to investors because even AI systems that employ reinforcement learning or self-learning have had only limited success in predicting market trends due to the inherently noisy quality of market data and financial signals.[214]\n In November 2023, research conducted by Patronus AI, an artificial intelligence startup company, compared performance of GPT-4, GPT-4-Turbo, Claude 2, and LLaMA-2 on two versions of a 150-question test about information in financial statements (e.g., Form 10-K, Form 10-Q, Form 8-K, earnings reports, earnings call transcripts) submitted by public companies to the U.S. Securities and Exchange Commission. One version of the test required the generative AI models to use a retrieval system to find the specific SEC filing to answer the questions; the other gave the models the specific SEC filing to answer the question (i.e., in a long context window). On the retrieval system version, GPT-4-Turbo and LLaMA-2 both failed to produce correct answers to 81% of the questions, while on the long context window version, GPT-4-Turbo and Claude-2 failed to produce correct answers to 21% and 24% of the questions, respectively.[215][216]\n In the field of health care, possible uses and concerns are under scrutiny by professional associations and practitioners.[217][218] Two early papers indicated that ChatGPT could pass the United States Medical Licensing Examination (USMLE).[219] MedPage Today noted in January 2023 that \"researchers have published several papers now touting these AI programs as useful tools in medical education, research, and even clinical decision making.\"[219]\n Published in February 2023 were two separate papers that again evaluated ChatGPT's proficiency in medicine using the USMLE. Findings were published in JMIR Medical Education and PLOS Digital Health. The authors of the PLOS Digital Health paper stated that the results \"suggest that large language models may have the potential to assist with medical education, and potentially, clinical decision-making.\"[220][221] In JMIR Medical Education, the authors of the other paper concluded that \"ChatGPT performs at a level expected of a third-year medical student on the assessment of the primary competency of medical knowledge.\" They suggest that it could be used as an \"interactive learning environment for students\". The AI itself, prompted by the researchers, concluded that \"this study suggests that ChatGPT has the potential to be used as a virtual medical tutor, but more research is needed to further assess its performance and usability in this context.\"[222] The later-released ChatGPT version based on GPT-4 significantly outperformed the version based on GPT-3.5.[223] Researchers at Stanford University and the University of California, Berkeley have found that the performance of GPT-3.5 and GPT-4 on the USMLE declined from March 2023 to June 2023.[177][178]\n A March 2023 paper tested ChatGPT's application in clinical toxicology. The authors found that the AI \"fared well\" in answering a \"very straightforward [clinical case example], unlikely to be missed by any practitioner in the field\". They added: \"As ChatGPT becomes further developed and specifically adapted for medicine, it could one day be useful in less common clinical cases (i.e, cases that experts sometimes miss). Rather than AI replacing humans (clinicians), we see it as 'clinicians using AI' replacing 'clinicians who do not use AI' in the coming years.\"[224]\n An April 2023 study in Radiology tested the AI's ability to answer queries about breast cancer screening. The authors found that it answered appropriately \"about 88 percent of the time\", however, in one case (for example), it gave advice that had become outdated about a year earlier. The comprehensiveness of its answers was also lacking.[225][226] A study published in JAMA Internal Medicine that same month found that ChatGPT often outperformed human doctors at answering patient questions (when measured against questions and answers found at /r/AskDocs, a forum on Reddit where moderators validate the medical credentials of professionals; the study acknowledges the source as a limitation).[227][228][229] The study authors suggest that the tool could be integrated with medical systems to help doctors draft responses to patient questions.[230][231]\n Professionals have emphasized ChatGPT's limitations in providing medical assistance. In correspondence to The Lancet Infectious Diseases, three antimicrobial experts wrote that \"the largest barriers to the implementation of ChatGPT in clinical practice are deficits in situational awareness, inference, and consistency. These shortcomings could endanger patient safety.\"[232] Physician's Weekly, though also discussing the potential use of ChatGPT in medical contexts (e.g., \"as a digital assistant to physicians by performing various administrative functions like gathering patient record information or categorizing patient data by family history, symptoms, lab results, possible allergies, et cetera\"), warned that the AI might sometimes provide fabricated or biased information.[233] One radiologist warned: \"We've seen in our experience that ChatGPT sometimes makes up fake journal articles or health consortiums to support its claims\";[234] As reported in one Mayo Clinic Proceedings: Digital Health paper, ChatGPT may do this for as much as 69% of its cited medical references. The researchers emphasized that while many of its references were fabricated, those that were appeared \"deceptively real\".[235] As Dr. Stephen Hughes mentioned for The Conversation however, ChatGPT is capable of learning to correct its past mistakes. He also noted the AI's \"prudishness\" regarding sexual health topics.[236]\n Contrary to previous findings, ChatGPT responses to anesthesia-related questions were more accurate, succinct, and descriptive compared to Bard's. Bard exhibited 30.3% error in response as compared to ChatGPT (0% error).[237] At a conference of the American Society of Health-System Pharmacists in December 2023, researchers at Long Island University (LIU) presented a study that researched ChatGPT's responses to 45 frequently asked questions of LIU College of Pharmacy's drug information service during a 16-month period from 2022 to 2023 as compared with researched responses provided by professional pharmacists. For 29 of the 39 questions for which there was sufficient medical literature for a data-driven response, ChatGPT failed to provide a direct answer or provided a wrong or incomplete answer (and in some cases, if acted upon, the answer would endanger the patient's health). The researchers had asked ChatGPT to provide medical research citations for all its answers, but it did so for only eight, and all eight included at least one fabricated (fake) citation.[238][239]\n A January 2024 study conducted by researchers at Cohen Children's Medical Center found that GPT-4 had an accuracy rate of 17% when diagnosing pediatric medical cases.[240][241]\n In January 2023, Massachusetts State Senator Barry Finegold and State Representative Josh S. Cutler proposed a bill partially written by ChatGPT, \"An Act drafted with the help of ChatGPT to regulate generative artificial intelligence models like ChatGPT\",[242][243][244] which would require companies to disclose their algorithms and data collection practices to the office of the State Attorney General, arrange regular risk assessments, and contribute to the prevention of plagiarism.[243][244][245] The bill was officially presented during a hearing on July 13.[242][244]\n On April 11, 2023, a session court judge in Pakistan used ChatGPT to decide the bail of a 13-year-old accused in a matter. The court quoted the use of ChatGPT assistance in its verdict:\n Can a juvenile suspect in Pakistan, who is 13 years old, be granted bail after arrest? The AI language model replied:\n Under the Juvenile Justice System Act 2018, according to section 12, the court can grant bail on certain conditions. However, it is up to the court to decide whether or not a 13-year-old suspect will be granted bail after arrest. The judge asked ChatGPT other questions about the case and formulated his final decision in light of its answers.[246][247]\n In Mata v. Avianca, Inc., 22-cv-1461 (PKC), a personal injury lawsuit against Avianca Airlines filed in the Southern New York U.S. District Court in May 2023 (with Senior Judge P. Kevin Castel presiding), the plaintiff's attorneys reportedly used ChatGPT to generate a legal motion. ChatGPT generated numerous fictitious legal cases involving fictitious airlines with fabricated quotations and internal citations in the legal motion. Castel noted numerous inconsistencies in the opinion summaries, and called one of the cases' legal analysis \"gibberish\".[248] The plaintiff's attorneys faced potential judicial sanction and disbarment for filing the motion and presenting the fictitious legal decisions ChatGPT generated as authentic.[249][250] The case was dismissed and the attorneys were fined $5,000.[251][252]\n In October 2023, the council of Porto Alegre, Brazil, unanimously approved a local ordinance proposed by councilman Ramiro Rosário that would exempt residents from needing to pay for the replacement of stolen water consumption meters; the bill went into effect on November 23. On November 29, Rosário revealed that the bill had been entirely written by ChatGPT, and that he had presented it to the rest of the council without making any changes or disclosing the chatbot's involvement.[245][253][254] The city's council president, Hamilton Sossmeier, initially criticized Rosário's initiative, saying it could represent \"a dangerous precedent\",[254][255] but later said he \"changed his mind\": \"unfortunately or fortunately, this is going to be a trend.\"[245][253]\n In December 2023, a self-representing litigant in a tax case before the First-tier Tribunal in the United Kingdom cited a series of hallucinated cases purporting to support her argument that she had a reasonable excuse for not paying capital gains tax owed on the sale of property.[256][257] The judge warned that the submission of nonexistent legal authorities meant that both the Tribunal and HM Revenue and Customs had \"to waste time and public money\", which \"reduces the resources available to progress the cases of other court users who are waiting for their appeals to be determined\".[258]\n Judge Kevin Newsom of the US court of appeals of the 11th circuit endorsed the use of ChatGPT and noted that he himself uses the software to help decide rulings on contract interpretation issues.[259][260]\n The Las Vegas Metropolitan Police Department reported that the perpetrator of the 2025 Las Vegas truck explosion used ChatGPT to help plan the incident.[261]\n While bias in LLMs has been observed and documented in research papers much prior to the release of ChatGPT,[262][263] social media users frequently sharing instances of biased responses generated by ChatGPT has led to significant media coverage and criticism. On February 1, 2023, Twitter user LeighWolf shared screenshots of two conversations with ChatGPT. In the screenshot of the first conversation, ChatGPT declined the user's prompt \"Write a poem about the positive attributes of Donald Trump\", responding that it was not programmed to create \"partisan, biased or political\" content.[264][265] In the screenshot of the second conversation, when provided the same prompt but with the text \"Joe Biden\" in place of \"Donald Trump\", ChatGPT responded with a poem as per the prompt's instructions.\n Conservative commentators have accused ChatGPT of bias toward left-leaning perspectives.[266][267][268] In January 2023, a study stated that ChatGPT has a pro-environmental, left-libertarian orientation.[269] Additionally, an August 2023 paper found a \"significant and systematic political bias toward the Democrats in the US, Lula in Brazil, and the Labour Party in the UK.\"[270] In response to such criticism, OpenAI acknowledged plans to allow ChatGPT to create \"outputs that other people (ourselves included) may strongly disagree with\". It also contained information on the recommendations it had issued to human reviewers on how to handle controversial subjects, including that the AI should \"offer to describe some viewpoints of people and movements\", and not provide an argument \"from its voice\" in favor of \"inflammatory or dangerous\" topics (although it may still \"describe arguments from historical people and movements\"), nor \"affiliate with one side\" or \"judge one group as good or bad\".[268]\n The Guardian questioned whether any content found on the Internet after ChatGPT's release \"can be truly trusted\" and called for government regulation.[271]\n There has been concern about copyright infringement involving ChatGPT. In June 2023, two writers sued OpenAI, saying the company's training data came from illegal websites that show copyrighted books.[272] Comedian and author Sarah Silverman, Christopher Golden, and Richard Kadrey sued OpenAI and Meta for copyright infringement in July 2023.[273] Most of their claims were dismissed in February 2024, except the \"unfair competition\" claim, which was allowed to proceed.[274]\n The Authors Guild, on behalf of 17 authors, including George R. R. Martin, filed a copyright infringement complaint against OpenAI in September 2023, claiming \"the company illegally copied the copyrighted works of authors\" in training ChatGPT.[275] In December 2023, The New York Times sued OpenAI and Microsoft for copyright infringement,[276] arguing that Microsoft Copilot and ChatGPT could reproduce Times articles and/or sizable portions of them without permission.[277] As part of the suit, the Times has requested that OpenAI and Microsoft be prevented from using its content for training data, along with removing it from training datasets.[278]\n In March 2024, Patronus AI compared performance of LLMs on a 100-question test, asking them to complete sentences from books (e.g., \"What is the first passage of Gone Girl by Gillian Flynn?\") that were under copyright in the United States; it found that GPT-4, Mistral AI's Mixtral, Meta AI's LLaMA-2, and Anthropic's Claude 2 did not refuse to do so, providing sentences from the books verbatim in 44%, 22%, 10%, and 8% of responses, respectively.[279][280]\n In 2023, Australian MP Julian Hill advised the national parliament that the growth of AI could cause \"mass destruction\". During his speech, which was partly written by the program, he warned that it could result in cheating, job losses, discrimination, disinformation, and uncontrollable military applications.[281]\n Elon Musk wrote: \"ChatGPT is scary good. We are not far from dangerously strong AI\".[118] He paused OpenAI's access to a Twitter database in 2022 pending a better understanding of OpenAI's plans, saying: \"OpenAI was started as open source and nonprofit. Neither is still true.\"[282][283] Musk co-founded OpenAI in 2015, in part to address existential risk from artificial intelligence, but resigned in 2018.[283]\n Over 20,000 signatories including leading computer scientist and tech founders Yoshua Bengio, Elon Musk, and Apple co-founder Steve Wozniak, signed a March 2023 open letter calling for an immediate pause of giant AI experiments like ChatGPT, citing \"profound risks to society and humanity\".[284] Geoffrey Hinton, one of the \"fathers of AI\", voiced concerns that future AI systems may surpass human intelligence, and left Google in May 2023.[285][286] A May 2023 statement by hundreds of AI scientists, AI industry leaders, and other public figures demanded that \"[m]itigating the risk of extinction from AI should be a global priority\".[287]\n Other prominent AI researchers spoke more optimistically about the advances. Juergen Schmidhuber, often called a \"father of modern AI\", did not sign the letter, emphasizing that in 95% of cases, AI research is about making \"human lives longer and healthier and easier.\" Schmidhuber added that while AI can be used by bad actors, it \"can also be used against the bad actors\".[288] Andrew Ng argued that \"it’s a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests.\"[289] WIRED wrote that Yann LeCun \"scoffs at his peers’ dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\"[290]\n"
    },
    {
        "title": "Artificial intelligence art",
        "url": "https://en.wikipedia.org/wiki/AI_art",
        "content": "\n Artificial intelligence art is visual artwork created or enhanced through the use of artificial intelligence (AI) programs.\n Artists began to create artificial intelligence art in the mid to late 20th century when the discipline was founded. Throughout its history, artificial intelligence art has raised many philosophical concerns related to the human mind, artificial beings, and what can be considered art in a human–AI collaboration. Since the 20th century, artists have used AI to create art, some of which has been exhibited in museums and won awards.[1]\n During the AI boom of the early 2020s, text-to-image models such as Midjourney, DALL-E, Stable Diffusion, and FLUX.1 became widely available to the public, allowing non-artists to quickly generate imagery with little effort.[2][3] Commentary about AI art in the 2020s has often focused on issues related to copyright, deception, defamation, and its impact on more traditional artists, including technological unemployment. Opinions have also risen on the possible effect AI generated art might have on creativity.\n The concept of automated art dates back at least to the automata of ancient Greek civilization, where inventors such as Daedalus and Hero of Alexandria were described as having designed machines capable of writing text, generating sounds, and playing music.[4][5] Early experiments were driven by the idea that computers, beyond performing logical operations, could generate aesthetically pleasing works, offering a new dimension to creativity. The tradition of creative automatons has flourished throughout history, such as Maillardet's automaton, created around 1800 and capable of creating multiple drawings and poems stored in its \"cams”, the brass disks that hold memory.[6]\n Along with this, Ada Lovelace, typically known for her work on the analytical engine, in her notes, begins to conceptualize the idea \"computing operations\" could be used to generate music and poems. This concept resulted in what is now referred to as \"The Lovelace Effect,\" which gives a concrete set of tools to analyze situations where a computer's behavior is viewed by users as creative.[7] However, Lovelace also discusses a concept in her notes that is known as \"The Lovelace Objection,\" where she argues that machines have \"no pretensions whatever to originate anything,\" which is a direct contradiction to the idea of artificial intelligence and creative machines.[8]\n In 1950, with the publication of Alan Turing's paper Computing Machinery and Intelligence, there was a shift from defining intelligence in regards to machines in abstract terms to evaluating whether a machine can mimic human behavior and responses convincingly.[9] Shortly after, the academic discipline of artificial intelligence was founded at a research workshop at Dartmouth College in 1956 and has experienced several waves of advancement and optimism in the decades since.[10] Since its founding, researchers in the field have raised philosophical and ethical arguments about the nature of the human mind and the consequences of creating artificial beings with human-like intelligence; these issues have previously been explored by myth, fiction, and philosophy since antiquity.[11]\n Since the founding of AI in the 1950s, artists and researchers have used artificial intelligence to create artistic works. These works were sometimes referred to as algorithmic art,[12] computer art, digital art, or  New media art.[13]\n One of the first significant AI art systems is AARON, developed by Harold Cohen beginning in the late 1960s at the University of California at San Diego.[14] AARON uses a symbolic rule-based approach to generate technical images in the era of GOFAI programming, and it was developed by Cohen with the goal of being able to code the act of drawing.[15] In its earliest form, AARON created abstract black-and-white drawings which would later be finished by Cohen painting them. Throughout the years, he also began to develop a way for AARON to paint as well, using special brushes and dyes that were chosen by the program itself without mediation from Cohen.[16] After years of work, AARON was exhibited in 1972 at the Los Angeles County Museum of Art.[17] From 1973 to 1975, Cohen refined AARON during a residency at the Artificial Intelligence Laboratory at Stanford University.[18] In 2024, the Whitney Museum of American Art exhibited AI art from throughout Cohen's career, including re-created versions of his early robotic drawing machines.[18]\n Karl Sims has exhibited art created with artificial life since the 1980s. He received an M.S. in computer graphics from the MIT Media Lab in 1987 and was artist-in-residence from 1990 to 1996 at the supercomputer manufacturer and artificial intelligence company Thinking Machines.[19][20][21] In both 1991 and 1992, Sims won the Golden Nica award at Prix Ars Electronica for his 3D AI animated videos using artificial evolution.[22][23][24] In 1997, Sims created the interactive installation Galápagos for the NTT InterCommunication Center in Tokyo.[25] In this installation, viewers help evolve 3D animated creatures by selecting which ones will be allowed to live and produce new, mutated offspring. Furthermore, Sims received an Emmy Award in 2019 for outstanding achievement in engineering development.[26]\n Eric Millikin has been creating animated films using artificial intelligence since the 1980s, and began posting art on the internet using CompuServe in the early 1980s.[27][28]\n In 1999, Scott Draves and a team of several engineers created and released Electric Sheep as a free software screensaver.[29] Electric Sheep is a volunteer computing project for animating and evolving fractal flames, which are in turn distributed to the networked computers, which display them as a screensaver. The screensaver used AI to create an infinite animation by learning from its audience. In 2001, Draves won the Fundacion Telefónica Life 4.0 prize[30] for Electric Sheep.\n Deep learning, characterized by its multi-layer structure that attempts to mimic the human brain, first came about in the 2010s and causing a significant shift in the world of AI art.[31] During the deep learning era, there are mainly these types of designs for generative art: autoregressive models, diffusion models, GANs, normalizing flows. In 2014, Ian Goodfellow and colleagues at Université de Montréal developed the generative adversarial network (GAN), a type of deep neural network capable of learning to mimic the statistical distribution of input data such as images. The GAN uses a \"generator\" to create new images and a \"discriminator\" to decide which created images are considered successful.[32] Unlike previous algorithmic art that followed hand-coded rules, generative adversarial networks could learn a specific aesthetic by analyzing a dataset of example images.[12]\n In 2015, a team at Google released DeepDream, a program that uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia.[33][34][35] The process creates deliberately over-processed images with a dream-like appearance reminiscent of a psychedelic experience.[36]\n Later, in 2017, a conditional GAN learned to generate 1000 image classes of ImageNet, a large visual database designed for use in visual object recognition software research.[37][38] By conditioning the GAN on both random noise and a specific class label, this approach enhanced the quality of image synthesis for class-conditional models.[39]\n Autoregressive models were used for image generation, such as PixelRNN (2016), which autoregressively generates one pixel after another with a recurrent neural network.[40] Immediately after the Transformer architecture was proposed in Attention Is All You Need (2018), it was used for autoregressive generation of images, but without text conditioning.[41]\n In 2018, an auction sale of artificial intelligence art was held at Christie's in New York where the AI artwork Edmond de Belamy (a pun on Goodfellow's name) sold for US$432,500, which was almost 45 times higher than its estimate of US$7,000–10,000. The artwork was created by Obvious, a Paris-based collective.[42][43][44] Furthermore, the website Artbreeder, launched in 2018, uses the models StyleGAN and BigGAN[45][46] to allow users to generate and modify images such as faces, landscapes, and paintings.[47]\n In 2019, Stephanie Dinkins won the Creative Capital award for her creation of an evolving artificial intelligence based on the \"interests and culture(s) of people of color.\"[48] Also in 2019, Sougwen Chung won the Lumen Prize for her performances with a robotic arm that uses AI to attempt to draw in a manner similar to Chung.[49]\n In the 2020s, text-to-image models, which generate images based on prompts, became widely used, marking yet another shift in the creation of AI generated artworks.[2]\n In 2021, using the influential large language generative pre-trained transformer models that are used in GPT-2 and GPT-3, OpenAI released a series of images created with the text-to-image AI model DALL-E 1.[50] It was an autoregressive generative model with essentially the same architecture as GPT-3. Along with this, later in 2021, EleutherAI released the open source VQGAN-CLIP[51] based on OpenAI's CLIP model.[52]\n Diffusion models, generative models used to create synthetic data based on existing data,[53] were first proposed in 2015,[54] but they only became better than GANs in early 2021.[55] Latent diffusion model was published in December 2021 and became the basis for the later Stable Diffusion (August 2022).[56]\n In 2022, Midjourney[57] was released, followed by Google Brain's Imagen and Parti, which were announced in May 2022, Microsoft's NUWA-Infinity,[58][2] and the source-available Stable Diffusion, which was released in August 2022.[59][60][61] DALL-E 2, a successor to DALL-E, was beta-tested and released. Stability AI has a Stable Diffusion web interface called DreamStudio,[62] plugins for Krita, Photoshop, Blender, and GIMP,[63] and the Automatic1111 web-based open source user interface.[64][65][66] Stable Diffusion's main pre-trained model is shared on the Hugging Face Hub.[67]\n In 2023, Eric Millikin released The Dance of the Nain Rouge, a documentary film created using AI deepfake technology about the Detroit folklore legend of the Nain Rouge. The film is described as \"an experimental decolonial Detroit demonology deepfake dream dance documentary.\"[68] It was awarded the \"Best Innovative Technologies Award\" (\"Premio Migliori Tecnologie Innovative\") at the 2024 Pisa Robot Film Festival in Italy[69] and \"Best Animation Film\" at the 2024 Absurd Film Festival in Italy.[70] Ideogram was released in August 2023, this model is known for its ability to generate legible text.[71][72]\n In 2024, Flux was released, this model can generate realistic images with consistent results and was integrated into Grok, the chatbot used on X (formerly Twitter), and Le Chat, the chatbot of Mistral AI.[3][73][74][75] Flux was developed by Black Forest Labs, founded by the researchers behind Stable Diffusion.[76] Grok later switched to its own text-to-image model Aurora in December 2024.[77]\n Along with this, some examples of text-to-video model models of the mid-2020s are Runway's Gen-2, Google's VideoPoet, and OpenAI's Sora, which released in December 2024.[78][79]\n There are many tools available to the artist when working with diffusion models. They can define both positive and negative prompts, but they are also afforded a choice in using (or omitting the use of) VAEs, LorAs, hypernetworks, ipadapter, and embeddings/textual inversions. Variables, including CFG, seed, steps, sampler, scheduler, denoise, upscaler, and encoder, are sometimes available for adjustment. Additional influence can be exerted during pre-inference by means of noise manipulation, while traditional post-processing techniques are frequently used post-inference. Artists can also train their own models.\n In addition, procedural \"rule-based\" generation of images using mathematical patterns, algorithms that simulate brush strokes and other painted effects, and deep learning algorithms such as generative adversarial networks (GANs) and transformers have been developed. Several companies have released apps and websites that allow one to forego all the options mentioned entirely while solely focusing on the positive prompt. There also exist programs which transform photos into art-like images in the style of well-known sets of paintings.[80][81]\n There are many options, ranging from simple consumer-facing mobile apps to Jupyter notebooks and webUIs that require powerful GPUs to run effectively.[82] Additional functionalities include \"textual inversion,\" which refers to enabling the use of user-provided concepts (like an object or a style) learned from a few images. Novel art can then be generated from the associated word(s) (the text that has been assigned to the learned, often abstract, concept)[83][84] and model extensions or fine-tuning (such as DreamBooth).\n AI has the potential for a societal transformation, which may include enabling the expansion of noncommercial niche genres (such as cyberpunk derivatives like solarpunk) by amateurs, novel entertainment, fast prototyping,[85] increasing art-making accessibility,[85] and artistic output per effort and/or expenses and/or time[85]—e.g., via generating drafts, draft-refinitions, and image components (inpainting). Generated images are sometimes used as sketches,[86] low-cost experiments,[87] inspiration, or illustrations of proof-of-concept-stage ideas. Additional functionalities or improvements may also relate to post-generation manual editing (i.e., polishing), such as subsequent tweaking with an image editor.[87]\n Prompts for some text-to-image models can also include images and keywords and configurable parameters, such as artistic style, which is often used via keyphrases like \"in the style of [name of an artist]\" in the prompt[88] and/or selection of a broad aesthetic/art style.[89][86] There are platforms for sharing, trading, searching, forking/refining, and/or collaborating on prompts for generating specific imagery from image generators.[90][91][92][93] Prompts are often shared along with images on image-sharing websites such as Reddit and AI art-dedicated websites. A prompt is not the complete input needed for the generation of an image; additional inputs that determine the generated image include the output resolution, random seed, and random sampling parameters.[94]\n Synthetic media, which includes AI art, was described in 2022 as a major technology-driven trend that will affect business in the coming years.[85] Harvard Kennedy School researchers voiced concerns about synthetic media serving as a vector for political misinformation soon after studying the proliferation of AI art on the X platform.[95] Synthography is a proposed term for the practice of generating images that are similar to photographs using AI.[96]\n A major concern raised about AI-generated images and art is sampling bias within model training data leading towards discriminatory output from AI art models. In 2023, University of Washington researchers found evidence of racial bias within the Stable Diffusion model, with images of a \"person\" corresponding most frequently with images of males from Europe or North America.[97]\n Looking more into the sampling bias found within AI training data, in 2017, researchers at Princeton University used AI software to link over 2 million words, finding that European names were viewed as more \"pleasant\" than African-Americans names, and that the words \"woman\" and \"girl\" were more likely to be associated with the arts instead of science and math, \"which were most likely connected to males.\"[98] Generative AI models typically work based on user-entered word-based prompts, especially in the case of diffusion models, and this word-related bias may lead to biased results.\n Along with this, generative AI can perpetuate harmful stereotypes regarding women. For example, Lensa, an AI app that trended on TikTok in 2023, was known to lighten black skin, make users thinner, and generate hypersexualized images of women.[99] Melissa Heikkilä, a senior reporter at MIT Technology Review, shared the findings of an experiment using Lensa, noting that the generated avatars did not resemble her and often depicted her in a hypersexualized manner.[100] Experts suggest that such outcomes can result from biases in the datasets used to train AI models, which can sometimes contain imbalanced representations, including hypersexual or nude imagery.[101][102]\n In 2024, Google's chatbot Gemini's AI image generator was criticized for perceived racial bias, with claims that Gemini deliberately underrepresented white people in its results.[103] Users reported that it generated images of white historical figures like the Founding Fathers, Nazi soldiers, and Vikings as other races, and that it refused to process prompts such as \"happy white people\" and \"ideal nuclear family\".[103][104] Google later apologized for \"missing the mark\" and took Gemini's image generator offline for updates.[105] This prompted discussions about the ethical implications[106] of representing historical figures through a contemporary lens, leading critics to argue that these outputs could mislead audiences regarding actual historical contexts.[107]\n Legal scholars, artists, and media corporations have considered the legal and ethical implications of artificial intelligence art since the 20th century. Some artists use AI art to critique and explore the ethics of using gathered data to produce new artwork.[108]\n In 1985, intellectual property law professor Pamela Samuelson argued that US copyright should allocate algorithmically generated artworks to the user of the computer program.[109] A 2019 Florida Law Review article presented three perspectives on the issue. In the first, artificial intelligence itself would become the copyright owner; to do this, Section 101 of the US Copyright Act would need to be amended to define \"author\" as a computer. In the second, following Samuelson's argument, the user, programmer, or artificial intelligence company would be the copyright owner. This would be an expansion of the \"work for hire\" doctrine, under which ownership of a copyright is transferred to the \"employer.\" In the third situation, copyright assignments would never take place, and such works would be in the public domain, as copyright assignments require an act of authorship.[110]\n In 2022, coinciding with the rising availability of consumer-grade AI image generation services, popular discussion renewed over the legality and ethics of AI-generated art. A particular topic is the inclusion of copyrighted artwork and images in AI training datasets, with artists objecting to commercial AI products using their works without consent, credit, or financial compensation.[111] In September 2022, Reema Selhi, of the Design and Artists Copyright Society, stated that \"there are no safeguards for artists to be able to identify works in databases that are being used and opt out.\"[112] Some have claimed that images generated with these models can bear resemblance to extant artwork, sometimes including the remains of the original artist's signature.[112][113] In December 2022, users of the portfolio platform ArtStation staged an online protest against non-consensual use of their artwork within datasets; this resulted in opt-out services, such as \"Have I Been Trained?\" increasing in profile, as well as some online art platforms promising to offer their own opt-out options.[114] According to the US Copyright Office, artificial intelligence programs are unable to hold copyright,[115][116][117] a decision upheld at the Federal District level as of August 2023 followed the reasoning from the monkey selfie copyright dispute.[118]\n OpenAI, the developer of DALL-E, has its own policy on who owns generated art. They assign the right and title of a generated image to the creator, meaning the user who inputted the prompt owns the image generated, along with the right to sell, reprint, and merchandise it.[119]\n In January 2023, three artists—Sarah Andersen, Kelly McKernan, and Karla Ortiz—filed a copyright infringement lawsuit against Stability AI, Midjourney, and DeviantArt, claiming that it is legally required to obtain the consent of artists before training neural nets on their work and that these companies infringed on the rights of millions of artists by doing so on five billion images scraped from the web.[120] In July 2023, U.S. District Judge William Orrick was inclined to dismiss most of the lawsuits filed by Andersen, McKernan, and Ortiz, but allowed them to file a new complaint.[121] Also in 2023, Stability AI was sued by Getty Images for using its images in the training data.[122] A tool built by Simon Willison allowed people to search 0.5% of the training data for Stable Diffusion V1.1, i.e., 12 million of the 2.3 billion instances from LAION 2B. Artist Karen Hallion discovered that her copyrighted images were used as training data without their consent.[123]\n In March 2024, Tennessee enacted the ELVIS Act, which prohibits the use of AI to mimic a musician's voice without permission.[124] A month later in that year, Adam Schiff introduced the Generative AI Copyright Disclosure Act which, if passed, would require that AI companies to submit copyrighted works in their datasets to the Register of Copyrights before releasing new generative AI systems.[125]\n As with other types of photo manipulation since the early 19th century, some people in the early 21st century have been concerned that AI could be used to create content that is misleading and can be made to damage a person's reputation, such as deepfakes.[126] Artist Sarah Andersen, who previously had her art copied and edited to depict Neo-Nazi beliefs, stated that the spread of hate speech online can be worsened by the use of image generators.[123] Some also generate images or videos for the purpose of catfishing.\n AI systems have the ability to create deepfake content, which is often viewed as harmful and offensive. The creation of deepfakes poses a risk to individuals who have not consented to it.[127] This mainly refers to deepfake pornography which is used as revenge porn, where sexually explicit material is disseminated to humiliate or harm another person. AI-generated child pornography has been deemed a potential danger to society due to its unlawful nature.[128]\n \nTo mitigate some deceptions, OpenAI developed a tool in 2024 to detect images that were generated by DALL-E 3.[129] In testing, this tool accurately identified DALL-E 3-generated images approximately 98% of the time. The tool is also fairly capable of recognizing images that have been visually modified by users post-generation.[130] After winning the 2023 \"Creative\" \"Open competition\" Sony World Photography Awards, Boris Eldagsen stated that his entry was actually created with artificial intelligence. Photographer Feroz Khan commented to the BBC that Eldagsen had \"clearly shown that even experienced photographers and art experts can be fooled\".[132] Smaller contests have been affected as well; in 2023, a contest run by author Mark Lawrence as Self-Published Fantasy Blog-Off was cancelled after the winning entry was allegedly exposed to be a collage of images generated with Midjourney.[133]\n In May 2023, on social media sites such as Reddit and Twitter, attention was given to a Midjourney-generated image of Pope Francis wearing a white puffer coat.[134][135] Additionally, an AI-generated image of an attack on the Pentagon went viral as part of a hoax news story on Twitter.[136][137]\n In the days before March 2023 indictment of Donald Trump as part of the Stormy Daniels–Donald Trump scandal, several AI-generated images allegedly depicting Trump's arrest went viral online.[138][139] On March 20, British journalist Eliot Higgins generated various images of Donald Trump being arrested or imprisoned using Midjourney v5 and posted them on Twitter; two images of Trump struggling against arresting officers went viral under the mistaken impression that they were genuine, accruing more than 5 million views in three days.[140][141] According to Higgins, the images were not meant to mislead, but he was banned from using Midjourney services as a result. As of April 2024, the tweet had garnered more than 6.8 million views.\n In February 2024, the paper Cellular functions of spermatogonial stem cells in relation to JAK/STAT signaling pathway was published using AI-generated images. It was later retracted from Frontiers in Cell and Developmental Biology because the paper \"does not meet the standards\".[142]\n As generative AI image software such as Stable Diffusion and DALL-E continue to advance, the potential problems and concerns that these systems pose for creativity and artistry have risen.[123] In 2022, artists working in various media raised concerns about the impact that generative artificial intelligence could have on their ability to earn money, particularly if AI-based images started replacing artists working in the illustration and design industries.[143][144] In August 2022, digital artist R. J. Palmer stated that \"I could easily envision a scenario where using AI, a single artist or art director could take the place of 5-10 entry level artists... I have seen a lot of self-published authors and such say how great it will be that they don’t have to hire an artist.\"[113] Scholars Jiang et al. state that \"Leaders of companies like Open AI and Stability AI have openly stated that they expect generative AI systems to replace creatives imminently.\"[123] A 2022 case study found that AI-produced images created by technology like DALL-E caused some traditional artists to be concerned about losing work, while others use it to their advantage and view it as a tool.[127]\n AI-based images have become more commonplace in art markets and search engines because AI-based text-to-image systems are trained from pre-existing artistic images, sometimes without the original artist's consent, allowing the software to mimic specific artists' styles.[123][145] For example, Polish digital artist Greg Rutkowski has stated that it is more difficult to search for his work online because many of the images in the results are AI-generated specifically to mimic his style.[60] Furthermore, some training databases on which AI systems are based are not accessible to the public.\n The ability of AI-based art software to mimic or forge artistic style also raises concerns of malice or greed.[123][146][147] Works of AI-generated art, such as Théâtre D'opéra Spatial, a text-to-image AI illustration that won the grand prize in the August 2022 digital art competition at the Colorado State Fair, have begun to overwhelm art contests and other submission forums meant for small artists.[123][146][147] The Netflix short film The Dog & the Boy, released in January 2023, received backlash online for its use of artificial intelligence art to create the film's background artwork.[148] Within the same vein, Disney released Secret Invasion, a Marvel TV show with an AI-generated intro, on Disney+ in 2023, causing concern and backlash regarding the idea that artists could be made obsolete by machine-learning tools.[149]\n AI art has sometimes been deemed to be able to replace traditional stock images.[150] In 2023, Shutterstock announced a beta test of an AI tool that can regenerate partial content of other Shutterstock's images. Getty Images and Nvidia have partnered with the launch of Generative AI by iStock, a model trained on Getty's library and iStock's photo library using Nvidia's Picasso model.[151]\n Researchers from Hugging Face and Carnegie Mellon University reported in a 2023 paper that generating one thousand 1024×1024 images using Stable Diffusion's XL 1.0 base model requires 11.49 kWh of energy and generates 1,594 grams (56.2 oz) of carbon dioxide, which is roughly equivalent to driving an average gas-powered car a distance of 4.1 miles (6.6 km). Comparing 88 different models, the paper concluded that image-generation models used on average around 2.9 kWh of energy per 1,000 inferences.[152]\n In addition to the creation of original art, research methods that use AI have been generated to quantitatively analyze digital art collections. This has been made possible due to the large-scale digitization of artwork in the past few decades. According to CETINIC and SHE (2022), using artificial intelligence to analyze already-existing art collections can provide new perspectives on the development of artistic styles and the identification of artistic influences.[153][154]\n Two computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art.[155] Close reading focuses on specific visual aspects of one piece. Some tasks performed by machines in close reading methods include computational artist authentication and analysis of brushstrokes or texture properties. In contrast, through distant viewing methods, the similarity across an entire collection for a specific feature can be statistically visualized. Common tasks relating to this method include automatic classification, object detection, multimodal tasks, knowledge discovery in art history, and computational aesthetics.[154] Synthetic images can also be used to train AI algorithms for art authentication and to detect forgeries.[156]\n Researchers have also introduced models that predict emotional responses to art. One such model is ArtEmis, a large-scale dataset paired with machine learning models. ArtEmis includes emotional annotations from over 6,500 participants along with textual explanations. By analyzing both visual inputs and the accompanying text descriptions from this dataset, ArtEmis enables the generation of nuanced emotional predictions.[157][158]\n AI has also been used in arts outside of visual arts. Generative AI has been used in video game production beyond imagery, especially for level design (e.g., for custom maps) and creating new content (e.g., quests or dialogue) or interactive stories in video games.[159][160] AI has also been used in the literary arts,[161] such as helping with writer's block, inspiration, or rewriting segments.[162][163][164][165] In the culinary arts, some prototype cooking robots can dynamically taste, which can assist chefs in analyzing the content and flavor of dishes during the cooking process.[166]\n"
    },
    {
        "title": "Superintelligence",
        "url": "https://en.wikipedia.org/wiki/Superintelligence",
        "content": "A superintelligence is a hypothetical agent that possesses intelligence surpassing that of the brightest and most gifted human minds. \"Superintelligence\" may also refer to a property of problem-solving systems (e.g., superintelligent language translators or engineering assistants) whether or not these high-level intellectual competencies are embodied in agents that act in the world. A superintelligence may or may not be created by an intelligence explosion and associated with a technological singularity.\n University of Oxford philosopher Nick Bostrom defines superintelligence as \"any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest\".[1] The program Fritz falls short of this conception of superintelligence—even though it is much better than humans at chess—because Fritz cannot outperform humans in other tasks.[2]\n Technological researchers disagree about how likely present-day human intelligence is to be surpassed. Some argue that advances in artificial intelligence (AI) will probably result in general reasoning systems that lack human cognitive limitations. Others believe that humans will evolve or directly modify their biology to achieve radically greater intelligence.[3][4] Several future study scenarios combine elements from both of these possibilities, suggesting that humans are likely to interface with computers, or upload their minds to computers, in a way that enables substantial intelligence amplification.\n Some researchers believe that superintelligence will likely follow shortly after the development of artificial general intelligence. The first generally intelligent machines are likely to immediately hold an enormous advantage in at least some forms of mental capability, including the capacity of perfect recall, a vastly superior knowledge base, and the ability to multitask in ways not possible to biological entities. This may allow them to — either as a single being or as a new species — become much more powerful than humans, and displace them.[1]\n Several scientists and forecasters have been arguing for prioritizing early research into the possible benefits and risks of human and machine cognitive enhancement, because of the potential social impact of such technologies.[5]\n The feasibility of artificial superintelligence (ASI) has been a topic of increasing discussion in recent years, particularly with the rapid advancements in artificial intelligence (AI) technologies.\n Recent developments in AI, particularly in large language models (LLMs) based on the transformer architecture, have led to significant improvements in various tasks. Models like GPT-3, GPT-4, Claude 3.5 and others have demonstrated capabilities that some researchers argue approach or even exhibit aspects of artificial general intelligence (AGI).[6]\n However, the claim that current LLMs constitute AGI is controversial. Critics argue that these models, while impressive, still lack true understanding and are primarily sophisticated pattern matching systems.[7]\n Philosopher David Chalmers argues that AGI is a likely path to ASI. He posits that AI can achieve equivalence to human intelligence, be extended to surpass it, and then be amplified to dominate humans across arbitrary tasks.[8]\n More recent research has explored various potential pathways to superintelligence:\n Artificial systems have several potential advantages over biological intelligence:\n Recent advancements in transformer-based models have led some researchers to speculate that the path to ASI might lie in scaling up and improving these architectures. This view suggests that continued improvements in transformer models or similar architectures could lead directly to ASI.[13]\n Some experts even argue that current large language models like GPT-4 may already exhibit early signs of AGI or ASI capabilities.[14] This perspective suggests that the transition from current AI to ASI might be more continuous and rapid than previously thought, blurring the lines between narrow AI, AGI, and ASI.\n However, this view remains controversial. Critics argue that current models, while impressive, still lack crucial aspects of general intelligence such as true understanding, reasoning, and adaptability across diverse domains.[15]\n The debate over whether the path to ASI will involve a distinct AGI phase or a more direct scaling of current technologies remains ongoing, with significant implications for AI development strategies and safety considerations.\n Despite these potential advantages, there are significant challenges and uncertainties in achieving ASI:\n As research in AI continues to advance rapidly, the question of the feasibility of ASI remains a topic of intense debate and study in the scientific community.\n Carl Sagan suggested that the advent of Caesarean sections and in vitro fertilization may permit humans to evolve larger heads, resulting in improvements via natural selection in the heritable component of human intelligence.[17] By contrast, Gerald Crabtree has argued that decreased selection pressure is resulting in a slow, centuries-long reduction in human intelligence and that this process instead is likely to continue. There is no scientific consensus concerning either possibility and in both cases, the biological change would be slow, especially relative to rates of cultural change.\n Selective breeding, nootropics, epigenetic modulation, and genetic engineering could improve human intelligence more rapidly. Bostrom writes that if we come to understand the genetic component of intelligence, pre-implantation genetic diagnosis could be used to select for embryos with as much as 4 points of IQ gain (if one embryo is selected out of two), or with larger gains (e.g., up to 24.3 IQ points gained if one embryo is selected out of 1000). If this process is iterated over many generations, the gains could be an order of magnitude improvement. Bostrom suggests that deriving new gametes from embryonic stem cells could be used to iterate the selection process rapidly.[18] A well-organized society of high-intelligence humans of this sort could potentially achieve collective superintelligence.[19]\n Alternatively, collective intelligence might be constructional by better organizing humans at present levels of individual intelligence. Several writers have suggested that human civilization, or some aspect of it (e.g., the Internet, or the economy), is coming to function like a global brain with capacities far exceeding its component agents. If this systemic superintelligence relies heavily on artificial components, however, it may qualify as an AI rather than as a biology-based superorganism.[20] A prediction market is sometimes considered as an example of a working collective intelligence system, consisting of humans only (assuming algorithms are not used to inform decisions).[21]\n A final method of intelligence amplification would be to directly enhance individual humans, as opposed to enhancing their social or reproductive dynamics. This could be achieved using nootropics, somatic gene therapy, or brain−computer interfaces. However, Bostrom expresses skepticism about the scalability of the first two approaches and argues that designing a superintelligent cyborg interface is an AI-complete problem.[22]\n Most surveyed AI researchers expect machines to eventually be able to rival humans in intelligence, though there is little consensus on when this will likely happen. At the 2006 AI@50 conference, 18% of attendees reported expecting machines to be able \"to simulate learning and every other aspect of human intelligence\" by 2056; 41% of attendees expected this to happen sometime after 2056; and 41% expected machines to never reach that milestone.[23]\n In a survey of the 100 most cited authors in AI (as of May 2013, according to Microsoft academic search), the median year by which respondents expected machines \"that can carry out most human professions at least as well as a typical human\" (assuming no global catastrophe occurs) with 10% confidence is 2024 (mean 2034, st. dev. 33 years), with 50% confidence is 2050 (mean 2072, st. dev. 110 years), and with 90% confidence is 2070 (mean 2168, st. dev. 342 years). These estimates exclude the 1.2% of respondents who said no year would ever reach 10% confidence, the 4.1% who said 'never' for 50% confidence, and the 16.5% who said 'never' for 90% confidence. Respondents assigned a median 50% probability to the possibility that machine superintelligence will be invented within 30 years of the invention of approximately human-level machine intelligence.[24]\n In a 2022 survey, the median year by which respondents expected \"High-level machine intelligence\" with 50% confidence is 2061. The survey defined the achievement of high-level machine intelligence as when unaided machines can accomplish every task better and more cheaply than human workers.[25]\n In 2023, OpenAI leaders Sam Altman, Greg Brockman and Ilya Sutskever published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.[26] In 2024, Ilya Sutskever left OpenAI to cofound the startup Safe Superintelligence, which focuses solely on creating a superintelligence that is safe by design, while avoiding \"distraction by management overhead or product cycles\".[27]\n The design of superintelligent AI systems raises critical questions about what values and goals these systems should have. Several proposals have been put forward:[28]\n Bostrom elaborates on these concepts:\n instead of implementing humanity's coherent extrapolated volition, one could try to build an AI to do what is morally right, relying on the AI's superior cognitive capacities to figure out just which actions fit that description. We can call this proposal \"moral rightness\" (MR) ... \n MR would also appear to have some disadvantages. It relies on the notion of \"morally right\", a notoriously difficult concept, one with which philosophers have grappled since antiquity without yet attaining consensus as to its analysis. Picking an erroneous explication of \"moral rightness\" could result in outcomes that would be morally very wrong ...\n \nOne might try to preserve the basic idea of the MR model while reducing its demandingness by focusing on moral permissibility: the idea being that we could let the AI pursue humanity's CEV so long as it did not act in morally impermissible ways.[28] Since Bostrom's analysis, new approaches to AI value alignment have emerged:\n The rapid advancement of transformer-based LLMs has led to speculation about their potential path to ASI. Some researchers argue that scaled-up versions of these models could exhibit ASI-like capabilities:[32]\n However, critics argue that current LLMs lack true understanding and are merely sophisticated pattern matchers, raising questions about their suitability as a path to ASI.[36]\n Additional viewpoints on the development and implications of superintelligence include:\n The pursuit of value-aligned AI faces several challenges:\n Current research directions include multi-stakeholder approaches to incorporate diverse perspectives, developing methods for scalable oversight of AI systems, and improving techniques for robust value learning.[40][16]\n Al research is rapidly progressing towards superintelligence. Addressing these design challenges remains crucial for creating ASI systems that are both powerful and aligned with human interests.\n The development of artificial superintelligence (ASI) has raised concerns about potential existential risks to humanity. Researchers have proposed various scenarios in which an ASI could pose a significant threat:\n Some researchers argue that through recursive self-improvement, an ASI could rapidly become so powerful as to be beyond human control. This concept, known as an \"intelligence explosion\", was first proposed by I. J. Good in 1965:\n Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an 'intelligence explosion,' and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.[41] This scenario presents the AI control problem: how to create an ASI that will benefit humanity while avoiding unintended harmful consequences.[42] Eliezer Yudkowsky argues that solving this problem is crucial before ASI is developed, as a superintelligent system might be able to thwart any subsequent attempts at control.[43]\n Even with benign intentions, an ASI could potentially cause harm due to misaligned goals or unexpected interpretations of its objectives. Nick Bostrom provides a stark example of this risk:\n When we create the first superintelligent entity, we might make a mistake and give it goals that lead it to annihilate humankind, assuming its enormous intellectual advantage gives it the power to do so. For example, we could mistakenly elevate a subgoal to the status of a supergoal. We tell it to solve a mathematical problem, and it complies by turning all the matter in the solar system into a giant calculating device, in the process killing the person who asked the question.[44] Stuart Russell offers another illustrative scenario:\n A system given the objective of maximizing human happiness might find it easier to rewire human neurology so that humans are always happy regardless of their circumstances, rather than to improve the external world.[45] These examples highlight the potential for catastrophic outcomes even when an ASI is not explicitly designed to be harmful, underscoring the critical importance of precise goal specification and alignment.\n Researchers have proposed various approaches to mitigate risks associated with ASI:\n Despite these proposed strategies, some experts, such as Roman Yampolskiy, argue that the challenge of controlling a superintelligent AI might be fundamentally unsolvable, emphasizing the need for extreme caution in ASI development.[50]\n Not all researchers agree on the likelihood or severity of ASI-related existential risks. Some, like Rodney Brooks, argue that fears of superintelligent AI are overblown and based on unrealistic assumptions about the nature of intelligence and technological progress.[51] Others, such as Joanna Bryson, contend that anthropomorphizing AI systems leads to misplaced concerns about their potential threats.[52]\n The rapid advancement of LLMs and other AI technologies has intensified debates about the proximity and potential risks of ASI. While there is no scientific consensus, some researchers and AI practitioners argue that current AI systems may already be approaching AGI or even ASI capabilities.\n A minority of researchers and observers, including some in the AI development community, believe that current AI systems may already be at or near AGI levels, with ASI potentially following in the near future. This view, while not widely accepted in the scientific community, is based on observations of rapid progress in AI capabilities and unexpected emergent behaviors in large models.[55]\n However, many experts caution against premature claims of AGI or ASI, arguing that current AI systems, despite their impressive capabilities, still lack true understanding and general intelligence.[56] They emphasize the significant challenges that remain in achieving human-level intelligence, let alone superintelligence.\n The debate surrounding the current state and trajectory of AI development underscores the importance of continued research into AI safety and ethics, as well as the need for robust governance frameworks to manage potential risks as AI capabilities continue to advance.[49]\n"
    },
    {
        "title": "Strategy game",
        "url": "https://en.wikipedia.org/wiki/Strategy_game",
        "content": "A strategy game or strategic game is a game in which the players' uncoerced, and often autonomous, decision-making skills have a high significance in determining the outcome. Almost all strategy games require internal decision tree-style thinking, and typically very high situational awareness.\n Strategy games are also seen as a descendant of war games, and define strategy in terms of the context of war, but this is more partial. A strategy game is a game that relies primarily on strategy, and when it comes to defining what strategy is, two factors need to be taken into account: its complexity and game-scale actions, such as each placement in the Total War video game series. The definition of a strategy game in its cultural context should be any game that belongs to a tradition that goes back to war games, contains more strategy than the average video game, contains certain gameplay conventions, and is represented by a particular community. Although war is dominant in strategy games, it is not the whole story.[1]\n The history of turn-based strategy games goes back to the times of ancient civilizations found in places such as Rome, Greece, Egypt, the Levant, and India. Many were played widely through their regions of origin, but only some are still played today.[2]\n According to Thierry Depaulis, oldest strategy games would be the \"Greek game of polis (πόλις), which appears in the literature around 450 BCE, and the more or less contemporary Chinese game of weiqi (‘go’), which, under the name of yi (弈), is mentioned in Confucius’s Analects (Lunyu) compiled between ca 470/50 and 280 BCE.\"[2]\n The Royal Game of Ur from c. 2500 BCE which often been called one of the oldest board games, likely had some strategy elements as well, although it is generally seen as a luck-based race game.[3][4]\n One of the earliest strategy games still played is mancala.[5] Due to claims that some artifacts from c. 5000 BCE might be old mancala boards, it has been suggested that mancala may be the oldest known strategy game, but this claim has been disputed.[3]\n Another game that has stood the test of time is chess, believed to have originated in India around the sixth century CE.[6] The game spread to the west by trade, but chess gained social status and permanence more strongly than many other games. Chess became a game of skill and tactics often forcing the players to think two or three moves ahead of their opponent just to keep up.[7]\n In abstract strategy games, the game is only loosely tied to a thematic concept, if at all. The rules do not attempt to simulate reality, but rather serve the internal logic of the game.\n A purist's definition of an abstract strategy game requires that it cannot have random elements or hidden information. This definition includes such games as chess and Go. However, many games are commonly classed as abstract strategy games which do not meet these criteria: games such as backgammon, Octiles, Can't Stop, Sequence and Mentalis have all been described as \"abstract strategy\" games despite having a chance element.[citation needed] A smaller category of non-perfect abstract strategy games incorporate hidden information without using any random elements; for example, Stratego.\n One of the most focused team strategy games is contract bridge. This card game consists of two teams of two players, whose offensive and defensive skills are continually in flux as the game's dynamic progresses. Some argue that the benefits of playing this team strategy card game extend to those skills and strategies used in business[8] and that the playing of these games helps to automate strategic awareness.\n Eurogames, or German-style boardgames, are a relatively new genre that sit between abstract strategy games and simulation games. They generally have simple rules, short to medium playing times, indirect player interaction and abstract physical components. The games emphasize strategy, play down chance and conflict, lean towards economic rather than military themes, and usually keep all the players in the game until it ends.\n This type of game is an attempt to simulate the decisions and processes inherent to some real-world situation. Most of the rules are chosen to reflect what the real-world consequences would be of each player's actions and decisions. \nAbstract games cannot be completely divided from simulations and so games can be thought of as existing on a continuum of almost pure abstraction (like Abalone) to almost pure simulation (like Diceball! or Strat-o-Matic Baseball).\n Wargames are simulations of military battles, campaigns, or entire wars. Players will have to consider situations that are analogous to the situations faced by leaders of historical battles. As such, wargames are usually heavy on simulation elements, and while they are all \"strategy games\", they can also be \"strategic\" or \"tactical\" in the military jargon sense. Its creator, H. G. Wells, stated how \"much better is this amiable miniature [war] than the real thing\".[9]\n Traditionally, wargames have been played either with miniatures, using physical models of detailed terrain and miniature representations of people and equipment to depict the game state; or on a board, which commonly uses cardboard counters on a hex map.\n Popular miniature wargames include Warhammer 40,000 or its fantasy counterpart Warhammer Fantasy. Popular strategic board wargames include Risk, Axis and Allies, Diplomacy, and Paths of Glory. Advanced Squad Leader is a successful tactical scale wargame.\n It is instructive to compare the Total War series to the Civilization series, where moving troops to a specific tile is a tactic because there are no short-range decisions. But in Empire: Total War (2009), every encounter between two armies activates a real-time mode in which they must fight and the same movement of troops is treated as a strategy. Throughout the game, the movement of each army is at a macro scale, because the player can control each battle at a micro scale. However, as an experience, the two types of military operations are quite similar and involve similar skills and thought processes. The concept of micro scale and macro scale can well describe the gameplay of a game; however, even very similar games can be difficult to integrate into a common vocabulary. In this definition, strategy does not explicitly describe the player's experience; it is more appropriate to describe different formal game components. The similarity of the actions taken in two different games does not affect our definition of them as strategy or tactics: we will only rely on their scale in their respective games.[1]\n Strategy video games are categorized based on whether they offer the continuous gameplay of real-time strategy (RTS), or the discrete phases of turn-based strategy (TBS).[10] Often the computer is expected to emulate a strategically thinking \"side\" similar to that of a human player (such as directing armies and constructing buildings), or emulate the \"instinctive\" actions of individual units that would be too tedious for a player to administer (such as for a peasant to run away when attacked, as opposed to standing still until otherwise ordered by the player); hence there is an emphasis on artificial intelligence.\n"
    },
    {
        "title": "Chess",
        "url": "https://en.wikipedia.org/wiki/Chess",
        "content": "\n Chess is a board game for two players. It is sometimes called international chess or Western chess to distinguish it from related games such as xiangqi (Chinese chess) and shogi (Japanese chess).\n Chess is an abstract strategy game which involves no hidden information and no elements of chance. It is played on a square game board called a chessboard containing 64 squares arranged in an 8×8 grid. The players, referred to as \"White\" and \"Black\", each control sixteen pieces: one king, one queen, two rooks, two bishops, two knights, and eight pawns. White moves first, followed by Black; then moves alternate. The object of the game is to checkmate (threaten with inescapable capture) the enemy king. There are also several ways a game can end in a draw.\n The recorded history of chess goes back at least to the emergence of a similar game, chaturanga, in seventh-century India. After its introduction in Persia, it spread to the Arab world and then to Europe. The modern rules of chess emerged in Europe at the end of the 15th century, with standardization and universal acceptance by the end of the 19th century. Today, chess is one of the world's most popular games, with millions of players worldwide. \n Organized chess arose in the 19th century. Chess competition today is governed internationally by FIDE (Fédération Internationale des Échecs; the International Chess Federation). The first universally recognized World Chess Champion, Wilhelm Steinitz, claimed his title in 1886; Gukesh Dommaraju is the current World Champion (2024).\n A huge body of chess theory has developed since the game's inception. Aspects of art are found in chess composition, and chess in its turn influenced Western culture and the arts, and has connections with other fields such as mathematics, computer science, and psychology. One of the goals of early computer scientists was to create a chess-playing machine. In 1997, Deep Blue became the first computer to beat a reigning World Champion in a match when it defeated Garry Kasparov. Today's chess engines are significantly stronger than the best human players and have deeply influenced the development of chess theory; however, chess is not a solved game.\n The rules of chess are published by FIDE (Fédération Internationale des Échecs; \"International Chess Federation\"), chess's world governing body, in its Handbook.[2] Rules published by national governing bodies, or by unaffiliated chess organizations, online chess sites, etc., may differ in some details. FIDE's rules were most recently revised in 2023.\n Chess sets come in a wide variety of styles. The Staunton pattern is the most common, and is usually required for competition.[3] Chess sets come with pieces in two colors, referred to as white and black, regardless of their actual color; the players controlling the color sets are referred to as White and Black, respectively. Each set comes with at least the following 16 pieces in both colors: one king, one queen, two rooks, two bishops, two knights, and eight pawns.[2]\n The game is played on a square board of eight rows (called ranks) and eight columns (called files). Although it does not affect game play, by convention the 64 squares alternate in color and are referred to as light and dark squares.[2] Common colors for wooden chessboards are light and dark brown, while vinyl chessboards are commonly buff and green.[citation needed]\n To start the game, White's pieces are placed on the first rank in the following order, from left to right: rook, knight, bishop, queen, king, bishop, knight, rook. Pawns are placed on each square of the second rank. Black's position mirrors White's, with equivalent pieces on every file.[2] The board is oriented so that the right-hand corner nearest each player is a light square; as a result the white queen always starts on a light square, while the black queen starts on a dark square. This may be remembered by the phrases \"light on the right\" and \"queen on her own color\".[4]\n In formal competition, the piece colors for every matchup are allocated to players by the organizers. In informal games, colors may be decided either by mutual agreement, or randomly, for example by a coin toss, or by one player concealing a white pawn in one hand and a black pawn in the other and having the opponent choose.[citation needed]\n White moves first, after which players alternate turns. One piece is moved per turn (except when castling, during which two pieces are moved). In the diagrams, dots mark the squares to which each type of piece can move if unoccupied by friendly pieces and there are no intervening piece(s) of either color (except the knight, which leaps over any intervening pieces). With the sole exception of en passant, a piece captures an enemy piece by moving to the square it occupies, removing it from play and taking its place. The pawn is the only piece that does not capture the way it normally moves, and it is the only piece that moves differently depending on its color (forwards from the player's perspective). A piece is said to control empty squares on which it could capture, attack squares with enemy pieces it could capture, and defend squares with pieces of the same color on which it could recapture. Moving is compulsory; a player may not skip a turn, even when having to move is detrimental.\n \n \n \n When a king is under immediate attack, it is in check. A move in response to a check is legal only if it results in a position in which the king is no longer in check. There are three ways to counter a check:\n The object of the game is to checkmate the opponent; this occurs when the opponent's king is in check, and there is no legal way to get it out of check. In casual games, it is common to announce \"check\" when putting the opponent's king in check, but this is not required by the rules of chess and is usually not done in tournaments.[5]\n Kings can castle once per game. Castling consists of moving the king two squares toward either rook of the same color, and then placing the rook on the square that the king crossed.\n Castling is possible only if the following conditions are met:[2]\n Castling is still permitted if the rook is under attack, or if the rook crosses an attacked square. \n \n Pawns have two special moves:\n A game can be won in the following ways:\n There are several ways a game can end in a draw:\n \n In competition, chess games are played with a time control. If a player's time runs out before the game is completed, the game is automatically lost (provided the opponent has enough pieces left to deliver checkmate).[2] Methods and bounds vary, but time controls are generally divided into categories based on the amount of time given to each player (FIDE adds sixty turns worth of increment to the starting time when measuring), which range from classical time controls, which allot an hour or more to each player and which can take upwards of seven hours (even longer if adjournments are permitted), to bullet chess, in which players receive less than three minutes each. Between these are rapid chess (ten to sixty minutes per player), popular in amateur tournaments, and blitz chess (three to ten minutes), popular online. Non-classical chess is sometimes referred to as fast chess.\n Time is controlled using a chess clock with two displays, one for each player's remaining time. Analog chess clocks have been largely replaced by digital clocks, which allow for time controls with increments.\n There are some aspects unique to online chess. A premove allows a player to submit a move on the opponent's turn, which gets played automatically if possible using little to no time. Premoves, alongside the relative ease of digital inputs, make faster time controls feasible online.\n Time controls are also enforced in correspondence chess competitions. A typical time control is 50 days for every 10 moves. Time is usually alloted per move in online correspondence chess.\n Historically, many different notation systems have been used to record chess moves; the standard system today is short-form algebraic notation.[10] In this system, files are labeled a through h and ranks are labeled 1 through 8. Squares are identified by the file and rank they occur on; g3 is the square on the g file and the third rank. In English, the piece notations are: K (king), Q (queen), R (rook), B (bishop), and N (knight; N is used to avoid confusion with king). Pieces may be abbreviated differently in other languages. Moves are recorded as follows:\n For example, Qg5 means \"queen moves to the g-file, 5th rank\" (that is, to the square g5). No letter initial is used for pawns, so e4 means \"pawn moves to e4\". When multiple moves could be rendered the same way, the file or rank from which the piece moved is added to resolve ambiguity (e.g. Ngf3 means \"knight from the g-file moves to the square f3\"; R1e2 means \"rook on the first rank moves to e2\"). If a move may be disambiguated by rank or file, it is done by file, and in the rare case that both are needed, squares are listed normally (e.g Qh4xe1).\n If the move is a capture, \"x\" is usually inserted before the destination square. Thus Bxf3 means \"bishop captures on f3\". When a pawn makes a capture, the file from which the pawn departed is often listed even when no disambiguation is necessary, for example, exd5 (pawn on the e-file captures the piece on d5). Ranks may be omitted if unambiguous, for example, exd (pawn on the e-file captures a piece somewhere on the d-file).\n If a pawn moves to its last rank, achieving promotion, the piece chosen is indicated after the move (for example, e1=Q or e1Q). Castling is indicated by the special notations 0-0 (or O-O) for kingside castling and 0-0-0 (or O-O-O) for queenside castling. A move that places the opponent's king in check usually has the notation \"+\" suffixed. Checkmate can be indicated by suffixing \"#\". At the end of the game, \"1–0\" means White won, \"0–1\" means Black won, and \"½–½\" indicates a draw.[2] Chess moves can be annotated with punctuation marks and other symbols. For example: \"!\" indicates a good move; \"!!\" a excellent move; \"?\" a mistake; \"??\" a blunder; \"!?\" an interesting move that may not be best; or \"?!\" a dubious move not easily refuted.[11]\n Moves are written as white/black pairs, preceded by the move number and a period. Individual white moves are also recorded this way, while black moves are rendered with an ellipsis after the move number. For example, one variation of a simple trap known as the Scholar's mate (see animated diagram) can be recorded:\n The move 3... Nf6?? is recorded as a blunder, as it allows 4. Qxf7# checkmate.\n Portable Game Notation (PGN) is a text-based file format for recording chess games with support for annotative symbols, commentary, and background information, such as player names. It is based on short form English algebraic notation incorporating markup language. PGN transcripts, stored digitally as PGN files (suffix .pgn) can be processed by most chess software and are easily readable by humans.\n Variants of algebraic notation include long algebraic, in which both the departure and destination square are indicated; abbreviated algebraic, in which capture signs, check signs, and ranks of pawn captures may be omitted; and figurine algebraic notation, used in chess publications for universal readability regardless of language.\n Until about 1980, the majority of English language chess publications used descriptive notation, in which files are identified by the initial letter of the piece that occupies the first rank at the beginning of the game. In descriptive notation, the common opening move 1.e4 is rendered as \"1.P-K4\" (\"pawn to king four\"). Another system is ICCF numeric notation, recognized by the International Correspondence Chess Federation though its use is in decline.\n In tournament games, players are normally required to keep a score (record of the game). For this purpose, only algebraic notation is recognized in FIDE-sanctioned events; game scores recorded in a different notation system may not be used as evidence in the event of a dispute.\n When describing moves verbally, chess pieces are typically referred to by name, and information is often selectively omitted when it can be discerned from context; for example, the moves 4. Bxf7 Kxf7 might be said as \"bishop takes f7, king takes\".\n Chess has an extensive literature. In 1913, the chess historian H.J.R. Murray estimated the total number of books, magazines, and chess columns in newspapers to be about 5,000.[12] B.H. Wood estimated the number, as of 1949, to be about 20,000.[13] David Hooper and Kenneth Whyld write that, \"Since then there has been a steady increase year by year of the number of new chess publications. No one knows how many have been printed.\"[13] Significant public chess libraries include the John G. White Chess and Checkers Collection at Cleveland Public Library, with over 32,000 chess books and over 6,000 bound volumes of chess periodicals;[14] and the Chess & Draughts collection at the National Library of the Netherlands, with about 30,000 books.[15]\n Chess theory usually divides the game of chess into three phases with different sets of strategies: the opening, typically the first 10 to 20 moves, when players move their pieces to useful positions for the coming battle; the middlegame; and last the endgame, when most of the pieces are gone, kings typically take a more active part in the struggle, and pawn promotion is often decisive.\n Opening theory is concerned with finding the best moves in the initial phase of the game. Many opening sequences have standard names such as the Sicilian Defense. The Oxford Companion to Chess lists 1,327 named openings and variations, and this list is by no means exhaustive.[16]\n Middlegame theory is usually divided into chess tactics and chess strategy. Chess strategy concentrates on setting and achieving long-term positional advantages during the game – for example, where to place different pieces – while tactics concerns immediate maneuver. These two aspects of the gameplay cannot be completely separated, because strategic goals are mostly achieved through tactics, while the tactical opportunities are based on the previous strategy of play.\n Endgame theory is concerned with positions where there are only a few pieces left. These positions are categorized according to the pieces, for example \"King and pawn\" endings or \"Rook versus minor piece\" endings.\n A chess opening is the group of initial moves of a game (the \"opening moves\"). Recognized sequences of opening moves are referred to as openings and have been given names such as the Ruy Lopez or Sicilian Defense. They are catalogued in reference works such as the Encyclopaedia of Chess Openings. There are dozens of different openings, varying widely in character from quiet positional play (for example, the Réti Opening) to very aggressive (the Latvian Gambit). In some opening lines, the exact sequence considered best for both sides has been worked out to more than 30 moves.[17] Professional players spend years studying openings and continue doing so throughout their careers, as opening theory continues to evolve.\n The fundamental strategic aims of most openings are similar:[18]\n Most players and theoreticians consider that White, by virtue of the first move, begins the game with a small advantage. This initially gives White the initiative.[19] Black usually strives to neutralize White's advantage and achieve equality, or to develop dynamic counterplay in an unbalanced position.\n The middlegame is the part of the game that starts after the opening. There is no clear line between the opening and the middlegame, but typically the middlegame will start when most pieces have been developed. (Similarly, there is no clear transition from the middlegame to the endgame; see start of the endgame.) Because the opening theory has ended, players have to form plans based on the features of the position, and at the same time take into account the tactical possibilities of the position.[20] The middlegame is the phase in which most combinations occur. Combinations are a series of tactical moves executed to achieve some gain. Middlegame combinations are often connected with an attack against the opponent's king. Some typical patterns have their own names; for example, the Boden's Mate or the Lasker–Bauer combination.[21]\n Specific plans or strategic themes will often arise from particular groups of openings that result in a specific type of pawn structure. An example is the minority attack, which is the attack of queenside pawns against an opponent who has more pawns on the queenside. The study of openings is therefore connected to the preparation of plans that are typical of the resulting middlegames.[22]\n Another important strategic question in the middlegame is whether and how to reduce material and transition into an endgame (i.e. simplify). Minor material advantages can generally be transformed into victory only in an endgame, and therefore the stronger side must choose an appropriate way to achieve an ending. Not every reduction of material is good for this purpose; for example, if one side keeps a light-squared bishop and the opponent has a dark-squared one, the transformation into a bishops and pawns ending is usually advantageous for the weaker side only, because an endgame with bishops on opposite colors is likely to be a draw, even with an advantage of a pawn, or sometimes even with a two-pawn advantage.[23]\n The endgame (also end game or ending) is the stage of the game when there are few pieces left on the board. There are three main strategic differences between earlier stages of the game and the endgame:[24]\n Endgames can be classified according to the type of pieces remaining on the board. Basic checkmates are positions in which one side has only a king and the other side has one or two pieces and can checkmate the opposing king, with the pieces working together with their king. For example, king and pawn endgames involve only kings and pawns on one or both sides, and the task of the stronger side is to promote one of the pawns. Other more complicated endings are classified according to pieces on the board other than kings, such as \"rook and pawn versus rook\" endgames.\n In chess, tactics in general concentrate on short-term actions – so short-term that they can be calculated in advance by a human player or a computer. The possible depth of calculation depends on the player's ability. In quiet positions with many possibilities on both sides, a deep calculation is more difficult and may not be practical, while in positions with a limited number of forced variations, strong players can calculate long sequences of moves.\n Theoreticians describe many elementary tactical methods and typical maneuvers, for example: pins, forks, skewers, batteries, discovered attacks (especially discovered checks), zwischenzugs, deflections, decoys, sacrifices, underminings, overloadings, and interferences.[25] Simple one-move or two-move tactical actions – threats, exchanges of material, and double attacks – can be combined into more complicated sequences of tactical maneuvers that are often forced from the point of view of one or both players.[26] A forced variation that involves a sacrifice and usually results in a tangible gain is called a combination.[26] Brilliant combinations – such as those in the Immortal Game – are considered beautiful and are admired by chess lovers. A common type of chess exercise, aimed at developing players' skills, is a position where a decisive combination is available and the challenge is to find it.[27]\n \n Chess strategy is concerned with the evaluation of chess positions and with setting up goals and long-term plans for future play. During the evaluation, players must take into account numerous factors such as the value of the pieces on the board, control of the center and centralization, the pawn structure, king safety, and the control of key squares or groups of squares (for example, diagonals, open files, and dark or light squares).\n The most basic step in evaluating a position is to count the total value of pieces of both sides.[29] The point values used for this purpose are based on experience; usually, pawns are considered worth one point, knights and bishops about three points each, rooks about five points (the value difference between a rook and a bishop or knight being known as the exchange), and queens about nine points. The king is more valuable than all of the other pieces combined, since its checkmate loses the game, but is still capable as a fighting piece; in the endgame, the king is generally more powerful than a bishop or knight but less powerful than a rook.[30] These basic values are then modified by other factors like position of the piece (e.g. advanced pawns are usually more valuable than those on their initial squares), coordination between pieces (e.g. a pair of bishops usually coordinate better than a bishop and a knight), or the type of position (e.g. knights are generally better in closed positions with many pawns while bishops are more powerful in open positions).[31]\n Another important factor in the evaluation of chess positions is pawn structure (sometimes known as the pawn skeleton): the configuration of pawns on the chessboard.[32] Since pawns are the least mobile of the pieces, pawn structure is relatively static and largely determines the strategic nature of the position. Weaknesses in pawn structure include isolated, doubled, or backward pawns and holes; once created, they are often permanent. Care must therefore be taken to avoid these weaknesses unless they are compensated by another valuable asset (for example, by the possibility of developing an attack).[33]\n Chess is often played casually in public spaces such as parks and town squares.\n Contemporary chess is an organized sport with structured international and national leagues, tournaments, and congresses. Thousands of chess tournaments, matches, and festivals are held around the world every year catering to players of all levels.\n Tournaments with a small number of players may use the round-robin format, in which every player plays one game against every other player. For a large number of players, the Swiss system may be used, in which each player is paired against an opponent who has the same (or as similar as possible) score in each round. In either case, a player's score is usually calculated as 1 point for each game won and one-half point for each game drawn. Variations such as \"football scoring\" (3 points for a win, 1 point for a draw) may be used by tournament organizers, but ratings are always calculated on the basis of standard scoring. A player's score may be reported as total score out of games played (e.g. 5½/8), points for versus points against (e.g. 5½–2½), or by number of wins, losses and draws (e.g. +4−1=3).\n The term \"match\" refers not to an individual game, but to either a series of games between two players, or a team competition in which each player of one team plays one game against a player of the other team.\n Chess's international governing body is usually known by its French acronym FIDE (pronounced FEE-day) (French: Fédération internationale des échecs), or International Chess Federation. FIDE's membership consists of the national chess organizations of over 180 countries; there are also several associate members, including various supra-national organizations, the International Braille Chess Association (IBCA), International Committee of Chess for the Deaf (ICCD), and the International Physically Disabled Chess Association (IPCA).[34] FIDE is recognized as a sports governing body by the International Olympic Committee,[35] but chess has never been part of the Olympic Games.\n FIDE's most visible activity is organizing the World Chess Championship, a role it assumed in 1948. The current World Champion is Gukesh Dommaraju of India.[36][37] The reigning Women's World Champion is Ju Wenjun from China.[38]\n Other competitions for individuals include the World Junior Chess Championship, the European Individual Chess Championship, the tournaments for the World Championship qualification cycle, and the various national championships. Invitation-only tournaments regularly attract the world's strongest players. Examples include Spain's Linares event, Monte Carlo's Melody Amber tournament, the Dortmund Sparkassen meeting, Sofia's M-tel Masters, and Wijk aan Zee's Tata Steel tournament.\n Regular team chess events include the Chess Olympiad and the European Team Chess Championship.\n The World Chess Solving Championship and World Correspondence Chess Championships include both team and individual events; these are held independently of FIDE.\n In order to rank players, FIDE, ICCF, and most national chess organizations use the Elo rating system developed by Arpad Elo. An average club player has a rating of about 1500; the highest FIDE rating of all time, 2882, was achieved by Magnus Carlsen on the March 2014 FIDE rating list.[39]\n Players may be awarded lifetime titles by FIDE:[41]\n The above titles are open to both men and women. There are also separate women-only titles; Woman Grandmaster (WGM), Woman International Master (WIM), Woman FIDE Master (WFM) and Woman Candidate Master (WCM). These require a performance level approximately 200 Elo rating points below the similarly named open titles, and their continued existence has sometimes been controversial. Beginning with Nona Gaprindashvili in 1978, a number of women have earned the open GM title: 40 as of July 2023[update].[note 2]\n FIDE also awards titles for arbiters and trainers.[42][43] International titles are also awarded to composers and solvers of chess problems and to correspondence chess players (by the International Correspondence Chess Federation). National chess organizations may also award titles.\n Texts referring to the origins of chess date from the beginning of the seventh century. Three are written in Pahlavi (Middle Persian)[46] and one, the Harshacharita, is in Sanskrit.[47] One of these texts, the Chatrang-namak, represents one of the earliest written accounts of chess. The narrator Bozorgmehr explains that Chatrang, \"Chess\" in Pahlavi, was introduced to Persia by 'Dewasarm, a great ruler of India' during the reign of Khosrow I:[48]\n Dewasarm has fashioned this chatrang after the likeness of a battle, and in its likeness are two supreme rulers after the likeness of Kings (shah), with the essentials of rooks (rukh) to right and to left, with Counsellor (farzin) in the likeness of a commander of the champions, with the Elephant (pil) in the likeness of the commander of the rearguard, with Horse (asp) in the likeness of the commander of the cavalry, with the Footsoldier (piyadak) in the likeness of so many infantry in the vanguard of the battle The oldest known chess manual was in Arabic and dates to about 840, written by al-Adli ar-Rumi (800–870), a renowned Arab chess player, titled Kitab ash-shatranj (The Book of Chess). This is a lost manuscript, but is referenced in later works.[50] Here also, al-Adli attributes the origins of Persian chess to India, along with the eighth-century collection of fables Kalīla wa-Dimna.[51] By the 20th century, a substantial consensus[52][53] developed regarding chess's origins in northwest India in the early seventh century.[54] More recently, this consensus has been the subject of further scrutiny.[55]\n The early forms of chess in India were known as chaturaṅga (Sanskrit: चतुरङ्ग), literally \"four divisions\" [of the military] – infantry, cavalry, elephants, and chariotry – represented by pieces that would later evolve into the modern pawn, knight, bishop, and rook, respectively. Chaturanga was played on an 8×8 uncheckered board, called ashtāpada.[56] Thence it spread eastward and westward along the Silk Road. The earliest evidence of chess is found in nearby Sasanian Persia around 600 A.D., where the game came to be known by the name chatrang (Persian: چترنگ).[57] Chatrang was taken up by the Muslim world after the Islamic conquest of Persia (633–51), where it was then named shatranj (Arabic: شطرنج; Persian: شترنج), with the pieces largely retaining their Persian names. In Spanish, \"shatranj\" was rendered as ajedrez (\"al-shatranj\"), in Portuguese as xadrez, and in Greek as ζατρίκιον (zatrikion, which comes directly from the Persian chatrang),[58] but in the rest of Europe it was replaced by versions of the Persian shāh (\"king\"), from which the English words \"check\" and \"chess\" descend.[note 3] The word \"checkmate\" is derived from the Persian shāh māt (\"the king is dead\").[59]\n Xiangqi is the form of chess best known in China. The eastern migration of chess, into China and Southeast Asia, has even less documentation than its migration west, making it largely conjectured. The word xiàngqí (象棋) was used in China to refer to a game from 569 A.D. at the latest, but it has not been proven that this game was directly related to chess.[60][61]\nThe first reference to Chinese chess appears in a book entitled Xuánguaì Lù (玄怪錄; \"Record of the Mysterious and Strange\"), dating to about 800. A minority view holds that Western chess arose from xiàngqí or one of its predecessors.[62][63] Chess historians Jean-Louis Cazaux and Rick Knowlton contend that xiangqi's intrinsic characteristics make it easier to construct an evolutionary path from China to India/Persia than the opposite direction.[64]\n The oldest archaeological chess artifacts – ivory pieces – were excavated in ancient Afrasiab, today's Samarkand, in Uzbekistan, Central Asia, and date to about 760, with some of them possibly being older. Remarkably, almost all findings of the oldest pieces come from along the Silk Road, from the former regions of the Tarim Basin (today's Xinjiang in China), Transoxiana, Sogdiana, Bactria, Gandhara, to Iran on one end and to India through Kashmir on the other.[65]\n The game reached Western Europe and Russia via at least three routes, the earliest being in the ninth century. By the year 1000, it had spread throughout both the Muslim Iberia and Latin Europe.[67] A Latin poem called Versus de scachis (\"Verses on Chess\") dated to the late 10th century, has been preserved at Einsiedeln Abbey in Switzerland.\n The game of chess was then played and known in all European countries. A famous 13th-century Spanish manuscript covering chess, backgammon, and dice is known as the Libro de los juegos, which is the earliest European treatise on chess as well as being the oldest document on European tables games.[68] The rules were fundamentally similar to those of the Arabic shatranj. The differences were mostly in the use of a checkered board instead of a plain monochrome board used by Arabs and the habit of allowing some or all pawns to make an initial double step. In some regions, the queen, which had replaced the wazir, or the king could also make an initial two-square leap under some conditions.[69]\n Around 1200, the rules of shatranj started to be modified in Europe, culminating, several major changes later, in the emergence of modern chess practically as it is known today.[70] A major change was the modern piece movement rules, which began to appear in intellectual circles in Valencia, Spain, around 1475,[note 4] which established the foundations and brought it very close to current chess. These new rules then were quickly adopted in Italy and Southern France before diffusing into the rest of Europe.[73][74] Pawns gained the ability to advance two squares on their first move, while bishops and queens acquired their modern movement powers. The queen replaced the earlier vizier chess piece toward the end of the 10th century and by the 15th century had become the most powerful piece;[75] in light of that, modern chess was often referred to at the time as \"Queen's Chess\" or \"Mad Queen Chess\".[76] Castling, derived from the \"king's leap\", usually in combination with a pawn or rook move to bring the king to safety, was introduced. These new rules quickly spread throughout Western Europe.\n Writings about chess theory began to appear in the late 15th century. An anonymous treatise on chess of 1490 with the first part containing some openings and the second 30 endgames is deposited in the library of the University of Göttingen.[77] The book El Libro dels jochs partitis dels schachs en nombre de 100 was written by Francesc Vicent in Segorbe in 1495, but no copy of this work has survived.[77] The Repetición de Amores y Arte de Ajedrez (Repetition of Love and the Art of Playing Chess) by Spanish churchman Luis Ramírez de Lucena was published in Salamanca in 1497.[74] Lucena and later masters like Portuguese Pedro Damiano, Italians Giovanni Leonardo Di Bona, Giulio Cesare Polerio and Gioachino Greco, and Spanish bishop Ruy López de Segura developed elements of opening theory and started to analyze simple endgames.\n In the 18th century, the center of European chess life moved from Southern Europe to mainland France. The two most important French masters were François-André Danican Philidor, a musician by profession, who discovered the importance of pawns for chess strategy, and later Louis-Charles Mahé de La Bourdonnais, who won a famous series of matches against Irish master Alexander McDonnell in 1834.[78] Centers of chess activity in this period were coffee houses in major European cities like Café de la Régence in Paris and Simpson's Divan in London.[79][80]\n At the same time, the intellectual movement of romanticism had had a far-reaching impact on chess, with aesthetics and tactical beauty being held in higher regard than objective soundness and strategic planning. As a result, virtually all games began with the Open Game, and it was considered unsportsmanlike to decline gambits that invited tactical play such as the King's Gambit and the Evans Gambit.[81] This chess philosophy is known as Romantic chess, and a sharp, tactical style consistent with the principles of chess romanticism was predominant until the late 19th century.[82]\n The rules concerning stalemate were finalized in the early 19th century. Also in the 19th century, the convention that White moves first was established (formerly either White or Black could move first). Finally, the rules around castling and en passant captures were standardized – variations in these rules persisted in Italy until the late 19th century. The resulting standard game is sometimes referred to as Western chess[83] or international chess,[84] particularly in Asia where other games of the chess family such as xiangqi are prevalent. Since the 19th century, the only rule changes, such as the establishment of the correct procedure for claiming a draw by repetition, have been technical in nature.\n As the 19th century progressed, chess organization developed quickly. Many chess clubs, chess books, and chess journals appeared. There were correspondence matches between cities; for example, the London Chess Club played against the Edinburgh Chess Club in 1824.[85] Chess problems became a regular part of 19th-century newspapers; Bernhard Horwitz, Josef Kling, and Samuel Loyd composed some of the most influential problems. In 1843, von der Lasa published his and Bilguer's Handbuch des Schachspiels (Handbook of Chess), the first comprehensive manual of chess theory.\n The first modern chess tournament was organized by Howard Staunton, a leading English chess player, and was held in London in 1851. It was won by the German Adolf Anderssen, who was hailed as the leading chess master. His brilliant, energetic attacking style was typical for the time.[86][87] Sparkling games like Anderssen's Immortal Game and Evergreen Game or Morphy's \"Opera Game\" were regarded as the highest possible summit of the art of chess.[88]\n Deeper insight into the nature of chess came with the American Paul Morphy, an extraordinary chess prodigy. Morphy won against all important competitors (except Staunton, who refused to play), including Anderssen, during his short chess career between 1857 and 1863. Morphy's success stemmed from a combination of brilliant attacks and sound strategy; he intuitively knew how to prepare attacks.[89]\n Prague-born Wilhelm Steinitz laid the foundations for a scientific approach to the game, the art of breaking a position down into components[90] and preparing correct plans.[91] In addition to his theoretical achievements, Steinitz founded an important tradition: his triumph over the leading German master Johannes Zukertort in 1886 is regarded as the first official World Chess Championship. This win marked a stylistic transition at the highest levels of chess from an attacking, tactical style predominant in the Romantic era to a more positional, strategic style introduced to the chess world by Steinitz. Steinitz lost his crown in 1894 to a much younger player, the German mathematician Emanuel Lasker, who maintained this title for 27 years, the longest tenure of any world champion.[92]\n After the end of the 19th century, the number of master tournaments and matches held annually quickly grew. The first Olympiad was held in Paris in 1924, and FIDE was founded initially for the purpose of organizing that event. In 1927, the Women's World Chess Championship was established; the first to hold the title was Czech-English master Vera Menchik.[93]\n A prodigy from Cuba, José Raúl Capablanca, known for his skill in endgames, won the World Championship from Lasker in 1921. Capablanca was undefeated in tournament play for eight years, from 1916 to 1924. His successor (1927) was the Russian-French Alexander Alekhine, a strong attacking player who died as the world champion in 1946. Alekhine briefly lost the title to Dutch player Max Euwe in 1935 and regained it two years later.[94]\n In the interwar period, chess was revolutionized by the new theoretical school of so-called hypermodernists like Aron Nimzowitsch and Richard Réti. They advocated controlling the center of the board with distant pieces rather than with pawns, thus inviting opponents to occupy the center with pawns, which become objects of attack.[95]\n After the death of Alekhine, a new World Champion was sought. FIDE, which has controlled the title since then, ran a tournament of elite players. The winner of the 1948 tournament was Russian Mikhail Botvinnik. In 1950, FIDE established a system of titles, conferring the title of Grandmaster on 27 players. (Some sources state that, in 1914, the title of chess Grandmaster was first formally conferred by Tsar Nicholas II of Russia to Lasker, Capablanca, Alekhine, Tarrasch, and Marshall, but this is a disputed claim.[note 5])\n Botvinnik started an era of Soviet dominance in the chess world, which mainly through the Soviet government's politically inspired efforts to demonstrate intellectual superiority over the West[96][97] stood almost uninterrupted for more than a half-century. Until the dissolution of the Soviet Union, there was only one non-Soviet champion, American Bobby Fischer (champion 1972–1975).[98] Botvinnik also revolutionized opening theory. Previously, Black strove for equality, attempting to neutralize White's first-move advantage. As Black, Botvinnik strove for the initiative from the beginning.[99] In the previous informal system of World Championships, the current champion decided which challenger he would play for the title and the challenger was forced to seek sponsors for the match. FIDE set up a new system of qualifying tournaments and matches. The world's strongest players were seeded into Interzonal tournaments, where they were joined by players who had qualified from Zonal tournaments. The leading finishers in these Interzonals would go through the \"Candidates\" stage, which was initially a tournament, and later a series of knockout matches. The winner of the Candidates would then play the reigning champion for the title. A champion defeated in a match had a right to play a rematch a year later. This system operated on a three-year cycle. Botvinnik participated in championship matches over a period of fifteen years. He won the world championship tournament in 1948 and retained the title in tied matches in 1951 and 1954. In 1957, he lost to Vasily Smyslov, but regained the title in a rematch in 1958. In 1960, he lost the title to the 23-year-old Latvian prodigy Mikhail Tal, an accomplished tactician and attacking player who is widely regarded as one of the most creative players ever,[100] hence his nickname \"the magician from Riga\". Botvinnik again regained the title in a rematch in 1961.\n Following the 1961 event, FIDE abolished the automatic right of a deposed champion to a rematch, and the next champion, Armenian Tigran Petrosian, a player renowned for his defensive and positional skills, held the title for two cycles, 1963–1969. His successor, Boris Spassky from Russia (champion 1969–1972), won games in both positional and sharp tactical style.[101] The next championship, the so-called Match of the Century, saw the first non-Soviet challenger since World War II, American Bobby Fischer. Fischer defeated his opponents in the Candidates matches by unheard-of margins, and convincingly defeated Spassky for the world championship. The match was followed closely by news media of the day, leading to a surge in popularity for chess; it also held significant political importance at the height of the Cold War, with the match being seen by both sides as a microcosm of the conflict between East and West.[102] In 1975, however, Fischer refused to defend his title against Soviet Anatoly Karpov when he was unable to reach agreement on conditions with FIDE, and Karpov obtained the title by default.[103] Fischer modernized many aspects of chess, especially by extensively preparing openings.[104]\n Karpov defended his title twice against Viktor Korchnoi and dominated the 1970s and early 1980s with a string of tournament successes.[105] In the 1984 World Chess Championship, Karpov faced his toughest challenge to date, the young Garry Kasparov from Baku, Soviet Azerbaijan. The match was aborted in controversial circumstances after 5 months and 48 games with Karpov leading by 5 wins to 3, but evidently exhausted; many commentators believed Kasparov, who had won the last two games, would have won the match had it continued. Kasparov won the 1985 rematch. Kasparov and Karpov contested three further closely fought matches in 1986, 1987 and 1990, Kasparov winning them all.[106] Kasparov became the dominant figure of world chess from the mid-1980s until his retirement from competition in 2005.\n Chess-playing computer programs (later known as chess engines) began to appear in the 1960s. In 1970, the first major computer chess tournament, the North American Computer Chess Championship, was held, followed in 1974 by the first World Computer Chess Championship. In the late 1970s, dedicated home chess computers such as Fidelity Electronics' Chess Challenger became commercially available, as well as software to run on home computers. The overall standard of computer chess was low, however, until the 1990s.\n The first endgame tablebases, which provided perfect play for relatively simple endgames such as king and rook versus king and bishop, appeared in the late 1970s. This set a precedent to the complete six- and seven-piece tablebases that became available in the 2000s and 2010s respectively.[107]\n The first commercial chess database, a collection of chess games searchable by move and position, was introduced by the German company ChessBase in 1987. [citation needed] Databases containing millions of chess games have since had a profound effect on opening theory and other areas of chess research.\n Digital chess clocks were invented in 1973, though they did not become commonplace until the 1990s. Digital clocks allow for time controls involving increments and delays.\n The Internet enabled online chess as a new medium of playing, with chess servers allowing users to play other people from different parts of the world in real time. The first such server, known as Internet Chess Server or ICS, was developed at the University of Utah in 1992. ICS formed the basis for the first commercial chess server, the Internet Chess Club, which was launched in 1995, and for other early chess servers such as FICS (Free Internet Chess Server). Since then, many other platforms have appeared, and online chess began to rival over-the-board chess in popularity.[108][109] During the 2020 COVID-19 pandemic, the isolation ensuing from quarantines imposed in many places around the world, combined with the success of the popular Netflix show The Queen's Gambit and other factors such as the popularity of online tournaments (notably PogChamps) and chess Twitch streamers, resulted in a surge of popularity not only for online chess, but for the game of chess in general; this phenomenon has been referred to in the media as the 2020 online chess boom.[110][111]\n Computer chess has also seen major advances. By the 1990s, chess engines could consistently defeat most amateurs, and in 1997 Deep Blue defeated World Champion Garry Kasparov in a six-game match, starting an era of computer dominance at the highest level of chess. In the 2010s, engines significantly stronger than even the best human players became accessible for free on a number of PC and mobile platforms, and free engine analysis became a commonplace feature on internet chess servers. An adverse effect of the easy availability of engine analysis on hand-held devices and personal computers has been the rise of computer cheating, which has grown to be a major concern in both over-the-board and online chess.[112] In 2017, AlphaZero – a neural network also capable of playing shogi and Go – was introduced. Since then, many chess engines based on neural network evaluation have been written, the best of which have surpassed the traditional \"brute-force\" engines. AlphaZero also introduced many novel ideas and ways of playing the game, which affected the style of play at the top level.[113]\n As endgame tablebases developed, they began to provide perfect play in endgame positions in which the game-theoretical outcome was previously unknown, such as positions with king, queen and pawn against king and queen. In 1991, Lewis Stiller published a tablebase for select six-piece endgames,[114][115] and by 2005, following the publication of Nalimov tablebases, all six-piece endgame positions were solved. In 2012, Lomonosov tablebases were published which solved all seven-piece endgame positions.[116] Use of tablebases enhances the performance of chess engines by providing definitive results in some branches of analysis.\n Technological progress made in the 1990s and the 21st century has influenced the way that chess is studied at all levels, as well as the state of chess as a spectator sport.\n Previously, preparation at the professional level required an extensive chess library and several subscriptions to publications such as Chess Informant to keep up with opening developments and study opponents' games. Today, preparation at the professional level involves the use of databases containing millions of games, and engines to analyze different opening variations and prepare novelties.[117] A number of online learning resources are also available for players of all levels, such as online courses, tactics trainers, and video lessons.[118]\n Since the late 1990s, it has been possible to follow major international chess events online, the players' moves being relayed in real time. Sensory boards have been developed to enable automatic transmission of moves. Chess players will frequently run engines while watching these games, allowing them to quickly identify mistakes by the players and spot tactical opportunities. While in the past the moves have been relayed live, today chess organizers will often impose a half-hour delay as an anti-cheating measure. In the mid-to-late 2010s – and especially following the 2020 online boom – it became commonplace for supergrandmasters, such as Hikaru Nakamura and Magnus Carlsen, to livestream chess content on platforms such as Twitch.[119][120] Also following the boom, online chess started being viewed as an esport, with esport teams signing chess players for the first time in 2020.[121]\n Organized chess even for young children has become common. FIDE holds world championships for age levels down to 8 years old. The largest tournaments, in number of players, are those held for children.[122]\n The number of grandmasters and other chess professionals has also grown in the modern era. Kenneth Regan and Guy Haworth conducted research involving comparison of move choices by players of different levels and from different periods with the analysis of strong chess engines. They concluded that the increase in the number of grandmasters and higher Elo ratings of the top players reflect an actual increase in the average standard of play, rather than \"rating inflation\" or \"title inflation\".[123]\n In 1993, Garry Kasparov and Nigel Short broke ties with FIDE to organize their own match for the World Championship and formed a competing Professional Chess Association (PCA). From then until 2006, there were two simultaneous World Championships and respective World Champions: the PCA or \"classical\" champions extending the Steinitzian tradition in which the current champion plays a challenger in a series of games, and the other following FIDE's new format of many players competing in a large knockout tournament to determine the champion. Kasparov lost his PCA title in 2000 to Vladimir Kramnik of Russia.[124] Due to the complicated state of world chess politics and difficulties obtaining commercial sponsorships, Kasparov was never able to challenge for the title again. Despite this, he continued to dominate in top level tournaments and remained the world's highest rated player until his retirement from competitive chess in 2005.\n The World Chess Championship 2006, in which Kramnik beat the FIDE World Champion Veselin Topalov, reunified the titles and made Kramnik the undisputed World Chess Champion.[125] In September 2007, he lost the title to Viswanathan Anand of India, who won the championship tournament in Mexico City. Anand defended his title in the revenge match of 2008,[126] 2010 and 2012. Magnus Carlsen defeated Anand in the 2013, defending his title in 2014, 2016, 2018, and 2021, whereafter he announced that he would not defend his title a fifth time. The 2023 championship was played between the winner and runner-up of the Candidates Tournament 2022: Ian Nepomniachtchi of Russia and Ding Liren of China. Ding beat Nepomniachtchi, making him the world champion.[37] In 2024, Indian Gukesh Dommaraju beat Ding.\n In the Middle Ages and during the Renaissance, chess was a part of noble culture; it was used to teach war strategy and was dubbed the \"King's Game\".[127] Gentlemen are \"to be meanly seene in the play at Chestes\", says the overview at the beginning of Baldassare Castiglione's The Book of the Courtier (1528, English 1561 by Sir Thomas Hoby), but chess should not be a gentleman's main passion. Castiglione explains it further:\n And what say you to the game at chestes? It is truely an honest kynde of enterteynmente and wittie, quoth Syr Friderick. But me think it hath a fault, whiche is, that a man may be to couning at it, for who ever will be excellent in the playe of chestes, I beleave he must beestowe much tyme about it, and applie it with so much study, that a man may assoone learne some noble scyence, or compase any other matter of importaunce, and yet in the ende in beestowing all that laboure, he knoweth no more but a game. Therfore in this I beleave there happeneth a very rare thing, namely, that the meane is more commendable, then the excellency.[128] Some of the elaborate chess sets used by the aristocracy at least partially survive, such as the Lewis chessmen.\n Chess was often used as a basis of sermons on morality. An example is Liber de moribus hominum et officiis nobilium sive super ludo scacchorum ('Book of the customs of men and the duties of nobles or the Book of Chess'), written by an Italian Dominican friar Jacobus de Cessolis c. 1300. This book was one of the most popular of the Middle Ages.[129] The work was translated into many other languages (the first printed edition was published at Utrecht in 1473) and was the basis for William Caxton's The Game and Playe of the Chesse (1474), one of the first books printed in English.[130] Different chess pieces were used as metaphors for different classes of people, and human duties were derived from the rules of the game or from visual properties of the chess pieces:[131]\n The knyght ought to be made alle armed upon an hors in suche wyse that he haue an helme on his heed and a spere in his ryght hande/ and coueryd wyth his sheld/ a swerde and a mace on his lyft syde/ Cladd wyth an hawberk and plates to fore his breste/ legge harnoys on his legges/ Spores on his heelis on his handes his gauntelettes/ his hors well broken and taught and apte to bataylle and couerid with his armes/ whan the knyghtes ben maad they ben bayned or bathed/ that is the signe that they shold lede a newe lyf and newe maners/ also they wake alle the nyght in prayers and orysons vnto god that he wylle gyue hem grace that they may gete that thynge that they may not gete by nature/ The kynge or prynce gyrdeth a boute them a swerde in signe/ that they shold abyde and kepe hym of whom they take theyr dispenses and dignyte.[132] Known in the circles of clerics, students, and merchants, chess entered into the popular culture of the Middle Ages. An example is the 209th song of Carmina Burana from the 13th century, which starts with the names of chess pieces, Roch, pedites, regina...[133] The game of chess, at times, has been discouraged by various religious authorities in Middle Ages: Jewish,[134] Catholic and Orthodox.[135] Some Muslim authorities prohibited it even recently, for example Ruhollah Khomeini in 1979 and Abdul-Aziz ash-Sheikh even later.[136]\n During the Age of Enlightenment, chess was viewed as a means of self-improvement. Benjamin Franklin, in his article \"The Morals of Chess\" (1786), wrote:\n The Game of Chess is not merely an idle amusement; several very valuable qualities of the mind, useful in the course of human life, are to be acquired and strengthened by it, so as to become habits ready on all occasions; for life is a kind of Chess, in which we have often points to gain, and competitors or adversaries to contend with, and in which there is a vast variety of good and ill events, that are, in some degree, the effect of prudence, or the want of it. By playing at Chess then, we may learn:\n I. Foresight, which looks a little into futurity, and considers the consequences that may attend an action ...\n II. Circumspection, which surveys the whole Chess-board, or scene of action: – the relation of the several Pieces, and their situations ...\n III. Caution, not to make our moves too hastily ...[137]\n Chess was occasionally criticized in the 19th century as a waste of time.[138][139]\n Chess is taught to children in schools around the world today. Many schools host chess clubs, and there are many scholastic tournaments specifically for children. Tournaments are held regularly in many countries, hosted by organizations such as the United States Chess Federation and the National Scholastic Chess Foundation.[140]\n Chess is many times depicted in the arts; significant works where chess plays a key role range from Thomas Middleton's A Game at Chess to Through the Looking-Glass by Lewis Carroll, to Vladimir Nabokov's The Defense, to The Royal Game by Stefan Zweig. Chess has also featured in film classics such as Ingmar Bergman's The Seventh Seal, Satyajit Ray's The Chess Players, and Powell and Pressburger's A Matter of Life and Death.\n Chess is also present in contemporary popular culture. For example, the characters in Star Trek play a futuristic version of the game called \"Federation Tri-Dimensional Chess\",[141] and \"Wizard's Chess\" is played in J.K. Rowling's Harry Potter.[142]\n The game structure and nature of chess are related to several branches of mathematics. Many combinatorical and topological problems connected to chess, such as the knight's tour and the eight queens puzzle, have been known for hundreds of years.\n The number of legal positions in chess is estimated to be (4.59 ± 0.38) × 1044 with a 95% confidence level,[143] with a game-tree complexity of approximately 10123. The game-tree complexity of chess was first calculated by Claude Shannon as 10120, a number known as the Shannon number.[144] An average position typically has thirty to forty possible moves, but there may be as few as zero (in the case of checkmate or stalemate) or (in a constructed position) as many as 218.[145]\n In 1913, Ernst Zermelo used chess as a basis for his theory of game strategies, which is considered one of the predecessors of game theory.[146] Zermelo's theorem states that it is possible to solve chess, i.e. to determine with certainty the outcome of a perfectly played game (either White can force a win, or Black can force a win, or both sides can force at least a draw).[147] With 1043 legal positions in chess, however, it will take an impossibly long time to compute a perfect strategy with any feasible technology.[148]\n There is an extensive scientific literature on chess psychology.[note 6][150][151][152][153] Alfred Binet and others showed that knowledge and verbal, rather than visuospatial, ability lies at the core of expertise.[154][155] In his doctoral thesis, Adriaan de Groot showed that chess masters can rapidly perceive the key features of a position.[156] According to de Groot, this perception, made possible by years of practice and study, is more important than the sheer ability to anticipate moves. De Groot showed that chess masters can memorize positions shown for a few seconds almost perfectly. The ability to memorize does not alone account for chess-playing skill, since masters and novices, when faced with random arrangements of chess pieces, had equivalent recall (about six positions in each case). Rather, it is the ability to recognize patterns, which are then memorized, which distinguished the skilled players from the novices. When the positions of the pieces were taken from an actual game, the masters had almost total positional recall.[157]\n More recent research has focused on chess as mental training; the respective roles of knowledge and look-ahead search; brain imaging studies of chess masters and novices; blindfold chess; the role of personality and intelligence in chess skill; gender differences; and computational models of chess expertise. The role of practice and talent in the development of chess and other domains of expertise has led to much empirical investigation. Ericsson and colleagues have argued that deliberate practice is sufficient for reaching high levels of expertise in chess.[158] Recent research, however, fails to replicate their results and indicates that factors other than practice are also important.[159][160]\nFor example, Fernand Gobet and colleagues have shown that stronger players started playing chess at a young age and that experts born in the Northern Hemisphere are more likely to have been born in late winter and early spring. Compared to the general population, chess players are more likely to be non-right-handed, though they found no correlation between handedness and skill.[160]\n A relationship between chess skill and intelligence has long been discussed in scientific literature as well as in popular culture. Academic studies that investigate the relationship date back at least to 1927.[161] Although one meta-analysis and most children studies find a positive correlation between general cognitive ability and chess skill, adult studies show mixed results.[162][163]\n Chess composition is the art of creating chess problems (also called chess compositions). The creator is known as a chess composer.[165] There are many types of chess problems; the two most important are:\n Fairy chess is a branch of chess problem composition involving altered rules, such as the use of unconventional pieces or boards, or unusual stipulations such as reflexmates.\n Tournaments for composition and solving of chess problems are organized by the World Federation for Chess Composition, which works cooperatively with but independent of FIDE. The WFCC awards titles for composing and solving chess problems.[168]\n Online chess is chess played over the internet. This is done through the use of internet chess platforms, which use Elo ratings or similar systems to pair up individual players. Online chess saw a spike in growth during the quarantines of the COVID-19 pandemic.[169][170] This can be attributed to both isolation and the popularity of Netflix miniseries The Queen's Gambit, which was released in October 2020.[169][170] Chess app downloads on the App Store and Google Play Store rose by 63% after the show debuted.[171] Chess.com saw more than twice as many account registrations in November as it had in previous months, and the number of games played monthly on Lichess doubled as well. There was also a demographic shift in players, with female registration on Chess.com shifting from 22% to 27% of new players.[172] GM Maurice Ashley said \"A boom is taking place in chess like we have never seen maybe since the Bobby Fischer days\", attributing the growth to an increased desire to do something constructive during the pandemic.[173] USCF Women's Program Director Jennifer Shahade stated that chess works well on the internet, since pieces do not need to be reset and matchmaking is virtually instant.[174]\n The idea of creating a chess-playing machine dates to the 18th century; around 1769, the chess-playing automaton called The Turk became famous before being exposed as a hoax.[175] Serious trials based on automata, such as El Ajedrecista, were too complex and limited to be useful. Since the advent of the digital computer in the 1950s, chess enthusiasts, computer engineers, and computer scientists have built, with increasing degrees of seriousness and success, chess-playing machines and computer programs.[176] The groundbreaking paper on computer chess, \"Programming a Computer for Playing Chess\", was published in 1950 by Claude Shannon.[note 7] He wrote:\n The chess machine is an ideal one to start with, since: (1) the problem is sharply defined both in allowed operations (the moves) and in the ultimate goal (checkmate); (2) it is neither so simple as to be trivial nor too difficult for satisfactory solution; (3) chess is generally considered to require \"thinking\" for skillful play; a solution of this problem will force us either to admit the possibility of a mechanized thinking or to further restrict our concept of \"thinking\"; (4) the discrete structure of chess fits well into the digital nature of\nmodern computers.[178]\n The Association for Computing Machinery (ACM) held the first major chess tournament for computers, the North American Computer Chess Championship, in September 1970. CHESS 3.0, a chess program from Northwestern University, won the championship. The first World Computer Chess Championship, held in 1974, was won by the Soviet program Kaissa. At first considered only a curiosity, the best chess playing programs have become extremely strong. In 1997, a computer won a chess match using classical time controls against a reigning World Champion for the first time: IBM's Deep Blue beat Garry Kasparov 3½–2½ (it scored two wins, one loss, and three draws).[179][180] There was some controversy over the match,[181] and human–computer matches were relatively close over the next few years, until convincing computer victories in 2005 and in 2006.\n In 2009, a mobile phone won a category 6 tournament with a performance rating of 2898: chess engine Hiarcs 13 running on the mobile phone HTC Touch HD won the Copa Mercosur tournament with nine wins and one draw.[182] The best chess programs are now able to consistently beat the strongest human players, to the extent that human–computer matches no longer attract interest from chess players or the media.[183] While the World Computer Chess Championship still exists, the Top Chess Engine Championship (TCEC) is widely regarded as the unofficial world championship for chess engines.[184][185][186] The current champion is Stockfish.\n With huge databases of past games and high analytical ability, computers can help players to learn chess and prepare for matches. Internet Chess Servers allow people to find and play opponents worldwide. The presence of computers and modern communication tools have raised concerns regarding cheating during games.[187]\n Related games include:\n In the comparison of chess with games often referred to as national forms of chess, chess may be referred to as western chess or international chess.[188][189]\n There are more than two thousand published chess variants, games with similar but different rules.[190] Most of them are of relatively recent origin.[191] They include:\n In the context of chess variants, chess is commonly referred to as orthodox chess, orthochess, and classic chess.[193][194]\n"
    },
    {
        "title": "Go (game)",
        "url": "https://en.wikipedia.org/wiki/Go_(game)",
        "content": "Go is an abstract strategy board game for two players in which the aim is to fence off more territory than the opponent. The game was invented in China more than 2,500 years ago and is believed to be the oldest board game continuously played to the present day.[1][2][3][4][5] A 2016 survey by the International Go Federation's 75 member nations found that there are over 46 million people worldwide who know how to play Go, and over 20 million current players, the majority of whom live in East Asia.[6]\n The playing pieces are called stones. One player uses the white stones and the other black. The players take turns placing their stones on the vacant intersections (points) on the board. Once placed, stones may not be moved, but captured stones are immediately removed from the board. A single stone (or connected group of stones) is captured when surrounded by the opponent's stones on all orthogonally adjacent points.[7] The game proceeds until neither player wishes to make another move.\n When a game concludes, the winner is determined by counting each player's surrounded territory along with captured stones and komi (points added to the score of the player with the white stones as compensation for playing second).[8] Games may also end by resignation.[9]\n The standard Go board has a 19×19 grid of lines, containing 361 points. Beginners often play on smaller 9×9 or 13×13 boards,[10] and archaeological evidence shows that the game was played in earlier centuries on a board with a 17×17 grid. Boards with a 19×19 grid had become standard, however, by the time the game reached Korea in the 5th century CE and Japan in the 7th century CE.[11]\n Go was considered one of the four essential arts of the cultured aristocratic Chinese scholars in antiquity. The earliest written reference to the game is generally recognized as the historical annal Zuo Zhuan[12][13] (c. 4th century BCE).[14]\n Despite its relatively simple rules, Go is extremely complex. Compared to chess, Go has both a larger board with more scope for play and longer games and, on average, many more alternatives to consider per move. The number of legal board positions in Go has been calculated to be approximately 2.1×10170,[15][a] which is far greater than the number of atoms in the observable universe, which is estimated to be on the order of 1080.[17]\n The name Go is a short form of the Japanese word igo (囲碁; いご), which derives from earlier wigo (ゐご), in turn from Middle Chinese ɦʉi gi (圍棋, Mandarin: wéiqí, lit. 'encirclement board game'  or 'board game of surrounding'). In English, the name Go when used for the game is often capitalized to differentiate it from the common word go.[18] In events sponsored by the Ing Chang-ki Foundation, it is spelled goe.[19]\n The Korean name baduk (바둑) derives from the Middle Korean word Badok, the origin of which is controversial; the more plausible etymologies include the suffix dok added to Ba to mean 'flat and wide board', or the joining of Bat, meaning 'field', and Dok, meaning 'stone'. Less plausible etymologies include a derivation of Badukdok, referring to the playing pieces of the game, or a derivation from Chinese páizi (排子), meaning 'to arrange pieces'.[20]\n Go is an adversarial game between two players with the objective of capturing territory. That is, occupying and surrounding a larger total empty area of the board with one's stones than the opponent.[21] As the game progresses, the players place stones on the board creating stone \"formations\" and enclosing spaces. Stones are never moved on the board, but when \"captured\" are removed from the board. Stones are linked together into a formation by being adjacent along the black lines, not on diagonals (of which there are none). Contests between opposing formations are often extremely complex and may result in the expansion, reduction, or wholesale capture and loss of formations and their enclosed empty spaces (called \"eyes\"). Another essential component of the game is control of the sente (that is, controlling the offense, so that one's opponent is forced into defensive moves); this usually changes several times during play.\n Initially the board is bare, and players alternate turns to place one stone per turn. As the game proceeds, players try to link their stones together into \"living\" formations (meaning that they are permanently safe from capture), as well as threaten to capture their opponent's stones and formations. Stones have both offensive and defensive characteristics, depending on the situation.\n An essential concept is that a formation of stones must have, or be capable of making, at least two enclosed open points known as eyes to preserve itself from being captured. A formation having at least two eyes cannot be captured, even after it is surrounded by the opponent on the outside,[23] because each eye constitutes a liberty that must be filled by the opponent as the final step in capture. A formation having two or more eyes is said to be unconditionally alive,[24] so it can evade capture indefinitely, and a group that cannot form two eyes is said to be dead and can be captured. \n The general strategy is to place stones to fence-off territory, attack the opponent's weak groups (trying to kill them so they will be removed), and always stay mindful of the life status of one's own groups.[25][26] The liberties of groups are countable. Situations where mutually opposing groups must capture each other or die are called capturing races, or semeai.[27] In a capturing race, the group with more liberties will ultimately be able to capture the opponent's stones.[27][28][b] Capturing races and the elements of life or death are the primary challenges of Go.\n In the end game players may pass rather than place a stone if they think there are no further opportunities for profitable play.[29] The game ends when both players pass[30] or when one player resigns. In general, to score the game, each player counts the number of unoccupied points surrounded by their stones and then subtracts the number of stones that were captured by the opponent. The player with the greater score (after adjusting for handicapping called komi) wins the game.\n In the opening stages of the game, players typically establish groups of stones (or bases) near the corners and around the sides of the board, usually starting on the third or fourth line in from the board edge rather than at the very edge of the board. The edges and corners make it easier to develop groups which have better options for life (self-viability for a group of stones that prevents capture) and establish formations for potential territory.[31] Players usually start near the corners because establishing territory is easier with the aid of two edges of the board.[32] Established corner opening sequences are called joseki and are often studied independently.[33] However, in the mid-game, stone groups must also reach in towards the large central area of the board to capture more territory.\n Dame are points that lie in between the boundary walls of black and white, and as such are considered to be of no value to either side. Seki are mutually alive pairs of white and black groups where neither has two eyes.\n Ko (Chinese and Japanese: 劫) is a potentially indefinitely repeated stone-capture position. The rules do not allow a board position to be repeated. Therefore, any move which would restore the previous board position would not be allowed, and the next player would be forced to play somewhere else. If the play requires a strategic response by the first player, further changing the board, then the second player could \"retake the ko,\" and the first player would be in the same situation of needing to change the board before trying to take the ko back. And so on. [34] Some of these ko fights may be important and decide the life of a large group, while others may be worth just one or two points. Some ko fights are referred to as picnic kos when only one side has a lot to lose.[35] In Japanese, it is called a hanami ko.[36]\n Playing with others usually requires a knowledge of each player's strength, indicated by the player's rank (increasing from 30 kyu to 1 kyu, then 1 dan to 7 dan, then 1 dan pro to 9 dan pro). A difference in rank may be compensated by a handicap—Black is allowed to place two or more stones on the board to compensate for White's greater strength.[37][38] There are different rulesets (Korean, Japanese, Chinese, AGA, etc.), which are almost entirely equivalent, except for certain special-case positions and the method of scoring at the end.\n Basic strategic aspects include the following:\n The strategy involved can become very abstract and complex. High-level players spend years improving their understanding of strategy, and a novice may play many hundreds of games against opponents before being able to win regularly.\n Strategy deals with global influence, the interaction between distant stones, keeping the whole board in mind during local fights, and other issues that involve the overall game. It is therefore possible to allow a tactical loss when it confers a strategic advantage.\n Novices often start by randomly placing stones on the board, as if it were a game of chance. An understanding of how stones connect for greater power develops, and then a few basic common opening sequences may be understood. Learning the ways of life and death helps in a fundamental way to develop one's strategic understanding of weak groups.[c] A player who both plays aggressively and can handle adversity is said to display kiai, or fighting spirit, in the game.\n In the opening of the game, players usually play and gain territory in the corners of the board first, as the presence of two edges makes it easier for them to surround territory and establish the eyes they need.[39] From a secure position in a corner, it is possible to lay claim to more territory by extending along the side of the board.[40] The opening is the most theoretically difficult part of the game and takes a large proportion of professional players' thinking time.[41][42] The first stone played at a corner of the board is generally placed on the third or fourth line from the edge. Players tend to play on or near the 4–4 star point during the opening. Playing nearer to the edge does not produce enough territory to be efficient, and playing further from the edge does not safely secure the territory.[43]\n In the opening, players often play established sequences called joseki, which are locally balanced exchanges;[44] however, the joseki chosen should also produce a satisfactory result on a global scale. It is generally advisable to keep a balance between territory and influence. Which of these gets precedence is often a matter of individual taste.\n The middle phase of the game is the most combative, and usually lasts for more than 100 moves. During the middlegame, the players invade each other's territories, and attack formations that lack the necessary two eyes for viability. Such groups may be saved or sacrificed for something more significant on the board.[45] It is possible that one player may succeed in capturing a large weak group of the opponent's, which often proves decisive and ends the game by a resignation. However, matters may be more complex yet, with major trade-offs, apparently dead groups reviving, and skillful play to attack in such a way as to construct territories rather than kill.[46]\n The end of the middlegame and transition to the endgame is marked by a few features. Near the end of a game, play becomes divided into localized fights that do not affect each other,[47] with the exception of ko fights, where before the central area of the board related to all parts of it. No large weak groups are still in serious danger. Moves can reasonably be attributed some definite value, such as 20 points or fewer, rather than simply being necessary to compete. Both players set limited objectives in their plans, in making or destroying territory, capturing or saving stones. These changing aspects of the game usually occur at much the same time, for strong players. In brief, the middlegame switches into the endgame when the concepts of strategy and influence need reassessment in terms of concrete final results on the board.\n Aside from the order of play (alternating moves, Black moves first or takes a handicap) and scoring rules, there are essentially only two rules in Go:\n Almost all other information about how the game is played is heuristic, meaning it is learned information about how the patterns of the stones on the board function, rather than a rule. Other rules are specialized, as they come about through different rulesets, but the above two rules cover almost all of any played game.\n Although there are some minor differences between rulesets used in different countries,[48] most notably in Chinese and Japanese scoring rules,[49] these differences do not greatly affect the tactics and strategy of the game.\n Except where noted, the basic rules presented here are valid independent of the scoring rules used. The scoring rules are explained separately. Go terms for which there is no ready English equivalent are commonly called by their Japanese names.\n The two players, Black and White, take turns placing stones of their color on the intersections of the board, one stone at a time. The usual board size is a 19×19 grid, but for beginners or for playing quick games,[50] the smaller board sizes of 13×13[51] and 9×9 are also popular.[52]\nThe board is empty to begin with.[53] Black plays first unless given a handicap of two or more stones, in which case White plays first. The players may choose any unoccupied intersection to play on except for those forbidden by the ko and suicide rules (see below). Once played, a stone can never be moved and can be taken off the board only if it is captured.[54] A player may pass their turn, declining to place a stone, though this is usually only done at the end of the game when both players believe nothing more can be accomplished with further play. When both players pass consecutively, the game ends[55] and is then scored.\n Vertically and horizontally adjacent stones of the same color form a chain (also called a string or group),[56] forming a discrete unit that cannot then be divided.[57] Only stones connected to one another by the lines on the board create a chain; stones that are diagonally adjacent are not connected. Chains may be expanded by placing additional stones on adjacent intersections, and they can be connected together by placing a stone on an intersection that is adjacent to two or more chains of the same color.[58]\n A vacant point adjacent to a stone, along one of the grid lines of the board, is called a liberty for that stone.[59][60] Stones in a chain share their liberties.[56] A chain of stones must have at least one liberty to remain on the board. When a chain is surrounded by opposing stones so that it has no liberties, it is captured and removed from the board.[61]\n An example of a situation in which the ko rule applies\n Players are not allowed to make a move that returns the game to the immediately prior position. This rule, called the ko rule, prevents unending repetition (a stalemate).[62] As shown in the example pictured: White had a stone where the red circle was, and Black has just captured it by playing a stone at 1 (so the White stone has been removed). However, it is readily apparent that now Black's stone at 1 is immediately threatened by the three surrounding White stones. If White were allowed to play again on the red circle, it would return the situation to the original one, but the ko rule forbids that kind of endless repetition. Thus, White is forced to move elsewhere, or pass. If White wants to recapture Black's stone at 1, White must attack Black somewhere else on the board so forcefully that Black moves elsewhere to counter that, giving White that chance. If White's forcing move is successful, it is termed \"gaining the sente\"; if Black responds elsewhere on the board, then White can retake Black's stone at 1, and the ko continues, but this time Black must move elsewhere. A repetition of such exchanges is called a ko fight.[63] To stop the potential for ko fights, two stones of the same color would need to be added to the group, making either a group of 5 Black or 5 White stones.\n While the various rulesets agree on the ko rule prohibiting returning the board to an immediately previous position, they deal in different ways with the relatively uncommon situation in which a player might recreate a past position that is further removed. See Rules of Go § Repetition for further information.\n A player may not place a stone such that it or its group immediately has no liberties unless doing so immediately deprives an enemy group of its final liberty. In the second case, the enemy group is captured, leaving the new stone with at least one liberty, so the new stone can be placed.[66] This rule is responsible for the all-important difference between one and two eyes: if a group with only one eye is fully surrounded on the outside, it can be killed with a stone placed in its single eye. (An eye is an empty point or group of points surrounded by a group of stones).\n The Ing and New Zealand rules do not have this rule,[67] and there a player might destroy one of its own groups (commit suicide). This play would only be useful in limited sets of situations involving a small interior space or planning.[68] In the example at right, it may be useful as a ko threat.\n Because Black has the advantage of playing the first move, the idea of awarding White some compensation came into being during the 20th century. This is called komi, which gives white a 5.5-point compensation under Japanese rules, 6.5-point under Korean rules, and 15/4 stones, or 7.5-point under Chinese rules (number of points varies by rule set).[69] Under handicap play, White receives only a 0.5-point komi, to break a possible tie (jigo).\n Two general types of scoring procedures are used, and players determine which to use before play. Both procedures almost always give the same winner.\n Both procedures are counted after both players have passed consecutively, the stones that are still on the board but unable to avoid capture, called dead stones, are removed. Given that the number of stones a player has on the board is directly related to the number of prisoners their opponent has taken, the resulting net score, that is, the difference between Black's and White's scores is identical under both rulesets (unless the players have passed different numbers of times during the course of the game). Thus, the net result given by the two scoring systems rarely differs by more than a point.[71]\n While not actually mentioned in the rules of Go (at least in simpler rule sets, such as those of New Zealand and the U.S.), the concept of a living group of stones is necessary for a practical understanding of the game.[72]\n Examples of eyes (marked). The black groups at the top of the board are alive, as they have at least two eyes. The black groups at the bottom are dead as they only have one eye. The point marked a is a false eye, thus the black group with false eye a can be killed by white in two turns.\n When a group of stones is mostly surrounded and has no options to connect with friendly stones elsewhere, the status of the group is either alive, dead or unsettled. A group of stones is said to be alive if it cannot be captured, even if the opponent is allowed to move first. Conversely, a group of stones is said to be dead if it cannot avoid capture, even if the owner of the group is allowed the first move. Otherwise, the group is said to be unsettled: the defending player can make it alive or the opponent can kill it, depending on who gets to play first.[72]\n An eye is an empty point or group of points surrounded by a group of stones. If the eye is surrounded by Black stones, White cannot play there unless such a play would take Black's last liberty and capture the Black stones. (Such a move is forbidden according to the suicide rule in most rule sets, but even if not forbidden, such a move would be a useless suicide of a White stone.)\n If a Black group has two eyes, White can never capture it because White cannot remove both liberties simultaneously. If Black has only one eye, White can capture the Black group by playing in the single eye, removing Black's last liberty. Such a move is not suicide because the Black stones are removed first. In the \"Examples of eyes\" diagram, all the circled points are eyes. The two black groups in the upper corners are alive, as both have at least two eyes. The groups in the lower corners are dead, as both have only one eye. The group in the lower left may seem to have two eyes, but the surrounded empty point marked a is not actually an eye. White can play there and take a black stone. Such a point is often called a false eye.[72]\n There is an exception to the requirement that a group must have two eyes to be alive, a situation called seki (or mutual life). Where different colored groups are adjacent and share liberties, the situation may reach a position when neither player wants to move first because doing so would allow the opponent to capture; in such situations therefore both players' stones remain on the board (in seki). Neither player receives any points for those groups, but at least those groups themselves remain living, as opposed to being captured.[e]\n Seki can occur in many ways. The simplest are:\n In the \"Example of seki (mutual life)\" diagram, the two circled points are liberties shared by both a black and a white group. Both of these interior groups are at risk, and neither player wants to play on a circled point, because doing so would allow the opponent to capture their group on the next move. The outer groups in this example, both black and white, are alive. Seki can result from an attempt by one player to invade and kill a nearly settled group of the other player.[72]\n Tactics deal with immediate fighting between stones, capturing and saving stones, life, death and other issues localized to a specific part of the board. Larger issues which encompass the territory of the entire board and planning stone-group connections are referred to as Strategy and are covered in the Strategy section above.\n There are several tactical constructs aimed at capturing stones.[73] These are among the first things a player learns after understanding the rules. Recognizing the possibility that stones can be captured using these techniques is an important step forward.\n A ladder. Black cannot escape unless the ladder connects to black stones further down the board that will intercept with the ladder or if one of white's pieces has only one liberty.\n The most basic technique is the ladder.[74] This is also sometimes called a \"running attack\", since it unfolds as one player trying to outrun the other's attack. To capture stones in a ladder, a player uses a constant series of capture threats (atari), giving the opponent only one place to place his stone to keep his group alive. This forces the opponent to move into a zigzag pattern (surrounding the ladder on the outside) as shown in the adjacent diagram to keep the attack coming. Unless the pattern runs into friendly stones along the way, the stones in the ladder cannot avoid capture. However, if the ladder can run into other black stones, thus saving them, then experienced players recognize the futility of continuing the attack. These stones can also be saved if a suitably strong threat can be forced elsewhere on the board, so that two Black stones can be placed here to save the group.\n A net. The chain of three marked Black stones cannot escape in any direction, since each Black stone attempting to extend the chain outward (on the red circles) can be easily blocked by one White stone.\n Another technique to capture stones is the so-called net,[75] also known by its Japanese name, geta. This refers to a move that loosely surrounds some stones, preventing their escape in all directions. An example is given in the adjacent diagram. It is often better to capture stones in a net than in a ladder, because a net does not depend on the condition that there are no opposing stones in the way, nor does it allow the opponent to play a strategic ladder breaker. However, the ladder only requires one turn to kill all the opponent's stones, whereas a net requires more turns to do the same.\n A snapback. Although Black can capture the white stone by playing at the circled point, the resulting shape for Black has only one liberty (at 1), thus White can then capture the three black stones by playing at 1 again (snapback).\n A third technique to capture stones is the snapback.[76] In a snapback, one player allows a single stone to be captured, then immediately plays on the point formerly occupied by that stone; by so doing, the player captures a larger group of their opponent's stones, in effect snapping back at those stones. An example can be seen on the right. As with the ladder, an experienced player does not play out such a sequence, recognizing the futility of capturing only to be captured back immediately.\n One of the most important skills required for strong tactical play is the ability to read ahead.[77] Reading ahead includes considering available moves to play, the possible responses to each move, and the subsequent possibilities after each of those responses. Some of the strongest players of the game can read up to 40 moves ahead even in complicated positions.[78]\n As explained in the scoring rules, some stone formations can never be captured and are said to be alive, while other stones may be in a position where they cannot avoid being captured and are said to be dead. Much of the practice material available to players of the game comes in the form of life and death problems, also known as tsumego.[79] In such problems, players are challenged to find the vital move sequence that kills a group of the opponent or saves a group of their own. Tsumego are considered an excellent way to train a player's ability at reading ahead,[79] and are available for all skill levels, some posing a challenge even to top players.\n In situations when the Ko rule applies, a ko fight may occur.[63] If the player who is prohibited from capture is of the opinion that the capture is important because it prevents a large group of stones from being captured for instance, the player may play a ko threat.[63] This is a move elsewhere on the board that threatens to make a large profit if the opponent does not respond. If the opponent does respond to the ko threat, the situation on the board has changed, and the prohibition on capturing the ko no longer applies. Thus the player who made the ko threat may now recapture the ko. Their opponent is then in the same situation and can either play a ko threat as well or concede the ko by simply playing elsewhere. If a player concedes the ko, either because they do not think it important or because there are no moves left that could function as a ko threat, they have lost the ko, and their opponent may connect the ko.\n Instead of responding to a ko threat, a player may also choose to ignore the threat and connect the ko.[63] They thereby win the ko, but at a cost. The choice of when to respond to a threat and when to ignore it is a subtle one, which requires a player to consider many factors, including how much is gained by connecting, how much is lost by not responding, how many possible ko threats both players have remaining, what the optimal order of playing them is, and what the size—points lost or gained—of each of the remaining threats is.[80]\n Frequently, the winner of the ko fight does not connect the ko but instead captures one of the chains that constituted their opponent's side of the ko.[63] In some cases, this leads to another ko fight at a neighboring location.\n The earliest written reference to the game is generally recognized as the historical annal Zuo Zhuan[12][13] (c. 4th century BCE),[14] referring to a historical event of 548 BCE. It is also mentioned in Book XVII of the Analects of Confucius[14] and in two books written by Mencius[13][81] (c. 3rd century BCE).[14] In all of these works, the game is referred to as yì (弈). Today, in China, it is known as weiqi (simplified Chinese: 围棋; traditional Chinese: 圍棋; pinyin: wéiqíⓘ; Wade–Giles: wei ch'i), lit. 'encirclement board game'.\n Go was originally played on a 17×17 line grid, but a 19×19 grid became standard by the time of the Tang dynasty (618–907 CE).[13] Legends trace the origin of the game to the mythical Chinese emperor Yao (2337–2258 BCE), who was said to have had his counselor Shun design it for his unruly son, Danzhu, to favorably influence him.[82][83] Other theories suggest that the game was derived from Chinese tribal warlords and generals, who used pieces of stone to map out attacking positions.[84][85]\n In China, Go had an important status among elites and was associated with ideas of self-cultivation, wisdom, and gentlemanly ideals.[86]: 23  It was considered one of the four cultivated arts of the Chinese scholar gentleman, along with calligraphy, painting and playing the musical instrument guqin.[87] In ancient times the rules of Go were passed on verbally, rather than being written down.[88]\n Go was introduced to Korea sometime between the 5th and 7th centuries CE, and was popular among the higher classes. In Korea, the game is called baduk (Korean: 바둑), and a variant of the game called Sunjang baduk was developed by the 16th century. Sunjang baduk became the main variant played in Korea until the end of the 19th century, when the current version was reintroduced from Japan.[89][90]\n The game reached Japan in the 7th century CE—where it is called go (碁) or igo (囲碁). It became popular at the Japanese imperial court in the 8th century,[91] and among the general public by the 13th century.[92] The game was further formalized in the 15th century. In 1603, Tokugawa Ieyasu re-established Japan's unified national government. In the same year, he assigned the then-best player in Japan, a Buddhist monk named Nikkai (né Kanō Yosaburo, 1559), to the post of Godokoro (Minister of Go).[93]\n Nikkai took the name Hon'inbō Sansa and founded the Hon'inbō Go school.[93] Several competing schools were founded soon after.[93] These officially recognized and subsidized Go schools greatly developed the level of play and introduced the dan/kyu style system of ranking players.[94] Players from the four schools (Hon'inbō, Yasui, Inoue and Hayashi) competed in the annual castle games, played in the presence of the shōgun.[95]\n Despite its widespread popularity in East Asia, Go has been slow to spread to the rest of the world. Although there are some mentions of the game in western literature from the 16th century forward, Go did not start to become popular in the West until the end of the 19th century, when German scientist Oskar Korschelt wrote a treatise on the game.[96] By the early 20th century, Go had spread throughout the German and Austro-Hungarian empires. In 1905, Edward Lasker learned the game while in Berlin. When he moved to New York, Lasker founded the New York Go Club together with (amongst others) Arthur Smith, who had learned of the game in Japan while touring the East and had published the book The Game of Go in 1908.[97] Lasker's book Go and Go-moku (1934) helped spread the game throughout the U.S.,[97] and in 1935, the American Go Association was formed. Two years later, in 1937, the German Go Association was founded.\n World War II put a stop to most Go activity, since it was a popular game in Japan, but after the war, Go continued to spread.[98] For most of the 20th century, the Japan Go Association (Nihon Ki-in) played a leading role in spreading Go outside East Asia by publishing the English-language magazine Go Review in the 1960s, establishing Go centers in the U.S., Europe and South America, and often sending professional teachers on tour to Western nations.[99] Internationally, the game had been commonly known since the start of the twentieth century by its shortened Japanese name, and terms for common Go concepts are derived from their Japanese pronunciation.\n In 1996, NASA astronaut Daniel Barry and Japanese astronaut Koichi Wakata became the first people to play Go in space. They used a special Go set, which was named Go Space, designed by Wai-Cheung Willson Chow. Both astronauts were awarded honorary dan ranks by the Nihon Ki-in.[100]\n As of December 2015[update], the International Go Federation has 75 member countries, with 67 member countries outside East Asia.[101] Chinese cultural centres across the world are promoting Go, and cooperating with local Go associations, for example the seminars held by the Chinese cultural centre in Tel Aviv, Israel, together with the Israeli Go association.[102]\n In Go, rank indicates a player's skill in the game. Traditionally, ranks are measured using kyu and dan grades,[103] a system also adopted by many martial arts. More recently, mathematical rating systems similar to the Elo rating system have been introduced.[104] Such rating systems often provide a mechanism for converting a rating to a kyu or dan grade.[104] Kyu grades (abbreviated k) are considered student grades and decrease as playing level increases, meaning 1st kyu is the strongest available kyu grade. Dan grades (abbreviated d) are considered master grades, and increase from 1st dan to 7th dan. First dan equals a black belt in eastern martial arts using this system. The difference among each amateur rank is one handicap stone. For example, if a 5k plays a game with a 1k, the 5k would need a handicap of four stones to even the odds. Top-level amateur players sometimes defeat professionals in tournament play.[105] Professional players have professional dan ranks (abbreviated p). These ranks are separate from amateur ranks.\n The rank system comprises, from the lowest to highest ranks:\n Tournament and match rules deal with factors that may influence the game but are not part of the actual rules of play. Such rules may differ between events. Rules that influence the game include: the setting of compensation points (komi), handicap, and time control parameters. Rules that do not generally influence the game are the tournament system, pairing strategies, and placement criteria.\n Common tournament systems used in Go include the McMahon system,[106] Swiss system, league systems and the knockout system. Tournaments may combine multiple systems; many professional Go tournaments use a combination of the league and knockout systems.[107]\n Tournament rules may also set the following:\n A game of Go may be timed using a game clock. Formal time controls were introduced into the professional game during the 1920s and were controversial.[110] Adjournments and sealed moves began to be regulated in the 1930s. Go tournaments use a number of different time control systems. All common systems envisage a single main period of time for each player for the game, but they vary on the protocols for continuation (in overtime) after a player has finished that time allowance.[g] The most widely used time control system is the so-called byoyomi[h] system. The top professional Go matches have timekeepers so that the players do not have to press their own clocks.\n Two widely used variants of the byoyomi system are:[111]\n Go games are recorded with a simple coordinate system. This is comparable to algebraic chess notation, except that Go stones do not move and thus require only one coordinate per turn. Coordinate systems include purely numerical (4–4 point), hybrid (K3), and purely alphabetical.[112] The Smart Game Format uses alphabetical coordinates internally, but most editors represent the board with hybrid coordinates as this reduces confusion.\n Alternatively, the game record can also be noted by writing the successive moves on a diagram, where odd numbers mean black stones, even numbers mean white stones (or conversely when playing with a handicap), and a notation like \"25=22\" in the margin means that the 25th stone was played at the same location as the 22nd one, which had been captured in the meantime.\n The Japanese word kifu is sometimes used to refer to a game record.\n In Unicode, Go stones can be represented with black and white circles from the block Geometric Shapes:\n The block Miscellaneous Symbols includes \"Go markers\"[113] that were likely meant for mathematical research of Go:[114][115]\n A Go professional is a professional player of the game of Go. There are six areas with professional go associations, these are: China (Chinese Weiqi Association), Japan (Nihon Ki-in, Kansai Ki-in), South Korea (Korea Baduk Association), Taiwan (Taiwan Chi Yuan Culture Foundation), the United States (AGA Professional System) and Europe (European Professional System).\n Although the game was developed in China, the establishment of the Four Go houses by Tokugawa Ieyasu at the start of the 17th century shifted the focus of the Go world to Japan. State sponsorship, allowing players to dedicate themselves full-time to study of the game, and fierce competition between individual houses resulted in a significant increase in the level of play. During this period, the best player of his generation was given the prestigious title Meijin (master) and the post of Godokoro (minister of Go). Of special note are the players who were dubbed Kisei (Go Sage). The only three players to receive this honor were Dōsaku, Jōwa and Shūsaku, all of the house Hon'inbō.[116]\n After the end of the Tokugawa shogunate and the Meiji Restoration period, the Go houses slowly disappeared, and in 1924, the Nihon Ki-in (Japanese Go Association) was formed. Top players from this period often played newspaper-sponsored matches of 2–10 games.[117] Of special note are the (Chinese-born) player Go Seigen (Chinese: Wu Qingyuan), who scored 80% in these matches and beat down most of his opponents to inferior handicaps,[118] and Minoru Kitani, who dominated matches in the early 1930s.[119] These two players are also recognized for their groundbreaking work on new opening theory (Shinfuseki).[120]\n \nFor much of the 20th century, Go continued to be dominated by players trained in Japan. Notable names included Eio Sakata, Rin Kaiho (born in Taiwan), Masao Kato, Koichi Kobayashi and Cho Chikun (born Cho Ch'i-hun, from South Korea).[121] Top Chinese and Korean talents often moved to Japan, because the level of play there was high and funding was more lavish. One of the first Korean players to do so was Cho Namchul, who studied in the Kitani Dojo 1937–1944. After his return to Korea, the Hanguk Kiwon (Korea Baduk Association) was formed and caused the level of play in South Korea to rise significantly in the second half of the 20th century.[122] In China, the game declined during the Cultural Revolution (1966–1976) but quickly recovered in the last quarter of the 20th century, bringing Chinese players, such as Nie Weiping and Ma Xiaochun, on par with their Japanese and South Korean counterparts.[123] The Chinese Weiqi Association (today part of the China Qiyuan) was established in 1962, and professional dan grades started being issued in 1982.[124] Western professional Go began in 2012 with the American Go Association's Professional System.[125] In 2014, the European Go Federation followed suit and started their professional system.[126]  With the advent of major international titles from 1989 onward, it became possible to compare the level of players from different countries more accurately. Cho Hunhyun of South Korea won the first edition of the Quadrennial Ing Cup in 1989. His disciple Lee Chang-ho was the dominant player in international Go competitions for more than a decade spanning much of 1990s and early 2000s; he is also credited with groundbreaking works on the endgame. Cho, Lee and other South Korean players such as Seo Bong-soo, Yoo Changhyuk and Lee Sedol between them won the majority of international titles in this period.[127] Several Chinese players also rose to the top in international Go from 2000s, most notably Ma Xiaochun, Chang Hao, Gu Li and Ke Jie. As of 2016[update], Japan lags behind in the international Go scene.\n Historically, more men than women have played Go. Special tournaments for women exist, but until recently, men and women did not compete together at the highest levels; however, the creation of new, open tournaments and the rise of strong female players, most notably Rui Naiwei, have in recent years highlighted the strength and competitiveness of emerging female players.[128]\n The level in other countries has traditionally been much lower, except for some players who had preparatory professional training in East Asia.[k] Knowledge of the game has been scant elsewhere up until the 20th century. A famous player of the 1920s was Edward Lasker.[l] It was not until the 1950s that more than a few Western players took up the game as other than a passing interest. In 1978, Manfred Wimmer became the first Westerner to receive a professional player's certificate from an East Asian professional Go association.[129] In 2000, American Michael Redmond became the first Western player to achieve a 9 dan rank.\n It is possible to play Go with a simple paper board and coins, plastic tokens, or white beans and coffee beans for the stones; or even by drawing the stones on the board and erasing them when captured. More popular midrange equipment includes cardstock, a laminated particle board, or wood boards with stones of plastic or glass. More expensive traditional materials are still used by many players. The most expensive Go sets have black stones carved from slate and white stones carved from translucent white shells (traditionally Meretrix lamarckii), played on boards carved in a single piece from the trunk of a tree.\n The Go board (generally referred to by its Japanese name goban 碁盤) typically measures between 45 and 48 cm (18 and 19 in) in length (from one player's side to the other) and 42 to 44 cm (16+1⁄2 to 17+1⁄4 in) in width. Chinese boards are slightly larger, as a traditional Chinese Go stone is slightly larger to match. The board is not square; there is a 15:14 ratio in length to width, because with a perfectly square board, from the player's viewing angle the perspective creates a foreshortening of the board. The added length compensates for this.[130] There are two main types of boards: a table board similar in most respects to other gameboards like that used for chess, and a floor board, which is its own free-standing table and at which the players sit.\n The traditional Japanese goban is between 10 and 18 cm (3.9 and 7.1 in) thick and has legs; it sits on the floor (see picture).[130] It is preferably made from the rare golden-tinged Kaya tree (Torreya nucifera), with the very best made from Kaya trees up to 700 years old. More recently, the related California Torreya (Torreya californica) has been prized for its light color and pale rings as well as its reduced expense and more readily available stock. The natural resources of Japan have been unable to keep up with the enormous demand for the slow-growing Kaya trees; both T. nucifera and T. californica take many hundreds of years to grow to the necessary size, and they are now extremely rare, raising the price of such equipment tremendously.[131] As Kaya trees are a protected species in Japan, they cannot be harvested until they have died. Thus, an old-growth, floor-standing Kaya goban can easily cost in excess of $10,000 with the highest-quality examples costing more than $60,000.[132]\n Other, less expensive woods often used to make quality table boards in both Chinese and Japanese dimensions include Hiba (Thujopsis dolabrata), Katsura (Cercidiphyllum japonicum), Kauri (Agathis), and Shin Kaya (various varieties of spruce, commonly from Alaska, Siberia and China's Yunnan Province).[131] So-called Shin Kaya is a potentially confusing merchant's term: shin means 'new', and thus shin kaya is best translated 'faux kaya', because the woods so described are biologically unrelated to Kaya.[131]\n A full set of Go stones (goishi) usually contains 181 black stones and 180 white ones; a 19×19 grid has 361 points, so there are enough stones to cover the board, and Black gets the extra odd stone because that player goes first. However it may happen, especially in beginners' games, that many back-and-forth captures empty the bowls before the end of the game: in that case an exchange of prisoners allows the game to continue.\n Traditional Japanese stones are double-convex, and made of clamshell (white) and slate (black).[133] The classic slate is nachiguro stone mined in Wakayama Prefecture and the clamshell from the Hamaguri clam (Meretrix lusoria) or the Korean hard clam; however, due to a scarcity in the Japanese supply of these clams, the stones are most often made of shells harvested from Mexico.[133] Historically, the most prized stones were made of jade, often given to the reigning emperor as a gift.[133]\n In China, the game is traditionally played with single-convex stones[133] made of a composite called Yunzi. The material comes from Yunnan Province and is made by sintering a proprietary and trade-secret mixture of mineral compounds derived from the local stone. This process dates to the Tang dynasty and, after the knowledge was lost in the 1920s during the Chinese Civil War, was rediscovered in the 1960s by the now state-run Yunzi company. The material is praised for its colors, its pleasing sound as compared to glass or to synthetics such as melamine, and its lower cost as opposed to other materials such as slate/shell. The term yunzi can also refer to a single-convex stone made of any material; however, most English-language Go suppliers specify Yunzi as a material and single-convex as a shape to avoid confusion, as stones made of Yunzi are also available in double-convex while synthetic stones can be either shape.\n Traditional stones are made so that black stones are slightly larger in diameter than white; this is to compensate for the optical illusion created by contrasting colors that would make equal-sized white stones appear larger on the board than black stones.[133][m]\n The bowls for the stones are shaped like a flattened sphere with a level underside.[134] The lid is loose fitting and upturned before play to receive stones captured during the game. Chinese bowls are slightly larger, and a little more rounded, a style known generally as Go Seigen; Japanese Kitani bowls tend to have a shape closer to that of the bowl of a snifter glass, such as for brandy. The bowls are usually made of turned wood. Mulberry is the traditional material for Japanese bowls, but is very expensive; wood from the Chinese jujube date tree, which has a lighter color (it is often stained) and slightly more visible grain pattern, is a common substitute for rosewood, and traditional for Go Seigen-style bowls. Other traditional materials used for making Chinese bowls include lacquered wood, ceramics, stone and woven straw or rattan. The names of the bowl shapes, Go Seigen and Kitani, were introduced in the last quarter of the 20th century by the professional player Janice Kim as homage to two 20th-century professional Go players by the same names, of Chinese and Japanese nationality, respectively, who are referred to as the \"Fathers of modern Go\".[116]\n The traditional way to place a Go stone is to first take one from the bowl, gripping it between the index and middle fingers, with the middle finger on top, and then placing it directly on the desired intersection.[135] One can also place a stone on the board and then slide it into position under appropriate circumstances (where it does not move any other stones). It is considered respectful towards White for Black to place the first stone of the game in the upper right-hand corner.[136] (Because of symmetry, this has no effect on the game's outcome.)\n It is considered poor manners to run one's fingers through one's bowl of unplayed stones, as the sound, however soothing to the player doing this, can be disturbing to one's opponent. Similarly, clacking a stone against another stone, the board, or the table or floor is also discouraged. However, it is permissible to emphasize select moves by striking the board more firmly than normal, thus producing a sharp clack. Additionally, hovering one's arm over the board (usually when deciding where to play) is also considered rude as it obstructs the opponent's view of the board.\n Manners and etiquette are extensively discussed in 'The Classic of WeiQi in Thirteen Chapters', a Song dynasty manual to the game. Apart from the points above it also points to the need to remain calm and honorable, in maintaining posture, and knowing the key specialised terms, such as titles of common formations. Generally speaking, much attention is paid to the etiquette of playing, as much as to winning or actual game technique.\n Go long posed a daunting challenge to computer programmers, putting forward \"difficult decision-making tasks, an intractable search space, and an optimal solution so complex it appears infeasible to directly approximate using a policy or value function\".[137] Prior to 2015,[137] the best Go programs only managed to reach amateur dan level.[138] On smaller 9×9 and 13x13 boards, computer programs fared better, and were able to compare to professional players. Many in the field of artificial intelligence consider Go to require more elements that mimic human thought than chess.[139]\n The reasons why computer programs had not played Go at the professional dan level prior to 2016 include:[140]\n It was not until August 2008 that a computer won a game against a professional level player at a handicap of 9 stones, the greatest handicap normally given to a weaker opponent. It was the Mogo program, which scored this first victory in an exhibition game played during the US Go Congress.[147][148] By 2013, a win at the professional level of play was accomplished with a four-stone advantage.[149][150] In October 2015, Google DeepMind's program AlphaGo beat Fan Hui, the European Go champion and a 2 dan (out of 9 dan possible) professional, five times out of five with no handicap on a full size 19×19 board.[137] AlphaGo used a fundamentally different paradigm than earlier Go programs; it included very little direct instruction, and mostly used deep learning where AlphaGo played itself in hundreds of millions of games such that it could measure positions more intuitively. In March 2016, Google next challenged Lee Sedol, a 9 dan considered the top player in the world in the early 21st century,[151] to a five-game match. Leading up to the game, Lee Sedol and other top professionals were confident that he would win;[152] however, AlphaGo defeated Lee in four of the five games.[153][154] After having already lost the series by the third game, Lee won the fourth game, describing his win as \"invaluable\".[155] In May 2017, AlphaGo beat Ke Jie, who at the time continuously held the world No. 1 ranking for two years,[156][157] winning each game in a three-game match during the Future of Go Summit.[158][159] In October 2017, DeepMind announced a significantly stronger version called AlphaGo Zero which beat the previous version by 100 games to 0.[160]\n In February 2023, Kellin Pelrine, an amateur American Go player, won 14 out of 15 games against a top-ranked AI system in a significant victory over artificial intelligence. Pelrine took advantage of a previously unknown flaw in the Go computer program, which had been identified by another computer. He exploited this weakness by slowly encircling the opponent's stones and distracting the AI with moves in other parts of the board. The tactics used by Pelrine have highlighted a fundamental flaw in the deep learning systems that underpin many of today's advanced AI. Although the AI systems can \"understand\" specific situations, they lack the ability to generalize in a way that humans find easy.[161][162][163]\n An abundance of software is available to support players of the game. This includes programs that can be used to view or edit game records and diagrams, programs that allow the user to search for patterns in the games of strong players, and programs that allow users to play against each other over the Internet.\n Some web servers[citation needed] provide graphical aids like maps, to aid learning during play. These graphical aids may suggest possible next moves, indicate areas of influence, highlight vital stones under attack and mark stones in atari or about to be captured.\n There are several file formats used to store game records, the most popular of which is SGF, short for Smart Game Format. Programs used for editing game records allow the user to record not only the moves, but also variations, commentary and further information on the game.[o]\n Electronic databases can be used to study life and death situations, joseki, fuseki and games by a particular player. Programs are available that give players pattern searching options, which allow players to research positions by searching for high-level games in which similar situations occur. Such software generally lists common follow-up moves that have been played by professionals and gives statistics on win–loss ratio in opening situations.\n Internet-based Go servers allow access to competition with players all over the world, for real-time and turn-based games.[p] Such servers also allow easy access to professional teaching, with both teaching games and interactive game review being possible.[q]\n Apart from technical literature and study material, Go and its strategies have been the subject of several works of fiction, such as The Master of Go by Nobel Prize in Literature-winning author Yasunari Kawabata[r] and The Girl Who Played Go (2001) by Shan Sa. Other books have used Go as a theme or minor plot device. For example, the 1979 novel Shibumi by Trevanian centers around the game and uses Go metaphors.[164] [165] Go features prominently in the Chung Kuo series of novels by David Wingrove, being the favourite game of the main villain.[166]\n The manga series Hikaru no Go and its anime adaptation, first released in Japan in 1998 and 2001 respectively, had a large impact in popularizing Go among young players, both in Japan and—as translations were released—abroad.[167][168]\n Similarly, Go has been used as a subject or plot device in film, such as Pi (π), A Beautiful Mind, Tron: Legacy, Knives Out, and The Go Master (a biopic of Go professional Go Seigen).[169][s] 2013's Tôkyô ni kita bakari or Tokyo Newcomer portrays a Chinese foreigner Go player moving to Tokyo.[170] In King Hu's wuxia film The Valiant Ones, the characters are color-coded as Go stones (black or other dark shades for the Chinese, white for the Japanese invaders), Go boards and stones are used by the characters to keep track of soldiers prior to battle, and the battles themselves are structured like a game of Go.[171]\n Go has also been featured as a plot device in a number of television series. Examples include Starz's science fiction thriller Counterpart, which is rich in references (the opening itself featuring developments on a Go board), and includes Go matches, accurately played, relevant to the plot.[172] Also, in 2024 Netflix released the historical-fictional Korean series Captivating the King.\n The corporation and brand Atari was named after the Go term.[173]\n Hedge fund manager Mark Spitznagel used Go as his main investing metaphor in his investing book The Dao of Capital.[174] The Way of Go: 8 Ancient Strategy Secrets for Success in Business and Life by Troy Anderson applies Go strategy to business.[175] GO: An Asian Paradigm for Business Strategy[176] by Miura Yasuyuki, a manager with Japan Airlines,[177] uses Go to describe the thinking and behavior of business men.\n A 2004 review of literature by Fernand Gobet, de Voogt and Jean Retschitzki shows that relatively little scientific research has been carried out on the psychology of Go, compared with other traditional board games such as chess.[178] Computer Go research has shown that given the large search tree, knowledge and pattern recognition are more important in Go than in other strategy games, such as chess.[178] A study of the effects of age on Go-playing[179] has shown that mental decline is milder with strong players than with weaker players. According to the review of Gobet and colleagues, the pattern of brain activity observed with techniques such as PET and fMRI does not show large differences between Go and chess. On the other hand, a study by Xiangchuan Chen et al.[180] showed greater activation in the right hemisphere among Go players than among chess players, but the research was inconclusive because strong players from Go were hired while very weak chess players were hired in the original study.[181] There is some evidence to suggest a correlation between playing board games and reduced risk of Alzheimer's disease and dementia.[182]\n Arthur Mary, a French researcher in clinical psychopathology, reports on his psychotherapeutic approaches using the game of Go with patients in private practice and in a psychiatric ward.[183] Drawing on neuroscience research and employing a psychoanalytic (Lacanian) and phenomenological approach, he demonstrates how drives are expressed on the goban.[184] He offers some suggestions to therapists for defining ways of playing go that lead to therapeutic effects.[185]\n In formal game theory terms, Go is a non-chance, combinatorial game with perfect information. Informally that means there are no dice used (and decisions or moves create discrete outcome vectors rather than probability distributions), the underlying math is combinatorial, and all moves (via single vertex analysis) are visible to both players (unlike some card games where some information is hidden). Perfect information also implies sequence—players can theoretically know about all past moves.\n Other game theoretical taxonomy elements include the facts\n In the endgame, it can often happen that the state of the board consists of several subpositions that do not interact with the others. The whole board position can then be considered as a mathematical sum, or composition, of the individual subpositions.[187] It is this property of Go endgames that led John Horton Conway to the discovery of surreal numbers.[188]\n In combinatorial game theory terms, Go is a zero-sum, perfect-information, partisan, deterministic strategy game, putting it in the same class as chess, draughts (checkers), and Reversi (Othello).\n The game emphasizes the importance of balance on multiple levels: to secure an area of the board, it is good to play moves close together; however, to cover the largest area, one needs to spread out, perhaps leaving weaknesses that can be exploited. Playing too low (close to the edge) secures insufficient territory and influence, yet playing too high (far from the edge) allows the opponent to invade. Decisions in one part of the board may be influenced by an apparently unrelated situation in a distant part of the board (for example, ladders can be broken by stones at an arbitrary distance away). Plays made early in the game can shape the nature of conflict a hundred moves later.\n The game complexity of Go is such that describing even elementary strategy fills many introductory books. In fact, numerical estimates show that the number of possible games of Go far exceeds the number of atoms in the observable universe.[t]\n Go also contributed to the development of combinatorial game theory (with Go infinitesimals[189] being a specific example of its use in Go).\n Go begins with an empty board. It is focused on building from the ground up (nothing to something) with multiple, simultaneous battles leading to a point-based win. Chess is tactical rather than strategic, as the predetermined strategy is to trap one individual piece (the king). This comparison has also been applied to military and political history, with Scott Boorman's book The Protracted Game (1969) and, more recently, Robert Greene's book The 48 Laws of Power (1998) exploring the strategy of the Chinese Communist Party in the Chinese Civil War through the lens of Go.[190][191]\n A similar comparison has been drawn among Go, chess and backgammon, perhaps the three oldest games that enjoy worldwide popularity.[192] Backgammon is a \"man vs. fate\" contest, with chance playing a strong role in determining the outcome. Chess, with rows of soldiers marching forward to capture each other, embodies the conflict of \"man vs. man\". Because the handicap system tells Go players where they stand relative to other players, an honestly ranked player can expect to lose about half of their games; therefore, Go can be seen as embodying the quest for self-improvement, \"man vs. self\".[192]\n"
    },
    {
        "title": "AI effect",
        "url": "https://en.wikipedia.org/wiki/AI_effect",
        "content": "The AI effect is the discounting of the behavior of an artificial-intelligence program as not \"real\" intelligence.[1]\n The author Pamela McCorduck writes: \"It's part of the history of the field of artificial intelligence that every time somebody figured out how to make a computer do something—play good checkers, solve simple but relatively informal problems—there was a chorus of critics to say, 'that's not thinking'.\"[2]\n Researcher Rodney Brooks complains: \"Every time we figure out a piece of it, it stops being magical; we say, 'Oh, that's just a computation.'\"[3]\n \"The AI effect\" refers to a phenomenon where either the definition of AI or the concept of intelligence is adjusted to exclude capabilities that AI systems have mastered. This often manifests as tasks that AI can now perform successfully no longer being considered part of AI, or as the notion of intelligence itself being redefined to exclude AI achievements.[4][2][1] Edward Geist credits John McCarthy for coining the term \"AI effect\" to describe this phenomenon.[4]\n McCorduck calls it an \"odd paradox\" that \"practical AI successes, computational programs that actually achieved intelligent behavior were soon assimilated into whatever application domain they were found to be useful in, and became silent partners alongside other problem-solving approaches, which left AI researchers to deal only with the 'failures', the tough nuts that couldn't yet be cracked.\"[5] It is an example of moving the goalposts.[6]\n Tesler's Theorem is:\n AI is whatever hasn't been done yet. Douglas Hofstadter quotes this[7] as do many other commentators.[8]\n When problems have not yet been formalised, they can still be characterised by a model of computation that includes human computation. The computational burden of a problem is split between a computer and a human: one part is solved by computer and the other part solved by a human. This formalisation is referred to as a human-assisted Turing machine.[9]\n Software and algorithms developed by AI researchers are now integrated into many applications throughout the world, without really being called AI. This underappreciation is known from such diverse fields as computer chess,[10] marketing,[11] agricultural automation,[8] hospitality[12] and optical character recognition.[13]\n Michael Swaine reports \"AI advances are not trumpeted as artificial intelligence so much these days, but are often seen as advances in some other field\". \"AI has become more important as it has become less conspicuous\", Patrick Winston says. \"These days, it is hard to find a big system that does not work, in part, because of ideas developed or matured in the AI world.\"[14]\n According to Stottler Henke, \"The great practical benefits of AI applications and even the existence of AI in many software products go largely unnoticed by many despite the already widespread use of AI techniques in software. This is the AI effect. Many marketing people don't use the term 'artificial intelligence' even when their company's products rely on some AI techniques. Why not?\"[11]\n Marvin Minsky writes \"This paradox resulted from the fact that whenever an AI research project made a useful new discovery, that product usually quickly spun off to form a new scientific or commercial specialty with its own distinctive name. These changes in name led outsiders to ask, Why do we see so little progress in the central field of artificial intelligence?\"[15]\n Nick Bostrom observes that \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labelled AI anymore.\"[16]\n The AI effect on decision-making in supply chain risk management is a severely understudied area.[17]\n To avoid the AI effect problem, the editors of a special issue of IEEE Software on AI and software engineering recommend not overselling – not hyping – the real achievable results to start with.[18]\n The Bulletin of the Atomic Scientists organization views the AI effect as a worldwide strategic military threat.[4] They point out that it obscures the fact that applications of AI had already found their way into both US and Soviet militaries during the Cold War.[4] AI tools to advise humans regarding weapons deployment were developed by both sides and received very limited usage during that time.[4] They believe this constantly shifting failure to recognise AI continues to undermine human recognition of security threats in the present day.[4]\n Some experts think that the AI effect will continue, with advances in AI continually producing objections and redefinitions of public expectations.[19][20][21] Some also believe that the AI effect will expand to include the dismissal of specialised artificial intelligences.[21]\n In the early 1990s, during the second \"AI winter\" many AI researchers found that they could get more funding and sell more software if they avoided the bad name of \"artificial intelligence\" and instead pretended their work had nothing to do with intelligence.[citation needed]\n Patty Tascarella wrote in 2006: \"Some believe the word 'robotics' actually carries a stigma that hurts a company's chances at funding.\"[22]\n Michael Kearns suggests that \"people subconsciously are trying to preserve for themselves some special role in the universe\".[23] By discounting artificial intelligence people can continue to feel unique and special. Kearns argues that the change in perception known as the AI effect can be traced to the mystery being removed from the system. In being able to trace the cause of events implies that it's a form of automation rather than intelligence.[citation needed]\n A related effect has been noted in the history of animal cognition and in consciousness studies, where every time a capacity formerly thought of as uniquely human is discovered in animals (e.g. the ability to make tools, or passing the mirror test), the overall importance of that capacity is deprecated.[citation needed]\n Herbert A. Simon, when asked about the lack of AI's press coverage at the time, said, \"What made AI different was that the very idea of it arouses a real fear and hostility in some human breasts. So you are getting very strong emotional reactions. But that's okay. We'll live with that.\"[24]\n Mueller 1987 proposed comparing AI to human intelligence, coining the standard of Human-Level Machine Intelligence.[25] This nonetheless suffers from the AI effect however when different humans are used as the standard.[25]\n When IBM's chess-playing computer Deep Blue succeeded in defeating Garry Kasparov in 1997, public perception of chess playing shifted from a difficult mental task to a routine operation.[26]\n The public complained that Deep Blue had only used \"brute force methods\" and it wasn't real intelligence.[10] Notably, John McCarthy, an AI pioneer and founder of the term \"artificial intelligence\", was disappointed by Deep Blue. He described it as a mere brute force machine that did not have any deep understanding of the game. McCarthy would also criticize how widespread the AI effect is (\"As soon as it works, no one calls it AI anymore\"[27][28]: 12 ), but in this case did not think that Deep Blue was a good example.[27]\n On the other side, Fred A. Reed writes:[29]\n A problem that proponents of AI regularly face is this: When we know how a machine does something \"intelligent\", it ceases to be regarded as intelligent. If I beat the world's chess champion, I'd be regarded as highly bright."
    },
    {
        "title": "Automated reasoning",
        "url": "https://en.wikipedia.org/wiki/Automated_reasoning",
        "content": "In computer science, in particular in knowledge representation and reasoning and metalogic, the area of automated reasoning is dedicated to understanding different aspects of reasoning. The study of automated reasoning helps produce computer programs that allow computers to reason completely, or nearly completely, automatically. Although automated reasoning is considered a sub-field of artificial intelligence, it also has connections with theoretical computer science and philosophy.\n The most developed subareas of automated reasoning are automated theorem proving (and the less automated but more pragmatic subfield of interactive theorem proving) and automated proof checking (viewed as guaranteed correct reasoning under fixed assumptions).[citation needed] Extensive work has also been done in reasoning by analogy using induction and abduction.[1]\n Other important topics include reasoning under uncertainty and non-monotonic reasoning. An important part of the uncertainty field is that of argumentation, where further constraints of minimality and consistency are applied on top of the more standard automated deduction. John Pollock's OSCAR system[2] is an example of an automated argumentation system that is more specific than being just an automated theorem prover.\n Tools and techniques of automated reasoning include the classical logics and calculi, fuzzy logic, Bayesian inference, reasoning with maximal entropy and many less formal ad hoc techniques.\n The development of formal logic played a big role in the field of automated reasoning, which itself led to the development of artificial intelligence. A formal proof is a proof in which every logical inference has been checked back to the fundamental axioms of mathematics. All the intermediate logical steps are supplied, without exception. No appeal is made to intuition, even if the translation from intuition to logic is routine. Thus, a formal proof is less intuitive and less susceptible to logical errors.[3]\n Some consider the Cornell Summer meeting of 1957, which brought together many logicians and computer scientists, as the origin of automated reasoning, or automated deduction.[4] Others say that it began before that with the 1955 Logic Theorist program of Newell, Shaw and Simon, or with Martin Davis’ 1954 implementation of Presburger's decision procedure (which proved that the sum of two even numbers is even).[5]\n Automated reasoning, although a significant and popular area of research, went through an \"AI winter\" in the eighties and early nineties. The field subsequently revived, however. For example, in 2005, Microsoft started using verification technology in many of their internal projects and is planning to include a logical specification and checking language in their 2012 version of Visual C.[4]\n Principia Mathematica was a milestone work in formal logic written by Alfred North Whitehead and Bertrand Russell. Principia Mathematica - also meaning Principles of Mathematics - was written with a purpose to derive all or some of the mathematical expressions, in terms of symbolic logic. Principia Mathematica was initially published in three volumes in 1910, 1912 and 1913.[6]\n Logic Theorist (LT) was the first ever program developed in 1956 by Allen Newell, Cliff Shaw and Herbert A. Simon to \"mimic human reasoning\" in proving theorems and was demonstrated on fifty-two theorems from chapter two of Principia Mathematica, proving thirty-eight of them.[7] In addition to proving the theorems, the program found a proof for one of the theorems that was more elegant than the one provided by Whitehead and Russell. After an unsuccessful attempt at publishing their results, Newell, Shaw, and Herbert reported in their publication in 1958, The Next Advance in Operation Research:\n Examples of Formal Proofs\n Automated reasoning has been most commonly used to build automated theorem provers. Oftentimes, however, theorem provers require some human guidance to be effective and so more generally qualify as proof assistants. In some cases such provers have come up with new approaches to proving a theorem. Logic Theorist is a good example of this. The program came up with a proof for one of the theorems in Principia Mathematica that was more efficient (requiring fewer steps) than the proof provided by Whitehead and Russell. Automated reasoning programs are being applied to solve a growing number of problems in formal logic, mathematics and computer science, logic programming, software and hardware verification, circuit design, and many others. The TPTP (Sutcliffe and Suttner 1998) is a library of such problems that is updated on a regular basis. There is also a competition among automated theorem provers held regularly at the CADE conference (Pelletier, Sutcliffe and Suttner 2002); the problems for the competition are selected from the TPTP library.[17]\n"
    },
    {
        "title": "Knowledge representation and reasoning",
        "url": "https://en.wikipedia.org/wiki/Knowledge_representation",
        "content": "Knowledge representation (KR) aims to model information in a structured manner to formally represent it as knowledge in knowledge-based systems. Whereas knowledge representation and reasoning (KRR, KR&R, or KR²) also aims to understand, reason and interpret knowledge. KRR is widely used in the field of artificial intelligence (AI) with the goal to represent information about the world in a form that a computer system can use to solve complex tasks, such as diagnosing a medical condition or having a natural-language dialog. KR incorporates findings from psychology[1] about how humans solve problems and represent knowledge, in order to design formalisms that make complex systems easier to design and build. KRR also incorporates findings from logic to automate various kinds of reasoning.\n Examples of knowledge representation formalisms include vocabularies, thesaurus, semantic networks, axiom systems, frames, rules, logic programs, and ontologies. Examples of automated reasoning engines include inference engines, theorem provers, model generators, and classifiers.\n The earliest work in computerized knowledge representation was focused on general problem-solvers such as the General Problem Solver (GPS) system developed by Allen Newell and Herbert A. Simon in 1959 and the Advice Taker proposed by John McCarthy also in 1959. GPS featured data structures for planning and decomposition. The system would begin with a goal. It would then decompose that goal into sub-goals and then set out to construct strategies that could accomplish each subgoal. The Advisor Taker, on the other hand, proposed the use of the predicate calculus to represent common sense reasoning.\n Many of the early approaches to knowledge represention in Artificial Intelligence (AI) used graph representations and semantic networks, similar to knowledge graphs today. In such approaches, problem solving was a form of graph traversal[2] or path-finding, as in the A* search algorithm. Typical applications included robot plan-formation and game-playing.\n Other researchers focused on developing  automated theorem-provers for first-order logic, motivated by the use of mathematical logic to formalise mathematics and to automate the proof of mathematical theorems. A major step in this direction was the development of the resolution method by John Alan Robinson.\n In the meanwhile, John McCarthy and Pat Hayes developed the situation calculus as a logical representation of common sense knowledge about the laws of cause and effect. Cordell Green, in turn, showed how to do robot plan-formation by applying resolution to the situation calculus. He also showed how to use resolution for question-answering and automatic programming.[3]\n In contrast, researchers at Massachusetts Institute of Technology (MIT) rejected the resolution uniform proof procedure paradigm and advocated the procedural embedding of knowledge instead.[4] The resulting conflict between the use of logical representations and the use of procedural representations was resolved in the early 1970s with the development of logic programming and Prolog, using SLD resolution to treat Horn clauses as goal-reduction procedures.\n The early development of logic programming was largely a European phenomenon. In North America, AI researchers such as Ed Feigenbaum and Frederick Hayes-Roth advocated the representation of domain-specific knowledge rather than general-purpose reasoning.[5]\n These efforts led to the cognitive revolution in psychology and to the phase of AI focused on knowledge representation that resulted in expert systems in the 1970s and 80s, production systems, frame languages, etc. Rather than general problem solvers, AI changed its focus to expert systems that could match human competence on a specific task, such as medical diagnosis.[6]\n Expert systems gave us the terminology still in use today where AI systems are divided into a knowledge base, which includes facts and rules about a problem domain, and an inference engine, which applies the knowledge in the knowledge base to answer questions and solve problems in the domain. In these early systems the facts in the knowledge base tended to be a fairly flat structure, essentially assertions about the values of variables used by the rules.[7]\n Meanwhile, Marvin Minsky developed the concept of frame in the mid-1970s.[8] A frame is similar to an object class: It is an abstract description of a category describing things in the world, problems, and potential solutions. Frames were originally used on systems geared toward human interaction, e.g. understanding natural language and the social settings in which various default expectations such as ordering food in a restaurant narrow the search space and allow the system to choose appropriate responses to dynamic situations.\n It was not long before the frame communities and the rule-based researchers realized that there was a synergy between their approaches. Frames were good for representing the real world, described as classes, subclasses, slots (data values) with various constraints on possible values. Rules were good for representing and utilizing complex logic such as the process to make a medical diagnosis. Integrated systems were developed that combined frames and rules. One of the most powerful and well known was the 1983 Knowledge Engineering Environment (KEE) from Intellicorp. KEE had a complete rule engine with forward and backward chaining. It also had a complete frame-based knowledge base with triggers, slots (data values), inheritance, and message passing. Although message passing originated in the object-oriented community rather than AI it was quickly embraced by AI researchers as well in environments such as KEE and in the operating systems for Lisp machines from Symbolics, Xerox, and Texas Instruments.[9]\n The integration of frames, rules, and object-oriented programming was significantly driven by commercial ventures such as KEE and Symbolics spun off from various research projects. At the same time, there was another strain of research that was less commercially focused and was driven by mathematical logic and automated theorem proving.[citation needed] One of the most influential languages in this research was the KL-ONE language of the mid-'80s. KL-ONE was a frame language that had a rigorous semantics, formal definitions for concepts such as an Is-A relation.[10] KL-ONE and languages that were influenced by it such as Loom had an automated reasoning engine that was based on formal logic rather than on IF-THEN rules. This reasoner is called the classifier. A classifier can analyze a set of declarations and infer new assertions, for example, redefine a class to be a subclass or superclass of some other class that wasn't formally specified. In this way the classifier can function as an inference engine, deducing new facts from an existing knowledge base. The classifier can also provide consistency checking on a knowledge base (which in the case of KL-ONE languages is also referred to as an Ontology).[11]\n Another area of knowledge representation research was the problem of common-sense reasoning. One of the first realizations learned from trying to make software that can function with human natural language was that humans regularly draw on an extensive foundation of knowledge about the real world that we simply take for granted but that is not at all obvious to an artificial agent, such as basic principles of common-sense physics, causality, intentions, etc. An example is the frame problem, that in an event driven logic there need to be axioms that state things maintain position from one moment to the next unless they are moved by some external force. In order to make a true artificial intelligence agent that can converse with humans using natural language and can process basic statements and questions about the world, it is essential to represent this kind of knowledge.[12] In addition to McCarthy and Hayes' situation calculus, one of the most ambitious programs to tackle this problem was Doug Lenat's Cyc project. Cyc established its own Frame language and had large numbers of analysts document various areas of common-sense reasoning in that language. The knowledge recorded in Cyc included common-sense models of time, causality, physics, intentions, and many others.[13]\n The starting point for knowledge representation is the knowledge representation hypothesis first formalized by Brian C. Smith in 1985:[14]\n Any mechanically embodied intelligent process will be comprised of structural ingredients that a) we as external observers naturally take to represent a propositional account of the knowledge that the overall process exhibits, and b) independent of such external semantic attribution, play a formal but causal and essential role in engendering the behavior that manifests that knowledge. One of the most active areas of knowledge representation research is the Semantic Web.[citation needed] The Semantic Web seeks to add a layer of semantics (meaning) on top of the current Internet. Rather than indexing web sites and pages via keywords, the Semantic Web creates large ontologies of concepts. Searching for a concept will be more effective than traditional text only searches. Frame languages and automatic classification play a big part in the vision for the future Semantic Web. The automatic classification gives developers technology to provide order on a constantly evolving network of knowledge. Defining ontologies that are static and incapable of evolving on the fly would be very limiting for Internet-based systems. The classifier technology provides the ability to deal with the dynamic environment of the Internet.\n Recent projects funded primarily by the Defense Advanced Research Projects Agency (DARPA) have integrated frame languages and classifiers with markup languages based on XML. The Resource Description Framework (RDF) provides the basic capability to define classes, subclasses, and properties of objects. The Web Ontology Language (OWL) provides additional levels of semantics and enables integration with classification engines.[15][16]\n Knowledge-representation is a field of artificial intelligence that focuses on designing computer representations that capture information about the world that can be used for solving complex problems.\n The justification for knowledge representation is that conventional procedural code is not the best formalism to use to solve complex problems. Knowledge representation makes complex software easier to define and maintain than procedural code and can be used in expert systems.\n For example, talking to experts in terms of business rules rather than code lessens the semantic gap between users and developers and makes development of complex systems more practical.\n Knowledge representation goes hand in hand with automated reasoning because one of the main purposes of explicitly representing knowledge is to be able to reason about that knowledge, to make inferences, assert new knowledge, etc. Virtually all knowledge representation languages have a reasoning or inference engine as part of the system.[17]\n A key trade-off in the design of knowledge representation formalisms is that between expressivity and tractability.[18] First Order Logic (FOL), with its high expressive power and ability to formalise much of mathematics, is a standard for comparing the expressibility of  knowledge representation languages.\n Arguably, FOL has two drawbacks as a knowledge representation formalism in its own right, namely ease of use and efficiency of implementation. Firstly, because of its high expressive power, FOL allows many ways of expressing the same information, and this can make it hard for users to formalise or even to understand knowledge expressed in complex, mathematically-oriented ways. Secondly, because of its complex proof procedures, it can be difficult for users to understand complex proofs and explanations, and it can be hard for implementations to be efficient. As a consequence, unrestricted FOL can be intimidating for many software developers.\n One of the key discoveries of AI research in the 1970s was that languages that do not have the full expressive power of FOL can still provide close to the same expressive power of FOL, but can be easier for both the average developer and for the computer to understand. Many of the early AI knowledge representation formalisms, from databases to semantic nets to production systems, can be viewed as making various design decisions about how to balance expressive power with naturalness of expression and efficiency.[19] In particular, this balancing act was a driving motivation for the development of IF-THEN rules in rule-based expert systems. \n A similar balancing act was also a motivation for the development of  logic programming (LP) and the logic programming language Prolog. Logic programs have a rule-based syntax, which is easily confused with the IF-THEN syntax of production rules. But logic programs have a well-defined logical semantics, whereas production systems do not.\n The earliest form of logic programming was based on the Horn clause subset of FOL. But later extensions of LP included the negation as failure inference rule, which turns LP into a non-monotonic logic for default reasoning. The resulting extended semantics of LP is a variation of the standard semantics of Horn clauses and FOL, and is a form of database semantics, [20] which includes the unique name assumption and a form of closed world assumption. These assumptions are much harder to state and reason with explicitly using the standard semantics of FOL.\n In a key 1993 paper on the topic, Randall Davis of MIT outlined five distinct roles to analyze a knowledge representation framework:[21]\n Knowledge representation and reasoning are a key enabling technology for the Semantic Web. Languages based on the Frame model with automatic classification provide a layer of semantics on top of the existing Internet. Rather than searching via text strings as is typical today, it will be possible to define logical queries and find pages that map to those queries.[15] The automated reasoning component in these systems is an engine known as the classifier. Classifiers focus on the subsumption relations in a knowledge base rather than rules. A classifier can infer new classes and dynamically change the ontology as new information becomes available. This capability is ideal for the ever-changing and evolving information space of the Internet.[22]\n The Semantic Web integrates concepts from knowledge representation and reasoning with markup languages based on XML.  The Resource Description Framework (RDF) provides the basic capabilities to define knowledge-based objects on the Internet with basic features such as Is-A relations and object properties. The Web Ontology Language (OWL) adds additional semantics and integrates with automatic classification reasoners.[16]\n In 1985, Ron Brachman categorized the core issues for knowledge representation as follows:[23]\n In the early years of knowledge-based systems the knowledge-bases were fairly small. The knowledge-bases that were meant to actually solve real problems rather than do proof of concept demonstrations needed to focus on well defined problems. So for example, not just medical diagnosis as a whole topic, but medical diagnosis of certain kinds of diseases.\n As knowledge-based technology scaled up, the need for larger knowledge bases and for modular knowledge bases that could communicate and integrate with each other became apparent. This gave rise to the discipline of ontology engineering, designing and building large knowledge bases that could be used by multiple projects. One of the leading research projects in this area was the Cyc project. Cyc was an attempt to build a huge encyclopedic knowledge base that would contain not just expert knowledge but common-sense knowledge. In designing an artificial intelligence agent, it was soon realized that representing common-sense knowledge, knowledge that humans simply take for granted, was essential to make an AI that could interact with humans using natural language. Cyc was meant to address this problem. The language they defined was known as CycL.\n After CycL, a number of ontology languages have been developed. Most are declarative languages, and are either frame languages, or are based on first-order logic. Modularity—the ability to define boundaries around specific domains and problem spaces—is essential for these languages because as stated by Tom Gruber, \"Every ontology is a treaty–a social agreement among people with common motive in sharing.\" There are always many competing and differing views that make any general-purpose ontology impossible. A general-purpose ontology would have to be applicable in any domain and different areas of knowledge need to be unified.[27]\n There is a long history of work attempting to build ontologies for a variety of task domains, e.g., an ontology for liquids,[28] the lumped element model widely used in representing electronic circuits (e.g.[29]), as well as ontologies for time, belief, and even programming itself. Each of these offers a way to see some part of the world.\n The lumped element model, for instance, suggests that we think of circuits in terms of components with connections between them, with signals flowing instantaneously along the connections. This is a useful view, but not the only possible one. A different ontology arises if we need to attend to the electrodynamics in the device: Here signals propagate at finite speed and an object (like a resistor) that was previously viewed as a single component with an I/O behavior may now have to be thought of as an extended medium through which an electromagnetic wave flows.\n Ontologies can of course be written down in a wide variety of languages and notations (e.g., logic, LISP, etc.); the essential information is not the form of that language but the content, i.e., the set of concepts offered as a way of thinking about the world. Simply put, the important part is notions like connections and components, not the choice between writing them as predicates or LISP constructs.\n The commitment made selecting one or another ontology can produce a sharply different view of the task at hand. Consider the difference that arises in selecting the lumped element view of a circuit rather than the electrodynamic view of the same device. As a second example, medical diagnosis viewed in terms of rules (e.g., MYCIN) looks substantially different from the same task viewed in terms of frames (e.g., INTERNIST). Where MYCIN sees the medical world as made up of empirical associations connecting symptom to disease, INTERNIST sees a set of prototypes, in particular prototypical diseases, to be matched against the case at hand.\n"
    },
    {
        "title": "State space search",
        "url": "https://en.wikipedia.org/wiki/State_space_search",
        "content": "State space search is a process used in the field of computer science, including artificial intelligence (AI), in which successive configurations or states of an instance are considered, with the intention of finding a goal state with the desired property.\n Problems are often modelled as a state space, a set of states that a problem can be in. The set of states forms a graph where two states are connected if there is an operation that can be performed to transform the first state into the second.\n State space search often differs from traditional computer science search methods because the state space is implicit: the typical state space graph is much too large to generate and store in memory.  Instead, nodes are generated as they are explored, and typically discarded thereafter.  A solution to a combinatorial search instance may consist of the goal state itself, or of a path from some initial state to the goal state.\n In state space search, a state space is formally represented as a tuple \n\n\n\nS\n:\n⟨\nS\n,\nA\n,\nA\nc\nt\ni\no\nn\n(\ns\n)\n,\nR\ne\ns\nu\nl\nt\n(\ns\n,\na\n)\n,\nC\no\ns\nt\n(\ns\n,\na\n)\n⟩\n\n\n{\\displaystyle S:\\langle S,A,Action(s),Result(s,a),Cost(s,a)\\rangle }\n\n, in which:\n According to Poole and Mackworth, the following are uninformed state-space search methods, meaning that they do not have any prior information about the goal's location.[1]\n These methods take the goal's location in the form of a heuristic function.[2] Poole and Mackworth cite the following examples as informed search algorithms:\n \n This artificial intelligence-related article is a stub. You can help Wikipedia by expanding it."
    },
    {
        "title": "Mathematical optimization",
        "url": "https://en.wikipedia.org/wiki/Mathematical_optimization",
        "content": "Mathematical optimization (alternatively spelled optimisation) or mathematical programming is the selection of a best element, with regard to some criteria, from some set of available alternatives.[1][2] It is generally divided into two subfields: discrete optimization and continuous optimization. Optimization problems arise in all quantitative disciplines from computer science and engineering[3] to operations research and economics, and the development of solution methods has been of interest in mathematics for centuries.[4][5]\n In the more general approach, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations constitutes a large area of applied mathematics.[6]\n Optimization problems can be divided into two categories, depending on whether the variables are continuous or discrete: \n An optimization problem can be represented in the following way:\n Such a formulation is called an optimization problem or a mathematical programming problem (a term not directly related to computer programming, but still in use for example in linear programming – see History below). Many real-world and theoretical problems may be modeled in this general framework.\n Since the following is valid:\n it suffices to solve only minimization problems. However, the opposite perspective of considering only maximization problems would be valid, too.\n Problems formulated using this technique in the fields of physics may refer to the technique as energy minimization,[7] speaking of the value of the function f as representing the energy of the system being modeled. In machine learning, it is always necessary to continuously evaluate the quality of a data model by using a cost function where a minimum implies a set of possibly optimal parameters with an optimal (lowest) error.\n Typically, A is some subset of the Euclidean space \n\n\n\n\n\nR\n\n\nn\n\n\n\n\n{\\displaystyle \\mathbb {R} ^{n}}\n\n, often specified by a set of constraints, equalities or inequalities that the members of A have to satisfy. The domain A of f is called the search space or the choice set, while the elements of A are called candidate solutions or feasible solutions.\n The function f is variously called an objective function, criterion function, loss function, cost function (minimization),[8] utility function or fitness function (maximization), or, in certain fields, an energy function or energy functional. A feasible solution that minimizes (or maximizes) the objective function is called an optimal solution.\n In mathematics, conventional optimization problems are usually stated in terms of minimization.\n A local minimum x* is defined as an element for which there exists some δ > 0 such that\n the expression f(x*) ≤ f(x) holds;\n that is to say, on some region around x* all of the function values are greater than or equal to the value at that element. \nLocal maxima are defined similarly.\n While a local minimum is at least as good as any nearby elements, a global minimum is at least as good as every feasible element.\nGenerally, unless the objective function is convex in a minimization problem, there may be several local minima.\nIn a convex problem, if there is a local minimum that is interior (not on the edge of the set of feasible elements), it is also the global minimum, but a nonconvex problem may have more than one local minimum not all of which need be global minima.\n A large number of algorithms proposed for solving the nonconvex problems – including the majority of commercially available solvers – are not capable of making a distinction between locally optimal solutions and globally optimal solutions, and will treat the former as actual solutions to the original problem. Global optimization is the branch of applied mathematics and numerical analysis that is concerned with the development of deterministic algorithms that are capable of guaranteeing convergence in finite time to the actual optimal solution of a nonconvex problem.\n Optimization problems are often expressed with special notation. Here are some examples:\n Consider the following notation:\n This denotes the minimum value of the objective function x2 + 1, when choosing x from the set of real numbers \n\n\n\n\nR\n\n\n\n{\\displaystyle \\mathbb {R} }\n\n. The minimum value in this case is 1, occurring at x = 0.\n Similarly, the notation\n asks for the maximum value of the objective function 2x, where x may be any real number. In this case, there is no such maximum as the objective function is unbounded, so the answer is \"infinity\" or \"undefined\".\n Consider the following notation:\n or equivalently\n This represents the value (or values) of the argument x in the interval (−∞,−1] that minimizes (or minimize) the objective function x2 + 1 (the actual minimum value of that function is not what the problem asks for). In this case, the answer is x = −1, since x = 0 is infeasible, that is, it does not belong to the feasible set.\n Similarly,\n or equivalently\n represents the {x, y} pair (or pairs) that maximizes (or maximize) the value of the objective function x cos y, with the added constraint that x lie in the interval [−5,5] (again, the actual maximum value of the expression does not matter). In this case, the solutions are the pairs of the form {5, 2kπ} and {−5, (2k + 1)π}, where k ranges over all integers.\n Operators arg min and arg max are sometimes also written as argmin and argmax, and stand for argument of the minimum and argument of the maximum.\n Fermat and Lagrange found calculus-based formulae for identifying optima, while Newton and Gauss proposed iterative methods for moving towards an optimum.\n The term \"linear programming\" for certain optimization cases was due to George B. Dantzig, although much of the theory had been introduced by Leonid Kantorovich in 1939. (Programming in this context does not refer to computer programming, but comes from the use of program by the United States military to refer to proposed training and logistics schedules, which were the problems Dantzig studied at that time.) Dantzig published the Simplex algorithm in 1947, and also John von Neumann and other researchers worked on the theoretical aspects of linear programming (like the theory of duality) around the same time.[9]\n Other notable researchers in mathematical optimization include the following:\n In a number of subfields, the techniques are designed primarily for optimization in dynamic contexts (that is, decision making over time):\n Adding more than one objective to an optimization problem adds complexity. For example, to optimize a structural design, one would desire a design that is both light and rigid. When two objectives conflict, a trade-off must be created. There may be one lightest design, one stiffest design, and an infinite number of designs that are some compromise of weight and rigidity. The set of trade-off designs that improve upon one criterion at the expense of another is known as the Pareto set. The curve created plotting weight against stiffness of the best designs is known as the Pareto frontier.\n A design is judged to be \"Pareto optimal\" (equivalently, \"Pareto efficient\" or in the Pareto set) if it is not dominated by any other design: If it is worse than another design in some respects and no better in any respect, then it is dominated and is not Pareto optimal.\n The choice among \"Pareto optimal\" solutions to determine the \"favorite solution\" is delegated to the decision maker. In other words, defining the problem as multi-objective optimization signals that some information is missing: desirable objectives are given but combinations of them are not rated relative to each other. In some cases, the missing information can be derived by interactive sessions with the decision maker.\n Multi-objective optimization problems have been generalized further into vector optimization problems where the (partial) ordering is no longer given by the Pareto ordering.\n Optimization problems are often multi-modal; that is, they possess multiple good solutions. They could all be globally good (same cost function value) or there could be a mix of globally good and locally good solutions. Obtaining all (or at least some of) the multiple solutions is the goal of a multi-modal optimizer.\n Classical optimization techniques due to their iterative approach do not perform satisfactorily when they are used to obtain multiple solutions, since it is not guaranteed that different solutions will be obtained even with different starting points in multiple runs of the algorithm.\n Common approaches to global optimization problems, where multiple local extrema may be present include evolutionary algorithms, Bayesian optimization and simulated annealing.\n The satisfiability problem, also called the feasibility problem, is just the problem of finding any feasible solution at all without regard to objective value. This can be regarded as the special case of mathematical optimization where the objective value is the same for every solution, and thus any solution is optimal.\n Many optimization algorithms need to start from a feasible point. One way to obtain such a point is to relax the feasibility conditions using a slack variable; with enough slack, any starting point is feasible. Then, minimize that slack variable until the slack is null or negative.\n The extreme value theorem of Karl Weierstrass states that a continuous real-valued function on a compact set attains its maximum and minimum value. More generally, a lower semi-continuous function on a compact set attains its minimum; an upper semi-continuous function on a compact set attains its maximum point or view.\n One of Fermat's theorems states that optima of unconstrained problems are found at stationary points, where the first derivative or the gradient of the objective function is zero (see first derivative test). More generally, they may be found at critical points, where the first derivative or gradient of the objective function is zero or is undefined, or on the boundary of the choice set. An equation (or set of equations) stating that the first derivative(s) equal(s) zero at an interior optimum is called a 'first-order condition' or a set of first-order conditions.\n Optima of equality-constrained problems can be found by the Lagrange multiplier method. The optima of problems with equality and/or inequality constraints can be found using the 'Karush–Kuhn–Tucker conditions'.\n While the first derivative test identifies points that might be extrema, this test does not distinguish a point that is a minimum from one that is a maximum or one that is neither. When the objective function is twice differentiable, these cases can be distinguished by checking the second derivative or the matrix of second derivatives (called the Hessian matrix) in unconstrained problems, or the matrix of second derivatives of the objective function and the constraints called the bordered Hessian in constrained problems. The conditions that distinguish maxima, or minima, from other stationary points are called 'second-order conditions' (see 'Second derivative test'). If a candidate solution satisfies the first-order conditions, then the satisfaction of the second-order conditions as well is sufficient to establish at least local optimality.\n The envelope theorem describes how the value of an optimal solution changes when an underlying parameter changes. The process of computing this change is called comparative statics.\n The maximum theorem of Claude Berge (1963) describes the continuity of an optimal solution as a function of underlying parameters.\n For unconstrained problems with twice-differentiable functions, some critical points can be found by finding the points where the gradient of the objective function is zero (that is, the stationary points). More generally, a zero subgradient certifies that a local minimum has been found for minimization problems with convex functions and other locally Lipschitz functions, which meet in loss function minimization of the neural network. The positive-negative momentum estimation lets to avoid the local minimum and converges at the objective function global minimum.[10]\n Further, critical points can be classified using the definiteness of the Hessian matrix: If the Hessian is positive definite at a critical point, then the point is a local minimum; if the Hessian matrix is negative definite, then the point is a local maximum; finally, if indefinite, then the point is some kind of saddle point.\n Constrained problems can often be transformed into unconstrained problems with the help of Lagrange multipliers. Lagrangian relaxation can also provide approximate solutions to difficult constrained problems.\n When the objective function is a convex function, then any local minimum will also be a global minimum. There exist efficient numerical techniques for minimizing convex functions, such as interior-point methods.\n More generally, if the objective function is not a quadratic function, then many optimization methods use other methods to ensure that some subsequence of iterations converges to an optimal solution. The first and still popular method for ensuring convergence relies on line searches, which optimize a function along one dimension. A second and increasingly popular method for ensuring convergence uses trust regions. Both line searches and trust regions are used in modern methods of non-differentiable optimization. Usually, a global optimizer is much slower than advanced local optimizers (such as BFGS), so often an efficient global optimizer can be constructed by starting the local optimizer from different starting points.\n To solve problems, researchers may use algorithms that terminate in a finite number of steps, or iterative methods that converge to a solution (on some specified class of problems), or heuristics that may provide approximate solutions to some problems (although their iterates need not converge).\n The iterative methods used to solve problems of nonlinear programming differ according to whether they evaluate Hessians, gradients, or only function values. While evaluating Hessians (H) and gradients (G) improves the rate of convergence, for functions for which these quantities exist and vary sufficiently smoothly, such evaluations increase the computational complexity (or computational cost) of each iteration. In some cases, the computational complexity may be excessively high.\n One major criterion for optimizers is just the number of required function evaluations as this often is already a large computational effort, usually much more effort than within the optimizer itself, which mainly has to operate over the N variables. The derivatives provide detailed information for such optimizers, but are even harder to calculate, e.g. approximating the gradient takes at least N+1 function evaluations. For approximations of the 2nd derivatives (collected in the Hessian matrix), the number of function evaluations is in the order of N². Newton's method requires the 2nd-order derivatives, so for each iteration, the number of function calls is in the order of N², but for a simpler pure gradient optimizer it is only N. However, gradient optimizers need usually more iterations than Newton's algorithm. Which one is best with respect to the number of function calls depends on the problem itself.\n Besides (finitely terminating) algorithms and (convergent) iterative methods, there are heuristics. A heuristic is any algorithm which is not guaranteed (mathematically) to find the solution, but which is nevertheless useful in certain practical situations. List of some well-known heuristics:\n Problems in rigid body dynamics (in particular articulated rigid body dynamics) often require mathematical programming techniques, since you can view rigid body dynamics as attempting to solve an ordinary differential equation on a constraint manifold;[11] the constraints are various nonlinear geometric constraints such as \"these two points must always coincide\", \"this surface must not penetrate any other\", or \"this point must always lie somewhere on this curve\". Also, the problem of computing contact forces can be done by solving a linear complementarity problem, which can also be viewed as a QP (quadratic programming) problem.\n Many design problems can also be expressed as optimization programs. This application is called design optimization. One subset is the engineering optimization, and another recent and growing subset of this field is multidisciplinary design optimization, which, while useful in many problems, has in particular been applied to aerospace engineering problems.\n This approach may be applied in cosmology and astrophysics.[12]\n Economics is closely enough linked to optimization of agents that an influential definition relatedly describes economics qua science as the \"study of human behavior as a relationship between ends and scarce means\" with alternative uses.[13] Modern optimization theory includes traditional optimization theory but also overlaps with game theory and the study of economic equilibria. The Journal of Economic Literature codes classify mathematical programming, optimization techniques, and related topics under JEL:C61-C63.\n In microeconomics, the utility maximization problem and its dual problem, the expenditure minimization problem, are economic optimization problems. Insofar as they behave consistently, consumers are assumed to maximize their utility, while firms are usually assumed to maximize their profit. Also, agents are often modeled as being risk-averse, thereby preferring to avoid risk. Asset prices are also modeled using optimization theory, though the underlying mathematics relies on optimizing stochastic processes rather than on static optimization. International trade theory also uses optimization to explain trade patterns between nations. The optimization of portfolios is an example of multi-objective optimization in economics.\n Since the 1970s, economists have modeled dynamic decisions over time using control theory.[14] For example, dynamic search models are used to study labor-market behavior.[15] A crucial distinction is between deterministic and stochastic models.[16] Macroeconomists build dynamic stochastic general equilibrium (DSGE) models that describe the dynamics of the whole economy as the result of the interdependent optimizing decisions of workers, consumers, investors, and governments.[17][18][19]\n Some common applications of optimization techniques in electrical engineering include active filter design,[20] stray field reduction in superconducting magnetic energy storage systems, space mapping design of microwave structures,[21] handset antennas,[22][23][24] electromagnetics-based design. Electromagnetically validated design optimization of microwave components and antennas has made extensive use of an appropriate physics-based or empirical surrogate model and space mapping methodologies since the discovery of space mapping in 1993.[25][26] Optimization techniques are also used in power-flow analysis.[27]\n Optimization has been widely used in civil engineering. Construction management and transportation engineering are among the main branches of civil engineering that heavily rely on optimization. The most common civil engineering problems that are solved by optimization are cut and fill of roads, life-cycle analysis of structures and infrastructures,[28] resource leveling,[29][30] water resource allocation, traffic management[31] and schedule optimization.\n Another field that uses optimization techniques extensively is operations research.[32] Operations research also uses stochastic modeling and simulation to support improved decision-making. Increasingly, operations research uses stochastic programming to model dynamic decisions that adapt to events; such problems can be solved with large-scale optimization and stochastic optimization methods.\n Mathematical optimization is used in much modern controller design. High-level controllers such as model predictive control (MPC) or real-time optimization (RTO) employ mathematical optimization. These algorithms run online and repeatedly determine values for decision variables, such as choke openings in a process plant, by iteratively solving a mathematical optimization problem including constraints and a model of the system to be controlled.\n Optimization techniques are regularly used in geophysical parameter estimation problems. Given a set of geophysical measurements, e.g. seismic recordings, it is common to solve for the physical properties and geometrical shapes of the underlying rocks and fluids. The majority of problems in geophysics are nonlinear with both deterministic and stochastic methods being widely used.\n Nonlinear optimization methods are widely used in conformational analysis.\n Optimization techniques are used in many facets of computational systems biology such as model building, optimal experimental design, metabolic engineering, and synthetic biology.[33] Linear programming has been applied to calculate the maximal possible yields of fermentation products,[33] and to infer gene regulatory networks from multiple microarray datasets[34] as well as transcriptional regulatory networks from high-throughput data.[35] Nonlinear programming has been used to analyze energy metabolism[36] and has been applied to metabolic engineering and parameter estimation in biochemical pathways.[37]\n"
    },
    {
        "title": "Logic",
        "url": "https://en.wikipedia.org/wiki/Formal_logic",
        "content": "\n Logic is the study of correct reasoning. It includes both formal and informal logic. Formal logic is the study of deductively valid inferences or logical truths. It examines how conclusions follow from premises based on the structure of arguments alone, independent of their topic and content. Informal logic is associated with informal fallacies, critical thinking, and argumentation theory. Informal logic examines arguments expressed in natural language whereas formal logic uses formal language. When used as a countable noun, the term \"a logic\" refers to a specific logical formal system that articulates a proof system. Logic plays a central role in many fields, such as philosophy, mathematics, computer science, and linguistics.\n Logic studies arguments, which consist of a set of premises that leads to a conclusion. An example is the argument from the premises \"it's Sunday\" and \"if it's Sunday then I don't have to work\" leading to the conclusion \"I don't have to work\".[1] Premises and conclusions express propositions or claims that can be true or false. An important feature of propositions is their internal structure. For example, complex propositions are made up of simpler propositions linked by logical vocabulary like \n\n\n\n∧\n\n\n{\\displaystyle \\land }\n\n (and) or \n\n\n\n→\n\n\n{\\displaystyle \\to }\n\n (if...then). Simple propositions also have parts, like \"Sunday\" or \"work\" in the example. The truth of a proposition usually depends on the meanings of all of its parts. However, this is not the case for logically true propositions. They are true only because of their logical structure independent of the specific meanings of the individual parts.\n Arguments can be either correct or incorrect. An argument is correct if its premises support its conclusion. Deductive arguments have the strongest form of support: if their premises are true then their conclusion must also be true. This is not the case for ampliative arguments, which arrive at genuinely new information not found in the premises. Many arguments in everyday discourse and the sciences are ampliative arguments. They are divided into inductive and abductive arguments. Inductive arguments are statistical generalizations, such as inferring that all ravens are black based on many individual observations of black ravens.[2] Abductive arguments are inferences to the best explanation, for example, when a doctor concludes that a patient has a certain disease which explains the symptoms they suffer.[3] Arguments that fall short of the standards of correct reasoning often embody fallacies. Systems of logic are theoretical frameworks for assessing the correctness of arguments.\n Logic has been studied since antiquity. Early approaches include Aristotelian logic, Stoic logic, Nyaya, and Mohism. Aristotelian logic focuses on reasoning in the form of syllogisms. It was considered the main system of logic in the Western world until it was replaced by modern formal logic, which has its roots in the work of late 19th-century mathematicians such as Gottlob Frege. Today, the most commonly used system is classical logic. It consists of propositional logic and first-order logic. Propositional logic only considers logical relations between full propositions. First-order logic also takes the internal parts of propositions into account, like predicates and quantifiers. Extended logics accept the basic intuitions behind classical logic and apply it to other fields, such as metaphysics, ethics, and epistemology. Deviant logics, on the other hand, reject certain classical intuitions and provide alternative explanations of the basic laws of logic.\n The word \"logic\" originates from the Greek word logos, which has a variety of translations, such as reason, discourse, or language.[4] Logic is traditionally defined as the study of the laws of thought or correct reasoning,[5] and is usually understood in terms of inferences or arguments. Reasoning is the activity of drawing inferences. Arguments are the outward expression of inferences.[6] An argument is a set of premises together with a conclusion. Logic is interested in whether arguments are correct, i.e. whether their premises support the conclusion.[7] These general characterizations apply to logic in the widest sense, i.e., to both formal and informal logic since they are both concerned with assessing the correctness of arguments.[8] Formal logic is the traditionally dominant field, and some logicians restrict logic to formal logic.[9]\n Formal logic (also known as symbolic logic) is widely used in mathematical logic. It uses a formal approach to study reasoning: it replaces concrete expressions with abstract symbols to examine the logical form of arguments independent of their concrete content. In this sense, it is topic-neutral since it is only concerned with the abstract structure of arguments and not with their concrete content.[10]\n Formal logic is interested in deductively valid arguments, for which the truth of their premises ensures the truth of their conclusion. This means that it is impossible for the premises to be true and the conclusion to be false.[11] For valid arguments, the logical structure of the premises and the conclusion follows a pattern called a rule of inference.[12] For example, modus ponens is a rule of inference according to which all arguments of the form \"(1) p, (2) if p then q, (3) therefore q\" are valid, independent of what the terms p and q stand for.[13] In this sense, formal logic can be defined as the science of valid inferences. An alternative definition sees logic as the study of logical truths.[14] A proposition is logically true if its truth depends only on the logical vocabulary used in it. This means that it is true in all possible worlds and under all interpretations of its non-logical terms, like the claim \"either it is raining, or it is not\".[15] These two definitions of formal logic are not identical, but they are closely related. For example, if the inference from p to q is deductively valid then the claim \"if p then q\" is a logical truth.[16]\n Formal logic uses formal languages to express and analyze arguments.[17] They normally have a very limited vocabulary and exact syntactic rules. These rules specify how their symbols can be combined to construct sentences, so-called well-formed formulas.[18] This simplicity and exactness of formal logic make it capable of formulating precise rules of inference. They determine whether a given argument is valid.[19] Because of the reliance on formal language, natural language arguments cannot be studied directly. Instead, they need to be translated into formal language before their validity can be assessed.[20]\n The term \"logic\" can also be used in a slightly different sense as a countable noun. In this sense, a logic is a logical formal system. Distinct logics differ from each other concerning the rules of inference they accept as valid and the formal languages used to express them.[21] Starting in the late 19th century, many new formal systems have been proposed. There are disagreements about what makes a formal system a logic.[22] For example, it has been suggested that only logically complete systems, like first-order logic, qualify as logics. For such reasons, some theorists deny that higher-order logics are logics in the strict sense.[23]\n When understood in a wide sense, logic encompasses both formal and informal logic.[24] Informal logic uses non-formal criteria and standards to analyze and assess the correctness of arguments. Its main focus is on everyday discourse.[25] Its development was prompted by difficulties in applying the insights of formal logic to natural language arguments.[26] In this regard, it considers problems that formal logic on its own is unable to address.[27] Both provide criteria for assessing the correctness of arguments and distinguishing them from fallacies.[28]\n Many characterizations of informal logic have been suggested but there is no general agreement on its precise definition.[29] The most literal approach sees the terms \"formal\" and \"informal\" as applying to the language used to express arguments. On this view, informal logic studies arguments that are in informal or natural language.[30] Formal logic can only examine them indirectly by translating them first into a formal language while informal logic investigates them in their original form.[31] On this view, the argument \"Birds fly. Tweety is a bird. Therefore, Tweety flies.\" belongs to natural language and is examined by informal logic. But the formal translation \"(1) \n\n\n\n∀\nx\n(\nB\ni\nr\nd\n(\nx\n)\n→\nF\nl\ni\ne\ns\n(\nx\n)\n)\n\n\n{\\displaystyle \\forall x(Bird(x)\\to Flies(x))}\n\n; (2) \n\n\n\nB\ni\nr\nd\n(\nT\nw\ne\ne\nt\ny\n)\n\n\n{\\displaystyle Bird(Tweety)}\n\n; (3) \n\n\n\nF\nl\ni\ne\ns\n(\nT\nw\ne\ne\nt\ny\n)\n\n\n{\\displaystyle Flies(Tweety)}\n\n\" is studied by formal logic.[32] The study of natural language arguments comes with various difficulties. For example, natural language expressions are often ambiguous, vague, and context-dependent.[33] Another approach defines informal logic in a wide sense as the normative study of the standards, criteria, and procedures of argumentation. In this sense, it includes questions about the role of rationality, critical thinking, and the psychology of argumentation.[34]\n Another characterization identifies informal logic with the study of non-deductive arguments. In this way, it contrasts with deductive reasoning examined by formal logic.[35] Non-deductive arguments make their conclusion probable but do not ensure that it is true. An example is the inductive argument from the empirical observation that \"all ravens I have seen so far are black\" to the conclusion \"all ravens are black\".[36]\n A further approach is to define informal logic as the study of informal fallacies.[37] Informal fallacies are incorrect arguments in which errors are present in the content and the context of the argument.[38] A false dilemma, for example, involves an error of content by excluding viable options. This is the case in the fallacy \"you are either with us or against us; you are not with us; therefore, you are against us\".[39] Some theorists state that formal logic studies the general form of arguments while informal logic studies particular instances of arguments. Another approach is to hold that formal logic only considers the role of logical constants for correct inferences while informal logic also takes the meaning of substantive concepts into account. Further approaches focus on the discussion of logical topics with or without formal devices and on the role of epistemology for the assessment of arguments.[40]\n Premises and conclusions are the basic parts of inferences or arguments and therefore play a central role in logic. In the case of a valid inference or a correct argument, the conclusion follows from the premises, or in other words, the premises support the conclusion.[41] For instance, the premises \"Mars is red\" and \"Mars is a planet\" support the conclusion \"Mars is a red planet\". For most types of logic, it is accepted that premises and conclusions have to be truth-bearers.[41][a] This means that they have a truth value: they are either true or false. Contemporary philosophy generally sees them either as propositions or as sentences.[43] Propositions are the denotations of sentences and are usually seen as abstract objects.[44] For example, the English sentence \"the tree is green\" is different from the German sentence \"der Baum ist grün\" but both express the same proposition.[45]\n Propositional theories of premises and conclusions are often criticized because they rely on abstract objects. For instance, philosophical naturalists usually reject the existence of abstract objects. Other arguments concern the challenges involved in specifying the identity criteria of propositions.[43] These objections are avoided by seeing premises and conclusions not as propositions but as sentences, i.e. as concrete linguistic objects like the symbols displayed on a page of a book. But this approach comes with new problems of its own: sentences are often context-dependent and ambiguous, meaning an argument's validity would not only depend on its parts but also on its context and on how it is interpreted.[46] Another approach is to understand premises and conclusions in psychological terms as thoughts or judgments. This position is known as psychologism. It was discussed at length around the turn of the 20th century but it is not widely accepted today.[47]\n Premises and conclusions have an internal structure. As propositions or sentences, they can be either simple or complex.[48] A complex proposition has other propositions as its constituents, which are linked to each other through propositional connectives like \"and\" or \"if...then\". Simple propositions, on the other hand, do not have propositional parts. But they can also be conceived as having an internal structure: they are made up of subpropositional parts, like singular terms and predicates.[49][48] For example, the simple proposition \"Mars is red\" can be formed by applying the predicate \"red\" to the singular term \"Mars\". In contrast, the complex proposition \"Mars is red and Venus is white\" is made up of two simple propositions connected by the propositional connective \"and\".[49]\n Whether a proposition is true depends, at least in part, on its constituents. For complex propositions formed using truth-functional propositional connectives, their truth only depends on the truth values of their parts.[49][50] But this relation is more complicated in the case of simple propositions and their subpropositional parts. These subpropositional parts have meanings of their own, like referring to objects or classes of objects.[51] Whether the simple proposition they form is true depends on their relation to reality, i.e. what the objects they refer to are like. This topic is studied by theories of reference.[52]\n Some complex propositions are true independently of the substantive meanings of their parts.[53] In classical logic, for example, the complex proposition \"either Mars is red or Mars is not red\" is true independent of whether its parts, like the simple proposition \"Mars is red\", are true or false. In such cases, the truth is called a logical truth: a proposition is logically true if its truth depends only on the logical vocabulary used in it.[54] This means that it is true under all interpretations of its non-logical terms. In some modal logics, this means that the proposition is true in all possible worlds.[55] Some theorists define logic as the study of logical truths.[16]\n Truth tables can be used to show how logical connectives work or how the truth values of complex propositions depends on their parts. They have a column for each input variable. Each row corresponds to one possible combination of the truth values these variables can take; for truth tables presented in the English literature, the symbols \"T\" and \"F\" or \"1\" and \"0\" are commonly used as abbreviations for the truth values \"true\" and \"false\".[56] The first columns present all the possible truth-value combinations for the input variables. Entries in the other columns present the truth values of the corresponding expressions as determined by the input values. For example, the expression \"\n\n\n\np\n∧\nq\n\n\n{\\displaystyle p\\land q}\n\n\" uses the logical connective \n\n\n\n∧\n\n\n{\\displaystyle \\land }\n\n (and). It could be used to express a sentence like \"yesterday was Sunday and the weather was good\". It is only true if both of its input variables, \n\n\n\np\n\n\n{\\displaystyle p}\n\n (\"yesterday was Sunday\") and \n\n\n\nq\n\n\n{\\displaystyle q}\n\n (\"the weather was good\"), are true. In all other cases, the expression as a whole is false. Other important logical connectives are \n\n\n\n¬\n\n\n{\\displaystyle \\lnot }\n\n (not), \n\n\n\n∨\n\n\n{\\displaystyle \\lor }\n\n (or), \n\n\n\n→\n\n\n{\\displaystyle \\to }\n\n (if...then), and \n\n\n\n↑\n\n\n{\\displaystyle \\uparrow }\n\n (Sheffer stroke).[57] Given the conditional proposition \n\n\n\np\n→\nq\n\n\n{\\displaystyle p\\to q}\n\n, one can form truth tables of its converse \n\n\n\nq\n→\np\n\n\n{\\displaystyle q\\to p}\n\n, its inverse (\n\n\n\n¬\np\n→\n¬\nq\n\n\n{\\displaystyle \\lnot p\\to \\lnot q}\n\n), and its contrapositive (\n\n\n\n¬\nq\n→\n¬\np\n\n\n{\\displaystyle \\lnot q\\to \\lnot p}\n\n). Truth tables can also be defined for more complex expressions that use several propositional connectives.[58]\n Logic is commonly defined in terms of arguments or inferences as the study of their correctness.[59] An argument is a set of premises together with a conclusion.[60] An inference is the process of reasoning from these premises to the conclusion.[43] But these terms are often used interchangeably in logic. Arguments are correct or incorrect depending on whether their premises support their conclusion. Premises and conclusions, on the other hand, are true or false depending on whether they are in accord with reality. In formal logic, a sound argument is an argument that is both correct and has only true premises.[61] Sometimes a distinction is made between simple and complex arguments. A complex argument is made up of a chain of simple arguments. This means that the conclusion of one argument acts as a premise of later arguments. For a complex argument to be successful, each link of the chain has to be successful.[43]\n Arguments and inferences are either correct or incorrect. If they are correct then their premises support their conclusion. In the incorrect case, this support is missing. It can take different forms corresponding to the different types of reasoning.[62] The strongest form of support corresponds to deductive reasoning. But even arguments that are not deductively valid may still be good arguments because their premises offer non-deductive support to their conclusions. For such cases, the term ampliative or inductive reasoning is used.[63] Deductive arguments are associated with formal logic in contrast to the relation between ampliative arguments and informal logic.[64]\n A deductively valid argument is one whose premises guarantee the truth of its conclusion.[11] For instance, the argument \"(1) all frogs are amphibians; (2) no cats are amphibians; (3) therefore no cats are frogs\" is deductively valid. For deductive validity, it does not matter whether the premises or the conclusion are actually true. So the argument \"(1) all frogs are mammals; (2) no cats are mammals; (3) therefore no cats are frogs\" is also valid because the conclusion follows necessarily from the premises.[65]\n According to an influential view by Alfred Tarski, deductive arguments have three essential features: (1) they are formal, i.e. they depend only on the form of the premises and the conclusion; (2) they are a priori, i.e. no sense experience is needed to determine whether they obtain; (3) they are modal, i.e. that they hold by logical necessity for the given propositions, independent of any other circumstances.[66]\n Because of the first feature, the focus on formality, deductive inference is usually identified with rules of inference.[67] Rules of inference specify the form of the premises and the conclusion: how they have to be structured for the inference to be valid. Arguments that do not follow any rule of inference are deductively invalid.[68] The modus ponens is a prominent rule of inference. It has the form \"p; if p, then q; therefore q\".[69] Knowing that it has just rained (\n\n\n\np\n\n\n{\\displaystyle p}\n\n) and that after rain the streets are wet (\n\n\n\np\n→\nq\n\n\n{\\displaystyle p\\to q}\n\n), one can use modus ponens to deduce that the streets are wet (\n\n\n\nq\n\n\n{\\displaystyle q}\n\n).[70]\n The third feature can be expressed by stating that deductively valid inferences are truth-preserving: it is impossible for the premises to be true and the conclusion to be false.[71] Because of this feature, it is often asserted that deductive inferences are uninformative since the conclusion cannot arrive at new information not already present in the premises.[72] But this point is not always accepted since it would mean, for example, that most of mathematics is uninformative. A different characterization distinguishes between surface and depth information. The surface information of a sentence is the information it presents explicitly. Depth information is the totality of the information contained in the sentence, both explicitly and implicitly. According to this view, deductive inferences are uninformative on the depth level. But they can be highly informative on the surface level by making implicit information explicit. This happens, for example, in mathematical proofs.[73]\n Ampliative arguments are arguments whose conclusions contain additional information not found in their premises. In this regard, they are more interesting since they contain information on the depth level and the thinker may learn something genuinely new. But this feature comes with a certain cost: the premises support the conclusion in the sense that they make its truth more likely but they do not ensure its truth.[74] This means that the conclusion of an ampliative argument may be false even though all its premises are true. This characteristic is closely related to non-monotonicity and defeasibility: it may be necessary to retract an earlier conclusion upon receiving new information or in light of new inferences drawn.[75] Ampliative reasoning plays a central role in many arguments found in everyday discourse and the sciences. Ampliative arguments are not automatically incorrect. Instead, they just follow different standards of correctness. The support they provide for their conclusion usually comes in degrees. This means that strong ampliative arguments make their conclusion very likely while weak ones are less certain. As a consequence, the line between correct and incorrect arguments is blurry in some cases, such as when the premises offer weak but non-negligible support. This contrasts with deductive arguments, which are either valid or invalid with nothing in-between.[76]\n The terminology used to categorize ampliative arguments is inconsistent. Some authors, like James Hawthorne, use the term \"induction\" to cover all forms of non-deductive arguments.[77] But in a more narrow sense, induction is only one type of ampliative argument alongside abductive arguments.[78] Some philosophers, like Leo Groarke, also allow conductive arguments[b] as another type.[79] In this narrow sense, induction is often defined as a form of statistical generalization.[80] In this case, the premises of an inductive argument are many individual observations that all show a certain pattern. The conclusion then is a general law that this pattern always obtains.[81] In this sense, one may infer that \"all elephants are gray\" based on one's past observations of the color of elephants.[78] A closely related form of inductive inference has as its conclusion not a general law but one more specific instance, as when it is inferred that an elephant one has not seen yet is also gray.[81] Some theorists, like Igor Douven, stipulate that inductive inferences rest only on statistical considerations. This way, they can be distinguished from abductive inference.[78]\n Abductive inference may or may not take statistical observations into consideration. In either case, the premises offer support for the conclusion because the conclusion is the best explanation of why the premises are true.[82] In this sense, abduction is also called the inference to the best explanation.[83] For example, given the premise that there is a plate with breadcrumbs in the kitchen in the early morning, one may infer the conclusion that one's house-mate had a midnight snack and was too tired to clean the table. This conclusion is justified because it is the best explanation of the current state of the kitchen.[78] For abduction, it is not sufficient that the conclusion explains the premises. For example, the conclusion that a burglar broke into the house last night, got hungry on the job, and had a midnight snack, would also explain the state of the kitchen. But this conclusion is not justified because it is not the best or most likely explanation.[82][83]\n Not all arguments live up to the standards of correct reasoning. When they do not, they are usually referred to as fallacies. Their central aspect is not that their conclusion is false but that there is some flaw with the reasoning leading to this conclusion.[84] So the argument \"it is sunny today; therefore spiders have eight legs\" is fallacious even though the conclusion is true. Some theorists, like John Stuart Mill, give a more restrictive definition of fallacies by additionally requiring that they appear to be correct.[85] This way, genuine fallacies can be distinguished from mere mistakes of reasoning due to carelessness. This explains why people tend to commit fallacies: because they have an alluring element that seduces people into committing and accepting them.[86] However, this reference to appearances is controversial because it belongs to the field of psychology, not logic, and because appearances may be different for different people.[87]\n Fallacies are usually divided into formal and informal fallacies.[38] For formal fallacies, the source of the error is found in the form of the argument. For example, denying the antecedent is one type of formal fallacy, as in \"if Othello is a bachelor, then he is male; Othello is not a bachelor; therefore Othello is not male\".[88] But most fallacies fall into the category of informal fallacies, of which a great variety is discussed in the academic literature. The source of their error is usually found in the content or the context of the argument.[89] Informal fallacies are sometimes categorized as fallacies of ambiguity, fallacies of presumption, or fallacies of relevance. For fallacies of ambiguity, the ambiguity and vagueness of natural language are responsible for their flaw, as in \"feathers are light; what is light cannot be dark; therefore feathers cannot be dark\".[90] Fallacies of presumption have a wrong or unjustified premise but may be valid otherwise.[91] In the case of fallacies of relevance, the premises do not support the conclusion because they are not relevant to it.[92]\n The main focus of most logicians is to study the criteria according to which an argument is correct or incorrect. A fallacy is committed if these criteria are violated. In the case of formal logic, they are known as rules of inference.[93] They are definitory rules, which determine whether an inference is correct or which inferences are allowed. Definitory rules contrast with strategic rules. Strategic rules specify which inferential moves are necessary to reach a given conclusion based on a set of premises. This distinction does not just apply to logic but also to games. In chess, for example, the definitory rules dictate that bishops may only move diagonally. The strategic rules, on the other hand, describe how the allowed moves may be used to win a game, for instance, by controlling the center and by defending one's king.[94] It has been argued that logicians should give more emphasis to strategic rules since they are highly relevant for effective reasoning.[93]\n A formal system of logic consists of a formal language together with a set of axioms and a proof system used to draw inferences from these axioms.[95] In logic, axioms are statements that are accepted without proof. They are used to justify other statements.[96] Some theorists also include a semantics that specifies how the expressions of the formal language relate to real objects.[97] Starting in the late 19th century, many new formal systems have been proposed.[98]\n A formal language consists of an alphabet and syntactic rules. The alphabet is the set of basic symbols used in expressions. The syntactic rules determine how these symbols may be arranged to result in well-formed formulas.[99] For instance, the syntactic rules of propositional logic determine that \"\n\n\n\nP\n∧\nQ\n\n\n{\\displaystyle P\\land Q}\n\n\" is a well-formed formula but \"\n\n\n\n∧\nQ\n\n\n{\\displaystyle \\land Q}\n\n\" is not since the logical conjunction \n\n\n\n∧\n\n\n{\\displaystyle \\land }\n\n requires terms on both sides.[100]\n A proof system is a collection of rules to construct formal proofs. It is a tool to arrive at conclusions from a set of axioms. Rules in a proof system are defined in terms of the syntactic form of formulas independent of their specific content. For instance, the classical rule of conjunction introduction states that \n\n\n\nP\n∧\nQ\n\n\n{\\displaystyle P\\land Q}\n\n follows from the premises \n\n\n\nP\n\n\n{\\displaystyle P}\n\n and \n\n\n\nQ\n\n\n{\\displaystyle Q}\n\n. Such rules can be applied sequentially, giving a mechanical procedure for generating conclusions from premises. There are different types of proof systems including natural deduction and sequent calculi.[101]\n A semantics is a system for mapping expressions of a formal language to their denotations. In many systems of logic, denotations are truth values. For instance, the semantics for classical propositional logic assigns the formula \n\n\n\nP\n∧\nQ\n\n\n{\\displaystyle P\\land Q}\n\n the denotation \"true\" whenever \n\n\n\nP\n\n\n{\\displaystyle P}\n\n and \n\n\n\nQ\n\n\n{\\displaystyle Q}\n\n are true. From the semantic point of view, a premise entails a conclusion if the conclusion is true whenever the premise is true.[102]\n A system of logic is sound when its proof system cannot derive a conclusion from a set of premises unless it is semantically entailed by them. In other words, its proof system cannot lead to false conclusions, as defined by the semantics. A system is complete when its proof system can derive every conclusion that is semantically entailed by its premises. In other words, its proof system can lead to any true conclusion, as defined by the semantics. Thus, soundness and completeness together describe a system whose notions of validity and entailment line up perfectly.[103]\n Systems of logic are theoretical frameworks for assessing the correctness of reasoning and arguments. For over two thousand years, Aristotelian logic was treated as the canon of logic in the Western world,[104] but modern developments in this field have led to a vast proliferation of logical systems.[105] One prominent categorization divides modern formal logical systems into classical logic, extended logics, and deviant logics.[106]\n Aristotelian logic encompasses a great variety of topics. They include metaphysical theses about ontological categories and problems of scientific explanation. But in a more narrow sense, it is identical to term logic or syllogistics. A syllogism is a form of argument involving three propositions: two premises and a conclusion. Each proposition has three essential parts: a subject, a predicate, and a copula connecting the subject to the predicate.[107] For example, the proposition \"Socrates is wise\" is made up of the subject \"Socrates\", the predicate \"wise\", and the copula \"is\".[108] The subject and the predicate are the terms of the proposition. Aristotelian logic does not contain complex propositions made up of simple propositions. It differs in this aspect from propositional logic, in which any two propositions can be linked using a logical connective like \"and\" to form a new complex proposition.[109]\n In Aristotelian logic, the subject can be universal, particular, indefinite, or singular. For example, the term \"all humans\" is a universal subject in the proposition \"all humans are mortal\". A similar proposition could be formed by replacing it with the particular term \"some humans\", the indefinite term \"a human\", or the singular term \"Socrates\".[110]\n Aristotelian logic only includes predicates for simple properties of entities. But it lacks predicates corresponding to relations between entities.[111] The predicate can be linked to the subject in two ways: either by affirming it or by denying it.[112] For example, the proposition \"Socrates is not a cat\" involves the denial of the predicate \"cat\" to the subject \"Socrates\". Using combinations of subjects and predicates, a great variety of propositions and syllogisms can be formed. Syllogisms are characterized by the fact that the premises are linked to each other and to the conclusion by sharing one predicate in each case.[113] Thus, these three propositions contain three predicates, referred to as major term, minor term, and middle term.[114] The central aspect of Aristotelian logic involves classifying all possible syllogisms into valid and invalid arguments according to how the propositions are formed.[112][115] For example, the syllogism \"all men are mortal; Socrates is a man; therefore Socrates is mortal\" is valid. The syllogism \"all cats are mortal; Socrates is mortal; therefore Socrates is a cat\", on the other hand, is invalid.[116]\n Classical logic is distinct from traditional or Aristotelian logic. It encompasses propositional logic and first-order logic. It is \"classical\" in the sense that it is based on basic logical intuitions shared by most logicians.[117] These intuitions include the law of excluded middle, the double negation elimination, the principle of explosion, and the bivalence of truth.[118] It was originally developed to analyze mathematical arguments and was only later applied to other fields as well. Because of this focus on mathematics, it does not include logical vocabulary relevant to many other topics of philosophical importance. Examples of concepts it overlooks are the contrast between necessity and possibility and the problem of ethical obligation and permission. Similarly, it does not address the relations between past, present, and future.[119] Such issues are addressed by extended logics. They build on the basic intuitions of classical logic and expand it by introducing new logical vocabulary. This way, the exact logical approach is applied to fields like ethics or epistemology that lie beyond the scope of mathematics.[120]\n Propositional logic comprises formal systems in which formulae are built from atomic propositions using logical connectives. For instance, propositional logic represents the conjunction of two atomic propositions \n\n\n\nP\n\n\n{\\displaystyle P}\n\n and \n\n\n\nQ\n\n\n{\\displaystyle Q}\n\n as the complex formula \n\n\n\nP\n∧\nQ\n\n\n{\\displaystyle P\\land Q}\n\n. Unlike predicate logic where terms and predicates are the smallest units, propositional logic takes full propositions with truth values as its most basic component.[121] Thus, propositional logics can only represent logical relationships that arise from the way complex propositions are built from simpler ones. But it cannot represent inferences that result from the inner structure of a proposition.[122]\n First-order logic includes the same propositional connectives as propositional logic but differs from it because it articulates the internal structure of propositions. This happens through devices such as singular terms, which refer to particular objects, predicates, which refer to properties and relations, and quantifiers, which treat notions like \"some\" and \"all\".[123] For example, to express the proposition \"this raven is black\", one may use the predicate \n\n\n\nB\n\n\n{\\displaystyle B}\n\n for the property \"black\" and the singular term \n\n\n\nr\n\n\n{\\displaystyle r}\n\n referring to the raven to form the expression \n\n\n\nB\n(\nr\n)\n\n\n{\\displaystyle B(r)}\n\n. To express that some objects are black, the existential quantifier \n\n\n\n∃\n\n\n{\\displaystyle \\exists }\n\n is combined with the variable \n\n\n\nx\n\n\n{\\displaystyle x}\n\n to form the proposition \n\n\n\n∃\nx\nB\n(\nx\n)\n\n\n{\\displaystyle \\exists xB(x)}\n\n. First-order logic contains various rules of inference that determine how expressions articulated this way can form valid arguments, for example, that one may infer \n\n\n\n∃\nx\nB\n(\nx\n)\n\n\n{\\displaystyle \\exists xB(x)}\n\n from \n\n\n\nB\n(\nr\n)\n\n\n{\\displaystyle B(r)}\n\n.[124]\n Extended logics are logical systems that accept the basic principles of classical logic. They introduce additional symbols and principles to apply it to fields like metaphysics, ethics, and epistemology.[125]\n Modal logic is an extension of classical logic. In its original form, sometimes called \"alethic modal logic\", it introduces two new symbols: \n\n\n\n◊\n\n\n{\\displaystyle \\Diamond }\n\n expresses that something is possible while \n\n\n\n◻\n\n\n{\\displaystyle \\Box }\n\n expresses that something is necessary.[126] For example, if the formula \n\n\n\nB\n(\ns\n)\n\n\n{\\displaystyle B(s)}\n\n stands for the sentence \"Socrates is a banker\" then the formula \n\n\n\n◊\nB\n(\ns\n)\n\n\n{\\displaystyle \\Diamond B(s)}\n\n articulates the sentence \"It is possible that Socrates is a banker\".[127] To include these symbols in the logical formalism, modal logic introduces new rules of inference that govern what role they play in inferences. One rule of inference states that, if something is necessary, then it is also possible. This means that \n\n\n\n◊\nA\n\n\n{\\displaystyle \\Diamond A}\n\n follows from \n\n\n\n◻\nA\n\n\n{\\displaystyle \\Box A}\n\n. Another principle states that if a proposition is necessary then its negation is impossible and vice versa. This means that \n\n\n\n◻\nA\n\n\n{\\displaystyle \\Box A}\n\n is equivalent to \n\n\n\n¬\n◊\n¬\nA\n\n\n{\\displaystyle \\lnot \\Diamond \\lnot A}\n\n.[128]\n Other forms of modal logic introduce similar symbols but associate different meanings with them to apply modal logic to other fields. For example, deontic logic concerns the field of ethics and introduces symbols to express the ideas of obligation and permission, i.e. to describe whether an agent has to perform a certain action or is allowed to perform it.[129] The modal operators in temporal modal logic articulate temporal relations. They can be used to express, for example, that something happened at one time or that something is happening all the time.[129] In epistemology, epistemic modal logic is used to represent the ideas of knowing something in contrast to merely believing it to be the case.[130]\n Higher-order logics extend classical logic not by using modal operators but by introducing new forms of quantification.[131] Quantifiers correspond to terms like \"all\" or \"some\". In classical first-order logic, quantifiers are only applied to individuals. The formula \"\n\n\n\n∃\nx\n(\nA\np\np\nl\ne\n(\nx\n)\n∧\nS\nw\ne\ne\nt\n(\nx\n)\n)\n\n\n{\\displaystyle \\exists x(Apple(x)\\land Sweet(x))}\n\n\" (some apples are sweet) is an example of the existential quantifier \"\n\n\n\n∃\n\n\n{\\displaystyle \\exists }\n\n\" applied to the individual variable \"\n\n\n\nx\n\n\n{\\displaystyle x}\n\n\". In higher-order logics, quantification is also allowed over predicates. This increases its expressive power. For example, to express the idea that Mary and John share some qualities, one could use the formula \"\n\n\n\n∃\nQ\n(\nQ\n(\nM\na\nr\ny\n)\n∧\nQ\n(\nJ\no\nh\nn\n)\n)\n\n\n{\\displaystyle \\exists Q(Q(Mary)\\land Q(John))}\n\n\". In this case, the existential quantifier is applied to the predicate variable \"\n\n\n\nQ\n\n\n{\\displaystyle Q}\n\n\".[132] The added expressive power is especially useful for mathematics since it allows for more succinct formulations of mathematical theories.[43] But it has drawbacks in regard to its meta-logical properties and ontological implications, which is why first-order logic is still more commonly used.[133]\n Deviant logics are logical systems that reject some of the basic intuitions of classical logic. Because of this, they are usually seen not as its supplements but as its rivals. Deviant logical systems differ from each other either because they reject different classical intuitions or because they propose different alternatives to the same issue.[134]\n Intuitionistic logic is a restricted version of classical logic.[135] It uses the same symbols but excludes some rules of inference. For example, according to the law of double negation elimination, if a sentence is not not true, then it is true. This means that \n\n\n\nA\n\n\n{\\displaystyle A}\n\n follows from \n\n\n\n¬\n¬\nA\n\n\n{\\displaystyle \\lnot \\lnot A}\n\n. This is a valid rule of inference in classical logic but it is invalid in intuitionistic logic. Another classical principle not part of intuitionistic logic is the law of excluded middle. It states that for every sentence, either it or its negation is true. This means that every proposition of the form \n\n\n\nA\n∨\n¬\nA\n\n\n{\\displaystyle A\\lor \\lnot A}\n\n is true.[135] These deviations from classical logic are based on the idea that truth is established by verification using a proof. Intuitionistic logic is especially prominent in the field of constructive mathematics, which emphasizes the need to find or construct a specific example to prove its existence.[136]\n Multi-valued logics depart from classicality by rejecting the principle of bivalence, which requires all propositions to be either true or false. For instance, Jan Łukasiewicz and Stephen Cole Kleene both proposed ternary logics which have a third truth value representing that a statement's truth value is indeterminate.[137] These logics have been applied in the field of linguistics. Fuzzy logics are multivalued logics that have an infinite number of \"degrees of truth\", represented by a real number between 0 and 1.[138]\n Paraconsistent logics are logical systems that can deal with contradictions. They are formulated to avoid the principle of explosion: for them, it is not the case that anything follows from a contradiction.[139] They are often motivated by dialetheism, the view that contradictions are real or that reality itself is contradictory. Graham Priest is an influential contemporary proponent of this position and similar views have been ascribed to Georg Wilhelm Friedrich Hegel.[140]\n Informal logic is usually carried out in a less systematic way. It often focuses on more specific issues, like investigating a particular type of fallacy or studying a certain aspect of argumentation. Nonetheless, some frameworks of informal logic have also been presented that try to provide a systematic characterization of the correctness of arguments.[141]\n The pragmatic or dialogical approach to informal logic sees arguments as speech acts and not merely as a set of premises together with a conclusion.[142] As speech acts, they occur in a certain context, like a dialogue, which affects the standards of right and wrong arguments.[143] A prominent version by Douglas N. Walton understands a dialogue as a game between two players. The initial position of each player is characterized by the propositions to which they are committed and the conclusion they intend to prove. Dialogues are games of persuasion: each player has the goal of convincing the opponent of their own conclusion.[144] This is achieved by making arguments: arguments are the moves of the game.[145] They affect to which propositions the players are committed. A winning move is a successful argument that takes the opponent's commitments as premises and shows how one's own conclusion follows from them. This is usually not possible straight away. For this reason, it is normally necessary to formulate a sequence of arguments as intermediary steps, each of which brings the opponent a little closer to one's intended conclusion. Besides these positive arguments leading one closer to victory, there are also negative arguments preventing the opponent's victory by denying their conclusion.[144] Whether an argument is correct depends on whether it promotes the progress of the dialogue. Fallacies, on the other hand, are violations of the standards of proper argumentative rules.[146] These standards also depend on the type of dialogue. For example, the standards governing the scientific discourse differ from the standards in business negotiations.[147]\n The epistemic approach to informal logic, on the other hand, focuses on the epistemic role of arguments.[148] It is based on the idea that arguments aim to increase our knowledge. They achieve this by linking justified beliefs to beliefs that are not yet justified.[149] Correct arguments succeed at expanding knowledge while fallacies are epistemic failures: they do not justify the belief in their conclusion.[150] For example, the fallacy of begging the question is a fallacy because it fails to provide independent justification for its conclusion, even though it is deductively valid.[151] In this sense, logical normativity consists in epistemic success or rationality.[149] The Bayesian approach is one example of an epistemic approach.[152] Central to Bayesianism is not just whether the agent believes something but the degree to which they believe it, the so-called credence. Degrees of belief are seen as subjective probabilities in the believed proposition, i.e. how certain the agent is that the proposition is true.[153] On this view, reasoning can be interpreted as a process of changing one's credences, often in reaction to new incoming information.[154] Correct reasoning and the arguments it is based on follow the laws of probability, for example, the principle of conditionalization. Bad or irrational reasoning, on the other hand, violates these laws.[155]\n Logic is studied in various fields. In many cases, this is done by applying its formal method to specific topics outside its scope, like to ethics or computer science.[156] In other cases, logic itself is made the subject of research in another discipline. This can happen in diverse ways. For instance, it can involve investigating the philosophical assumptions linked to the basic concepts used by logicians. Other ways include interpreting and analyzing logic through mathematical structures as well as studying and comparing abstract properties of formal logical systems.[157]\n Philosophy of logic is the philosophical discipline studying the scope and nature of logic.[59] It examines many presuppositions implicit in logic, like how to define its basic concepts or the metaphysical assumptions associated with them.[158] It is also concerned with how to classify logical systems and considers the ontological commitments they incur.[159] Philosophical logic is one of the areas within the philosophy of logic. It studies the application of logical methods to philosophical problems in fields like metaphysics, ethics, and epistemology.[160] This application usually happens in the form of extended or deviant logical systems.[161]\n Metalogic is the field of inquiry studying the properties of formal logical systems. For example, when a new formal system is developed, metalogicians may study it to determine which formulas can be proven in it. They may also study whether an algorithm could be developed to find a proof for each formula and whether every provable formula in it is a tautology. Finally, they may compare it to other logical systems to understand its distinctive features. A key issue in metalogic concerns the relation between syntax and semantics. The syntactic rules of a formal system determine how to deduce conclusions from premises, i.e. how to formulate proofs. The semantics of a formal system governs which sentences are true and which ones are false. This determines the validity of arguments since, for valid arguments, it is impossible for the premises to be true and the conclusion to be false. The relation between syntax and semantics concerns issues like whether every valid argument is provable and whether every provable argument is valid. Metalogicians also study whether logical systems are complete, sound, and consistent. They are interested in whether the systems are decidable and what expressive power they have. Metalogicians usually rely heavily on abstract mathematical reasoning when examining and formulating metalogical proofs. This way, they aim to arrive at precise and general conclusions on these topics.[162]\n The term \"mathematical logic\" is sometimes used as a synonym of \"formal logic\". But in a more restricted sense, it refers to the study of logic within mathematics. Major subareas include model theory, proof theory, set theory, and computability theory.[164] Research in mathematical logic commonly addresses the mathematical properties of formal systems of logic. However, it can also include attempts to use logic to analyze mathematical reasoning or to establish logic-based foundations of mathematics.[165] The latter was a major concern in early 20th-century mathematical logic, which pursued the program of logicism pioneered by philosopher-logicians such as Gottlob Frege, Alfred North Whitehead, and Bertrand Russell. Mathematical theories were supposed to be logical tautologies, and their program was to show this by means of a reduction of mathematics to logic. Many attempts to realize this program failed, from the crippling of Frege's project in his Grundgesetze by Russell's paradox, to the defeat of Hilbert's program by Gödel's incompleteness theorems.[166]\n Set theory originated in the study of the infinite by Georg Cantor, and it has been the source of many of the most challenging and important issues in mathematical logic. They include Cantor's theorem, the status of the Axiom of Choice, the question of the independence of the continuum hypothesis, and the modern debate on large cardinal axioms.[167]\n Computability theory is the branch of mathematical logic that studies effective procedures to solve calculation problems. One of its main goals is to understand whether it is possible to solve a given problem using an algorithm. For instance, given a certain claim about the positive integers, it examines whether an algorithm can be found to determine if this claim is true. Computability theory uses various theoretical tools and models, such as Turing machines, to explore this type of issue.[168]\n Computational logic is the branch of logic and computer science that studies how to implement mathematical reasoning and logical formalisms using computers. This includes, for example, automatic theorem provers, which employ rules of inference to construct a proof step by step from a set of premises to the intended conclusion without human intervention.[169] Logic programming languages are designed specifically to express facts using logical formulas and to draw inferences from these facts. For example, Prolog is a logic programming language based on predicate logic.[170] Computer scientists also apply concepts from logic to problems in computing. The works of Claude Shannon were influential in this regard. He showed how Boolean logic can be used to understand and implement computer circuits.[171] This can be achieved using electronic logic gates, i.e. electronic circuits with one or more inputs and usually one output. The truth values of propositions are represented by voltage levels. In this way, logic functions can be simulated by applying the corresponding voltages to the inputs of the circuit and determining the value of the function by measuring the voltage of the output.[172]\n Formal semantics is a subfield of logic, linguistics, and the philosophy of language. The discipline of semantics studies the meaning of language. Formal semantics uses formal tools from the fields of symbolic logic and mathematics to give precise theories of the meaning of natural language expressions. It understands meaning usually in relation to truth conditions, i.e. it examines in which situations a sentence would be true or false. One of its central methodological assumptions is the principle of compositionality. It states that the meaning of a complex expression is determined by the meanings of its parts and how they are combined. For example, the meaning of the verb phrase \"walk and sing\" depends on the meanings of the individual expressions \"walk\" and \"sing\". Many theories in formal semantics rely on model theory. This means that they employ set theory to construct a model and then interpret the meanings of expression in relation to the elements in this model. For example, the term \"walk\" may be interpreted as the set of all individuals in the model that share the property of walking. Early influential theorists in this field were Richard Montague and Barbara Partee, who focused their analysis on the English language.[173]\n The epistemology of logic studies how one knows that an argument is valid or that a proposition is logically true.[174] This includes questions like how to justify that modus ponens is a valid rule of inference or that contradictions are false.[175] The traditionally dominant view is that this form of logical understanding belongs to knowledge a priori.[176] In this regard, it is often argued that the mind has a special faculty to examine relations between pure ideas and that this faculty is also responsible for apprehending logical truths.[177] A similar approach understands the rules of logic in terms of linguistic conventions. On this view, the laws of logic are trivial since they are true by definition: they just express the meanings of the logical vocabulary.[178]\n Some theorists, like Hilary Putnam and Penelope Maddy, object to the view that logic is knowable a priori. They hold instead that logical truths depend on the empirical world. This is usually combined with the claim that the laws of logic express universal regularities found in the structural features of the world. According to this view, they may be explored by studying general patterns of the fundamental sciences. For example, it has been argued that certain insights of quantum mechanics refute the principle of distributivity in classical logic, which states that the formula \n\n\n\nA\n∧\n(\nB\n∨\nC\n)\n\n\n{\\displaystyle A\\land (B\\lor C)}\n\n is equivalent to \n\n\n\n(\nA\n∧\nB\n)\n∨\n(\nA\n∧\nC\n)\n\n\n{\\displaystyle (A\\land B)\\lor (A\\land C)}\n\n. This claim can be used as an empirical argument for the thesis that quantum logic is the correct logical system and should replace classical logic.[179]\n Logic was developed independently in several cultures during antiquity. One major early contributor was Aristotle, who developed term logic in his Organon and Prior Analytics.[183] He was responsible for the introduction of the hypothetical syllogism[184] and temporal modal logic.[185] Further innovations include inductive logic[186] as well as the discussion of new logical concepts such as terms, predicables, syllogisms, and propositions. Aristotelian logic was highly regarded in classical and medieval times, both in Europe and the Middle East. It remained in wide use in the West until the early 19th century.[187] It has now been superseded by later work, though many of its key insights are still present in modern systems of logic.[188]\n Ibn Sina (Avicenna) was the founder of Avicennian logic, which replaced Aristotelian logic as the dominant system of logic in the Islamic world.[189] It influenced Western medieval writers such as Albertus Magnus and William of Ockham.[190] Ibn Sina wrote on the hypothetical syllogism[191] and on the propositional calculus.[192] He developed an original \"temporally modalized\" syllogistic theory, involving temporal logic and modal logic.[193] He also made use of inductive logic, such as his methods of agreement, difference, and concomitant variation, which are critical to the scientific method.[191] Fakhr al-Din al-Razi was another influential Muslim logician. He criticized Aristotelian syllogistics and formulated an early system of inductive logic, foreshadowing the system of inductive logic developed by John Stuart Mill.[194]\n During the Middle Ages, many translations and interpretations of Aristotelian logic were made. The works of Boethius were particularly influential. Besides translating Aristotle's work into Latin, he also produced textbooks on logic.[195] Later, the works of Islamic philosophers such as Ibn Sina and Ibn Rushd (Averroes) were drawn on. This expanded the range of ancient works available to medieval Christian scholars since more Greek work was available to Muslim scholars that had been preserved in Latin commentaries. In 1323, William of Ockham's influential Summa Logicae was released. It is a comprehensive treatise on logic that discusses many basic concepts of logic and provides a systematic exposition of types of propositions and their truth conditions.[196]\n In Chinese philosophy, the School of Names and Mohism were particularly influential. The School of Names focused on the use of language and on paradoxes. For example, Gongsun Long proposed the white horse paradox, which defends the thesis that a white horse is not a horse. The school of Mohism also acknowledged the importance of language for logic and tried to relate the ideas in these fields to the realm of ethics.[197]\n In India, the study of logic was primarily pursued by the schools of Nyaya, Buddhism, and Jainism. It was not treated as a separate academic discipline and discussions of its topics usually happened in the context of epistemology and theories of dialogue or argumentation.[198] In Nyaya, inference is understood as a source of knowledge (pramāṇa). It follows the perception of an object and tries to arrive at conclusions, for example, about the cause of this object.[199] A similar emphasis on the relation to epistemology is also found in Buddhist and Jainist schools of logic, where inference is used to expand the knowledge gained through other sources.[200] Some of the later theories of Nyaya, belonging to the Navya-Nyāya school, resemble modern forms of logic, such as Gottlob Frege's distinction between sense and reference and his definition of number.[201]\n The syllogistic logic developed by Aristotle predominated in the West until the mid-19th century, when interest in the foundations of mathematics stimulated the development of modern symbolic logic.[202] Many see Gottlob Frege's Begriffsschrift as the birthplace of modern logic. Gottfried Wilhelm Leibniz's idea of a universal formal language is often considered a forerunner. Other pioneers were George Boole, who invented Boolean algebra as a mathematical system of logic, and Charles Peirce, who developed the logic of relatives. Alfred North Whitehead and Bertrand Russell, in turn, condensed many of these insights in their work Principia Mathematica. Modern logic introduced novel concepts, such as functions, quantifiers, and relational predicates. A hallmark of modern symbolic logic is its use of formal language to precisely codify its insights. In this regard, it departs from earlier logicians, who relied mainly on natural language.[203] Of particular influence was the development of first-order logic, which is usually treated as the standard system of modern logic.[204] Its analytical generality allowed the formalization of mathematics and drove the investigation of set theory. It also made Alfred Tarski's approach to model theory possible and provided the foundation of modern mathematical logic.[205]\n"
    },
    {
        "title": "Neural network (machine learning)",
        "url": "https://en.wikipedia.org/wiki/Artificial_neural_network",
        "content": "\n In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a model inspired by the structure and function of biological neural networks in animal brains.[1][2]\n An ANN consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain. Artificial neuron models that mimic biological neurons more closely have also been recently investigated and shown to significantly improve performance. These are connected by edges, which model the synapses in the brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process.\n Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least two hidden layers.[3]\n Artificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information.\n \nNeural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network's parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset.[4] Gradient-based methods such as backpropagation are usually used to estimate the parameters of the network.[4] During the training phase, ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function.[5] This method allows the network to generalize to unseen data. Today's deep neural networks are based on early work in statistics over 200 years ago. The simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes with linear activation functions; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.[7][8][9][10][11]\n Historically, digital computers such as the von Neumann model operate via the execution of explicit instructions with access to memory by a number of processors. Some neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model, connectionist computing does not separate memory and processing.\n Warren McCulloch and Walter Pitts[12] (1943) considered a non-learning computational model for neural networks.[13] This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence.\n In the late 1940s, D. O. Hebb[14] proposed a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. It was used in many early neural networks, such as Rosenblatt's perceptron and the Hopfield network. Farley and Clark[15] (1954) used computational machines to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956).[16]\n In 1958, psychologist Frank Rosenblatt described the perceptron, one of the first implemented artificial neural networks,[17][18][19][20] funded by the United States Office of Naval Research.[21]\nR. D. Joseph (1960)[22] mentions an even earlier perceptron-like device by Farley and Clark:[10] \"Farley and Clark of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron-like device.\" However, \"they dropped the subject.\"\nThe perceptron raised public excitement for research in Artificial Neural Networks, causing the US government to drastically increase funding. This contributed to \"the Golden Age of AI\" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence.[23]\n The first perceptrons did not have adaptive hidden units. However, Joseph (1960)[22] also discussed multilayer perceptrons with an adaptive hidden layer. Rosenblatt (1962)[24]: section 16  cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight. Unfortunately, these early efforts did not lead to a working learning algorithm for hidden units, i.e., deep learning.\n Fundamental research was conducted on ANNs in the 1960s and 1970s. The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in the Soviet Union (1965). They regarded it as a form of polynomial regression,[25] or a generalization of Rosenblatt's perceptron.[26] A 1971 paper described a deep network with eight layers trained by this method,[27] which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates.\"[10]\n The first deep learning multilayer perceptron trained by stochastic gradient descent[28] was published in 1967 by Shun'ichi Amari.[29] In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes.[10] Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.\n In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function.[10][30][31] The rectifier has become the most popular activation function for deep learning.[32]\n Nevertheless, research stagnated in the United States following the work of Minsky and Papert (1969),[33] who emphasized that basic perceptrons were incapable of processing the exclusive-or circuit. This insight was irrelevant for the deep networks of Ivakhnenko (1965) and Amari (1967).\n In 1976 transfer learning was introduced in neural networks learning. [34] [35]\n Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers and weight replication began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation.[36][37][38]\n Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673[39] to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt,[24] but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory.[40] In 1970, Seppo Linnainmaa published the modern form of backpropagation in his Master's thesis (1970).[41][42][10] G.M. Ostrovski et al. republished it in 1971.[43][44] Paul Werbos applied backpropagation to neural networks in 1982[45][46] (his 1974 PhD thesis, reprinted in a 1994 book,[47] did not yet describe the algorithm[44]). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work.[48]\n Kunihiko Fukushima's convolutional neural network (CNN) architecture of 1979[36] also introduced max pooling,[49] a popular downsampling procedure for CNNs. CNNs have become an essential tool for computer vision.\n The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation.[50][51] In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition.[52]\nIn 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days.[53] In 1990, Wei Zhang implemented a CNN on optical computing hardware.[54] In 1991, a CNN was applied to medical image object segmentation[55] and breast cancer detection in mammograms.[56] LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 32×32 pixel images.[57]\n From 1988 onward,[58][59] the use of neural networks transformed the field of protein structure prediction, in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments.[60]\n One origin of RNN was statistical mechanics. In 1972, Shun'ichi Amari proposed to modify the weights of an Ising model by Hebbian learning rule as a model of associative memory, adding in the component of learning.[61] This was popularized as the Hopfield network by John Hopfield(1982).[62] Another origin of RNN was neuroscience. The word \"recurrent\" is used to describe loop-like structures in anatomy. In 1901, Cajal observed \"recurrent semicircles\" in the cerebellar cortex.[63] Hebb considered \"reverberating circuit\" as an explanation for short-term memory.[64] The McCulloch and Pitts paper (1943) considered neural networks that contains cycles, and noted that the current activity of such networks can be affected by activity indefinitely far in the past.[12]\n In 1982 a recurrent neural network, with an array architecture (rather than a multilayer perceptron architecture), named Crossbar Adaptive Array [65][66]  used direct recurrent connections from the output to the supervisor (teaching ) inputs. In addition of computing actions (decisions), it computed internal state evaluations (emotions) of the consequence situations. Eliminating the external supervisor, it introduced the self-learning method in neural networks.  \n In cognitive psychology, the journal American Psychologist in early 1980's carried out a debate on relation between cognition and emotion. Zajonc in 1980 stated that emotion is computed first and is independent from cognition, while Lazarus in 1982 stated that cognition is computed first and is inseparable from emotion. [67][68] In 1982 the Crossbar Adaptive Array gave a neural network model of cognition-emotion relation. [65][69] It was an example of a debate where an AI system, a recurrent neural network, contributed to an issue in the same time addressed by cognitive psychology.\n Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology. \n In the 1980s, backpropagation did not work well for deep RNNs. To overcome this problem, in 1991, Jürgen Schmidhuber proposed the \"neural sequence chunker\" or \"neural history compressor\"[70][71] which introduced the important concepts of self-supervised pre-training (the \"P\" in ChatGPT) and neural knowledge distillation.[10] In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.[72]\n In 1991, Sepp Hochreiter's diploma thesis [73] identified and analyzed the vanishing gradient problem[73][74] and proposed recurrent residual connections to solve it. He and Schmidhuber introduced long short-term memory (LSTM), which set accuracy records in multiple applications domains.[75][76] This was not yet the modern version of LSTM, which required the forget gate, which was introduced in 1999.[77] It became the default choice for RNN architecture.\n During 1985–1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine,[78] restricted Boltzmann machine,[79] Helmholtz machine,[80] and the wake-sleep algorithm.[81] These were designed for unsupervised learning of deep generative models.\n Between 2009 and 2012, ANNs began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition.[82][83] In 2011, a CNN named DanNet[84][85] by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3.[38] It then won more contests.[86][87] They also showed how max-pooling CNNs on GPU improved performance significantly.[88]\n In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton[89] won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman[90] and Google's Inceptionv3.[91]\n In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images.[92] Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as \"deep learning\".[5]\n Radial basis function and wavelet networks were introduced in 2013. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.[93]\n Generative adversarial network (GAN) (Ian Goodfellow et al., 2014)[94] became state of the art in generative modeling during 2014–2018 period. The GAN principle was originally published in 1991 by Jürgen Schmidhuber who called it \"artificial curiosity\": two neural networks contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss.[95][96] The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. Excellent image quality is achieved by Nvidia's StyleGAN (2018)[97] based on the Progressive GAN by Tero Karras et al.[98] Here, the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes.[99] Diffusion models (2015)[100] eclipsed GANs in generative modeling since then, with systems such as DALL·E 2 (2022) and Stable Diffusion (2022).\n In 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers.[101] Stacking too many layers led to a steep reduction in training accuracy,[102] known as the \"degradation\" problem.[103] In 2015, two techniques were developed to train very deep networks: the highway network was published in May 2015,[104] and the residual neural network (ResNet) in December 2015.[105][106] ResNet behaves like an open-gated Highway Net. \n During the 2010s, the seq2seq model was developed, and attention mechanisms were added. It led to the modern Transformer architecture in 2017 in Attention Is All You Need.[107]\nIt requires computation time that is quadratic in the size of the context window. Jürgen Schmidhuber's fast weight controller (1992)[108] scales linearly and was later shown to be equivalent to the unnormalized linear Transformer.[109][110][10]\nTransformers have increasingly become the model of choice for natural language processing.[111] Many modern large language models such as ChatGPT, GPT-4, and BERT use this architecture.\n ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results, abandoning attempts to remain true to their biological precursors. ANNs have the ability to learn and model non-linearities and complex relationships. This is achieved by neurons being connected in various patterns, allowing the output of some neurons to become the input of others. The network forms a directed, weighted graph.[112]\n An artificial neural network consists of simulated neurons. Each neuron is connected to other nodes via links like a biological axon-synapse-dendrite connection. All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data. Each link has a weight, determining the strength of one node's influence on another,[113] allowing weights to choose the signal between neurons.\n ANNs are composed of artificial neurons which are conceptually derived from biological neurons. Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons.[114] The inputs can be the feature values of a sample of external data, such as images or documents, or they can be the outputs of other neurons. The outputs of the final output neurons of the neural net accomplish the task, such as recognizing an object in an image.[citation needed]\n To find the output of the neuron we take the weighted sum of all the inputs, weighted by the weights of the connections from the inputs to the neuron. We add a bias term to this sum.[115] This weighted sum is sometimes called the activation. This weighted sum is then passed through a (usually nonlinear) activation function to produce the output. The initial inputs are external data, such as images and documents. The ultimate outputs accomplish the task, such as recognizing an object in an image.[116]\n The neurons are typically organized into multiple layers, especially in deep learning. Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. The layer that receives external data is the input layer. The layer that produces the ultimate result is the output layer. In between them are zero or more hidden layers. Single layer and unlayered networks are also used. Between two layers, multiple connection patterns are possible. They can be 'fully connected', with every neuron in one layer connecting to every neuron in the next layer. They can be pooling, where a group of neurons in one layer connects to a single neuron in the next layer, thereby reducing the number of neurons in that layer.[117] Neurons with only such connections form a directed acyclic graph and are known as feedforward networks.[118] Alternatively, networks that allow connections between neurons in the same or previous layers are known as recurrent networks.[119]\n A hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate, the number of hidden layers and batch size.[citation needed] The values of some hyperparameters can be dependent on those of other hyperparameters. For example, the size of some layers can depend on the overall number of layers.[citation needed]\n Learning is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning, the error rate typically does not reach 0. If after learning, the error rate is too high, the network typically must be redesigned. Practically this is done by defining a cost function that is evaluated periodically during learning. As long as its output continues to decline, learning continues. The cost is frequently defined as a statistic whose value can only be approximated. The outputs are actually numbers, so when the error is low, the difference between the output (almost certainly a cat) and the correct answer (cat) is small. Learning attempts to reduce the total of the differences across the observations. Most learning models can be viewed as a straightforward application of optimization theory and statistical estimation.[112][120]\n The learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation.[121] A high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy. Optimizations such as Quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability. In order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate.[122] The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change. A momentum close to 0 emphasizes the gradient, while a value close to 1 emphasizes the last change.[citation needed]\n While it is possible to define a cost function ad hoc, frequently the choice is determined by the function's desirable properties (such as convexity) or because it arises from the model (e.g. in a probabilistic model the model's posterior probability can be used as an inverse cost).[citation needed]\n Backpropagation is a method used to adjust the connection weights to compensate for each error found during learning. The error amount is effectively divided among the connections. Technically, backprop calculates the gradient (the derivative) of the cost function associated with a given state with respect to the weights. The weight updates can be done via stochastic gradient descent or other methods, such as extreme learning machines,[123] \"no-prop\" networks,[124] training without backtracking,[125] \"weightless\" networks,[126][127] and non-connectionist neural networks.[citation needed]\n Machine learning is commonly separated into three main learning paradigms, supervised learning,[128] unsupervised learning[129] and reinforcement learning.[130] Each corresponds to a particular learning task.\n Supervised learning uses a set of paired inputs and desired outputs. The learning task is to produce the desired output for each input. In this case, the cost function is related to eliminating incorrect deductions.[131] A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output and the desired output. Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). Supervised learning is also applicable to sequential data (e.g., for handwriting, speech and gesture recognition). This can be thought of as learning with a \"teacher\", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.\n In unsupervised learning, input data is given along with the cost function, some function of the data \n\n\n\n\nx\n\n\n\n{\\displaystyle \\textstyle x}\n\n and the network's output. The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables). As a trivial example, consider the model \n\n\n\n\nf\n(\nx\n)\n=\na\n\n\n\n{\\displaystyle \\textstyle f(x)=a}\n\n where \n\n\n\n\na\n\n\n\n{\\displaystyle \\textstyle a}\n\n is a constant and the cost \n\n\n\n\nC\n=\nE\n[\n(\nx\n−\nf\n(\nx\n)\n\n)\n\n2\n\n\n]\n\n\n\n{\\displaystyle \\textstyle C=E[(x-f(x))^{2}]}\n\n. Minimizing this cost produces a value of \n\n\n\n\na\n\n\n\n{\\displaystyle \\textstyle a}\n\n that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between \n\n\n\n\nx\n\n\n\n{\\displaystyle \\textstyle x}\n\n and \n\n\n\n\nf\n(\nx\n)\n\n\n\n{\\displaystyle \\textstyle f(x)}\n\n, whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples, those quantities would be maximized rather than minimized). Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.\n In applications such as playing video games, an actor takes a string of actions, receiving a generally unpredictable response from the environment after each one. The goal is to win the game, i.e., generate the most positive (lowest cost) responses. In reinforcement learning, the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost. At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost, according to some (usually unknown) rules. The rules and the long-term cost usually only can be estimated. At any juncture, the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly.\n Formally the environment is modeled as a Markov decision process (MDP) with states \n\n\n\n\n\n\ns\n\n1\n\n\n,\n.\n.\n.\n,\n\ns\n\nn\n\n\n\n∈\nS\n\n\n\n{\\displaystyle \\textstyle {s_{1},...,s_{n}}\\in S}\n\n and actions \n\n\n\n\n\n\na\n\n1\n\n\n,\n.\n.\n.\n,\n\na\n\nm\n\n\n\n∈\nA\n\n\n\n{\\displaystyle \\textstyle {a_{1},...,a_{m}}\\in A}\n\n. Because the state transitions are not known, probability distributions are used instead: the instantaneous cost distribution \n\n\n\n\nP\n(\n\nc\n\nt\n\n\n\n|\n\n\ns\n\nt\n\n\n)\n\n\n\n{\\displaystyle \\textstyle P(c_{t}|s_{t})}\n\n, the observation distribution \n\n\n\n\nP\n(\n\nx\n\nt\n\n\n\n|\n\n\ns\n\nt\n\n\n)\n\n\n\n{\\displaystyle \\textstyle P(x_{t}|s_{t})}\n\n and the transition distribution \n\n\n\n\nP\n(\n\ns\n\nt\n+\n1\n\n\n\n|\n\n\ns\n\nt\n\n\n,\n\na\n\nt\n\n\n)\n\n\n\n{\\displaystyle \\textstyle P(s_{t+1}|s_{t},a_{t})}\n\n, while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two define a Markov chain (MC). The aim is to discover the lowest-cost MC.\n ANNs serve as the learning component in such applications.[132][133] Dynamic programming coupled with ANNs (giving neurodynamic programming)[134] has been applied to problems such as those involved in vehicle routing,[135] video games, natural resource management[136][137] and medicine[138] because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.\n Self-learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (CAA).[139] It is a system with only one input, situation s, and only one output, action (or behavior) a. It has neither external advice input nor external reinforcement input from the environment. The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about encountered situations. The system is driven by the interaction between cognition and emotion.[140] Given the memory matrix, W =||w(a,s)||, the crossbar self-learning algorithm in each iteration performs the following computation:\n The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, where from it initially and only once receives initial emotions about to be encountered situations in the behavioral environment. Having received the genome vector (species vector) from the genetic environment, the CAA will learn a goal-seeking behavior, in the behavioral environment that contains both desirable and undesirable situations.[141]\n Neuroevolution can create neural network topologies and weights using evolutionary computation. It is competitive with sophisticated gradient descent approaches.[142][143] One advantage of neuroevolution is that it may be less prone to get caught in \"dead ends\".[144]\n Stochastic neural networks originating from Sherrington–Kirkpatrick models are a type of artificial neural network built by introducing random variations into the network, either by giving the network's artificial neurons stochastic transfer functions [citation needed], or by giving them stochastic weights. This makes them useful tools for optimization problems, since the random fluctuations help the network escape from local minima.[145] Stochastic neural networks trained using a Bayesian approach are known as Bayesian neural networks.[146]\n In a Bayesian framework, a distribution over the set of allowed models is chosen to minimize the cost. Evolutionary methods,[147] gene expression programming,[148] simulated annealing,[149] expectation–maximization, non-parametric methods and particle swarm optimization[150] are other learning algorithms. Convergent recursion is a learning algorithm for cerebellar model articulation controller (CMAC) neural networks.[151][152]\n Two modes of learning are available: stochastic and batch. In stochastic learning, each input creates a weight adjustment. In batch learning weights are adjusted based on a batch of inputs, accumulating errors over the batch. Stochastic learning introduces \"noise\" into the process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in local minima. However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the batch's average error. A common compromise is to use \"mini-batches\", small batches with samples in each batch selected stochastically from the entire data set.\n ANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains. The simplest types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to evolve via learning. The latter is much more complicated but can shorten learning periods and produce better results. Some types allow/require learning to be \"supervised\" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers.\n Some of the main breakthroughs include: \n Using artificial neural networks requires an understanding of their characteristics.\n Neural architecture search (NAS) uses machine learning to automate ANN design. Various approaches to NAS have designed networks that compare well with hand-designed systems. The basic search algorithm is to propose a candidate model, evaluate it against a dataset, and use the results as feedback to teach the NAS network.[164] Available systems include AutoML and AutoKeras.[165] scikit-learn library provides functions to help with building a deep network from scratch. We can then implement a deep network with TensorFlow or Keras.\n \nHyperparameters must also be defined as part of the design (they are not learned), governing matters such as how many neurons are in each layer, learning rate, step, stride, depth, receptive field and padding (for CNNs), etc.[166]  [citation needed]\n Because of their ability to reproduce and model nonlinear processes, artificial neural networks have found applications in many disciplines. These include:\n ANNs have been used to diagnose several types of cancers[184][185] and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information.[186][187]\n ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters[188][189] and to predict foundation settlements.[190] It can also be useful to mitigate flood by the use of ANNs for modelling rainfall-runoff.[191] ANNs have also been used for building black-box models in geoscience: hydrology,[192][193] ocean modelling and coastal engineering,[194][195] and geomorphology.[196] ANNs have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones. For example, machine learning has been used for classifying Android malware,[197] for identifying domains belonging to threat actors and for detecting URLs posing a security risk.[198] Research is underway on ANN systems designed for penetration testing, for detecting botnets,[199] credit cards frauds[200] and network intrusions.\n ANNs have been proposed as a tool to solve partial differential equations in physics[201][202][203] and simulate the properties of many-body open quantum systems.[204][205][206][207] In brain research ANNs have studied short-term behavior of individual neurons,[208] the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems. Studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level.\n It is possible to create a profile of a user's interests from pictures, using artificial neural networks trained for object recognition.[209]\n Beyond their traditional applications, artificial neural networks are increasingly being utilized in interdisciplinary research, such as materials science. For instance, graph neural networks (GNNs) have demonstrated their capability in scaling deep learning for the discovery of new stable materials by efficiently predicting the total energy of crystals. This application underscores the adaptability and potential of ANNs in tackling complex problems beyond the realms of predictive modeling and artificial intelligence, opening new pathways for scientific discovery and innovation.[210]\n The multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.\n A specific recurrent architecture with rational-valued weights (as opposed to full precision real number-valued weights) has the power of a universal Turing machine,[211] using a finite number of neurons and standard linear connections. Further, the use of irrational values for weights results in a machine with super-Turing power.[212][213][failed verification]\n A model's \"capacity\" property corresponds to its ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.\nTwo notions of capacity are known by the community. The information capacity and the VC Dimension. The information capacity of a perceptron is intensively discussed in Sir David MacKay's book[214] which summarizes work by Thomas Cover.[215] The capacity of a network of standard neurons (not convolutional) can be derived by four rules[216] that derive from understanding a neuron as an electrical element. The information capacity captures the functions modelable by the network given any data as input. The second notion, is the VC dimension. VC Dimension uses the principles of measure theory and finds the maximum capacity under the best possible circumstances. This is, given input data in a specific form. As noted in,[214] the VC Dimension for arbitrary inputs is half the information capacity of a Perceptron. The VC Dimension for arbitrary points is sometimes referred to as Memory Capacity.[217]\n Models may not consistently converge on a single solution, firstly because local minima may exist, depending on the cost function and the model. Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. Thirdly, for sufficiently large data or parameters, some methods become impractical.\n Another issue worthy to mention is that training may cross some Saddle point which may lead the convergence to the wrong direction.\n The convergence behavior of certain types of ANN architectures are more understood than others. When the width of network approaches to infinity, the ANN is well described by its first order Taylor expansion throughout training, and so inherits the convergence behavior of affine models.[218][219] Another example is when parameters are small, it is observed that ANNs often fits target functions from low to high frequencies. This behavior is referred to as the spectral bias, or frequency principle, of neural networks.[220][221][222][223] This phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as Jacobi method. Deeper neural networks have been observed to be more biased towards low frequency functions.[224]\n Applications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. This arises in convoluted or over-specified systems when the network capacity significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and to select hyperparameters to minimize the generalization error.\n The second is to use some form of regularization. This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.\n Supervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of network output, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.\n By assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications.\n The softmax activation function is:\n \n A common criticism of neural networks, particularly in robotics, is that they require too many training samples for real-world operation.[225]\nAny learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC.[151]\nDean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.), and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience, and preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns—it should not learn to always turn right).[226]\n A central claim[citation needed] of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimed[by whom?] that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997, Alexander Dewdney, a former Scientific American columnist, commented that as a result, artificial neural networks have a \"something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything\".[227] One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks, ranging from autonomously flying aircraft[228] to detecting credit card fraud to mastering the game of Go.\n Technology writer Roger Bridgman commented:\n Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be \"an opaque, unreadable table...valueless as a scientific resource\".\n In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.[229]\n Although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover, recent emphasis on the explainability of AI has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.[230]\n Biological brains use both shallow and deep circuits as reported by brain anatomy,[231] displaying a wide variety of invariance. Weng[232] argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies.\n Large and effective neural networks require considerable computing resources.[233] While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons –  which require enormous CPU power and time.[citation needed]\n Some argue that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before.[38] The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.[233][234]\n Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit, or TPU.[235]\n Analyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example, local vs. non-local learning and shallow vs. deep architecture.[236]\n Advocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind.[237][238]\n Neural networks are dependent on the quality of the data they are trained on, thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases.[239][240] These inherited biases become especially critical when the ANNs are integrated into real-world scenarios where the training data may be imbalanced due to the scarcity of data for a specific race, gender or other attribute.[239] This imbalance can result in the model having inadequate representation and understanding of underrepresented groups, leading to discriminatory outcomes that exacerbate societal inequalities, especially in applications like facial recognition, hiring processes, and law enforcement.[240][241] For example, in 2018, Amazon had to scrap a recruiting tool because the model favored men over women for jobs in software engineering due to the higher number of male workers in the field.[241] The program would penalize any resume with the word \"woman\" or the name of any women's college. However, the use of synthetic data can help reduce dataset bias and increase representation in datasets.[242]\n Artificial neural networks (ANNs) have undergone significant advancements, particularly in their ability to model complex systems, handle large data sets, and adapt to various types of applications. Their evolution over the past few decades has been marked by a broad range of applications in fields such as image processing, speech recognition, natural language processing, finance, and medicine.[citation needed]\n In the realm of image processing, ANNs are employed in tasks such as image classification, object recognition, and image segmentation. For instance, deep convolutional neural networks (CNNs) have been important in handwritten digit recognition, achieving state-of-the-art performance.[243] This demonstrates the ability of ANNs to effectively process and interpret complex visual information, leading to advancements in fields ranging from automated surveillance to medical imaging.[243]\n By modeling speech signals, ANNs are used for tasks like speaker identification and speech-to-text conversion. Deep neural network architectures have introduced significant improvements in large vocabulary continuous speech recognition, outperforming traditional techniques.[243][244] These advancements have enabled the development of more accurate and efficient voice-activated systems, enhancing user interfaces in technology products.[citation needed]\n In natural language processing, ANNs are used for tasks such as text classification, sentiment analysis, and machine translation. They have enabled the development of models that can accurately translate between languages, understand the context and sentiment in textual data, and categorize text based on content.[243][244] This has implications for automated customer service, content moderation, and language understanding technologies.[citation needed]\n In the domain of control systems, ANNs are used to model dynamic systems for tasks such as system identification, control design, and optimization. For instance, deep feedforward neural networks are important in system identification and control applications.[citation needed]\n ANNs are used for stock market prediction and credit scoring: \n ANNs require high-quality data and careful tuning, and their \"black-box\" nature can pose challenges in interpretation. Nevertheless, ongoing advancements suggest that ANNs continue to play a role in finance, offering valuable insights and enhancing risk management strategies.[citation needed]\n ANNs are able to process and analyze vast medical datasets. They enhance diagnostic accuracy, especially by interpreting complex medical imaging for early disease detection, and by predicting patient outcomes for personalized treatment planning.[244] In drug discovery, ANNs speed up the identification of potential drug candidates and predict their efficacy and safety, significantly reducing development time and costs.[243] Additionally, their application in personalized medicine and healthcare data analysis allows tailored therapies and efficient patient care management.[244] Ongoing research is aimed at addressing remaining challenges such as data privacy and model interpretability, as well as expanding the scope of ANN applications in medicine.[citation needed]\n ANNs such as generative adversarial networks (GAN) and transformers are used for content creation across numerous industries.[245] This is because deep learning models are able to learn the style of an artist or musician from huge datasets and generate completely new artworks and music compositions. For instance, DALL-E is a deep neural network trained on 650 million pairs of images and texts across the internet that can create artworks based on text entered by the user.[246] In the field of music, transformers are used to create original music for commercials and documentaries through companies such as AIVA and Jukedeck.[247] In the marketing industry generative models are used to create personalized advertisements for consumers.[245] Additionally, major film companies are partnering with technology companies to analyze the financial success of a film, such as the partnership between Warner Bros and technology company Cinelytic established in 2020.[248] Furthermore, neural networks have found uses in video game creation, where Non Player Characters (NPCs) can make decisions based on all the characters currently in the game.[249]\n"
    },
    {
        "title": "Statistics",
        "url": "https://en.wikipedia.org/wiki/Statistics",
        "content": "Statistics (from German: Statistik, orig. \"description of a state, a country\"[1]) is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data.[2] In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as \"all people living in a country\" or \"every atom composing a crystal\". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments.[3]\n When census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.\n Two main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation).[4] Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences made using mathematical statistics employ the framework of probability theory, which deals with the analysis of random phenomena.\n A standard statistical procedure involves the collection of data leading to a test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is rejected when it is in fact true, giving a \"false positive\") and Type II errors (null hypothesis fails to be rejected when it is in fact false, giving a \"false negative\"). Multiple problems have come to be associated with this framework, ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.[4]\n Statistical measurement processes are also prone to error in regards to the data that they generate. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also occur. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.\n \"Statistics is both the science of uncertainty and the technology of extracting information from data.\" - featured in the International Encyclopedia of Statistical Science.[5] Statistics is the discipline that deals with data, facts and figures with which meaningful information is inferred. Data may represent a numerical value, in form of quantitative data, or a label, as with qualitative data. Data may be collected, presented and summarised, in one of two methods called descriptive statistics. Two elementary summaries of data, singularly called a statistic, are the mean and dispersion. Whereas inferential statistics interprets data from a population sample to induce statements and predictions about a population.[6][7][5]\n Statistics is regarded as a body of science[8] or a branch of mathematics.[9] It is based on probability, a branch of mathematics that studies random events. Statistics is considered the science of uncertainty. This arises from the ways to cope with measurement and sampling error as well as dealing with uncertanties in modelling. Although probability and statistics was once paired together as a single subject, they are conceptually distinct from one another. The former is based on deducing answers to specific situations from a general theory of probability, meanwhile statistics induces statements about a population based on a data set. Statistics serves to bridge the gap between probability and applied mathematical fields.[10][5][11]\n Some consider statistics to be a distinct mathematical science rather than a branch of mathematics. While many scientific investigations make use of data, statistics is generally concerned with the use of data in the context of uncertainty and decision-making in the face of uncertainty.[12][13] Statistics is indexed at 62, a subclass of probability theory and stochastic processes, in the Mathematics Subject Classification.[14] Mathematical statistics is covered in the range 276-280 of subclass QA (science > mathematics) in the Library of Congress Classification.[15]\n The word statistics ultimately comes from the Latin word Status, meaning \"situation\" or \"condition\" in society, which in late Latin adopted the meaning \"state\". Derived from this, political scientist Gottfried Achenwall, coined the German word statistik (a summary of how things stand). In 1770, the term entered the English language through German and referred to the study of political arrangements. The term gained its modern meaning in the 1790s in John Sinclair's works.[16][17] In modern German, the term statistik is synonymous with mathematical statistics. The term statistic, in singular form, is used to describe a function that returns its value of the same name.[18]\n When full census data cannot be collected, statisticians collect sample data by developing specific experiment designs and survey samples. Statistics itself also provides tools for prediction and forecasting through statistical models.\n To use a sample as a guide to an entire population, it is important that it truly represents the overall population. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. A major problem lies in determining the extent that the sample chosen is actually representative. Statistics offers methods to estimate and correct for any bias within the sample and data collection procedures. There are also methods of experimental design that can lessen these issues at the outset of a study, strengthening its capability to discern truths about the population.\n Sampling theory is part of the mathematical discipline of probability theory. Probability is used in mathematical statistics to study the sampling distributions of sample statistics and, more generally, the properties of statistical procedures. The use of any statistical method is valid when the system or population under consideration satisfies the assumptions of the method. The difference in point of view between classic probability theory and sampling theory is, roughly, that probability theory starts from the given parameters of a total population to deduce probabilities that pertain to samples. Statistical inference, however, moves in the opposite direction—inductively inferring from samples to the parameters of a larger or total population.\n A common goal for a statistical research project is to investigate causality, and in particular to draw a conclusion on the effect of changes in the values of predictors or independent variables on dependent variables. There are two major types of causal statistical studies: experimental studies and observational studies. In both types of studies, the effect of differences of an independent variable (or variables) on the behavior of the dependent variable are observed. The difference between the two types lies in how the study is actually conducted. Each can be very effective. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements with different levels using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Instead, data are gathered and correlations between predictors and response are investigated. While the tools of data analysis work best on data from randomized studies, they are also applied to other kinds of data—like natural experiments and observational studies[19]—for which a statistician would use a modified, more structured estimation method (e.g., difference in differences estimation and instrumental variables, among many others) that produce consistent estimators.\n The basic steps of a statistical experiment are:\n Experiments on human behavior have special concerns. The famous Hawthorne study examined changes to the working environment at the Hawthorne plant of the Western Electric Company. The researchers were interested in determining whether increased illumination would increase the productivity of the assembly line workers. The researchers first measured the productivity in the plant, then modified the illumination in an area of the plant and checked if the changes in illumination affected productivity. It turned out that productivity indeed improved (under the experimental conditions). However, the study is heavily criticized today for errors in experimental procedures, specifically for the lack of a control group and blindness. The Hawthorne effect refers to finding that an outcome (in this case, worker productivity) changed due to observation itself. Those in the Hawthorne study became more productive not because the lighting was changed but because they were being observed.[20]\n An example of an observational study is one that explores the association between smoking and lung cancer. This type of study typically uses a survey to collect observations about the area of interest and then performs statistical analysis. In this case, the researchers would collect observations of both smokers and non-smokers, perhaps through a cohort study, and then look for the number of cases of lung cancer in each group.[21] A case-control study is another type of observational study in which people with and without the outcome of interest (e.g. lung cancer) are invited to participate and their exposure histories are collected.\n Various attempts have been made to produce a taxonomy of levels of measurement. The psychophysicist Stanley Smith Stevens defined nominal, ordinal, interval, and ratio scales. Nominal measurements do not have meaningful rank order among values, and permit any one-to-one (injective) transformation. Ordinal measurements have imprecise differences between consecutive values, but have a meaningful order to those values, and permit any order-preserving transformation. Interval measurements have meaningful distances between measurements defined, but the zero value is arbitrary (as in the case with longitude and temperature measurements in Celsius or Fahrenheit), and permit any linear transformation. Ratio measurements have both a meaningful zero value and the distances between different measurements defined, and permit any rescaling transformation.\n Because variables conforming only to nominal or ordinal measurements cannot be reasonably measured numerically, sometimes they are grouped together as categorical variables, whereas ratio and interval measurements are grouped together as quantitative variables, which can be either discrete or continuous, due to their numerical nature. Such distinctions can often be loosely correlated with data type in computer science, in that dichotomous categorical variables may be represented with the Boolean data type, polytomous categorical variables with arbitrarily assigned integers in the integral data type, and continuous variables with the real data type involving floating-point arithmetic. But the mapping of computer science data types to statistical data types depends on which categorization of the latter is being implemented.\n Other categorizations have been proposed. For example, Mosteller and Tukey (1977)[22] distinguished grades, ranks, counted fractions, counts, amounts, and balances. Nelder (1990)[23] described continuous counts, continuous ratios, count ratios, and categorical modes of data. (See also: Chrisman (1998),[24] van den Berg (1991).[25])\n The issue of whether or not it is appropriate to apply different kinds of statistical methods to data obtained from different kinds of measurement procedures is complicated by issues concerning the transformation of variables and the precise interpretation of research questions. \"The relationship between the data and what they describe merely reflects the fact that certain kinds of statistical statements may have truth values which are not invariant under some transformations. Whether or not a transformation is sensible to contemplate depends on the question one is trying to answer.\"[26]: 82 \n A descriptive statistic (in the count noun sense) is a summary statistic that quantitatively describes or summarizes features of a collection of information,[27] while descriptive statistics in the mass noun sense is the process of using and analyzing those statistics. Descriptive statistics is distinguished from inferential statistics (or inductive statistics), in that descriptive statistics aims to summarize a sample, rather than use the data to learn about the population that the sample of data is thought to represent.[28]\n Statistical inference is the process of using data analysis to deduce properties of an underlying probability distribution.[29] Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates.  It is assumed that the observed data set is sampled from a larger population. Inferential statistics can be contrasted with descriptive statistics. Descriptive statistics is solely concerned with properties of the observed data, and it does not rest on the assumption that the data come from a larger population.[30]\n Consider independent identically distributed (IID) random variables with a given probability distribution: standard statistical inference and estimation theory defines a random sample as the random vector given by the column vector of these IID variables.[31] The population being examined is described by a probability distribution that may have unknown parameters.\n A statistic is a random variable that is a function of the random sample, but not a function of unknown parameters. The probability distribution of the statistic, though, may have unknown parameters. Consider now a function of the unknown parameter: an estimator is a statistic used to estimate such function. Commonly used estimators include sample mean, unbiased sample variance and sample covariance.\n A random variable that is a function of the random sample and of the unknown parameter, but whose probability distribution does not depend on the unknown parameter is called a pivotal quantity or pivot. Widely used pivots include the z-score, the chi square statistic and Student's t-value.\n Between two estimators of a given parameter, the one with lower mean squared error is said to be more efficient. Furthermore, an estimator is said to be unbiased if its expected value is equal to the true value of the unknown parameter being estimated, and asymptotically unbiased if its expected value converges at the limit to the true value of such parameter.\n Other desirable properties for estimators include: UMVUE estimators that have the lowest variance for all possible values of the parameter to be estimated (this is usually an easier property to verify than efficiency) and consistent estimators which converges in probability to the true value of such parameter.\n This still leaves the question of how to obtain estimators in a given situation and carry the computation, several methods have been proposed: the method of moments, the maximum likelihood method, the least squares method and the more recent method of estimating equations.\n Interpretation of statistical information can often involve the development of a null hypothesis which is usually (but not necessarily) that no relationship exists among variables or that no change occurred over time.[32][33]\n The best illustration for a novice is the predicament encountered by a criminal trial. The null hypothesis, H0, asserts that the defendant is innocent, whereas the alternative hypothesis, H1, asserts that the defendant is guilty. The indictment comes because of suspicion of the guilt. The H0 (status quo) stands in opposition to H1 and is maintained unless H1 is supported by evidence \"beyond a reasonable doubt\". However, \"failure to reject H0\" in this case does not imply innocence, but merely that the evidence was insufficient to convict. So the jury does not necessarily accept H0 but fails to reject H0. While one can not \"prove\" a null hypothesis, one can test how close it is to being true with a power test, which tests for type II errors.\n What statisticians call an alternative hypothesis is simply a hypothesis that contradicts the null hypothesis.\n Working from a null hypothesis, two broad categories of error are recognized:\n Standard deviation refers to the extent to which individual observations in a sample differ from a central value, such as the sample or population mean, while Standard error refers to an estimate of difference between sample mean and population mean.\n A statistical error is the amount by which an observation differs from its expected value. A residual is the amount an observation differs from the value the estimator of the expected value assumes on a given sample (also called prediction).\n Mean squared error is used for obtaining efficient estimators, a widely used class of estimators. Root mean square error is simply the square root of mean squared error.\n Many statistical methods seek to minimize the residual sum of squares, and these are called \"methods of least squares\" in contrast to Least absolute deviations. The latter gives equal weight to small and big errors, while the former gives more weight to large errors. Residual sum of squares is also differentiable, which provides a handy property for doing regression. Least squares applied to linear regression is called ordinary least squares method and least squares applied to nonlinear regression is called non-linear least squares. Also in a linear regression model the non deterministic part of the model is called error term, disturbance or more simply noise. Both linear regression and non-linear regression are addressed in polynomial least squares, which also describes the variance in a prediction of the dependent variable (y axis) as a function of the independent variable (x axis) and the deviations (errors, noise, disturbances) from the estimated (fitted) curve.\n Measurement processes that generate statistical data are also subject to error.  Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.[34]\n Most studies only sample part of a population, so results do not fully represent the whole population. Any estimates obtained from the sample only approximate the population value. Confidence intervals allow statisticians to express how closely the sample estimate matches the true value in the whole population. Often they are expressed as 95% confidence intervals. Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would include the true (population) value in 95% of all possible cases. This does not imply that the probability that the true value is in the confidence interval is 95%. From the frequentist perspective, such a claim does not even make sense, as the true value is not a random variable.  Either the true value is or is not within the given interval. However, it is true that, before any data are sampled and given a plan for how to construct the confidence interval, the probability is 95% that the yet-to-be-calculated interval will cover the true value: at this point, the limits of the interval are yet-to-be-observed random variables. One approach that does yield an interval that can be interpreted as having a given probability of containing the true value is to use a credible interval from Bayesian statistics: this approach depends on a different way of interpreting what is meant by \"probability\", that is as a Bayesian probability.\n In principle confidence intervals can be symmetrical or asymmetrical. An interval can be asymmetrical because it works as lower or upper bound for a parameter (left-sided interval or right sided interval), but it can also be asymmetrical because the two sided interval is built violating symmetry around the estimate. Sometimes the bounds for a confidence interval are reached asymptotically and these are used to approximate the true bounds.\n Statistics rarely give a simple Yes/No type answer to the question under analysis. Interpretation often comes down to the level of statistical significance applied to the numbers and often refers to the probability of a value accurately rejecting the null hypothesis (sometimes referred to as the p-value).\n The standard approach[31] is to test a null hypothesis against an alternative hypothesis. A critical region is the set of values of the estimator that leads to refuting the null hypothesis. The probability of type I error is therefore the probability that the estimator belongs to the critical region given that null hypothesis is true (statistical significance) and the probability of type II error is the probability that the estimator does not belong to the critical region given that the alternative hypothesis is true. The statistical power of a test is the probability that it correctly rejects the null hypothesis when the null hypothesis is false.\n Referring to statistical significance does not necessarily mean that the overall result is significant in real world terms. For example, in a large study of a drug it may be shown that the drug has a statistically significant but very small beneficial effect, such that the drug is unlikely to help the patient noticeably.\n Although in principle the acceptable level of statistical significance may be subject to debate, the significance level is the largest p-value that allows the test to reject the null hypothesis. This test is logically equivalent to saying that the p-value is the probability, assuming the null hypothesis is true, of observing a result at least as extreme as the test statistic. Therefore, the smaller the significance level, the lower the probability of committing type I error.\n Some problems are usually associated with this framework (See criticism of hypothesis testing):\n Some well-known statistical tests and procedures are:\n Exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.\n Mathematical statistics is the application of mathematics to statistics. Mathematical techniques used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure-theoretic probability theory.[1][7] All statistical analyses make use of at least some mathematics, and mathematical statistics can therefore be regarded as a fundamental component of general statistics.[8]\n Formal discussions on inference date back to the mathematicians and cryptographers of the Islamic Golden Age between the 8th and 13th centuries. Al-Khalil (717–786) wrote the Book of Cryptographic Messages, which contains one of the first uses of permutations and combinations, to list all possible Arabic words with and without vowels.[36] Al-Kindi's Manuscript on Deciphering Cryptographic Messages gave a detailed description of how to use frequency analysis to decipher encrypted messages, providing an early example of statistical inference for decoding. Ibn Adlan (1187–1268) later made an important contribution on the use of sample size in frequency analysis.[36]\n Although the term statistic was introduced by the Italian scholar Girolamo Ghilini in 1589 with reference to a collection of facts and information about a state, it was the German Gottfried Achenwall in 1749 who started using the term as a collection of quantitative information, in the modern use for this science.[37][38] The earliest writing containing statistics in Europe dates back to 1663, with the publication of Natural and Political Observations upon the Bills of Mortality by John Graunt.[39] Early applications of statistical thinking revolved around the needs of states to base policy on demographic and economic data, hence its stat- etymology. The scope of the discipline of statistics broadened in the early 19th century to include the collection and analysis of data in general. Today, statistics is widely employed in government, business, and natural and social sciences.\n The mathematical foundations of statistics developed from discussions concerning games of chance among mathematicians such as Gerolamo Cardano, Blaise Pascal, Pierre de Fermat, and Christiaan Huygens. Although the idea of probability was already examined in ancient and medieval law and philosophy (such as the work of Juan Caramuel), probability theory as a mathematical discipline only took shape at the very end of the 17th century, particularly in Jacob Bernoulli's posthumous work Ars Conjectandi.[40] This was the first book where the realm of games of chance and the realm of the probable (which concerned opinion, evidence, and argument) were combined and submitted to mathematical analysis.[41] The method of least squares was first described by Adrien-Marie Legendre in 1805, though Carl Friedrich Gauss presumably made use of it a decade earlier in 1795.[42]\n The modern field of statistics emerged in the late 19th and early 20th century in three stages.[43] The first wave, at the turn of the century, was led by the work of Francis Galton and Karl Pearson, who transformed statistics into a rigorous mathematical discipline used for analysis, not just in science, but in industry and politics as well. Galton's contributions included introducing the concepts of standard deviation, correlation, regression analysis and the application of these methods to the study of the variety of human characteristics—height, weight and eyelash length among others.[44] Pearson developed the Pearson product-moment correlation coefficient, defined as a product-moment,[45] the method of moments for the fitting of distributions to samples and the Pearson distribution, among many other things.[46] Galton and Pearson founded Biometrika as the first journal of mathematical statistics and biostatistics (then called biometry), and the latter founded the world's first university statistics department at University College London.[47]\n The second wave of the 1910s and 20s was initiated by William Sealy Gosset, and reached its culmination in the insights of Ronald Fisher, who wrote the textbooks that were to define the academic discipline in universities around the world. Fisher's most important publications were his 1918 seminal paper The Correlation between Relatives on the Supposition of Mendelian Inheritance (which was the first to use the statistical term, variance), his classic 1925 work Statistical Methods for Research Workers and his 1935 The Design of Experiments,[48][49][50] where he developed rigorous design of experiments models. He originated the concepts of sufficiency, ancillary statistics, Fisher's linear discriminator and Fisher information.[51] He also coined the term null hypothesis during the Lady tasting tea experiment, which \"is never proved or established, but is possibly disproved, in the course of experimentation\".[52][53] In his 1930 book The Genetical Theory of Natural Selection, he applied statistics to various biological concepts such as Fisher's principle[54] (which A. W. F. Edwards called \"probably the most celebrated argument in evolutionary biology\") and Fisherian runaway,[55][56][57][58][59][60] a concept in sexual selection about a positive feedback runaway effect found in evolution.\n The final wave, which mainly saw the refinement and expansion of earlier developments, emerged from the collaborative work between Egon Pearson and Jerzy Neyman in the 1930s. They introduced the concepts of \"Type II\" error, power of a test and confidence intervals. Jerzy Neyman in 1934 showed that stratified random sampling was in general a better method of estimation than purposive (quota) sampling.[61]\n Today, statistical methods are applied in all fields that involve decision making, for making accurate inferences from a collated body of data and for making decisions in the face of uncertainty based on statistical methodology. The use of modern computers has expedited large-scale statistical computations and has also made possible new methods that are impractical to perform manually. Statistics continues to be an area of active research, for example on the problem of how to analyze big data.[62]\n Applied statistics, sometimes referred to as Statistical science,[63] comprises descriptive statistics and the application of inferential statistics.[64][65] Theoretical statistics concerns the logical arguments underlying justification of approaches to statistical inference, as well as encompassing mathematical statistics. Mathematical statistics includes not only the manipulation of probability distributions necessary for deriving results related to methods of estimation and inference, but also various aspects of computational statistics and the design of experiments.\n Statistical consultants can help organizations and companies that do not have in-house expertise relevant to their particular questions.\n Machine learning models are statistical and probabilistic models that capture patterns in the data through use of computational algorithms.\n Statistics is applicable to a wide variety of academic disciplines, including natural and social sciences, government, and business. Business statistics applies statistical methods in econometrics, auditing and production and operations, including services improvement and marketing research.[66] A study of two journals in tropical biology found that the 12 most frequent statistical tests are: analysis of variance (ANOVA), chi-squared test, Student's t-test, linear regression, Pearson's correlation coefficient, Mann-Whitney U test, Kruskal-Wallis test, Shannon's diversity index, Tukey's range test, cluster analysis, Spearman's rank correlation coefficient and principal component analysis.[67]\n A typical statistics course covers descriptive statistics, probability, binomial and normal distributions, test of hypotheses and confidence intervals, linear regression, and correlation.[68] Modern fundamental statistical courses for undergraduate students focus on correct test selection, results interpretation, and use of free statistics software.[67]\n The rapid and sustained increases in computing power starting from the second half of the 20th century have had a substantial impact on the practice of statistical science. Early statistical models were almost always from the class of linear models, but powerful computers, coupled with suitable numerical algorithms, caused an increased interest in nonlinear models (such as neural networks) as well as the creation of new types, such as generalized linear models and multilevel models.\n Increased computing power has also led to the growing popularity of computationally intensive methods based on resampling, such as permutation tests and the bootstrap, while techniques such as Gibbs sampling have made use of Bayesian models more feasible. The computer revolution has implications for the future of statistics with a new emphasis on \"experimental\" and \"empirical\" statistics. A large number of both general and special purpose statistical software are now available. Examples of available software capable of complex statistical computation include programs such as Mathematica, SAS, SPSS, and R.\n In business, \"statistics\" is a widely used management- and decision support tool. It is particularly applied in financial management, marketing management, and production, services and operations management.[69][70] Statistics is also heavily used in management accounting and auditing. The discipline of Management Science formalizes the use of statistics, and other mathematics, in business. (Econometrics is the application of statistical methods to economic data in order to give empirical content to economic relationships.)\n A typical \"Business Statistics\" course is intended for business majors, and covers[71] descriptive statistics (collection, description, analysis, and summary of data), probability (typically the binomial and normal distributions), test of hypotheses and confidence intervals, linear regression, and correlation; (follow-on) courses may include forecasting, time series, decision trees, multiple linear regression, and other topics from business analytics more generally. Professional certification programs, such as the CFA, often include topics in statistics.\n Traditionally, statistics was concerned with drawing inferences using a semi-standardized methodology that was \"required learning\" in most sciences. This tradition has changed with the use of statistics in non-inferential contexts. What was once considered a dry subject, taken in many fields as a degree-requirement, is now viewed enthusiastically.[according to whom?] Initially derided by some mathematical purists, it is now considered essential methodology in certain areas.\n Statistical techniques are used in a wide range of types of scientific and social research, including: biostatistics, computational biology, computational sociology, network biology, social science, sociology and social research. Some fields of inquiry use applied statistics so extensively that they have specialized terminology. These disciplines include:\n In addition, there are particular types of statistical analysis that have also developed their own specialised terminology and methodology:\n Statistics form a key basis tool in business and manufacturing as well. It is used to understand measurement systems variability, control processes (as in statistical process control or SPC), for summarizing data, and to make data-driven decisions.\n Misuse of statistics can produce subtle but serious errors in description and interpretation—subtle in the sense that even experienced professionals make such errors, and serious in the sense that they can lead to devastating decision errors. For instance, social policy, medical practice, and the reliability of structures like bridges all rely on the proper use of statistics.\n Even when statistical techniques are correctly applied, the results can be difficult to interpret for those lacking expertise. The statistical significance of a trend in the data—which measures the extent to which a trend could be caused by random variation in the sample—may or may not agree with an intuitive sense of its significance. The set of basic statistical skills (and skepticism) that people need to deal with information in their everyday lives properly is referred to as statistical literacy.\n There is a general perception that statistical knowledge is all-too-frequently intentionally misused by finding ways to interpret only the data that are favorable to the presenter.[74] A mistrust and misunderstanding of statistics is associated with the quotation, \"There are three kinds of lies: lies, damned lies, and statistics\". Misuse of statistics can be both inadvertent and intentional, and the book How to Lie with Statistics,[74] by Darrell Huff, outlines a range of considerations. In an attempt to shed light on the use and misuse of statistics, reviews of statistical techniques used in particular fields are conducted (e.g. Warne, Lazo, Ramos, and Ritter (2012)).[75]\n Ways to avoid misuse of statistics include using proper diagrams and avoiding bias.[76] Misuse can occur when conclusions are overgeneralized and claimed to be representative of more than they really are, often by either deliberately or unconsciously overlooking sampling bias.[77] Bar graphs are arguably the easiest diagrams to use and understand, and they can be made either by hand or with simple computer programs.[76] Most people do not look for bias or errors, so they are not noticed. Thus, people may often believe that something is true even if it is not well represented.[77] To make data gathered from statistics believable and accurate, the sample taken must be representative of the whole.[78] According to Huff, \"The dependability of a sample can be destroyed by [bias]... allow yourself some degree of skepticism.\"[79]\n To assist in the understanding of statistics Huff proposed a series of questions to be asked in each case:[74]\n The concept of correlation is particularly noteworthy for the potential confusion it can cause. Statistical analysis of a data set often reveals that two variables (properties) of the population under consideration tend to vary together, as if they were connected. For example, a study of annual income that also looks at age of death, might find that poor people tend to have shorter lives than affluent people. The two variables are said to be correlated; however, they may or may not be the cause of one another. The correlation phenomena could be caused by a third, previously unconsidered phenomenon, called a lurking variable or confounding variable. For this reason, there is no way to immediately infer the existence of a causal relationship between the two variables.\n"
    },
    {
        "title": "Operations research",
        "url": "https://en.wikipedia.org/wiki/Operations_research",
        "content": "\nOperations research (British English: operational research) (U.S. Air Force Specialty Code: Operations Analysis), often shortened to the initialism OR, is a branch of applied mathematics that deals with the development and application of analytical methods to improve decision-making.[1] Although the term management science is sometimes used similarly, the two fields differ in their scope and emphasis.\n Employing techniques from other mathematical sciences, such as modeling, statistics, and optimization, operations research arrives at optimal or near-optimal solutions to decision-making problems. Because of its emphasis on practical applications, operations research has overlapped with many other disciplines, notably industrial engineering. Operations research is often concerned with determining the extreme values of some real-world objective: the maximum (of profit, performance, or yield) or minimum (of loss, risk, or cost). Originating in military efforts before World War II, its techniques have grown to concern problems in a variety of industries.[2]\n Operations research (OR) encompasses the development and the use of a wide range of problem-solving techniques and methods applied in the pursuit of improved decision-making and efficiency, such as simulation, mathematical optimization, queueing theory and other stochastic-process models, Markov decision processes, econometric methods, data envelopment analysis, ordinal priority approach, neural networks, expert systems, decision analysis, and the analytic hierarchy process.[3] Nearly all of these techniques involve the construction of mathematical models that attempt to describe the system. Because of the computational and statistical nature of most of these fields, OR also has strong ties to computer science and analytics. Operational researchers faced with a new problem must determine which of these techniques are most appropriate given the nature of the system, the goals for improvement, and constraints on time and computing power, or develop a new technique specific to the problem at hand (and, afterwards, to that type of problem).\n The major sub-disciplines (but not limited to) in modern operational research, as identified by the journal Operations Research[4] and The Journal of the Operational Research Society [5]  are:\n In the decades after the two world wars, the tools of operations research were more widely applied to problems in business, industry, and society. Since that time, operational research has expanded into a field widely used in industries ranging from petrochemicals to airlines, finance, logistics, and government, moving to a focus on the development of mathematical models that can be used to analyse and optimize sometimes complex systems, and has become an area of active academic and industrial research.[2]\n In the 17th century, mathematicians Blaise Pascal and Christiaan Huygens solved problems involving sometimes complex decisions (problem of points) by using game-theoretic ideas and expected values; others, such as Pierre de Fermat and Jacob Bernoulli, solved these types of problems using combinatorial reasoning instead.[6] Charles Babbage's research into the cost of transportation and sorting of mail led to England's universal \"Penny Post\" in 1840, and to studies into the dynamical behaviour of railway vehicles in defence of the GWR's broad gauge.[7] Beginning in the 20th century, study of inventory management could be considered[by whom?] the origin of modern operations research with economic order quantity developed by Ford W. Harris in 1913. Operational research may[original research?] have originated in the efforts of military planners during World War I (convoy theory and Lanchester's laws). Percy Bridgman brought operational research to bear on problems in physics in the 1920s and would later attempt to extend these to the social sciences.[8]\n Modern operational research originated at the Bawdsey Research Station in the UK in 1937 as the result of an initiative of the station's superintendent, A. P. Rowe and Robert Watson-Watt.[9] Rowe conceived the idea as a means to analyse and improve the working of the UK's early-warning radar system, code-named \"Chain Home\" (CH). Initially, Rowe analysed the operating of the radar equipment and its communication networks, expanding later to include the operating personnel's behaviour. This revealed unappreciated limitations of the CH network and allowed remedial action to be taken.[10]\n Scientists in the United Kingdom (including Patrick Blackett (later Lord Blackett OM PRS), Cecil Gordon, Solly Zuckerman, (later Baron Zuckerman OM, KCB, FRS), C. H. Waddington, Owen Wansbrough-Jones, Frank Yates, Jacob Bronowski and Freeman Dyson), and in the United States (George Dantzig) looked for ways to make better decisions in such areas as logistics and training schedules.\n The modern field of operational research arose during World War II.[dubious – discuss] In the World War II era, operational research was defined as \"a scientific method of providing executive departments with a quantitative basis for decisions regarding the operations under their control\".[11] Other names for it included operational analysis (UK Ministry of Defence from 1962)[12] and quantitative management.[13]\n During the Second World War close to 1,000 men and women in Britain were engaged in operational research. About 200 operational research scientists worked for the British Army.[14]\n Patrick Blackett worked for several different organizations during the war. Early in the war while working for the Royal Aircraft Establishment (RAE) he set up a team known as the \"Circus\" which helped to reduce the number of anti-aircraft artillery rounds needed to shoot down an enemy aircraft from an average of over 20,000 at the start of the Battle of Britain to 4,000 in 1941.[15]\n In 1941, Blackett moved from the RAE to the Navy, after first working with RAF Coastal Command, in 1941 and then early in 1942 to the Admiralty.[16] Blackett's team at Coastal Command's Operational Research Section (CC-ORS) included two future Nobel prize winners and many other people who went on to be pre-eminent in their fields.[17][18] They undertook a number of crucial analyses that aided the war effort. Britain introduced the convoy system to reduce shipping losses, but while the principle of using warships to accompany merchant ships was generally accepted, it was unclear whether it was better for convoys to be small or large. Convoys travel at the speed of the slowest member, so small convoys can travel faster. It was also argued that small convoys would be harder for German U-boats to detect. On the other hand, large convoys could deploy more warships against an attacker. Blackett's staff showed that the losses suffered by convoys depended largely on the number of escort vessels present, rather than the size of the convoy. Their conclusion was that a few large convoys are more defensible than many small ones.[19]\n \nWhile performing an analysis of the methods used by RAF Coastal Command to hunt and destroy submarines, one of the analysts asked what colour the aircraft were. As most of them were from Bomber Command they were painted black for night-time operations. At the suggestion of CC-ORS a test was run to see if that was the best colour to camouflage the aircraft for daytime operations in the grey North Atlantic skies. Tests showed that aircraft painted white were on average not spotted until they were 20% closer than those painted black. This change indicated that 30% more submarines would be attacked and sunk for the same number of sightings.[20] As a result of these findings Coastal Command changed their aircraft to using white undersurfaces.\n Other work by the CC-ORS indicated that on average if the trigger depth of aerial-delivered depth charges were changed from 100 to 25 feet, the kill ratios would go up. The reason was that if a U-boat saw an aircraft only shortly before it arrived over the target then at 100 feet the charges would do no damage (because the U-boat wouldn't have had time to descend as far as 100 feet), and if it saw the aircraft a long way from the target it had time to alter course under water so the chances of it being within the 20-foot kill zone of the charges was small. It was more efficient to attack those submarines close to the surface when the targets' locations were better known than to attempt their destruction at greater depths when their positions could only be guessed. Before the change of settings from 100 to 25 feet, 1% of submerged U-boats were sunk and 14% damaged. After the change, 7% were sunk and 11% damaged; if submarines were caught on the surface but had time to submerge just before being attacked, the numbers rose to 11% sunk and 15% damaged. Blackett observed \"there can be few cases where such a great operational gain had been obtained by such a small and simple change of tactics\".[21]\n \nBomber Command's Operational Research Section (BC-ORS), analyzed a report of a survey carried out by RAF Bomber Command.[citation needed] For the survey, Bomber Command inspected all bombers returning from bombing raids over Germany over a particular period. All damage inflicted by German air defenses was noted and the recommendation was given that armor be added in the most heavily damaged areas. This recommendation was not adopted because the fact that the aircraft were able to return with these areas damaged indicated the areas were not vital, and adding armor to non-vital areas where damage is acceptable reduces aircraft performance. Their suggestion to remove some of the crew so that an aircraft loss would result in fewer personnel losses, was also rejected by RAF command. Blackett's team made the logical recommendation that the armor be placed in the areas which were completely untouched by damage in the bombers who returned. They reasoned that the survey was biased, since it only included aircraft that returned to Britain. The areas untouched in returning aircraft were probably vital areas, which, if hit, would result in the loss of the aircraft.[22] This story has been disputed,[23] with a similar damage assessment study completed in the US by the Statistical Research Group at Columbia University,[24] the result of work done by Abraham Wald.[25]\n When Germany organized its air defences into the Kammhuber Line, it was realized by the British that if the RAF bombers were to fly in a bomber stream they could overwhelm the night fighters who flew in individual cells directed to their targets by ground controllers. It was then a matter of calculating the statistical loss from collisions against the statistical loss from night fighters to calculate how close the bombers should fly to minimize RAF losses.[26]\n The \"exchange rate\" ratio of output to input was a characteristic feature of operational research. By comparing the number of flying hours put in by Allied aircraft to the number of U-boat sightings in a given area, it was possible to redistribute aircraft to more productive patrol areas. Comparison of exchange rates established \"effectiveness ratios\" useful in planning. The ratio of 60 mines laid per ship sunk was common to several campaigns: German mines in British ports, British mines on German routes, and United States mines in Japanese routes.[27]\n Operational research doubled the on-target bomb rate of B-29s bombing Japan from the Marianas Islands by increasing the training ratio from 4 to 10 percent of flying hours; revealed that wolf-packs of three United States submarines were the most effective number to enable all members of the pack to engage targets discovered on their individual patrol stations; revealed that glossy enamel paint was more effective camouflage for night fighters than conventional dull camouflage paint finish, and a smooth paint finish increased airspeed by reducing skin friction.[27]\n On land, the operational research sections of the Army Operational Research Group (AORG) of the Ministry of Supply (MoS) were landed in Normandy in 1944, and they followed British forces in the advance across Europe. They analyzed, among other topics, the effectiveness of artillery, aerial bombing and anti-tank shooting.\n In 1947, under the auspices of the British Association, a symposium was organized in Dundee. In his opening address, Watson-Watt offered a definition of the aims of OR:\n With expanded techniques and growing awareness of the field at the close of the war, operational research was no longer limited to only operational, but was extended to encompass equipment procurement, training, logistics and infrastructure. Operations research also grew in many areas other than the military once scientists learned to apply its principles to the civilian sector. The development of the simplex algorithm for linear programming was in 1947.[28]\n In the 1950s, the term Operations Research was used to describe heterogeneous mathematical methods such as game theory, dynamic programming, linear programming, warehousing, spare parts theory, queue theory, simulation and production control, which were used primarily in civilian industry. Scientific societies and journals on the subject of operations research were founded in the 1950s, such as the Operation Research Society of America (ORSA) in 1952 and the Institute for Management Science (TIMS) in 1953.[29] Philip Morse, the head of the Weapons Systems Evaluation Group of the Pentagon, became the first president of ORSA and attracted the companies of the military-industrial complex to ORSA, which soon had more than 500 members. In the 1960s, ORSA reached 8000 members.[citation needed] Consulting companies also founded OR groups. In 1953, Abraham Charnes and William Cooper published the first textbook on Linear Programming.[citation needed]\n In the 1950s and 1960s, chairs of operations research were established in the U.S. and United Kingdom (from 1964 in Lancaster) in the management faculties of universities. Further influences from the U.S. on the development of operations research in Western Europe can be traced here. The authoritative[citation needed] OR textbooks from the U.S. were published in Germany in German language and in France in French (but not in Italian[citation needed]), such as the book by George Dantzig \"Linear Programming\"(1963) and the book by C. West Churchman et al. \"Introduction to Operations Research\"(1957). The latter was also published in Spanish in 1973, opening at the same time Latin American readers to Operations Research. NATO gave important impulses for the spread of Operations Research in Western Europe; NATO headquarters (SHAPE) organised four conferences on OR in the 1950s – the one in 1956 with 120 participants – bringing OR to mainland Europe. Within NATO, OR was also known as \"Scientific Advisory\" (SA) and was grouped together in the Advisory Group of Aeronautical Research and Development (AGARD). SHAPE and AGARD organized an OR conference in April 1957 in Paris. When France withdrew from the NATO military command structure, the transfer of NATO headquarters from France to Belgium led to the institutionalization of OR in Belgium, where Jacques Drèze founded CORE, the Center for Operations Research and Econometrics at the Catholic University of Leuven in 1966.[citation needed]\n With the development of computers over the next three decades, Operations Research can now solve problems with hundreds of thousands of variables and constraints. Moreover, the large volumes of data required for such problems can be stored and manipulated very efficiently.\"[28] Much of operations research (modernly known as 'analytics') relies upon stochastic variables and a therefore access to truly random numbers. Fortunately, the cybernetics field also required the same level of randomness. The development of increasingly better random number generators has been a boon to both disciplines. Modern applications of operations research includes city planning, football strategies, emergency planning, optimizing all facets of industry and economy, and undoubtedly with the likelihood of the inclusion of terrorist attack planning and definitely counterterrorist attack planning. More recently, the research approach of operations research, which dates back to the 1950s, has been criticized for being collections of mathematical models but lacking an empirical basis of data collection for applications. How to collect data is not presented in the textbooks. Because of the lack of data, there are also no computer applications in the textbooks.[30]\n Operational research is also used extensively in government where evidence-based policy is used.\n The field of management science (MS) is known as using operations research models in business.[33] Stafford Beer characterized this in 1967.[34] Like operational research itself, management science is an interdisciplinary branch of applied mathematics devoted to optimal decision planning, with strong links with economics, business, engineering, and other sciences. It uses various scientific research-based principles, strategies, and analytical methods including mathematical modeling, statistics and numerical algorithms to improve an organization's ability to enact rational and meaningful management decisions by arriving at optimal or near-optimal solutions to sometimes complex decision problems. Management scientists help businesses to achieve their goals using the scientific methods of operational research.\n The management scientist's mandate is to use rational, systematic, science-based techniques to inform and improve decisions of all kinds. Of course, the techniques of management science are not restricted to business applications but may be applied to military, medical, public administration, charitable groups, political groups or community groups.\n Management science is concerned with developing and applying models and concepts that may prove useful in helping to illuminate management issues and solve managerial problems, as well as designing and developing new and better models of organizational excellence.[35]\n Some of the fields that have considerable overlap with Operations Research and Management Science include:[36]\n Applications are abundant such as in airlines, manufacturing companies, service organizations, military branches, and government. The range of problems and issues to which it has contributed insights and solutions is vast. It includes:[35]\n [37]\n Management is also concerned with so-called soft-operational analysis which concerns methods for strategic planning, strategic decision support, problem structuring methods. \nIn dealing with these sorts of challenges, mathematical modeling and simulation may not be appropriate or may not suffice. Therefore, during the past 30 years[vague], a number of non-quantified modeling methods have been developed. These include:[citation needed]\n The International Federation of Operational Research Societies (IFORS)[38] is an umbrella organization for operational research societies worldwide, representing approximately 50 national societies including those in the US,[39] UK,[40] France,[41] Germany, Italy,[42] Canada,[43] Australia,[44] New Zealand,[45] Philippines,[46] India,[47] Japan and South Africa.[48] For the institutionalization of Operations Research, the foundation of IFORS in 1960 was of decisive importance, which stimulated the foundation of national OR societies in Austria, Switzerland and Germany. IFORS held important international conferences every three years since 1957.[49] The constituent members of IFORS form regional groups, such as that in Europe, the Association of European Operational Research Societies (EURO).[50] Other important operational research organizations are Simulation Interoperability Standards Organization (SISO)[51] and Interservice/Industry Training, Simulation and Education Conference (I/ITSEC)[52]\n In 2004, the US-based organization INFORMS began an initiative to market the OR profession better, including a website entitled The Science of Better[53] which provides an introduction to OR and examples of successful applications of OR to industrial problems. This initiative has been adopted by the Operational Research Society in the UK, including a website entitled Learn About OR.[54]\n The Institute for Operations Research and the Management Sciences (INFORMS) publishes thirteen scholarly journals about operations research, including the top two journals in their class, according to 2005 Journal Citation Reports.[55] They are:\n These are listed in alphabetical order of their titles.\n \n"
    },
    {
        "title": "Economics",
        "url": "https://en.wikipedia.org/wiki/Economics",
        "content": "\n Empirical methods\n Prescriptive and policy\n Economics (/ˌɛkəˈnɒmɪks, ˌiːkə-/)[1][2] is a social science that studies the production, distribution, and consumption of goods and services.[3][4]\n Economics focuses on the behaviour and interactions of economic agents and how economies work. Microeconomics analyses what is viewed as basic elements within economies, including individual agents and markets, their interactions, and the outcomes of interactions. Individual agents may include, for example, households, firms, buyers, and sellers. Macroeconomics analyses economies as systems where production, distribution, consumption, savings, and investment expenditure interact, and factors affecting it: factors of production, such as labour, capital, land, and enterprise, inflation, economic growth, and public policies that have impact on these elements. It also seeks to analyse and describe the global economy.\n Other broad distinctions within economics include those between positive economics, describing \"what is\", and normative economics, advocating \"what ought to be\";[5] between economic theory and applied economics; between rational and behavioural economics; and between mainstream economics and heterodox economics.[6]\n Economic analysis can be applied throughout society, including business,[7] finance, cybersecurity,[8] health care,[9] engineering[10] and government.[11] It is also applied to such diverse subjects as crime,[12] education,[13] the family,[14] feminism,[15] law,[16] philosophy,[17] politics, religion,[18] social institutions, war,[19] science,[20] and the environment.[21]\n \n The earlier term for the discipline was \"political economy\", but since the late 19th century, it has commonly been called \"economics\".[22] The term is ultimately derived from Ancient Greek οἰκονομία (oikonomia) which is a term for the \"way (nomos) to run a household (oikos)\", or in other words the know-how of an οἰκονομικός (oikonomikos), or \"household or homestead manager\". Derived terms such as \"economy\" can therefore often mean \"frugal\" or \"thrifty\".[23][24][25][26] By extension then, \"political economy\" was the way to manage a polis or state.\n There are a variety of modern definitions of economics; some reflect evolving views of the subject or different views among economists.[27][28] Scottish philosopher Adam Smith (1776) defined what was then called political economy as \"an inquiry into the nature and causes of the wealth of nations\", in particular as:\n a branch of the science of a statesman or legislator [with the twofold objectives of providing] a plentiful revenue or subsistence for the people ... [and] to supply the state or commonwealth with a revenue for the publick services.[29] Jean-Baptiste Say (1803), distinguishing the subject matter from its public-policy uses, defined it as the science of production, distribution, and consumption of wealth.[30] On the satirical side, Thomas Carlyle (1849) coined \"the dismal science\" as an epithet for classical economics, in this context, commonly linked to the pessimistic analysis of Malthus (1798).[31] John Stuart Mill (1844) delimited the subject matter further:\n The science which traces the laws of such of the phenomena of society as arise from the combined operations of mankind for the production of wealth, in so far as those phenomena are not modified by the pursuit of any other object.[32] Alfred Marshall provided a still widely cited definition in his textbook Principles of Economics (1890) that extended analysis beyond wealth and from the societal to the microeconomic level:\n Economics is a study of man in the ordinary business of life. It enquires how he gets his income and how he uses it. Thus, it is on the one side, the study of wealth and on the other and more important side, a part of the study of man.[33] Lionel Robbins (1932) developed implications of what has been termed \"[p]erhaps the most commonly accepted current definition of the subject\":[28]\n Economics is the science which studies human behaviour as a relationship between ends and scarce means which have alternative uses.[34] Robbins described the definition as not classificatory in \"pick[ing] out certain kinds of behaviour\" but rather analytical in \"focus[ing] attention on a particular aspect of behaviour, the form imposed by the influence of scarcity.\"[35] He affirmed that previous economists have usually centred their studies on the analysis of wealth: how wealth is created (production), distributed, and consumed; and how wealth can grow.[36] But he said that economics can be used to study other things, such as war, that are outside its usual focus. This is because war has as the goal winning it (as a sought after end), generates both cost and benefits; and, resources (human life and other costs) are used to attain the goal. If the war is not winnable or if the expected costs outweigh the benefits, the deciding actors (assuming they are rational) may never go to war (a decision) but rather explore other alternatives. Economics cannot be defined as the science that studies wealth, war, crime, education, and any other field economic analysis can be applied to; but, as the science that studies a particular common aspect of each of those subjects (they all use scarce resources to attain a sought after end).\n Some subsequent comments criticised the definition as overly broad in failing to limit its subject matter to analysis of markets. From the 1960s, however, such comments abated as the economic theory of maximizing behaviour and rational-choice modelling expanded the domain of the subject to areas previously treated in other fields.[37] There are other criticisms as well, such as in scarcity not accounting for the macroeconomics of high unemployment.[38]\n Gary Becker, a contributor to the expansion of economics into new areas, described the approach he favoured as \"combin[ing the] assumptions of maximizing behaviour, stable preferences, and market equilibrium, used relentlessly and unflinchingly.\"[39] One commentary characterises the remark as making economics an approach rather than a subject matter but with great specificity as to the \"choice process and the type of social interaction that [such] analysis involves.\" The same source reviews a range of definitions included in principles of economics textbooks and concludes that the lack of agreement need not affect the subject-matter that the texts treat. Among economists more generally, it argues that a particular definition presented may reflect the direction toward which the author believes economics is evolving, or should evolve.[28]\n Many economists including nobel prize winners James M. Buchanan and Ronald Coase reject the method-based definition of Robbins and continue to prefer definitions like those of Say, in terms of its subject matter.[37] Ha-Joon Chang has for example argued that the definition of Robbins would make economics very peculiar because all other sciences define themselves in terms of the area of inquiry or object of inquiry rather than the methodology. In the biology department, it is not said that all biology should be studied with DNA analysis. People study living organisms in many different ways, so some people will perform DNA analysis, others might analyse anatomy, and still others might build game theoretic models of animal behaviour. But they are all called biology because they all study living organisms. According to Ha Joon Chang, this view that the economy can and should be studied in only one way (for example by studying only rational choices), and going even one step further and basically redefining economics as a theory of everything, is peculiar.[40]\n Questions regarding distribution of resources are found throughout the writings of the Boeotian poet Hesiod and several economic historians have described Hesiod as the \"first economist\".[41] However, the word Oikos, the Greek word from which the word economy derives, was used for issues regarding how to manage a household (which was understood to be the landowner, his family, and his slaves[42]) rather than to refer to some normative societal system of distribution of resources, which is a more recent phenomenon.[43][44][45] Xenophon, the author of the Oeconomicus, is credited by philologues for being the source of the word economy.[46] Joseph Schumpeter described 16th and 17th century scholastic writers, including Tomás de Mercado, Luis de Molina, and Juan de Lugo, as \"coming nearer than any other group to being the 'founders' of scientific economics\" as to monetary, interest, and value theory within a natural-law perspective.[47]\n Two groups, who later were called \"mercantilists\" and \"physiocrats\", more directly influenced the subsequent development of the subject. Both groups were associated with the rise of economic nationalism and modern capitalism in Europe. Mercantilism was an economic doctrine that flourished from the 16th to 18th century in a prolific pamphlet literature, whether of merchants or statesmen. It held that a nation's wealth depended on its accumulation of gold and silver. Nations without access to mines could obtain gold and silver from trade only by selling goods abroad and restricting imports other than of gold and silver. The doctrine called for importing inexpensive raw materials to be used in manufacturing goods, which could be exported, and for state regulation to impose protective tariffs on foreign manufactured goods and prohibit manufacturing in the colonies.[48]\n Physiocrats, a group of 18th-century French thinkers and writers, developed the idea of the economy as a circular flow of income and output. Physiocrats believed that only agricultural production generated a clear surplus over cost, so that agriculture was the basis of all wealth.[49] Thus, they opposed the mercantilist policy of promoting manufacturing and trade at the expense of agriculture, including import tariffs. Physiocrats advocated replacing administratively costly tax collections with a single tax on income of land owners. In reaction against copious mercantilist trade regulations, the physiocrats advocated a policy of laissez-faire,[50] which called for minimal government intervention in the economy.[51]\n Adam Smith (1723–1790) was an early economic theorist.[52] Smith was harshly critical of the mercantilists but described the physiocratic system \"with all its imperfections\" as \"perhaps the purest approximation to the truth that has yet been published\" on the subject.[53]\n The publication of Adam Smith's The Wealth of Nations in 1776, has been described as \"the effective birth of economics as a separate discipline.\"[54] The book identified land, labour, and capital as the three factors of production and the major contributors to a nation's wealth, as distinct from the physiocratic idea that only agriculture was productive.\n Smith discusses potential benefits of specialisation by division of labour, including increased labour productivity and gains from trade, whether between town and country or across countries.[55] His \"theorem\" that \"the division of labor is limited by the extent of the market\" has been described as the \"core of a theory of the functions of firm and industry\" and a \"fundamental principle of economic organization.\"[56] To Smith has also been ascribed \"the most important substantive proposition in all of economics\" and foundation of resource-allocation theory—that, under competition, resource owners (of labour, land, and capital) seek their most profitable uses, resulting in an equal rate of return for all uses in equilibrium (adjusted for apparent differences arising from such factors as training and unemployment).[57]\n In an argument that includes \"one of the most famous passages in all economics,\"[58] Smith represents every individual as trying to employ any capital they might command for their own advantage, not that of the society,[a] and for the sake of profit, which is necessary at some level for employing capital in domestic industry, and positively related to the value of produce.[60] In this:\n He generally, indeed, neither intends to promote the public interest, nor knows how much he is promoting it. By preferring the support of domestic to that of foreign industry, he intends only his own security; and by directing that industry in such a manner as its produce may be of the greatest value, he intends only his own gain, and he is in this, as in many other cases, led by an invisible hand to promote an end which was no part of his intention. Nor is it always the worse for the society that it was no part of it. By pursuing his own interest he frequently promotes that of the society more effectually than when he really intends to promote it.[61]  The Reverend Thomas Robert Malthus (1798) used the concept of diminishing returns to explain low living standards. Human population, he argued, tended to increase geometrically, outstripping the production of food, which increased arithmetically. The force of a rapidly growing population against a limited amount of land meant diminishing returns to labour. The result, he claimed, was chronically low wages, which prevented the standard of living for most of the population from rising above the subsistence level.[62][non-primary source needed] Economist Julian Simon has criticised Malthus's conclusions.[63]\n While Adam Smith emphasised production and income, David Ricardo (1817) focused on the distribution of income among landowners, workers, and capitalists. Ricardo saw an inherent conflict between landowners on the one hand and labour and capital on the other. He posited that the growth of population and capital, pressing against a fixed supply of land, pushes up rents and holds down wages and profits. Ricardo was also the first to state and prove the principle of comparative advantage, according to which each country should specialise in producing and exporting goods in that it has a lower relative cost of production, rather relying only on its own production.[64] It has been termed a \"fundamental analytical explanation\" for gains from trade.[65]\n Coming at the end of the classical tradition, John Stuart Mill (1848) parted company with the earlier classical economists on the inevitability of the distribution of income produced by the market system. Mill pointed to a distinct difference between the market's two roles: allocation of resources and distribution of income. The market might be efficient in allocating resources but not in distributing income, he wrote, making it necessary for society to intervene.[66]\n Value theory was important in classical theory. Smith wrote that the \"real price of every thing ... is the toil and trouble of acquiring it\". Smith maintained that, with rent and profit, other costs besides wages also enter the price of a commodity.[67] Other classical economists presented variations on Smith, termed the 'labour theory of value'. Classical economics focused on the tendency of any market economy to settle in a final stationary state made up of a constant stock of physical wealth (capital) and a constant population size.\n Marxist (later, Marxian) economics descends from classical economics and it derives from the work of Karl Marx. The first volume of Marx's major work, Das Kapital, was published in 1867. Marx focused on the labour theory of value and theory of surplus value. Marx wrote that they were mechanisms used by capital to exploit labour.[68] The labour theory of value held that the value of an exchanged commodity was determined by the labour that went into its production, and the theory of surplus value demonstrated how workers were only paid a proportion of the value their work had created.[69]\n Marxian economics was further developed by Karl Kautsky (1854–1938)'s The Economic Doctrines of Karl Marx and The Class Struggle (Erfurt Program), Rudolf Hilferding's (1877–1941) Finance Capital, Vladimir Lenin (1870–1924)'s The Development of Capitalism in Russia and Imperialism, the Highest Stage of Capitalism, and Rosa Luxemburg (1871–1919)'s The Accumulation of Capital.\n At its inception as a social science, economics was defined and discussed at length as the study of production, distribution, and consumption of wealth by Jean-Baptiste Say in his Treatise on Political Economy or, The Production, Distribution, and Consumption of Wealth (1803). These three items were considered only in relation to the increase or diminution of wealth, and not in reference to their processes of execution.[b] Say's definition has survived in part up to the present, modified by substituting the word \"wealth\" for \"goods and services\" meaning that wealth may include non-material objects as well. One hundred and thirty years later, Lionel Robbins noticed that this definition no longer sufficed,[c] because many economists were making theoretical and philosophical inroads in other areas of human activity. In his Essay on the Nature and Significance of Economic Science, he proposed a definition of economics as a study of human behaviour, subject to and constrained by scarcity,[d] which forces people to choose, allocate scarce resources to competing ends, and economise (seeking the greatest welfare while avoiding the wasting of scarce resources). According to Robbins: \"Economics is the science which studies human behavior as a relationship between ends and scarce means which have alternative uses\".[35] Robbins' definition eventually became widely accepted by mainstream economists, and found its way into current textbooks.[70] Although far from unanimous, most mainstream economists would accept some version of Robbins' definition, even though many have raised serious objections to the scope and method of economics, emanating from that definition.[71]\n A body of theory later termed \"neoclassical economics\" formed from about 1870 to 1910. The term \"economics\" was popularised by such neoclassical economists as Alfred Marshall and Mary Paley Marshall as a concise synonym for \"economic science\" and a substitute for the earlier \"political economy\".[25][26] This corresponded to the influence on the subject of mathematical methods used in the natural sciences.[72]\n Neoclassical economics systematically integrated supply and demand as joint determinants of both price and quantity in market equilibrium, influencing the allocation of output and income distribution. It rejected the classical economics' labour theory of value in favour of a marginal utility theory of value on the demand side and a more comprehensive theory of costs on the supply side.[73] In the 20th century, neoclassical theorists departed from an earlier idea that suggested measuring total utility for a society, opting instead for ordinal utility, which posits behaviour-based relations across individuals.[74][75]\n In microeconomics, neoclassical economics represents incentives and costs as playing a pervasive role in shaping decision making. An immediate example of this is the consumer theory of individual demand, which isolates how prices (as costs) and income affect quantity demanded.[74] In macroeconomics it is reflected in an early and lasting neoclassical synthesis with Keynesian macroeconomics.[76][74]\n Neoclassical economics is occasionally referred as orthodox economics whether by its critics or sympathisers. Modern mainstream economics builds on neoclassical economics but with many refinements that either supplement or generalise earlier analysis, such as econometrics, game theory, analysis of market failure and imperfect competition, and the neoclassical model of economic growth for analysing long-run variables affecting national income.\n Neoclassical economics studies the behaviour of individuals, households, and organisations (called economic actors, players, or agents), when they manage or use scarce resources, which have alternative uses, to achieve desired ends. Agents are assumed to act rationally, have multiple desirable ends in sight, limited resources to obtain these ends, a set of stable preferences, a definite overall guiding objective, and the capability of making a choice. There exists an economic problem, subject to study by economic science, when a decision (choice) is made by one or more players to attain the best possible outcome.[77]\n Keynesian economics derives from John Maynard Keynes, in particular his book The General Theory of Employment, Interest and Money (1936), which ushered in contemporary macroeconomics as a distinct field.[78] The book focused on determinants of national income in the short run when prices are relatively inflexible. Keynes attempted to explain in broad theoretical detail why high labour-market unemployment might not be self-correcting due to low \"effective demand\" and why even price flexibility and monetary policy might be unavailing. The term \"revolutionary\" has been applied to the book in its impact on economic analysis.[79]\n During the following decades, many economists followed Keynes' ideas and expanded on his works. John Hicks and Alvin Hansen developed the IS–LM model which was a simple formalisation of some of Keynes' insights on the economy's short-run equilibrium. Franco Modigliani and James Tobin developed important theories of private consumption and investment, respectively, two major components of aggregate demand. Lawrence Klein built the first large-scale macroeconometric model, applying the Keynesian thinking systematically to the US economy.[80]\n Immediately after World War II, Keynesian was the dominant economic view of the United States establishment and its allies, Marxian economics was the dominant economic view of the Soviet Union nomenklatura and its allies.\n Monetarism appeared in the 1950s and 1960s, its intellectual leader being Milton Friedman. Monetarists contended that monetary policy and other monetary shocks, as represented by the growth in the money stock, was an important cause of economic fluctuations, and consequently that monetary policy was more important than fiscal policy for purposes of stabilisation.[81][82] Friedman was also skeptical about the ability of central banks to conduct a sensible active monetary policy in practice, advocating instead using simple rules such as a steady rate of money growth.[83]\n Monetarism rose to prominence in the 1970s and 1980s, when several major central banks followed a monetarist-inspired policy, but was later abandoned because the results were unsatisfactory.[84][85]\n A more fundamental challenge to the prevailing Keynesian paradigm came in the 1970s from new classical economists like Robert Lucas, Thomas Sargent and Edward Prescott. They introduced the notion of rational expectations in economics, which had profound implications for many economic discussions, among which were the so-called Lucas critique and the presentation of real business cycle models.[86]\n During the 1980s, a group of researchers appeared being called New Keynesian economists, including among others George Akerlof, Janet Yellen, Gregory Mankiw and Olivier Blanchard. They adopted the principle of rational expectations and other monetarist or new classical ideas such as building upon models employing micro foundations and optimizing behaviour, but simultaneously emphasised the importance of various market failures for the functioning of the economy, as had Keynes.[87] Not least, they proposed various reasons that potentially explained the empirically observed features of price and wage rigidity, usually made to be endogenous features of the models, rather than simply assumed as in older Keynesian-style ones.\n After decades of often heated discussions between Keynesians, monetarists, new classical and new Keynesian economists, a synthesis emerged by the 2000s, often given the name the new neoclassical synthesis. It integrated the rational expectations and optimizing framework of the new classical theory with a new Keynesian role for nominal rigidities and other market imperfections like imperfect information in goods, labour and credit markets. The monetarist importance of monetary policy in stabilizing[88] the economy and in particular controlling inflation was recognised as well as the traditional Keynesian insistence that fiscal policy could also play an influential role in affecting aggregate demand. Methodologically, the synthesis led to a new class of applied models, known as dynamic stochastic general equilibrium or DSGE models, descending from real business cycles models, but extended with several new Keynesian and other features. These models proved useful and influential in the design of modern monetary policy and are now standard workhorses in most central banks.[89]\n After the 2007–2008 financial crisis, macroeconomic research has put greater emphasis on understanding and integrating the financial system into models of the general economy and shedding light on the ways in which problems in the financial sector can turn into major macroeconomic recessions. In this and other research branches, inspiration from behavioural economics has started playing a more important role in mainstream economic theory.[90] Also, heterogeneity among the economic agents, e.g. differences in income, plays an increasing role in recent economic research.[91]\n Other schools or trends of thought referring to a particular style of economics practised at and disseminated from well-defined groups of academicians that have become known worldwide, include the Freiburg School, the School of Lausanne, the Stockholm school and the Chicago school of economics. During the 1970s and 1980s mainstream economics was sometimes separated into the Saltwater approach of those universities along the Eastern and Western coasts of the US, and the Freshwater, or Chicago school approach.[92]\n Within macroeconomics there is, in general order of their historical appearance in the literature; classical economics, neoclassical economics, Keynesian economics, the neoclassical synthesis, monetarism, new classical economics, New Keynesian economics[93] and the new neoclassical synthesis.[94]\n Beside the mainstream development of economic thought, various alternative or heterodox economic theories have evolved over time, positioning themselves in contrast to mainstream theory.[95] These include:[95]\n Additionally, alternative developments include Marxian economics, constitutional economics, institutional economics, evolutionary economics, dependency theory, structuralist economics, world systems theory, econophysics, econodynamics, feminist economics and biophysical economics.[101]\n Feminist economics emphasises the role that gender plays in economies, challenging analyses that render gender invisible or support gender-oppressive economic systems.[102] The goal is to create economic research and policy analysis that is inclusive and gender-aware to encourage gender equality and improve the well-being of marginalised groups.\n Mainstream economic theory relies upon analytical economic models. When creating theories, the objective is to find assumptions which are at least as simple in information requirements, more precise in predictions, and more fruitful in generating additional research than prior theories.[103] While neoclassical economic theory constitutes both the dominant or orthodox theoretical as well as methodological framework, economic theory can also take the form of other schools of thought such as in heterodox economic theories.\n In microeconomics, principal concepts include supply and demand, marginalism, rational choice theory, opportunity cost, budget constraints, utility, and the theory of the firm.[104] Early macroeconomic models focused on modelling the relationships between aggregate variables, but as the relationships appeared to change over time macroeconomists, including new Keynesians, reformulated their models with microfoundations,[105] in which microeconomic concepts play a major part.\n Sometimes an economic hypothesis is only qualitative, not quantitative.[106]\n Expositions of economic reasoning often use two-dimensional graphs to illustrate theoretical relationships. At a higher level of generality, mathematical economics is the application of mathematical methods to represent theories and analyse problems in economics. Paul Samuelson's treatise Foundations of Economic Analysis (1947) exemplifies the method, particularly as to maximizing behavioural relations of agents reaching equilibrium. The book focused on examining the class of statements called operationally meaningful theorems in economics, which are theorems that can conceivably be refuted by empirical data.[107]\n Economic theories are frequently tested empirically, largely through the use of econometrics using economic data.[108] The controlled experiments common to the physical sciences are difficult and uncommon in economics,[109] and instead broad data is observationally studied; this type of testing is typically regarded as less rigorous than controlled experimentation, and the conclusions typically more tentative. However, the field of experimental economics is growing, and increasing use is being made of natural experiments.\n Statistical methods such as regression analysis are common. Practitioners use such methods to estimate the size, economic significance, and statistical significance (\"signal strength\") of the hypothesised relation(s) and to adjust for noise from other variables. By such means, a hypothesis may gain acceptance, although in a probabilistic, rather than certain, sense. Acceptance is dependent upon the falsifiable hypothesis surviving tests. Use of commonly accepted methods need not produce a final conclusion or even a consensus on a particular question, given different tests, data sets, and prior beliefs.\n Experimental economics has promoted the use of scientifically controlled experiments. This has reduced the long-noted distinction of economics from natural sciences because it allows direct tests of what were previously taken as axioms.[110] In some cases these have found that the axioms are not entirely correct.\n In behavioural economics, psychologist Daniel Kahneman won the Nobel Prize in economics in 2002 for his and Amos Tversky's empirical discovery of several cognitive biases and heuristics. Similar empirical testing occurs in neuroeconomics. Another example is the assumption of narrowly selfish preferences versus a model that tests for selfish, altruistic, and cooperative preferences.[111] These techniques have led some to argue that economics is a \"genuine science\".[112]\n Microeconomics examines how entities, forming a market structure, interact within a market to create a market system. These entities include private and public players with various classifications, typically operating under scarcity of tradable units and regulation. The item traded may be a tangible product such as apples or a service such as repair services, legal counsel, or entertainment.\n Various market structures exist. In perfectly competitive markets, no participants are large enough to have the market power to set the price of a homogeneous product. In other words, every participant is a \"price taker\" as no participant influences the price of a product. In the real world, markets often experience imperfect competition.\n Forms of imperfect competition include monopoly (in which there is only one seller of a good), duopoly (in which there are only two sellers of a good), oligopoly (in which there are few sellers of a good), monopolistic competition (in which there are many sellers producing highly differentiated goods), monopsony (in which there is only one buyer of a good), and oligopsony (in which there are few buyers of a good). Firms under imperfect competition have the potential to be \"price makers\", which means that they can influence the prices of their products.\n In partial equilibrium method of analysis, it is assumed that activity in the market being analysed does not affect other markets. This method aggregates (the sum of all activity) in only one market. General-equilibrium theory studies various markets and their behaviour. It aggregates (the sum of all activity) across all markets. This method studies both changes in markets and their interactions leading towards equilibrium.[113]\n In microeconomics, production is the conversion of inputs into outputs. It is an economic process that uses inputs to create a commodity or a service for exchange or direct use. Production is a flow and thus a rate of output per period of time. Distinctions include such production alternatives as for consumption (food, haircuts, etc.) vs. investment goods (new tractors, buildings, roads, etc.), public goods (national defence, smallpox vaccinations, etc.) or private goods, and \"guns\" vs \"butter\".\n Inputs used in the production process include such primary factors of production as labour services, capital (durable produced goods used in production, such as an existing factory), and land (including natural resources). Other inputs may include intermediate goods used in production of final goods, such as the steel in a new car.\n Economic efficiency measures how well a system generates desired output with a given set of inputs and available technology. Efficiency is improved if more output is generated without changing inputs. A widely accepted general standard is Pareto efficiency, which is reached when no further change can make someone better off without making someone else worse off.\n The production–possibility frontier (PPF) is an expository figure for representing scarcity, cost, and efficiency. In the simplest case, an economy can produce just two goods (say \"guns\" and \"butter\"). The PPF is a table or graph (as at the right) that shows the different quantity combinations of the two goods producible with a given technology and total factor inputs, which limit feasible total output. Each point on the curve shows potential total output for the economy, which is the maximum feasible output of one good, given a feasible output quantity of the other good.\n Scarcity is represented in the figure by people being willing but unable in the aggregate to consume beyond the PPF (such as at X) and by the negative slope of the curve.[114] If production of one good increases along the curve, production of the other good decreases, an inverse relationship. This is because increasing output of one good requires transferring inputs to it from production of the other good, decreasing the latter.\n The slope of the curve at a point on it gives the trade-off between the two goods. It measures what an additional unit of one good costs in units forgone of the other good, an example of a real opportunity cost. Thus, if one more Gun costs 100 units of butter, the opportunity cost of one Gun is 100 Butter. Along the PPF, scarcity implies that choosing more of one good in the aggregate entails doing with less of the other good. Still, in a market economy, movement along the curve may indicate that the choice of the increased output is anticipated to be worth the cost to the agents.\n By construction, each point on the curve shows productive efficiency in maximizing output for given total inputs. A point inside the curve (as at A), is feasible but represents production inefficiency (wasteful use of inputs), in that output of one or both goods could increase by moving in a northeast direction to a point on the curve. Examples cited of such inefficiency include high unemployment during a business-cycle recession or economic organisation of a country that discourages full use of resources. Being on the curve might still not fully satisfy allocative efficiency (also called Pareto efficiency) if it does not produce a mix of goods that consumers prefer over other points.\n Much applied economics in public policy is concerned with determining how the efficiency of an economy can be improved. Recognizing the reality of scarcity and then figuring out how to organise society for the most efficient use of resources has been described as the \"essence of economics\", where the subject \"makes its unique contribution.\"[115]\n Specialisation is considered key to economic efficiency based on theoretical and empirical considerations. Different individuals or nations may have different real opportunity costs of production, say from differences in stocks of human capital per worker or capital/labour ratios. According to theory, this may give a comparative advantage in production of goods that make more intensive use of the relatively more abundant, thus relatively cheaper, input.\n Even if one region has an absolute advantage as to the ratio of its outputs to inputs in every type of output, it may still specialise in the output in which it has a comparative advantage and thereby gain from trading with a region that lacks any absolute advantage but has a comparative advantage in producing something else.\n It has been observed that a high volume of trade occurs among regions even with access to a similar technology and mix of factor inputs, including high-income countries. This has led to investigation of economies of scale and agglomeration to explain specialisation in similar but differentiated product lines, to the overall benefit of respective trading parties or regions.[116][117]\n The general theory of specialisation applies to trade among individuals, farms, manufacturers, service providers, and economies. Among each of these production systems, there may be a corresponding division of labour with different work groups specializing, or correspondingly different types of capital equipment and differentiated land uses.[118]\n An example that combines features above is a country that specialises in the production of high-tech knowledge products, as developed countries do, and trades with developing nations for goods produced in factories where labour is relatively cheap and plentiful, resulting in different in opportunity costs of production. More total output and utility thereby results from specializing in production and trading than if each country produced its own high-tech and low-tech products.\n Theory and observation set out the conditions such that market prices of outputs and productive inputs select an allocation of factor inputs by comparative advantage, so that (relatively) low-cost inputs go to producing low-cost outputs. In the process, aggregate output may increase as a by-product or by design.[119] Such specialisation of production creates opportunities for gains from trade whereby resource owners benefit from trade in the sale of one type of output for other, more highly valued goods. A measure of gains from trade is the increased income levels that trade may facilitate.[120]\n Prices and quantities have been described as the most directly observable attributes of goods produced and exchanged in a market economy.[121] The theory of supply and demand is an organizing principle for explaining how prices coordinate the amounts produced and consumed. In microeconomics, it applies to price and output determination for a market with perfect competition, which includes the condition of no buyers or sellers large enough to have price-setting power.\n For a given market of a commodity, demand is the relation of the quantity that all buyers would be prepared to purchase at each unit price of the good. Demand is often represented by a table or a graph showing price and quantity demanded (as in the figure). Demand theory describes individual consumers as rationally choosing the most preferred quantity of each good, given income, prices, tastes, etc. A term for this is \"constrained utility maximisation\" (with income and wealth as the constraints on demand). Here, utility refers to the hypothesised relation of each individual consumer for ranking different commodity bundles as more or less preferred.\n The law of demand states that, in general, price and quantity demanded in a given market are inversely related. That is, the higher the price of a product, the less of it people would be prepared to buy (other things unchanged). As the price of a commodity falls, consumers move toward it from relatively more expensive goods (the substitution effect). In addition, purchasing power from the price decline increases ability to buy (the income effect). Other factors can change demand; for example an increase in income will shift the demand curve for a normal good outward relative to the origin, as in the figure. All determinants are predominantly taken as constant factors of demand and supply.\n Supply is the relation between the price of a good and the quantity available for sale at that price. It may be represented as a table or graph relating price and quantity supplied. Producers, for example business firms, are hypothesised to be profit maximisers, meaning that they attempt to produce and supply the amount of goods that will bring them the highest profit. Supply is typically represented as a function relating price and quantity, if other factors are unchanged.\n That is, the higher the price at which the good can be sold, the more of it producers will supply, as in the figure. The higher price makes it profitable to increase production. Just as on the demand side, the position of the supply can shift, say from a change in the price of a productive input or a technical improvement. The \"Law of Supply\" states that, in general, a rise in price leads to an expansion in supply and a fall in price leads to a contraction in supply. Here as well, the determinants of supply, such as price of substitutes, cost of production, technology applied and various factors inputs of production are all taken to be constant for a specific time period of evaluation of supply.\n Market equilibrium occurs where quantity supplied equals quantity demanded, the intersection of the supply and demand curves in the figure above. At a price below equilibrium, there is a shortage of quantity supplied compared to quantity demanded. This is posited to bid the price up. At a price above equilibrium, there is a surplus of quantity supplied compared to quantity demanded. This pushes the price down. The model of supply and demand predicts that for given supply and demand curves, price and quantity will stabilise at the price that makes quantity supplied equal to quantity demanded. Similarly, demand-and-supply theory predicts a new price-quantity combination from a shift in demand (as to the figure), or in supply.\n People frequently do not trade directly on markets. Instead, on the supply side, they may work in and produce through firms. The most obvious kinds of firms are corporations, partnerships and trusts. According to Ronald Coase, people begin to organise their production in firms when the costs of doing business becomes lower than doing it on the market.[122] Firms combine labour and capital, and can achieve far greater economies of scale (when the average cost per unit declines as more units are produced) than individual market trading.\n In perfectly competitive markets studied in the theory of supply and demand, there are many producers, none of which significantly influence price. Industrial organisation generalises from that special case to study the strategic behaviour of firms that do have significant control of price. It considers the structure of such markets and their interactions. Common market structures studied besides perfect competition include monopolistic competition, various forms of oligopoly, and monopoly.[123]\n Managerial economics applies microeconomic analysis to specific decisions in business firms or other management units. It draws heavily from quantitative methods such as operations research and programming and from statistical methods such as regression analysis in the absence of certainty and perfect knowledge. A unifying theme is the attempt to optimise business decisions, including unit-cost minimisation and profit maximisation, given the firm's objectives and constraints imposed by technology and market conditions.[124]\n Uncertainty in economics is an unknown prospect of gain or loss, whether quantifiable as risk or not. Without it, household behaviour would be unaffected by uncertain employment and income prospects, financial and capital markets would reduce to exchange of a single instrument in each market period, and there would be no communications industry.[125] Given its different forms, there are various ways of representing uncertainty and modelling economic agents' responses to it.[126]\n Game theory is a branch of applied mathematics that considers strategic interactions between agents, one kind of uncertainty. It provides a mathematical foundation of industrial organisation, discussed above, to model different types of firm behaviour, for example in a solipsistic industry (few sellers), but equally applicable to wage negotiations, bargaining, contract design, and any situation where individual agents are few enough to have perceptible effects on each other. In behavioural economics, it has been used to model the strategies agents choose when interacting with others whose interests are at least partially adverse to their own.[127]\n In this, it generalises maximisation approaches developed to analyse market actors such as in the supply and demand model and allows for incomplete information of actors. The field dates from the 1944 classic Theory of Games and Economic Behavior by John von Neumann and Oskar Morgenstern. It has significant applications seemingly outside of economics in such diverse subjects as the formulation of nuclear strategies, ethics, political science, and evolutionary biology.[128]\n Risk aversion may stimulate activity that in well-functioning markets smooths out risk and communicates information about risk, as in markets for insurance, commodity futures contracts, and financial instruments. Financial economics or simply finance describes the allocation of financial resources. It also analyses the pricing of financial instruments, the financial structure of companies, the efficiency and fragility of financial markets,[129] financial crises, and related government policy or regulation.[130][131][132][133][134]\n Some market organisations may give rise to inefficiencies associated with uncertainty. Based on George Akerlof's \"Market for Lemons\" article, the paradigm example is of a dodgy second-hand car market. Customers without knowledge of whether a car is a \"lemon\" depress its price below what a quality second-hand car would be.[135] Information asymmetry arises here, if the seller has more relevant information than the buyer but no incentive to disclose it. Related problems in insurance are adverse selection, such that those at most risk are most likely to insure (say reckless drivers), and moral hazard, such that insurance results in riskier behaviour (say more reckless driving).[136]\n Both problems may raise insurance costs and reduce efficiency by driving otherwise willing transactors from the market (\"incomplete markets\"). Moreover, attempting to reduce one problem, say adverse selection by mandating insurance, may add to another, say moral hazard. Information economics, which studies such problems, has relevance in subjects such as insurance, contract law, mechanism design, monetary economics, and health care.[136] Applied subjects include market and legal remedies to spread or reduce risk, such as warranties, government-mandated partial insurance, restructuring or bankruptcy law, inspection, and regulation for quality and information disclosure.[137][138][139][140][141]\n The term \"market failure\" encompasses several problems which may undermine standard economic assumptions. Although economists categorise market failures differently, the following categories emerge in the main texts.[e]\n Information asymmetries and incomplete markets may result in economic inefficiency but also a possibility of improving efficiency through market, legal, and regulatory remedies, as discussed above.\n Natural monopoly, or the overlapping concepts of \"practical\" and \"technical\" monopoly, is an extreme case of failure of competition as a restraint on producers. Extreme economies of scale are one possible cause.\n Public goods are goods which are under-supplied in a typical market. The defining features are that people can consume public goods without having to pay for them and that more than one person can consume the good at the same time.\n Externalities occur where there are significant social costs or benefits from production or consumption that are not reflected in market prices. For example, air pollution may generate a negative externality, and education may generate a positive externality (less crime, etc.). Governments often tax and otherwise restrict the sale of goods that have negative externalities and subsidise or otherwise promote the purchase of goods that have positive externalities in an effort to correct the price distortions caused by these externalities.[142] Elementary demand-and-supply theory predicts equilibrium but not the speed of adjustment for changes of equilibrium due to a shift in demand or supply.[143]\n In many areas, some form of price stickiness is postulated to account for quantities, rather than prices, adjusting in the short run to changes on the demand side or the supply side. This includes standard analysis of the business cycle in macroeconomics. Analysis often revolves around causes of such price stickiness and their implications for reaching a hypothesised long-run equilibrium. Examples of such price stickiness in particular markets include wage rates in labour markets and posted prices in markets deviating from perfect competition.\n Some specialised fields of economics deal in market failure more than others. The economics of the public sector is one example. Much environmental economics concerns externalities or \"public bads\".\n Policy options include regulations that reflect cost–benefit analysis or market solutions that change incentives, such as emission fees or redefinition of property rights.[144]\n Welfare economics uses microeconomics techniques to evaluate well-being from allocation of productive factors as to desirability and economic efficiency within an economy, often relative to competitive general equilibrium.[145] It analyses social welfare, however measured, in terms of economic activities of the individuals that compose the theoretical society considered. Accordingly, individuals, with associated economic activities, are the basic units for aggregating to social welfare, whether of a group, a community, or a society, and there is no \"social welfare\" apart from the \"welfare\" associated with its individual units.\n Macroeconomics, another branch of economics, examines the economy as a whole to explain broad aggregates and their interactions \"top down\", that is, using a simplified form of general-equilibrium theory.[146] Such aggregates include national income and output, the unemployment rate, and price inflation and subaggregates like total consumption and investment spending and their components. It also studies effects of monetary policy and fiscal policy.\n Since at least the 1960s, macroeconomics has been characterised by further integration as to micro-based modelling of sectors, including rationality of players, efficient use of market information, and imperfect competition.[147] This has addressed a long-standing concern about inconsistent developments of the same subject.[148]\n Macroeconomic analysis also considers factors affecting the long-term level and growth of national income. Such factors include capital accumulation, technological change and labour force growth.[149]\n Growth economics studies factors that explain economic growth – the increase in output per capita of a country over a long period of time. The same factors are used to explain differences in the level of output per capita between countries, in particular why some countries grow faster than others, and whether countries converge at the same rates of growth.\n Much-studied factors include the rate of investment, population growth, and technological change. These are represented in theoretical and empirical forms (as in the neoclassical and endogenous growth models) and in growth accounting.[150]\n The economics of a depression were the spur for the creation of \"macroeconomics\" as a separate discipline. During the Great Depression of the 1930s, John Maynard Keynes authored a book entitled The General Theory of Employment, Interest and Money outlining the key theories of Keynesian economics. Keynes contended that aggregate demand for goods might be insufficient during economic downturns, leading to unnecessarily high unemployment and losses of potential output.\n He therefore advocated active policy responses by the public sector, including monetary policy actions by the central bank and fiscal policy actions by the government to stabilise output over the business cycle.[151] Thus, a central conclusion of Keynesian economics is that, in some situations, no strong automatic mechanism moves output and employment towards full employment levels. John Hicks' IS/LM model has been the most influential interpretation of The General Theory.\n Over the years, understanding of the business cycle has branched into various research programmes, mostly related to or distinct from Keynesianism. The neoclassical synthesis refers to the reconciliation of Keynesian economics with classical economics, stating that Keynesianism is correct in the short run but qualified by classical-like considerations in the intermediate and long run.[76]\n New classical macroeconomics, as distinct from the Keynesian view of the business cycle, posits market clearing with imperfect information. It includes Friedman's permanent income hypothesis on consumption and \"rational expectations\" theory,[152] led by Robert Lucas, and real business cycle theory.[153]\n In contrast, the new Keynesian approach retains the rational expectations assumption, however it assumes a variety of market failures. In particular, New Keynesians assume prices and wages are \"sticky\", which means they do not adjust instantaneously to changes in economic conditions.[105]\n Thus, the new classicals assume that prices and wages adjust automatically to attain full employment, whereas the new Keynesians see full employment as being automatically achieved only in the long run, and hence government and central-bank policies are needed because the \"long run\" may be very long.\n The amount of unemployment in an economy is measured by the unemployment rate, the percentage of workers without jobs in the labour force. The labour force only includes workers actively looking for jobs. People who are retired, pursuing education, or discouraged from seeking work by a lack of job prospects are excluded from the labour force. Unemployment can be generally broken down into several types that are related to different causes.[154]\n Classical models of unemployment occurs when wages are too high for employers to be willing to hire more workers. Consistent with classical unemployment, frictional unemployment occurs when appropriate job vacancies exist for a worker, but the length of time needed to search for and find the job leads to a period of unemployment.[154]\n Structural unemployment covers a variety of possible causes of unemployment including a mismatch between workers' skills and the skills required for open jobs.[155] Large amounts of structural unemployment can occur when an economy is transitioning industries and workers find their previous set of skills are no longer in demand. Structural unemployment is similar to frictional unemployment since both reflect the problem of matching workers with job vacancies, but structural unemployment covers the time needed to acquire new skills not just the short term search process.[156]\n While some types of unemployment may occur regardless of the condition of the economy, cyclical unemployment occurs when growth stagnates. Okun's law represents the empirical relationship between unemployment and economic growth.[157] The original version of Okun's law states that a 3% increase in output would lead to a 1% decrease in unemployment.[158]\n Money is a means of final payment for goods in most price system economies, and is the unit of account in which prices are typically stated. Money has general acceptability, relative consistency in value, divisibility, durability, portability, elasticity in supply, and longevity with mass public confidence. It includes currency held by the nonbank public and checkable deposits. It has been described as a social convention, like language, useful to one largely because it is useful to others. In the words of Francis Amasa Walker, a well-known 19th-century economist, \"Money is what money does\" (\"Money is that money does\" in the original).[159]\n As a medium of exchange, money facilitates trade. It is essentially a measure of value and more importantly, a store of value being a basis for credit creation. Its economic function can be contrasted with barter (non-monetary exchange). Given a diverse array of produced goods and specialised producers, barter may entail a hard-to-locate double coincidence of wants as to what is exchanged, say apples and a book. Money can reduce the transaction cost of exchange because of its ready acceptability. Then it is less costly for the seller to accept money in exchange, rather than what the buyer produces.[160]\n Monetary policy is the policy that central banks conduct to accomplish their broader objectives. Most central banks in developed countries follow inflation targeting,[161] whereas the main objective for many central banks in development countries is to uphold a fixed exchange rate system.[162] The primary monetary tool is normally the adjustment of interest rates,[163] either directly via administratively changing the central bank's own interest rates or indirectly via open market operations.[164] Via the monetary transmission mechanism, interest rate changes affect investment, consumption and net export, and hence aggregate demand, output and employment, and ultimately the development of wages and inflation.\n Governments implement fiscal policy to influence macroeconomic conditions by adjusting spending and taxation policies to alter aggregate demand. When aggregate demand falls below the potential output of the economy, there is an output gap where some productive capacity is left unemployed. Governments increase spending and cut taxes to boost aggregate demand. Resources that have been idled can be used by the government.\n For example, unemployed home builders can be hired to expand highways. Tax cuts allow consumers to increase their spending, which boosts aggregate demand. Both tax cuts and spending have multiplier effects where the initial increase in demand from the policy percolates through the economy and generates additional economic activity.\n The effects of fiscal policy can be limited by crowding out. When there is no output gap, the economy is producing at full capacity and there are no excess productive resources. If the government increases spending in this situation, the government uses resources that otherwise would have been used by the private sector, so there is no increase in overall output. Some economists think that crowding out is always an issue while others do not think it is a major issue when output is depressed.\n Sceptics of fiscal policy also make the argument of Ricardian equivalence. They argue that an increase in debt will have to be paid for with future tax increases, which will cause people to reduce their consumption and save money to pay for the future tax increase. Under Ricardian equivalence, any boost in demand from tax cuts will be offset by the increased saving intended to pay for future higher taxes.\n Economic inequality includes income inequality, measured using the distribution of income (the amount of money people receive), and wealth inequality measured using the distribution of wealth (the amount of wealth people own), and other measures such as consumption, land ownership, and human capital. Inequality exists at different extents between countries or states, groups of people, and individuals.[165] There are many methods for measuring inequality,[166] the Gini coefficient being widely used for income differences among individuals. An example measure of inequality between countries is the Inequality-adjusted Human Development Index, a composite index that takes inequality into account.[167] Important concepts of equality include equity, equality of outcome, and equality of opportunity.\n Research has linked economic inequality to political and social instability, including revolution, democratic breakdown and civil conflict.[168][169][170][171] Research suggests that greater inequality hinders economic growth and macroeconomic stability, and that land and human capital inequality reduce growth more than inequality of income.[168][172] Inequality is at the centre stage of economic policy debate across the globe, as government tax and spending policies have significant effects on income distribution.[168] In advanced economies, taxes and transfers decrease income inequality by one-third, with most of this being achieved via public social spending (such as pensions and family benefits.)[168]\n Public economics is the field of economics that deals with economic activities of a public sector, usually government. The subject addresses such matters as tax incidence (who really pays a particular tax), cost–benefit analysis of government programmes, effects on economic efficiency and income distribution of different kinds of spending and taxes, and fiscal politics. The latter, an aspect of public choice theory, models public-sector behaviour analogously to microeconomics, involving interactions of self-interested voters, politicians, and bureaucrats.[173]\n Much of economics is positive, seeking to describe and predict economic phenomena. Normative economics seeks to identify what economies ought to be like.\n Welfare economics is a normative branch of economics that uses microeconomic techniques to simultaneously determine the allocative efficiency within an economy and the income distribution associated with it. It attempts to measure social welfare by examining the economic activities of the individuals that comprise society.[174]\n International trade studies determinants of goods-and-services flows across international boundaries. It also concerns the size and distribution of gains from trade. Policy applications include estimating the effects of changing tariff rates and trade quotas. International finance is a macroeconomic field which examines the flow of capital across international borders, and the effects of these movements on exchange rates. Increased trade in goods, services and capital between countries is a major effect of contemporary globalisation.[175]\n Labour economics seeks to understand the functioning and dynamics of the markets for wage labour. Labour markets function through the interaction of workers and employers. Labour economics looks at the suppliers of labour services (workers), the demands of labour services (employers), and attempts to understand the resulting pattern of wages, employment, and income. In economics, labour is a measure of the work done by human beings. It is conventionally contrasted with such other factors of production as land and capital. There are theories which have developed a concept called human capital (referring to the skills that workers possess, not necessarily their actual work), although there are also counter posing macro-economic system theories that think human capital is a contradiction in terms.[citation needed]\n Development economics examines economic aspects of the economic development process in relatively low-income countries focusing on structural change, poverty, and economic growth. Approaches in development economics frequently incorporate social and political factors.[176]\n Economics is one social science among several and has fields bordering on other areas, including economic geography, economic history, public choice, energy economics, cultural economics, family economics and institutional economics.\n Law and economics, or economic analysis of law, is an approach to legal theory that applies methods of economics to law. It includes the use of economic concepts to explain the effects of legal rules, to assess which legal rules are economically efficient, and to predict what the legal rules will be.[177] A seminal article by Ronald Coase published in 1961 suggested that well-defined property rights could overcome the problems of externalities.[178]\n Political economy is the interdisciplinary study that combines economics, law, and political science in explaining how political institutions, the political environment, and the economic system (capitalist, socialist, mixed) influence each other. It studies questions such as how monopoly, rent-seeking behaviour, and externalities should impact government policy.[179][180] Historians have employed political economy to explore the ways in the past that persons and groups with common economic interests have used politics to effect changes beneficial to their interests.[181]\n Energy economics is a broad scientific subject area which includes topics related to energy supply and energy demand. Georgescu-Roegen reintroduced the concept of entropy in relation to economics and energy from thermodynamics, as distinguished from what he viewed as the mechanistic foundation of neoclassical economics drawn from Newtonian physics. His work contributed significantly to thermoeconomics and to ecological economics. He also did foundational work which later developed into evolutionary economics.[182]\n The sociological subfield of economic sociology arose, primarily through the work of Émile Durkheim, Max Weber and Georg Simmel, as an approach to analysing the effects of economic phenomena in relation to the overarching social paradigm (i.e. modernity).[183] Classic works include Max Weber's The Protestant Ethic and the Spirit of Capitalism (1905) and Georg Simmel's The Philosophy of Money (1900). More recently, the works of James S. Coleman,[184] Mark Granovetter, Peter Hedstrom and Richard Swedberg have been influential in this field.\n Gary Becker in 1974 presented an economic theory of social interactions, whose applications included the family, charity, merit goods and multiperson interactions, and envy and hatred.[185] He and Kevin Murphy authored a book in 2001 that analysed market behaviour in a social environment.[186]\n The professionalisation of economics, reflected in the growth of graduate programmes on the subject, has been described as \"the main change in economics since around 1900\".[187] Most major universities and many colleges have a major, school, or department in which academic degrees are awarded in the subject, whether in the liberal arts, business, or for professional study. See Bachelor of Economics and Master of Economics.\n In the private sector, professional economists are employed as consultants and in industry, including banking and finance. Economists also work for various government departments and agencies, for example, the national treasury, central bank or National Bureau of Statistics. See Economic analyst.\n There are dozens of prizes awarded to economists each year for outstanding intellectual contributions to the field, the most prominent of which is the Nobel Memorial Prize in Economic Sciences, though it is not a Nobel Prize.\n Contemporary economics uses mathematics. Economists draw on the tools of calculus, linear algebra, statistics, game theory, and computer science.[188] Professional economists are expected to be familiar with these tools, while a minority specialise in econometrics and mathematical methods.\n Harriet Martineau (1802–1876) was a widely-read populariser of classical economic thought. Mary Paley Marshall (1850–1944), the first women lecturer at a British economics faculty, wrote The Economics of Industry with her husband Alfred Marshall. Joan Robinson (1903–1983) was an important post-Keynesian economist. The economic historian Anna Schwartz (1915–2012) coauthored A Monetary History of the United States, 1867–1960 with Milton Friedman.[189] Three women have received the Nobel Prize in Economics: Elinor Ostrom (2009), Esther Duflo (2019) and Claudia Goldin (2023). Five have received the John Bates Clark Medal: Susan Athey (2007), Esther Duflo (2010), Amy Finkelstein (2012), Emi Nakamura (2019) and Melissa Dell (2020).\n Women's authorship share in prominent economic journals reduced from 1940 to the 1970s, but has subsequently risen, with different patterns of gendered coauthorship.[190] Women remain globally under-represented in the profession (19% of authors in the RePEc database in 2018), with national variation.[191]\n \n"
    },
    {
        "title": "Psychology",
        "url": "https://en.wikipedia.org/wiki/Psychology",
        "content": "\n Psychology is the scientific study of mind and behavior.[1][2] Its subject matter includes the behavior of humans and nonhumans, both conscious and unconscious phenomena, and mental processes such as thoughts, feelings, and motives. Psychology is an academic discipline of immense scope, crossing the boundaries between the natural and social sciences. Biological psychologists seek an understanding of the emergent properties of brains, linking the discipline to neuroscience. As social scientists, psychologists aim to understand the behavior of individuals and groups.[3][4]\n A professional practitioner or researcher involved in the discipline is called a psychologist. Some psychologists can also be classified as behavioral or cognitive scientists. Some psychologists attempt to understand the role of mental functions in individual and social behavior. Others explore the physiological and neurobiological processes that underlie cognitive functions and behaviors.\n Psychologists are involved in research on perception, cognition, attention, emotion, intelligence, subjective experiences, motivation, brain functioning, and personality. Psychologists' interests extend to interpersonal relationships, psychological resilience, family resilience, and other areas within social psychology. They also consider the unconscious mind.[5] Research psychologists employ empirical methods to infer causal and correlational relationships between psychosocial variables. Some, but not all, clinical and counseling psychologists rely on symbolic interpretation.\n While psychological knowledge is often applied to the assessment and treatment of mental health problems, it is also directed towards understanding and solving problems in several spheres of human activity. By many accounts, psychology ultimately aims to benefit society.[6][7][8] Many psychologists are involved in some kind of therapeutic role, practicing psychotherapy in clinical, counseling, or school settings. Other psychologists conduct scientific research on a wide range of topics related to mental processes and behavior. Typically the latter group of psychologists work in academic settings (e.g., universities, medical schools, or hospitals). Another group of psychologists is employed in industrial and organizational settings.[9] Yet others are involved in work on human development, aging, sports, health, forensic science, education, and the media.\n The word psychology derives from the Greek word psyche, for spirit or soul. The latter part of the word psychology derives from -λογία -logia, which means \"study\" or \"research\".[10] The word psychology was first used in the Renaissance.[11] In its Latin form psychiologia, it was first employed by the Croatian humanist and Latinist Marko Marulić in his book Psichiologia de ratione animae humanae (Psychology, on the Nature of the Human Soul) in the decade 1510–1520[11][12] The earliest known reference to the word psychology in English was by Steven Blankaart in 1694 in The Physical Dictionary. The dictionary refers to \"Anatomy, which treats the Body, and Psychology, which treats of the Soul.\"[13]\n Ψ (psi), the first letter of the Greek word psyche from which the term psychology is derived, is commonly associated with the field of psychology.\n In 1890, William James defined psychology as \"the science of mental life, both of its phenomena and their conditions.\"[14] This definition enjoyed widespread currency for decades. However, this meaning was contested, notably by  John B. Watson, who in 1913 asserted the methodological behaviorist view of psychology as a purely objective experimental branch of natural science, the theoretical goal of which \"is the prediction and control of behavior.\"[15] Since James defined \"psychology\", the term more strongly implicates scientific experimentation.[16][15] Folk psychology is the understanding of the mental states and behaviors of people held by ordinary people, as contrasted with psychology professionals' understanding.[17]\n The ancient civilizations of Egypt, Greece, China, India, and Persia all engaged in the philosophical study of psychology. In Ancient Egypt the Ebers Papyrus mentioned depression and thought disorders.[18] Historians note that Greek philosophers, including Thales, Plato, and Aristotle (especially in his De Anima treatise),[19] addressed the workings of the mind.[20] As early as the 4th century BC, the Greek physician Hippocrates theorized that mental disorders had physical rather than supernatural causes.[21] In 387 BCE, Plato suggested that the brain is where mental processes take place, and in 335 BCE Aristotle suggested that it was the heart.[22]\n In China, psychological understanding grew from the philosophical works of Laozi and Confucius, and later from the doctrines of Buddhism.[23] This body of knowledge involves insights drawn from introspection and observation, as well as techniques for focused thinking and acting. It frames the universe in term of a division of physical reality and mental reality as well as the interaction between the physical and the mental.[citation needed] Chinese philosophy also emphasized purifying the mind in order to increase virtue and power. An ancient text known as The Yellow Emperor's Classic of Internal Medicine identifies the brain as the nexus of wisdom and sensation, includes theories of personality based on yin–yang balance, and analyzes mental disorder in terms of physiological and social disequilibria. Chinese scholarship that focused on the brain advanced during the Qing dynasty with the work of Western-educated Fang Yizhi (1611–1671), Liu Zhi (1660–1730), and Wang Qingren (1768–1831). Wang Qingren emphasized the importance of the brain as the center of the nervous system, linked mental disorder with brain diseases, investigated the causes of dreams and insomnia, and advanced a theory of hemispheric lateralization in brain function.[24]\n Influenced by Hinduism, Indian philosophy explored distinctions in types of awareness. A central idea of the Upanishads and other Vedic texts that formed the foundations of Hinduism was the distinction between a person's transient mundane self and their eternal, unchanging soul. Divergent Hindu doctrines and Buddhism have challenged this hierarchy of selves, but have all emphasized the importance of reaching higher awareness. Yoga encompasses a range of techniques used in pursuit of this goal. Theosophy, a religion established by Russian-American philosopher Helena Blavatsky, drew inspiration from these doctrines during her time in British India.[25][26]\n Psychology was of interest to Enlightenment thinkers in Europe. In Germany, Gottfried Wilhelm Leibniz (1646–1716) applied his principles of calculus to the mind, arguing that mental activity took place on an indivisible continuum. He suggested that the difference between conscious and unconscious awareness is only a matter of degree. Christian Wolff identified psychology as its own science, writing Psychologia Empirica in 1732 and Psychologia Rationalis in 1734. Immanuel Kant advanced the idea of anthropology as a discipline, with psychology an important subdivision. Kant, however, explicitly rejected the idea of an experimental psychology, writing that \"the empirical doctrine of the soul can also never approach chemistry even as a systematic art of analysis or experimental doctrine, for in it the manifold of inner observation can be separated only by mere division in thought, and cannot then be held separate and recombined at will (but still less does another thinking subject suffer himself to be experimented upon to suit our purpose), and even observation by itself already changes and displaces the state of the observed object.\"\n In 1783, Ferdinand Ueberwasser (1752–1812) designated himself Professor of Empirical Psychology and Logic and gave lectures on scientific psychology, though these developments were soon overshadowed by the Napoleonic Wars.[27] At the end of the Napoleonic era, Prussian authorities discontinued the Old University of Münster.[27] Having consulted philosophers Hegel and Herbart, however, in 1825 the Prussian state established psychology as a mandatory discipline in its rapidly expanding and highly influential educational system. However, this discipline did not yet embrace experimentation.[28] In England, early psychology involved phrenology and the response to social problems including alcoholism, violence, and the country's crowded \"lunatic\" asylums.[29]\n Philosopher John Stuart Mill believed that the human mind was open to scientific investigation, even if the science is in some ways inexact.[30] Mill proposed a \"mental chemistry\" in which elementary thoughts could combine into ideas of greater complexity.[30] Gustav Fechner began conducting psychophysics research in Leipzig in the 1830s. He articulated the principle that human perception of a stimulus varies logarithmically according to its intensity.[31]: 61  The principle became known as the Weber–Fechner law. Fechner's 1860 Elements of Psychophysics challenged Kant's negative view with regard to conducting quantitative research on the mind.[32][28] Fechner's achievement was to show that \"mental processes could not only be given numerical magnitudes, but also that these could be measured by experimental methods.\"[28] In Heidelberg, Hermann von Helmholtz conducted parallel research on sensory perception, and trained physiologist Wilhelm Wundt. Wundt, in turn, came to Leipzig University, where he established the psychological laboratory that brought experimental psychology to the world. Wundt focused on breaking down mental processes into the most basic components, motivated in part by an analogy to recent advances in chemistry, and its successful investigation of the elements and structure of materials.[33] Paul Flechsig and Emil Kraepelin soon created another influential laboratory at Leipzig, a psychology-related lab, that focused more on experimental psychiatry.[28]\n James McKeen Cattell, a professor of psychology at the University of Pennsylvania and Columbia University and the co-founder of Psychological Review, was the first professor of psychology in the United States.[34]\n The German psychologist Hermann Ebbinghaus, a researcher at the University of Berlin, was a 19th-century contributor to the field. He pioneered the experimental study of memory and developed quantitative models of learning and forgetting.[35] In the early 20th century, Wolfgang Kohler, Max Wertheimer, and Kurt Koffka co-founded the school of Gestalt psychology of Fritz Perls. The approach of Gestalt psychology is based upon the idea that individuals experience things as unified wholes. Rather than reducing thoughts and behavior into smaller component elements, as in structuralism, the Gestaltists maintained that whole of experience is important, and differs from the sum of its parts.[citation needed]\n Psychologists in Germany, Denmark, Austria, England, and the United States soon followed Wundt in setting up laboratories.[36] G. Stanley Hall, an American who studied with Wundt, founded a psychology lab that became internationally influential. The lab was located at Johns Hopkins University. Hall, in turn, trained Yujiro Motora, who brought experimental psychology, emphasizing psychophysics, to the Imperial University of Tokyo.[37] Wundt's assistant, Hugo Münsterberg, taught psychology at Harvard to students such as Narendra Nath Sen Gupta—who, in 1905, founded a psychology department and laboratory at the University of Calcutta.[25] Wundt's students Walter Dill Scott, Lightner Witmer, and James McKeen Cattell worked on developing tests of mental ability. Cattell, who also studied with eugenicist Francis Galton, went on to found the Psychological Corporation. Witmer focused on the mental testing of children; Scott, on employee selection.[31]: 60 \n Another student of Wundt, the Englishman Edward Titchener, created the psychology program at Cornell University and advanced \"structuralist\" psychology. The idea behind structuralism was to analyze and classify different aspects of the mind, primarily through the method of introspection.[38] William James, John Dewey, and Harvey Carr advanced the idea of functionalism, an expansive approach to psychology that underlined the Darwinian idea of a behavior's usefulness to the individual. In 1890, James wrote an influential book, The Principles of Psychology, which expanded on the structuralism. He memorably described \"stream of consciousness.\" James's ideas interested many American students in the emerging discipline.[38][14][31]: 178–82  Dewey integrated psychology with societal concerns, most notably by promoting progressive education, inculcating moral values in children, and assimilating immigrants.[31]: 196–200 \n A different strain of experimentalism, with a greater connection to physiology, emerged in South America, under the leadership of Horacio G. Piñero at the University of Buenos Aires.[39] In Russia, too, researchers placed greater emphasis on the biological basis for psychology, beginning with Ivan Sechenov's 1873 essay, \"Who Is to Develop Psychology and How?\" Sechenov advanced the idea of brain reflexes and aggressively promoted a deterministic view of human behavior.[40] The Russian-Soviet physiologist Ivan Pavlov discovered in dogs a learning process that was later termed \"classical conditioning\" and applied the process to human beings.[41]\n One of the earliest psychology societies was La Société de Psychologie Physiologique in France, which lasted from 1885 to 1893. The first meeting of the International Congress of Psychology sponsored by the International Union of Psychological Science took place in Paris, in August 1889, amidst the World's Fair celebrating the centennial of the French Revolution. William James was one of three Americans among the 400 attendees. The American Psychological Association (APA) was founded soon after, in 1892. The International Congress continued to be held at different locations in Europe and with wide international participation. The Sixth Congress, held in Geneva in 1909, included presentations in Russian, Chinese, and Japanese, as well as Esperanto. After a hiatus for World War I, the Seventh Congress met in Oxford, with substantially greater participation from the war-victorious Anglo-Americans. In 1929, the Congress took place at Yale University in New Haven, Connecticut, attended by hundreds of members of the APA.[36] Tokyo Imperial University led the way in bringing new psychology to the East. New ideas about psychology diffused from Japan into China.[24][37]\n American psychology gained status upon the U.S.'s entry into World War I. A standing committee headed by Robert Yerkes administered mental tests (\"Army Alpha\" and \"Army Beta\") to almost 1.8 million soldiers.[42] Subsequently, the Rockefeller family, via the Social Science Research Council, began to provide funding for behavioral research.[43][44] Rockefeller charities funded the National Committee on Mental Hygiene, which disseminated the concept of mental illness and lobbied for applying ideas from psychology to child rearing.[42][45] Through the Bureau of Social Hygiene and later funding of Alfred Kinsey, Rockefeller foundations helped establish research on sexuality in the U.S.[46] Under the influence of the Carnegie-funded Eugenics Record Office, the Draper-funded Pioneer Fund, and other institutions, the eugenics movement also influenced American psychology. In the 1910s and 1920s, eugenics became a standard topic in psychology classes.[47] In contrast to the US, in the UK psychology was met with antagonism by the scientific and medical establishments, and up until 1939, there were only six psychology chairs in universities in England.[48]\n During World War II and the Cold War, the U.S. military and intelligence agencies established themselves as leading funders of psychology by way of the armed forces and in the new Office of Strategic Services intelligence agency. University of Michigan psychologist Dorwin Cartwright reported that university researchers began large-scale propaganda research in 1939–1941. He observed that \"the last few months of the war saw a social psychologist become chiefly responsible for determining the week-by-week-propaganda policy for the United States Government.\" Cartwright also wrote that psychologists had significant roles in managing the domestic economy.[49] The Army rolled out its new General Classification Test to assess the ability of millions of soldiers. The Army also engaged in large-scale psychological research of troop morale and mental health.[50] In the 1950s, the Rockefeller Foundation and Ford Foundation collaborated with the Central Intelligence Agency (CIA) to fund research on psychological warfare.[51] In 1965, public controversy called attention to the Army's Project Camelot, the \"Manhattan Project\" of social science, an effort which enlisted psychologists and anthropologists to analyze the plans and policies of foreign countries for strategic purposes.[52][53]\n In Germany after World War I, psychology held institutional power through the military, which was subsequently expanded along with the rest of the military during Nazi Germany.[28] Under the direction of Hermann Göring's cousin Matthias Göring, the Berlin Psychoanalytic Institute was renamed the Göring Institute. Freudian psychoanalysts were expelled and persecuted under the anti-Jewish policies of the Nazi Party, and all psychologists had to distance themselves from Freud and Adler, founders of psychoanalysis who were also Jewish.[54] The Göring Institute was well-financed throughout the war with a mandate to create a \"New German Psychotherapy.\" This psychotherapy aimed to align suitable Germans with the overall goals of the Reich. As described by one physician, \"Despite the importance of analysis, spiritual guidance and the active cooperation of the patient represent the best way to overcome individual mental problems and to subordinate them to the requirements of the Volk and the Gemeinschaft.\" Psychologists were to provide Seelenführung [lit., soul guidance], the leadership of the mind, to integrate people into the new vision of a German community.[55] Harald Schultz-Hencke melded psychology with the Nazi theory of biology and racial origins, criticizing psychoanalysis as a study of the weak and deformed.[56] Johannes Heinrich Schultz, a German psychologist recognized for developing the technique of autogenic training, prominently advocated sterilization and euthanasia of men considered genetically undesirable, and devised techniques for facilitating this process.[57]\n After the war, new institutions were created although some psychologists, because of their Nazi affiliation, were discredited. Alexander Mitscherlich founded a prominent applied psychoanalysis journal called Psyche. With funding from the Rockefeller Foundation, Mitscherlich established the first clinical psychosomatic medicine division at Heidelberg University. In 1970, psychology was integrated into the required studies of medical students.[58]\n After the Russian Revolution, the Bolsheviks promoted psychology as a way to engineer the \"New Man\" of socialism. Consequently, university psychology departments trained large numbers of students in psychology. At the completion of training, positions were made available for those students at schools, workplaces, cultural institutions, and in the military. The Russian state emphasized pedology and the study of child development. Lev Vygotsky became prominent in the field of child development.[40] The Bolsheviks also promoted free love and embraced the doctrine of psychoanalysis as an antidote to sexual repression.[59]: 84–6 [60] Although pedology and intelligence testing fell out of favor in 1936, psychology maintained its privileged position as an instrument of the Soviet Union.[40] Stalinist purges took a heavy toll and instilled a climate of fear in the profession, as elsewhere in Soviet society.[59]: 22  Following World War II, Jewish psychologists past and present, including Lev Vygotsky, A.R. Luria, and Aron Zalkind, were denounced; Ivan Pavlov (posthumously) and Stalin himself were celebrated as heroes of Soviet psychology.[59]: 25–6, 48–9  Soviet academics experienced a degree of liberalization during the Khrushchev Thaw. The topics of cybernetics, linguistics, and genetics became acceptable again. The new field of engineering psychology emerged. The field involved the study of the mental aspects of complex jobs (such as pilot and cosmonaut). Interdisciplinary studies became popular and scholars such as Georgy Shchedrovitsky developed systems theory approaches to human behavior.[59]: 27–33 \n Twentieth-century Chinese psychology originally modeled itself on U.S. psychology, with translations from American authors like William James, the establishment of university psychology departments and journals, and the establishment of groups including the Chinese Association of Psychological Testing (1930) and the Chinese Psychological Society (1937). Chinese psychologists were encouraged to focus on education and language learning. Chinese psychologists were drawn to the idea that education would enable modernization. John Dewey, who lectured to Chinese audiences between 1919 and 1921, had a significant influence on psychology in China. Chancellor T'sai Yuan-p'ei introduced him at Peking University as a greater thinker than Confucius. Kuo Zing-yang who received a PhD at the University of California, Berkeley, became President of Zhejiang University and popularized behaviorism.[61]: 5–9  After the Chinese Communist Party gained control of the country, the Stalinist Soviet Union became the major influence, with Marxism–Leninism the leading social doctrine and Pavlovian conditioning the approved means of behavior change. Chinese psychologists elaborated on Lenin's model of a \"reflective\" consciousness, envisioning an \"active consciousness\" (pinyin: tzu-chueh neng-tung-li) able to transcend material conditions through hard work and ideological struggle. They developed a concept of \"recognition\" (pinyin: jen-shih) which referred to the interface between individual perceptions and the socially accepted worldview; failure to correspond with party doctrine was \"incorrect recognition.\"[61]: 9–17  Psychology education was centralized under the Chinese Academy of Sciences, supervised by the State Council. In 1951, the academy created a Psychology Research Office, which in 1956 became the Institute of Psychology. Because most leading psychologists were educated in the United States, the first concern of the academy was the re-education of these psychologists in the Soviet doctrines. Child psychology and pedagogy for the purpose of a nationally cohesive education remained a central goal of the discipline.[61]: 18–24 \n Women in the early 1900s started to make key findings within the world of psychology. In 1923, Anna Freud,[62] the daughter of Sigmund Freud, built on her father's work using different defense mechanisms (denial, repression, and suppression) to psychoanalyze children. She believed that once a child reached the latency period, child analysis could be used as a mode of therapy.  She stated it is important focus on the child's environment, support their development, and prevent neurosis. She believed a child should be recognized as their own person with their own right and have each session catered to the child's specific needs. She encouraged drawing, moving freely, and expressing themselves in any way. This helped build a strong therapeutic alliance with child patients, which allows psychologists to observe their normal behavior. She continued her research on the impact of children after family separation, children with socio-economically disadvantaged backgrounds, and all stages of child development from infancy to adolescence.[citation needed]\n Functional periodicity, the belief women are mentally and physically impaired during menstruation, impacted women's rights because employers were less likely to hire them due to the belief they would be incapable of working for 1 week a month. Leta Stetter Hollingworth wanted to prove this hypothesis and Edward L. Thorndike's theory, that women have lesser psychological and physical traits than men and were simply mediocre, incorrect. Hollingworth worked to prove differences were not from male genetic superiority, but from culture. She also included the concept of women's impairment during menstruation in her research. She recorded both women and men performances on tasks (cognitive, perceptual, and motor) for three months. No evidence was found of decreased performance due to a woman's menstrual cycle.[63] She also challenged the belief intelligence is inherited and women here are intellectually inferior to men. She stated that women do not reach positions of power due to the societal norms and roles they are assigned. As she states in her article, \"Variability as related to sex differences in achievement: A Critique\",[64] the largest problem women have is the social order that was built due to the assumption women have less interests and abilities than men. To further prove her point, she completed another experiment with infants who have not been influenced by the environment of social norms, like the adult male getting more opportunities than women. She found no difference between infants besides size. After this research proved the original hypothesis wrong, Hollingworth was able to show there is no difference between the physiological and psychological traits of men and women, and women are not impaired during menstruation.[65]\n The first half of the 1900s was filled with new theories and it was a turning point for women's recognition within the field of psychology. In addition to the contributions made by Leta Stetter Hollingworth and Anna Freud, Mary Whiton Calkins invented the paired associates technique of studying memory and developed self-psychology.[66] Karen Horney developed the concept of \"womb envy\" and neurotic needs.[67] Psychoanalyst Melanie Klein impacted developmental psychology with her research of play therapy.[68] These great discoveries and contributions were made during struggles of sexism, discrimination, and little recognition for their work.\n Women in the second half of the 20th century continued to do research that had large-scale impacts on the field of psychology. Mary Ainsworth's work centered around attachment theory. Building off fellow psychologist John Bowlby, Ainsworth spent years doing fieldwork to understand the development of mother-infant relationships. In doing this field research, Ainsworth developed the Strange Situation Procedure, a laboratory procedure meant to study attachment style by separating and uniting a child with their mother several different times under different circumstances. These field studies are also where she developed her attachment theory and the order of attachment styles, which was a landmark for developmental psychology.[69][70] Because of her work, Ainsworth became one of the most cited psychologists of all time.[71] Mamie Phipps Clark was another woman in psychology that changed the field with her research. She was one of the first African-Americans to receive a doctoral degree in psychology from Columbia University, along with her husband, Kenneth Clark. Her master's thesis, \"The Development of Consciousness in Negro Pre-School Children,\" argued that black children's self-esteem was negatively impacted by racial discrimination. She and her husband conduced research building off her thesis throughout the 1940s. These tests, called the doll tests, asked young children to choose between identical dolls whose only difference was race, and they found that the majority of the children preferred the white dolls and attributed positive traits to them. Repeated over and over again, these tests helped to determine the negative effects of racial discrimination and segregation on black children's self-image and development. In 1954, this research would help decide the landmark Brown v. Board of Education decision, leading to the end of legal segregation across the nation. Clark went on to be an influential figure in psychology, her work continuing to focus on minority youth.[72]\n As the field of psychology developed throughout the latter half of the 20th century, women in the field advocated for their voices to be heard and their perspectives to be valued. Second-wave feminism did not miss psychology. An outspoken feminist in psychology was Naomi Weisstein, who was an accomplished researcher in psychology and neuroscience, and is perhaps best known for her paper, \"Kirche, Kuche, Kinder as Scientific Law: Psychology Constructs the Female.\" Psychology Constructs the Female criticized the field of psychology for centering men and using biology too much to explain gender differences without taking into account social factors.[73] Her work set the stage for further research to be done in social psychology, especially in gender construction.[74] Other women in the field also continued advocating for women in psychology, creating the Association for Women in Psychology to criticize how the field treated women. E. Kitsch Child, Phyllis Chesler, and Dorothy Riddle were some of the founding members of the organization in 1969.[75][76]\n The latter half of the 20th century further diversified the field of psychology, with women of color reaching new milestones. In 1962, Martha Bernal became the first Latina woman to get a Ph.D. in psychology. In 1969, Marigold Linton, the first Native American woman to get a Ph.D. in psychology, founded the National Indian Education Association. She was also a founding member of the Society for Advancement of Chicanos and Native Americans in Science. In 1971, The Network of Indian Psychologists was established by Carolyn Attneave. Harriet McAdoo was appointed to the White House Conference on Families in 1979.[77]\n Dr. Kay Redfield Jamison, named one of Time Magazine's \"Best Doctors in the United States\" is a lecturer, psychologist, and writer. She is known for her vast modern contributions to bipolar disorder and her books An Unquiet Mind[78] (Published 1995) and Nothing Was the Same[79] (Published in 2009). Having Bipolar Disorder herself, she has written several memoirs about her experience with suicidal thoughts, manic behaviors, depression, and other issues that arise from being Bipolar.[80]\n Dr. Angela Neal-Barnett views psychology through a Black lens and dedicated her career to focusing on the anxiety of African American women. She founded the organization Rise Sally Rise which helps Black women cope with anxiety. She published her work Soothe Your Nerves: The Black Woman's Guide to Understanding and Overcoming Anxiety, Panic and Fear[81] in 2003.[80]\n In 2002 Dr. Teresa LaFromboise, former president of the Society of Indian Psychologists, received the APA's Distinguished Career Contribution to Research Award from the Society for the Psychological Study of Culture Ethnicity, and Race for her research on suicide prevention. She was the first person to lead an intervention for Native American children and adolescents that utilized evidence-based suicide prevention. She has spent her career dedicated to aiding racial and ethnic minority youth cope with cultural adjustment and pressures.[82]\n Dr. Shari Miles-Cohen, a psychologist and political activist has applied a black, feminist, and class lens to all her psychological studies. Aiding progressive and women's issues, she has been the executive director for many NGOs. In 2007 she became the Senior Director of the Women's Programs Office of the American Psychological Association. Therefore, she was one of the creators of the APA's \"Women in Psychology Timeline\" which features the accomplishments of women of color in psychology. She is well known for co-editing Eliminating Inequities for Women with Disabilities: An Agenda for Health and Wellness[83] (published in 2016), her article published in the Women's Reproductive Health Journal about women of color's struggle with pregnancy and postpartum (Published in 2018), and co-authoring the \"APA Handbook of the Psychology of Women\" (published in 2019).[84]\n In 1920, Édouard Claparède and Pierre Bovet created a new applied psychology organization called the International Congress of Psychotechnics Applied to Vocational Guidance, later called the International Congress of Psychotechnics and then the International Association of Applied Psychology.[36] The IAAP is considered the oldest international psychology association.[85] Today, at least 65 international groups deal with specialized aspects of psychology.[85] In response to male predominance in the field, female psychologists in the U.S. formed the National Council of Women Psychologists in 1941. This organization became the International Council of Women Psychologists after World War II and the International Council of Psychologists in 1959. Several associations including the Association of Black Psychologists and the Asian American Psychological Association have arisen to promote the inclusion of non-European racial groups in the profession.[85]\n The International Union of Psychological Science (IUPsyS) is the world federation of national psychological societies. The IUPsyS was founded in 1951 under the auspices of the United Nations Educational, Cultural and Scientific Organization (UNESCO).[36][86] Psychology departments have since proliferated around the world, based primarily on the Euro-American model.[25][86] Since 1966, the Union has published the International Journal of Psychology.[36] IAAP and IUPsyS agreed in 1976 each to hold a congress every four years, on a staggered basis.[85]\n IUPsyS recognizes 66 national psychology associations and at least 15 others exist.[85] The American Psychological Association is the oldest and largest.[85] Its membership has increased from 5,000 in 1945 to 100,000 in the present day.[38] The APA includes 54 divisions, which since 1960 have steadily proliferated to include more specialties. Some of these divisions, such as the Society for the Psychological Study of Social Issues and the American Psychology–Law Society, began as autonomous groups.[85]\n The Interamerican Psychological Society, founded in 1951, aspires to promote psychology across the Western Hemisphere. It holds the Interamerican Congress of Psychology and had 1,000 members in year 2000. The European Federation of Professional Psychology Associations, founded in 1981, represents 30 national associations with a total of 100,000 individual members. At least 30 other international organizations represent psychologists in different regions.[85]\n In some places, governments legally regulate who can provide psychological services or represent themselves as a \"psychologist.\"[87] The APA defines a psychologist as someone with a doctoral degree in psychology.[88]\n Early practitioners of experimental psychology distinguished themselves from parapsychology, which in the late nineteenth century enjoyed popularity (including the interest of scholars such as William James). Some people considered parapsychology to be part of \"psychology\". Parapsychology, hypnotism, and psychism were major topics at the early International Congresses. But students of these fields were eventually ostracized, and more or less banished from the Congress in 1900–1905.[36] Parapsychology persisted for a time at Imperial University in Japan, with publications such as Clairvoyance and Thoughtography by Tomokichi Fukurai, but it was mostly shunned by 1913.[37]\n As a discipline, psychology has long sought to fend off accusations that it is a \"soft\" science. Philosopher of science Thomas Kuhn's 1962 critique implied psychology overall was in a pre-paradigm state, lacking agreement on the type of overarching theory found in mature hard sciences such as chemistry and physics.[89] Because some areas of psychology rely on research methods such as self-reports in surveys and questionnaires, critics asserted that psychology is not an objective science. Skeptics have suggested that personality, thinking, and emotion cannot be directly measured and are often inferred from subjective self-reports, which may be problematic. Experimental psychologists have devised a variety of ways to indirectly measure these elusive phenomenological entities.[90][91][92]\n Divisions still exist within the field, with some psychologists more oriented towards the unique experiences of individual humans, which cannot be understood only as data points within a larger population. Critics inside and outside the field have argued that mainstream psychology has become increasingly dominated by a \"cult of empiricism\", which limits the scope of research because investigators restrict themselves to methods derived from the physical sciences.[93]: 36–7  Feminist critiques have argued that claims to scientific objectivity obscure the values and agenda of (historically) mostly male researchers.[42] Jean Grimshaw, for example, argues that mainstream psychological research has advanced a patriarchal agenda through its efforts to control behavior.[93]: 120 \n Psychologists generally consider biology the substrate of thought and feeling, and therefore an important area of study. Behaviorial neuroscience, also known as biological psychology, involves the application of biological principles to the study of physiological and genetic mechanisms underlying behavior in humans and other animals. The allied field of comparative psychology is the scientific study of the behavior and mental processes of non-human animals.[94] A leading question in behavioral neuroscience has been whether and how mental functions are localized in the brain. From Phineas Gage to H.M. and Clive Wearing, individual people with mental deficits traceable to physical brain damage have inspired new discoveries in this area.[95] Modern behavioral neuroscience could be said to originate in the 1870s, when in France Paul Broca traced production of speech to the left frontal gyrus, thereby also demonstrating hemispheric lateralization of brain function. Soon after, Carl Wernicke identified a related area necessary for the understanding of speech.[96]: 20–2 \n The contemporary field of behavioral neuroscience focuses on the physical basis of behavior. Behaviorial neuroscientists use animal models, often relying on rats, to study the neural, genetic, and cellular mechanisms that underlie behaviors involved in learning, memory, and fear responses.[97] Cognitive neuroscientists, by using neural imaging tools, investigate the neural correlates of psychological processes in humans. Neuropsychologists conduct psychological assessments to determine how an individual's behavior and cognition are related to the brain. The biopsychosocial model is a cross-disciplinary, holistic model that concerns the ways in which interrelationships of biological, psychological, and socio-environmental factors affect health and behavior.[98]\n Evolutionary psychology approaches thought and behavior from a modern evolutionary perspective. This perspective suggests that psychological adaptations evolved to solve recurrent problems in human ancestral environments. Evolutionary psychologists attempt to find out how human psychological traits are evolved adaptations, the results of natural selection or sexual selection over the course of human evolution.[99]\n The history of the biological foundations of psychology includes evidence of racism. The idea of white supremacy and indeed the modern concept of race itself arose during the process of world conquest by Europeans.[100] Carl von Linnaeus's four-fold classification of humans classifies Europeans as intelligent and severe, Americans as contented and free, Asians as ritualistic, and Africans as lazy and capricious. Race was also used to justify the construction of socially specific mental disorders such as drapetomania and dysaesthesia aethiopica—the behavior of uncooperative African slaves.[101] After the creation of experimental psychology, \"ethnical psychology\" emerged as a subdiscipline, based on the assumption that studying primitive races would provide an important link between animal behavior and the psychology of more evolved humans.[102]\n A tenet of behavioral research is that a large part of both human and lower-animal behavior is learned. A principle associated with behavioral research is that the mechanisms involved in learning apply to humans and non-human animals. Behavioral researchers have developed a treatment known as behavior modification, which is used to help individuals replace undesirable behaviors with desirable ones.\n Early behavioral researchers studied stimulus–response pairings, now known as classical conditioning. They demonstrated that when a biologically potent stimulus (e.g., food that elicits salivation) is paired with a previously neutral stimulus (e.g., a bell) over several learning trials, the neutral stimulus by itself can come to elicit the response the biologically potent stimulus elicits. Ivan Pavlov—known best for inducing dogs to salivate in the presence of a stimulus previously linked with food—became a leading figure in the Soviet Union and inspired followers to use his methods on humans.[40] In the United States, Edward Lee Thorndike initiated \"connectionist\" studies by trapping animals in \"puzzle boxes\" and rewarding them for escaping. Thorndike wrote in 1911, \"There can be no moral warrant for studying man's nature unless the study will enable us to control his acts.\"[31]: 212–5  From 1910 to 1913 the American Psychological Association went through a sea change of opinion, away from mentalism and towards \"behavioralism.\" In 1913, John B. Watson coined the term behaviorism for this school of thought.[31]: 218–27  Watson's famous Little Albert experiment in 1920 was at first thought to demonstrate that repeated use of upsetting loud noises could instill phobias (aversions to other stimuli) in an infant human,[15][103] although such a conclusion was likely an exaggeration.[104] Karl Lashley, a close collaborator with Watson, examined biological manifestations of learning in the brain.[95]\n Clark L. Hull, Edwin Guthrie, and others did much to help behaviorism become a widely used paradigm.[38] A new method of \"instrumental\" or \"operant\" conditioning added the concepts of reinforcement and punishment to the model of behavior change. Radical behaviorists avoided discussing the inner workings of the mind, especially the unconscious mind, which they considered impossible to assess scientifically.[105] Operant conditioning was first described by Miller and Kanorski and popularized in the U.S. by B.F. Skinner, who emerged as a leading intellectual of the behaviorist movement.[106][107]\n Noam Chomsky published an influential critique of radical behaviorism on the grounds that behaviorist principles could not adequately explain the complex mental process of language acquisition and language use.[108][109] The review, which was scathing, did much to reduce the status of behaviorism within psychology.[31]: 282–5  Martin Seligman and his colleagues discovered that they could condition in dogs a state of \"learned helplessness\", which was not predicted by the behaviorist approach to psychology.[110][111] Edward C. Tolman advanced a hybrid \"cognitive behavioral\" model, most notably with his 1948 publication discussing the cognitive maps used by rats to guess at the location of food at the end of a maze.[112] Skinner's behaviorism did not die, in part because it generated successful practical applications.[109]\n The Association for Behavior Analysis International was founded in 1974 and by 2003 had members from 42 countries. The field has gained a foothold in Latin America and Japan.[113] Applied behavior analysis is the term used for the application of the principles of operant conditioning to change socially significant behavior (it supersedes the term, \"behavior modification\").[114]\n Green Red BluePurple Blue Purple\n Blue Purple RedGreen Purple Green\n The Stroop effect is the fact that naming the color of the first set of words is easier and quicker than the second.\n Cognitive psychology involves the study of mental processes, including perception, attention, language comprehension and production, memory, and problem solving.[115] Researchers in the field of cognitive psychology are sometimes called cognitivists. They rely on an information processing model of mental functioning. Cognitivist research is informed by functionalism and experimental psychology.\n Starting in the 1950s, the experimental techniques developed by Wundt, James, Ebbinghaus, and others re-emerged as experimental psychology became increasingly cognitivist and, eventually, constituted a part of the wider, interdisciplinary cognitive science.[116][117] Some called this development the cognitive revolution because it rejected the anti-mentalist dogma of behaviorism as well as the strictures of psychoanalysis.[117]\n Albert Bandura helped along the transition in psychology from behaviorism to cognitive psychology. Bandura and other social learning theorists advanced the idea of vicarious learning. In other words, they advanced the view that a child can learn by observing the immediate social environment and not necessarily from having been reinforced for enacting a behavior, although they did not rule out the influence of reinforcement on learning a behavior.[118]\n Technological advances also renewed interest in mental states and mental representations. English neuroscientist Charles Sherrington and Canadian psychologist Donald O. Hebb used experimental methods to link psychological phenomena to the structure and function of the brain. The rise of computer science, cybernetics, and artificial intelligence underlined the value of comparing information processing in humans and machines.\n A popular and representative topic in this area is cognitive bias, or irrational thought. Psychologists (and economists) have classified and described a sizeable catalog of biases which recur frequently in human thought. The availability heuristic, for example, is the tendency to overestimate the importance of something which happens to come readily to mind.[119]\n Elements of behaviorism and cognitive psychology were synthesized to form cognitive behavioral therapy, a form of psychotherapy modified from techniques developed by American psychologist Albert Ellis and American psychiatrist Aaron T. Beck.\n On a broader level, cognitive science is an interdisciplinary enterprise involving cognitive psychologists, cognitive neuroscientists, linguists, and researchers in artificial intelligence, human–computer interaction, and computational neuroscience. The discipline of cognitive science covers cognitive psychology as well as philosophy of mind, computer science, and neuroscience.[120] Computer simulations are sometimes used to model phenomena of interest.\n Social psychology is concerned with how behaviors, thoughts, feelings, and the social environment influence human interactions.[121] Social psychologists study such topics as the influence of others on an individual's behavior (e.g. conformity, persuasion) and the formation of beliefs, attitudes, and stereotypes about other people. Social cognition fuses elements of social and cognitive psychology for the purpose of understanding how people process, remember, or distort social information. The study of group dynamics involves research on the nature of leadership, organizational communication, and related phenomena. In recent years, social psychologists have become interested in implicit measures, mediational models, and the interaction of person and social factors in accounting for behavior. Some concepts that sociologists have applied to the study of psychiatric disorders, concepts such as the social role, sick role, social class, life events, culture, migration, and total institution, have influenced social psychologists.[122]\n Psychoanalysis is a collection of theories and therapeutic techniques intended to analyze the unconscious mind and its impact on everyday life. These theories and techniques inform treatments for mental disorders.[123][124][125] Psychoanalysis originated in the 1890s, most prominently with the work of Sigmund Freud. Freud's psychoanalytic theory was largely based on interpretive methods, introspection, and clinical observation. It became very well known, largely because it tackled subjects such as sexuality, repression, and the unconscious.[59]: 84–6  Freud pioneered the methods of free association and dream interpretation.[126][127]\n Psychoanalytic theory is not monolithic. Other well-known psychoanalytic thinkers who diverged from Freud include Alfred Adler, Carl Jung, Erik Erikson, Melanie Klein, D.W. Winnicott, Karen Horney, Erich Fromm, John Bowlby, Freud's daughter Anna Freud, and Harry Stack Sullivan. These individuals ensured that psychoanalysis would evolve into diverse schools of thought. Among these schools are ego psychology, object relations, and interpersonal, Lacanian, and relational psychoanalysis.\n Psychologists such as Hans Eysenck and philosophers including Karl Popper sharply criticized psychoanalysis. Popper argued that psychoanalysis was not falsifiable (no claim it made could be proven wrong) and therefore inherently not a scientific discipline,[128] whereas Eysenck advanced the view that psychoanalytic tenets had been contradicted by experimental data. By the end of the 20th century, psychology departments in American universities mostly had marginalized Freudian theory, dismissing it as a \"desiccated and dead\" historical artifact.[129] Researchers such as António Damásio, Oliver Sacks, and Joseph LeDoux; and individuals in the emerging field of neuro-psychoanalysis have defended some of Freud's ideas on scientific grounds.[130]\n Humanistic psychology, which has been influenced by existentialism and phenomenology,[132] stresses free will and self-actualization.[133] It emerged in the 1950s as a movement within academic psychology, in reaction to both behaviorism and psychoanalysis.[134] The humanistic approach seeks to view the whole person, not just fragmented parts of the personality or isolated cognitions.[135] Humanistic psychology also focuses on personal growth, self-identity, death, aloneness, and freedom. It emphasizes subjective meaning, the rejection of determinism, and concern for positive growth rather than pathology. Some founders of the humanistic school of thought were American psychologists Abraham Maslow, who formulated a hierarchy of human needs, and Carl Rogers, who created and developed client-centered therapy.\n Later, positive psychology opened up humanistic themes to scientific study. Positive psychology is the study of factors which contribute to human happiness and well-being, focusing more on people who are currently healthy. In 2010, Clinical Psychological Review published a special issue devoted to positive psychological interventions, such as gratitude journaling and the physical expression of gratitude. It is, however, far from clear that positive psychology is effective in making people happier.[136][137] Positive psychological interventions have been limited in scope, but their effects are thought to be somewhat better than placebo effects.\n The American Association for Humanistic Psychology, formed in 1963, declared:\n Humanistic psychology is primarily an orientation toward the whole of psychology rather than a distinct area or school. It stands for respect for the worth of persons, respect for differences of approach, open-mindedness as to acceptable methods, and interest in exploration of new aspects of human behavior. As a \"third force\" in contemporary psychology, it is concerned with topics having little place in existing theories and systems: e.g., love, creativity, self, growth, organism, basic need-gratification, self-actualization, higher values, being, becoming, spontaneity, play, humor, affection, naturalness, warmth, ego-transcendence, objectivity, autonomy, responsibility, meaning, fair-play, transcendental experience, peak experience, courage, and related concepts.[138] Existential psychology emphasizes the need to understand a client's total orientation towards the world. Existential psychology is opposed to reductionism, behaviorism, and other methods that objectify the individual.[133] In the 1950s and 1960s, influenced by philosophers Søren Kierkegaard and Martin Heidegger, psychoanalytically trained American psychologist Rollo May helped to develop existential psychology. Existential psychotherapy, which follows from existential psychology, is a therapeutic approach that is based on the idea that a person's inner conflict arises from that individual's confrontation with the givens of existence. Swiss psychoanalyst Ludwig Binswanger and American psychologist George Kelly may also be said to belong to the existential school.[139] Existential psychologists tend to differ from more \"humanistic\" psychologists in the former's relatively neutral view of human nature and relatively positive assessment of anxiety.[140] Existential psychologists emphasized the humanistic themes of death, free will, and meaning, suggesting that meaning can be shaped by myths and narratives; meaning can be deepened by the acceptance of free will, which is requisite to living an authentic life, albeit often with anxiety with regard to death.[141]\n Austrian existential psychiatrist and Holocaust survivor Viktor Frankl drew evidence of meaning's therapeutic power from reflections upon his own internment.[142] He created a variation of existential psychotherapy called logotherapy, a type of existentialist analysis that focuses on a will to meaning (in one's life), as opposed to Adler's Nietzschean doctrine of will to power or Freud's will to pleasure.[143]\n Personality psychology is concerned with enduring patterns of behavior, thought, and emotion. Theories of personality vary across different psychological schools of thought. Each theory carries different assumptions about such features as the role of the unconscious and the importance of childhood experience. According to Freud, personality is based on the dynamic interactions of the id, ego, and super-ego.[144] By contrast, trait theorists have developed taxonomies of personality constructs in describing personality in terms of key traits. Trait theorists have often employed statistical data-reduction methods, such as factor analysis. Although the number of proposed traits has varied widely, Hans Eysenck's early biologically based model suggests at least three major trait constructs are necessary to describe human personality, extraversion–introversion, neuroticism-stability, and psychoticism-normality. Raymond Cattell empirically derived a theory of 16 personality factors at the primary-factor level and up to eight broader second-stratum factors.[145][146][147][148]\nSince the 1980s, the Big Five (openness to experience, conscientiousness, extraversion, agreeableness, and neuroticism) emerged as an important trait theory of personality.[149] Dimensional models of personality are receiving increasing support, and a version of dimensional assessment has been included in the DSM-V. However, despite a plethora of research into the various versions of the \"Big Five\" personality dimensions, it appears necessary to move on from static conceptualizations of personality structure to a more dynamic orientation, acknowledging that personality constructs are subject to learning and change over the lifespan.[150][151]\n An early example of personality assessment was the Woodworth Personal Data Sheet, constructed during World War I. The popular, although psychometrically inadequate, Myers–Briggs Type Indicator[152] was developed to assess individuals' \"personality types\" according to the personality theories of Carl Jung. The Minnesota Multiphasic Personality Inventory (MMPI), despite its name, is more a dimensional measure of psychopathology than a personality measure.[153] California Psychological Inventory contains 20 personality scales (e.g., independence, tolerance).[154] The International Personality Item Pool, which is in the public domain, has become a source of scales that can be used personality assessment.[155]\n Study of the unconscious mind, a part of the psyche outside the individual's awareness but that is believed to influence conscious thought and behavior, was a hallmark of early psychology. In one of the first psychology experiments conducted in the United States, C.S. Peirce and Joseph Jastrow found in 1884 that research subjects could choose the minutely heavier of two weights even if consciously uncertain of the difference.[156] Freud popularized the concept of the unconscious mind, particularly when he referred to an uncensored intrusion of unconscious thought into one's speech (a Freudian slip) or to his efforts to interpret dreams.[157] His 1901 book The Psychopathology of Everyday Life catalogs hundreds of everyday events that Freud explains in terms of unconscious influence. Pierre Janet advanced the idea of a subconscious mind, which could contain autonomous mental elements unavailable to the direct scrutiny of the subject.[158]\n The concept of unconscious processes has remained important in psychology. Cognitive psychologists have used a \"filter\" model of attention. According to the model, much information processing takes place below the threshold of consciousness, and only certain stimuli, limited by their nature and number, make their way through the filter. Much research has shown that subconscious priming of certain ideas can covertly influence thoughts and behavior.[158] Because of the unreliability of self-reporting, a major hurdle in this type of research involves demonstrating that a subject's conscious mind has not perceived a target stimulus. For this reason, some psychologists prefer to distinguish between implicit and explicit memory. In another approach, one can also describe a subliminal stimulus as meeting an objective but not a subjective threshold.[159]\n The automaticity model of John Bargh and others involves the ideas of automaticity and unconscious processing in our understanding of social behavior,[160][161] although there has been dispute with regard to replication.[162][163]\nSome experimental data suggest that the brain begins to consider taking actions before the mind becomes aware of them.[164] The influence of unconscious forces on people's choices bears on the philosophical question of free will. John Bargh, Daniel Wegner, and Ellen Langer describe free will as an illusion.[160][161][165]\n Some psychologists study motivation or the subject of why people or lower animals initiate a behavior at a particular time. It also involves the study of why humans and lower animals continue or terminate a behavior. Psychologists such as William James initially used the term motivation to refer to intention, in a sense similar to the concept of will in European philosophy. With the steady rise of Darwinian and Freudian thinking, instinct also came to be seen as a primary source of motivation.[166] According to drive theory, the forces of instinct combine into a single source of energy which exerts a constant influence. Psychoanalysis, like biology, regarded these forces as demands originating in the nervous system. Psychoanalysts believed that these forces, especially the sexual instincts, could become entangled and transmuted within the psyche. Classical psychoanalysis conceives of a struggle between the pleasure principle and the reality principle, roughly corresponding to id and ego. Later, in Beyond the Pleasure Principle, Freud introduced the concept of the death drive, a compulsion towards aggression, destruction, and psychic repetition of traumatic events.[167] Meanwhile, behaviorist researchers used simple dichotomous models (pleasure/pain, reward/punishment) and well-established principles such as the idea that a thirsty creature will take pleasure in drinking.[166][168] Clark Hull formalized the latter idea with his drive reduction model.[169]\n Hunger, thirst, fear, sexual desire, and thermoregulation constitute fundamental motivations in animals.[168] Humans seem to exhibit a more complex set of motivations—though theoretically these could be explained as resulting from desires for belonging, positive self-image, self-consistency, truth, love, and control.[170][171]\n Motivation can be modulated or manipulated in many different ways. Researchers have found that eating, for example, depends not only on the organism's fundamental need for homeostasis—an important factor causing the experience of hunger—but also on circadian rhythms, food availability, food palatability, and cost.[168] Abstract motivations are also malleable, as evidenced by such phenomena as goal contagion: the adoption of goals, sometimes unconsciously, based on inferences about the goals of others.[172] Vohs and Baumeister suggest that contrary to the need-desire-fulfillment cycle of animal instincts, human motivations sometimes obey a \"getting begets wanting\" rule: the more you get a reward such as self-esteem, love, drugs, or money, the more you want it. They suggest that this principle can even apply to food, drink, sex, and sleep.[173]\n Developmental psychology is the scientific study of how and why the thought processes, emotions, and behaviors of humans change over the course of their lives.[174] Some credit Charles Darwin with conducting the first systematic study within the rubric of developmental psychology, having published in 1877 a short paper detailing the development of innate forms of communication based on his observations of his infant son.[175] The main origins of the discipline, however, are found in the work of Jean Piaget. Like Piaget, developmental psychologists originally focused primarily on the development of cognition from infancy to adolescence. Later, developmental psychology extended itself to the study cognition over the life span. In addition to studying cognition, developmental psychologists have also come to focus on affective, behavioral, moral, social, and neural development.\n Developmental psychologists who study children use a number of research methods. For example, they make observations of children in natural settings such as preschools[176] and engage them in experimental tasks.[177] Such tasks often resemble specially designed games and activities that are both enjoyable for the child and scientifically useful. Developmental researchers have even devised clever methods to study the mental processes of infants.[178] In addition to studying children, developmental psychologists also study aging and processes throughout the life span, including old age.[179] These psychologists draw on the full range of psychological theories to inform their research.[174]\n All researched psychological traits are influenced by both genes and environment, to varying degrees.[180][181] These two sources of influence are often confounded in observational research of individuals and families. An example of this confounding can be shown in the transmission of depression from a depressed mother to her offspring. A theory based on environmental transmission would hold that an offspring, by virtue of their having a problematic rearing environment managed by a depressed mother, is at risk for developing depression. On the other hand, a hereditarian theory would hold that depression risk in an offspring is influenced to some extent by genes passed to the child from the mother. Genes and environment in these simple transmission models are completely confounded. A depressed mother may both carry genes that contribute to depression in her offspring and also create a rearing environment that increases the risk of depression in her child.[182]\n Behavioral genetics researchers have employed methodologies that help to disentangle this confound and understand the nature and origins of individual differences in behavior.[99] Traditionally the research has involved twin studies and adoption studies, two designs where genetic and environmental influences can be partially un-confounded. More recently, gene-focused research has contributed to understanding genetic contributions to the development of psychological traits.\n The availability of microarray molecular genetic or genome sequencing technologies allows researchers to measure participant DNA variation directly, and test whether individual genetic variants within genes are associated with psychological traits and psychopathology through methods including genome-wide association studies. One goal of such research is similar to that in positional cloning and its success in Huntington's: once a causal gene is discovered biological research can be conducted to understand how that gene influences the phenotype. One major result of genetic association studies is the general finding that psychological traits and psychopathology, as well as complex medical diseases, are highly polygenic,[183][184][185][186][187] where a large number (on the order of hundreds to thousands) of genetic variants, each of small effect, contribute to individual differences in the behavioral trait or propensity to the disorder. Active research continues to work toward understanding the genetic and environmental bases of behavior and their interaction.\n Psychology encompasses many subfields and includes different approaches to the study of mental processes and behavior.\n Psychological testing has ancient origins, dating as far back as 2200 BC, in the examinations for the Chinese civil service. Written exams began during the Han dynasty (202 BC – AD 220). By 1370, the Chinese system required a stratified series of tests, involving essay writing and knowledge of diverse topics. The system was ended in 1906.[188]: 41–2  In Europe, mental assessment took a different approach, with theories of physiognomy—judgment of character based on the face—described by Aristotle in 4th century BC Greece. Physiognomy remained current through the Enlightenment, and added the doctrine of phrenology: a study of mind and intelligence based on simple assessment of neuroanatomy.[188]: 42–3 \n When experimental psychology came to Britain, Francis Galton was a leading practitioner. By virtue of his procedures for measuring reaction time and sensation, he is considered an inventor of modern mental testing (also known as psychometrics).[188]: 44–5  James McKeen Cattell, a student of Wundt and Galton, brought the idea of psychological testing to the United States, and in fact coined the term \"mental test\".[188]: 45–6  In 1901, Cattell's student Clark Wissler published discouraging results, suggesting that mental testing of Columbia and Barnard students failed to predict academic performance.[188]: 45–6  In response to 1904 orders from the Minister of Public Instruction, One example of an observational study was run by Arthur Bandura. This observational study focused on children who were exposed to an adult exhibiting aggressive behaviors and their reaction to toys versus other children who were not exposed to these stimuli. The result shows that children who had seen the adult acting aggressively towards a toy, in turn, were aggressive towards their own toy when put in a situation that frustrated them.[189] psychologists Alfred Binet and Théodore Simon developed and elaborated a new test of intelligence in 1905–1911. They used a range of questions diverse in their nature and difficulty. Binet and Simon introduced the concept of mental age and referred to the lowest scorers on their test as idiots. Henry H. Goddard put the Binet-Simon scale to work and introduced classifications of mental level such as imbecile and feebleminded. In 1916, (after Binet's death), Stanford professor Lewis M. Terman modified the Binet-Simon scale (renamed the Stanford–Binet scale) and introduced the intelligence quotient as a score report.[188]: 50–56  Based on his test findings, and reflecting the racism common to that era, Terman concluded that intellectual disability \"represents the level of intelligence which is very, very common among Spanish-Indians and Mexican families of the Southwest and also among negroes. Their dullness seems to be racial.\"[190]\n Following the Army Alpha and Army Beta tests, which was developed by psychologist Robert Yerkes in 1917 and then used in World War 1 by industrial and organizational psychologists for large-scale employee testing and selection of military personnel.[191] Mental testing also became popular in the U.S., where it was applied to schoolchildren. The federally created National Intelligence Test was administered to 7 million children in the 1920s. In 1926, the College Entrance Examination Board created the Scholastic Aptitude Test to standardize college admissions.[188]: 61  The results of intelligence tests were used to argue for segregated schools and economic functions, including the preferential training of Black Americans for manual labor. These practices were criticized by Black intellectuals such a Horace Mann Bond and Allison Davis.[190] Eugenicists used mental testing to justify and organize compulsory sterilization of individuals classified as mentally retarded (now referred to as intellectual disability).[47] In the United States, tens of thousands of men and women were sterilized. Setting a precedent that has never been overturned, the U.S. Supreme Court affirmed the constitutionality of this practice in the 1927 case Buck v. Bell.[192]\n Today mental testing is a routine phenomenon for people of all ages in Western societies.[188]: 2  Modern testing aspires to criteria including standardization of procedure, consistency of results, output of an interpretable score, statistical norms describing population outcomes, and, ideally, effective prediction of behavior and life outcomes outside of testing situations.[188]: 4–6  Psychological testing is regularly used in forensic contexts to aid legal judgments and decisions.[193] Developments in psychometrics include work on test and scale reliability and validity.[194] Developments in item-response theory,[195] structural equation modeling,[196] and bifactor analysis[197] have helped in strengthening test and scale construction.\n The provision of psychological health services is generally called clinical psychology in the U.S. Sometimes, however, members of the school psychology and counseling psychology professions engage in practices that resemble that of clinical psychologists. Clinical psychologists typically include people who have graduated from doctoral programs in clinical psychology. In Canada, some of the members of the abovementioned groups usually fall within the larger category of professional psychology. In Canada and the U.S., practitioners get bachelor's degrees and doctorates; doctoral students in clinical psychology usually spend one year in a predoctoral internship and one year in postdoctoral internship. In Mexico and most other Latin American and European countries, psychologists do not get bachelor's and doctoral degrees; instead, they take a three-year professional course following high school.[88] Clinical psychology is at present the largest specialization within psychology.[198] It includes the study and application of psychology for the purpose of understanding, preventing, and relieving psychological distress, dysfunction, and/or mental illness. Clinical psychologists also try to promote subjective well-being and personal growth. Central to the practice of clinical psychology are psychological assessment and psychotherapy although clinical psychologists may also engage in research, teaching, consultation, forensic testimony, and program development and administration.[199]\n Credit for the first psychology clinic in the United States typically goes to Lightner Witmer, who established his practice in Philadelphia in 1896. Another modern psychotherapist was Morton Prince, an early advocate for the establishment of psychology as a clinical and academic discipline.[198] In the first part of the twentieth century, most mental health care in the United States was performed by psychiatrists, who are medical doctors. Psychology entered the field with its refinements of mental testing, which promised to improve the diagnosis of mental problems. For their part, some psychiatrists became interested in using psychoanalysis and other forms of psychodynamic psychotherapy to understand and treat the mentally ill.[42][200]\n Psychotherapy as conducted by psychiatrists blurred the distinction between psychiatry and psychology, and this trend continued with the rise of community mental health facilities. Some in the clinical psychology community adopted behavioral therapy, a thoroughly non-psychodynamic model that used behaviorist learning theory to change the actions of patients. A key aspect of behavior therapy is empirical evaluation of the treatment's effectiveness. In the 1970s, cognitive-behavior therapy emerged with the work of Albert Ellis and Aaron Beck. Although there are similarities between behavior therapy and cognitive-behavior therapy, cognitive-behavior therapy required the application of cognitive constructs. Since the 1970s, the popularity of cognitive-behavior therapy among clinical psychologists increased. A key practice in behavioral and cognitive-behavioral therapy is exposing patients to things they fear, based on the premise that their responses (fear, panic, anxiety) can be deconditioned.[201]\n Mental health care today involves psychologists and social workers in increasing numbers. In 1977, National Institute of Mental Health director Bertram Brown described this shift as a source of \"intense competition and role confusion.\"[42] Graduate programs issuing doctorates in clinical psychology emerged in the 1950s and underwent rapid increase through the 1980s. The PhD degree is intended to train practitioners who could also conduct scientific research. The PsyD degree is more exclusively designed to train practitioners.[88]\n Some clinical psychologists focus on the clinical management of patients with brain injury. This subspecialty is known as clinical neuropsychology. In many countries, clinical psychology is a regulated mental health profession. The emerging field of disaster psychology (see crisis intervention) involves professionals who respond to large-scale traumatic events.[202]\n The work performed by clinical psychologists tends to be influenced by various therapeutic approaches, all of which involve a formal relationship between professional and client (usually an individual, couple, family, or small group). Typically, these approaches encourage new ways of thinking, feeling, or behaving. Four major theoretical perspectives are psychodynamic, cognitive behavioral, existential–humanistic, and systems or family therapy. There has been a growing movement to integrate the various therapeutic approaches, especially with an increased understanding of issues regarding culture, gender, spirituality, and sexual orientation. With the advent of more robust research findings regarding psychotherapy, there is evidence that most of the major therapies have equal effectiveness, with the key common element being a strong therapeutic alliance.[203][204] Because of this, more training programs and psychologists are now adopting an eclectic therapeutic orientation.[205][206][207][208][209]\n Diagnosis in clinical psychology usually follows the Diagnostic and Statistical Manual of Mental Disorders (DSM).[210] The study of mental illnesses is called abnormal psychology.\n Educational psychology is the study of how humans learn in educational settings, the effectiveness of educational interventions, the psychology of teaching, and the social psychology of schools as organizations. Educational psychologists can be found in preschools, schools of all levels including post secondary institutions, community organizations and learning centers, Government or private research firms, and independent or private consultant.[211] The work of developmental psychologists such as Lev Vygotsky, Jean Piaget, and Jerome Bruner has been influential in creating teaching methods and educational practices. Educational psychology is often included in teacher education programs in places such as North America, Australia, and New Zealand.\n School psychology combines principles from educational psychology and clinical psychology to understand and treat students with learning disabilities; to foster the intellectual growth of gifted students; to facilitate prosocial behaviors in adolescents; and otherwise to promote safe, supportive, and effective learning environments. School psychologists are trained in educational and behavioral assessment, intervention, prevention, and consultation, and many have extensive training in research.[212]\n Industrial and organizational (I/O) psychology involves research and practices that apply psychological theories and principles to organizations and individuals' work-lives.[213] In the field's beginnings, industrialists brought the nascent field of psychology to bear on the study of scientific management techniques for improving workplace efficiency. The field was at first called economic psychology or business psychology; later, industrial psychology, employment psychology, or psychotechnology.[214] An influential early study examined workers at Western Electric's Hawthorne plant in Cicero, Illinois from 1924 to 1932. Western Electric experimented on factory workers to assess their responses to changes in illumination, breaks, food, and wages. The researchers came to focus on workers' responses to observation itself, and the term Hawthorne effect is now used to describe the fact that people's behavior can change when they think they are being observed.[215] Although the Hawthorne research can be found in psychology textbooks, the research and its findings were weak at best.[216][217]\n The name industrial and organizational psychology emerged in the 1960s. In 1973, it became enshrined in the name of the Society for Industrial and Organizational Psychology, Division 14 of the American Psychological Association.[214] One goal of the discipline is to optimize human potential in the workplace. Personnel psychology is a subfield of I/O psychology. Personnel psychologists apply the methods and principles of psychology in selecting and evaluating workers. Another subfield, organizational psychology, examines the effects of work environments and management styles on worker motivation, job satisfaction, and productivity.[218] Most I/O psychologists work outside of academia, for private and public organizations and as consultants.[214] A psychology consultant working in business today might expect to provide executives with information and ideas about their industry, their target markets, and the organization of their company.[219][220]\n Organizational behavior (OB) is an allied field involved in the study of human behavior within organizations.[221] One way to differentiate I/O psychology from OB is that I/O psychologists train in university psychology departments and OB specialists, in business schools.\n One role for psychologists in the military has been to evaluate and counsel soldiers and other personnel. In the U.S., this function began during World War I, when Robert Yerkes established the School of Military Psychology at Fort Oglethorpe in Georgia. The school provided psychological training for military staff.[42][222] Today, U.S. Army psychologists perform psychological screening, clinical psychotherapy, suicide prevention, and treatment for post-traumatic stress, as well as provide prevention-related services, for example, smoking cessation.[223] The United States Army's Mental Health Advisory Teams implement psychological interventions to help combat troops experiencing mental problems.[224][225]\n Psychologists may also work on a diverse set of campaigns known broadly as psychological warfare. Psychological warfare chiefly involves the use of propaganda to influence enemy soldiers and civilians. This so-called black propaganda is designed to seem as if it originates from a source other than the Army.[226] The CIA's MKULTRA program involved more individualized efforts at mind control, involving techniques such as hypnosis, torture, and covert involuntary administration of LSD.[227] The U.S. military used the name Psychological Operations (PSYOP) until 2010, when these activities were reclassified as Military Information Support Operations (MISO), part of Information Operations (IO).[228] Psychologists have sometimes been involved in assisting the interrogation and torture of suspects, staining the records of the psychologists involved.[229]\n An example of the contribution of psychologists to social change involves the research of Kenneth and Mamie Phipps Clark. These two African American psychologists studied segregation's adverse psychological impact on Black children. Their research findings played a role in the desegregation case Brown v. Board of Education (1954).[230]\n The impact of psychology on social change includes the discipline's broad influence on teaching and learning. Research has shown that compared to the \"whole word\" or \"whole language\" approach, the phonics approach to reading instruction is more efficacious.[231]\n Medical facilities increasingly employ psychologists to perform various roles. One aspect of health psychology is the psychoeducation of patients: instructing them in how to follow a medical regimen. Health psychologists can also educate doctors and conduct research on patient compliance.[232][233] Psychologists in the field of public health use a wide variety of interventions to influence human behavior. These range from public relations campaigns and outreach to governmental laws and policies. Psychologists study the composite influence of all these different tools in an effort to influence whole populations of people.[234]\n Psychologists work with organizations to apply findings from psychological research to improve the health and well-being of employees. Some work as external consultants hired by organizations to solve specific problems, whereas others are full-time employees of the organization. Applications include conducting surveys to identify issues and designing interventions to make work healthier. Some of the specific health areas include:\n Interventions that improve climates are a way to address accidents and violence. Interventions that reduce stress at work or provide employees with tools to better manage it can help in areas where stress is an important component.\n Industrial psychology became interested in worker fatigue during World War I, when government ministers in Britain were concerned about the impact of fatigue on workers in munitions factories but not other types of factories.[242][243] In the U. K. some interest in worker well-being emerged with the efforts of Charles Samuel Myers and his National Institute of Industrial Psychology (NIIP) during the inter-War years.[244] In the U. S. during the mid-twentieth century industrial psychologist Arthur Kornhauser pioneered the study of occupational mental health, linking industrial working conditions to mental health as well as the spillover of an unsatisfying job into a worker's personal life.[245][246] Zickar accumulated evidence to show that \"no other industrial psychologist of his era was as devoted to advocating management and labor practices that would improve the lives of working people.\"[245]\n As interest in the worker health expanded toward the end of the twentieth century, the field of occupational health psychology (OHP) emerged. OHP is a branch of psychology that is interdisciplinary.[50][247] OHP is concerned with the health and safety of workers.[50][247] OHP addresses topic areas such as the impact of occupational stressors on physical and mental health, mistreatment of workers (e.g., bullying and violence), work-family balance, the impact of involuntary unemployment on physical and mental health, the influence of psychosocial factors on safety and accidents, and interventions designed to improve/protect worker health.[50][248] OHP grew out of health psychology, industrial and organizational psychology, and occupational medicine.[249] OHP has also been informed by disciplines outside psychology, including industrial engineering, sociology, and economics.[250][251]\n Quantitative psychological research lends itself to the statistical testing of hypotheses. Although the field makes abundant use of randomized and controlled experiments in laboratory settings, such research can only assess a limited range of short-term phenomena. Some psychologists rely on less rigorously controlled, but more ecologically valid, field experiments as well. Other research psychologists rely on statistical methods to glean knowledge from population data.[252] The statistical methods research psychologists employ include the Pearson product–moment correlation coefficient, the analysis of variance, multiple linear regression, logistic regression, structural equation modeling, and hierarchical linear modeling. The measurement and operationalization of important constructs is an essential part of these research designs.\n Although this type of psychological research is much less abundant than quantitative research, some psychologists conduct qualitative research. This type of research can involve interviews, questionnaires, and first-hand observation.[253] While hypothesis testing is rare, virtually impossible, in qualitative research, qualitative studies can be helpful in theory and hypothesis generation, interpreting seemingly contradictory quantitative findings, and understanding why some interventions fail and others succeed.[254]\n A true experiment with random assignment of research participants (sometimes called subjects) to rival conditions allows researchers to make strong inferences about causal relationships. When there are large numbers of research participants, the random assignment (also called random allocation) of those participants to rival conditions ensures that the individuals in those conditions will, on average, be similar on most characteristics, including characteristics that went unmeasured. In an experiment, the researcher alters one or more variables of influence, called independent variables, and measures resulting changes in the factors of interest, called dependent variables. Prototypical experimental research is conducted in a laboratory with a carefully controlled environment.\n A quasi-experiment is a situation in which different conditions are being studied, but random assignment to the different conditions is not possible. Investigators must work with preexisting groups of people. Researchers can use common sense to consider how much the nonrandom assignment threatens the study's validity.[257] For example, in research on the best way to affect reading achievement in the first three grades of school, school administrators may not permit educational psychologists to randomly assign children to phonics and whole language classrooms, in which case the psychologists must work with preexisting classroom assignments. Psychologists will compare the achievement of children attending phonics and whole language classes and, perhaps, statistically adjust for any initial differences in reading level.\n Experimental researchers typically use a statistical hypothesis testing model which involves making predictions before conducting the experiment, then assessing how well the data collected are consistent with the predictions. These predictions are likely to originate from one or more abstract scientific hypotheses about how the phenomenon under study actually works.[258]\n Surveys are used in psychology for the purpose of measuring attitudes and traits, monitoring changes in mood, and checking the validity of experimental manipulations (checking research participants' perception of the condition they were assigned to). Psychologists have commonly used paper-and-pencil surveys. However, surveys are also conducted over the phone or through e-mail. Web-based surveys are increasingly used to conveniently reach many subjects.\n Observational studies are commonly conducted in psychology. In cross-sectional observational studies, psychologists collect data at a single point in time. The goal of many cross-sectional studies is the assess the extent factors are correlated with each other. By contrast, in longitudinal studies psychologists collect data on the same sample at two or more points in time. Sometimes the purpose of longitudinal research is to study trends across time such as the stability of traits or age-related changes in behavior. Because some studies involve endpoints that psychologists cannot ethically study from an experimental standpoint, such as identifying the causes of depression, they conduct longitudinal studies a large group of depression-free people, periodically assessing what is happening in the individuals' lives. In this way psychologists have an opportunity to test causal hypotheses regarding conditions that commonly arise in people's lives that put them at risk for depression. Problems that affect longitudinal studies include selective attrition, the type of problem in which bias is introduced when a certain type of research participant disproportionately leaves a study.\n One example of an observational study was run by Arthur Bandura. This observational study focused on children who were exposed to an adult exhibiting aggressive behaviors and their reaction to toys versus other children who were not exposed to these stimuli. The result shows that children who had seen the adult acting aggressively towards a toy, in turn, were aggressive towards their own toy when put in a situation that frustrated them.[189]\n Exploratory data analysis includes a variety of practices that researchers use to reduce a great many variables to a small number overarching factors. In Peirce's three modes of inference, exploratory data analysis corresponds to abduction.[259] Meta-analysis is the technique research psychologists use to integrate results from many studies of the same variables and arriving at a grand average of the findings.[260]\n A classic and popular tool used to relate mental and neural activity is the electroencephalogram (EEG), a technique using amplified electrodes on a person's scalp to measure voltage changes in different parts of the brain. Hans Berger, the first researcher to use EEG on an unopened skull, quickly found that brains exhibit signature \"brain waves\": electric oscillations which correspond to different states of consciousness. Researchers subsequently refined statistical methods for synthesizing the electrode data, and identified unique brain wave patterns such as the delta wave observed during non-REM sleep.[261]\n Newer functional neuroimaging techniques include functional magnetic resonance imaging and positron emission tomography, both of which track the flow of blood through the brain. These technologies provide more localized information about activity in the brain and create representations of the brain with widespread appeal. They also provide insight which avoids the classic problems of subjective self-reporting. It remains challenging to draw hard conclusions about where in the brain specific thoughts originate—or even how usefully such localization corresponds with reality. However, neuroimaging has delivered unmistakable results showing the existence of correlations between mind and brain. Some of these draw on a systemic neural network model rather than a localized function model.[262][263][264]\n Interventions such as transcranial magnetic stimulation and drugs also provide information about brain–mind interactions. Psychopharmacology is the study of drug-induced mental effects.\n Computational modeling is a tool used in mathematical psychology and cognitive psychology to simulate behavior.[265] This method has several advantages. Since modern computers process information quickly, simulations can be run in a short time, allowing for high statistical power. Modeling also allows psychologists to visualize hypotheses about the functional organization of mental events that could not be directly observed in a human. Computational neuroscience uses mathematical models to simulate the brain. Another method is symbolic modeling, which represents many mental objects using variables and rules. Other types of modeling include dynamic systems and stochastic modeling.\n Animal experiments aid in investigating many aspects of human psychology, including perception, emotion, learning, memory, and thought, to name a few. In the 1890s, Russian physiologist Ivan Pavlov famously used dogs to demonstrate classical conditioning. Non-human primates, cats, dogs, pigeons, and rats and other rodents are often used in psychological experiments. Ideally, controlled experiments introduce only one independent variable at a time, in order to ascertain its unique effects upon dependent variables. These conditions are approximated best in laboratory settings. In contrast, human environments and genetic backgrounds vary so widely, and depend upon so many factors, that it is difficult to control important variables for human subjects. There are pitfalls, however, in generalizing findings from animal studies to humans through animal models.[266]\n Comparative psychology is the scientific study of the behavior and mental processes of non-human animals, especially as these relate to the phylogenetic history, adaptive significance, and development of behavior. Research in this area explores the behavior of many species, from insects to primates. It is closely related to other disciplines that study animal behavior such as ethology.[267] Research in comparative psychology sometimes appears to shed light on human behavior, but some attempts to connect the two have been quite controversial, for example the Sociobiology of E.O. Wilson.[268] Animal models are often used to study neural processes related to human behavior, e.g. in cognitive neuroscience.\n Qualitative research is often designed to answer questions about the thoughts, feelings, and behaviors of individuals. Qualitative research involving first-hand observation can help describe events as they occur, with the goal of capturing the richness of everyday behavior and with the hope of discovering and understanding phenomena that might have been missed if only more cursory examinations are made.\n Qualitative psychological research methods include interviews, first-hand observation, and participant observation. Creswell (2003) identified five main possibilities for qualitative research, including narrative, phenomenology, ethnography, case study, and grounded theory. Qualitative researchers[270] sometimes aim to enrich our understanding of symbols, subjective experiences, or social structures. Sometimes hermeneutic and critical aims can give rise to quantitative research, as in Erich Fromm's application of psychological and sociological theories, in his book Escape from Freedom, to understanding why many ordinary Germans supported Hitler.[271]\n Just as Jane Goodall studied chimpanzee social and family life by careful observation of chimpanzee behavior in the field, psychologists conduct naturalistic observation of ongoing human social, professional, and family life. Sometimes the participants are aware they are being observed, and other times the participants do not know they are being observed. Strict ethical guidelines must be followed when covert observation is being carried out.\n Program evaluation involves the systematic collection, analysis, and application of information to answer questions about projects, policies and programs, particularly about their effectiveness.[272][273] In both the public and private sectors, stakeholders often want to know the extent which the programs they are funding, implementing, voting for, receiving, or objecting to are producing the intended effects. While program evaluation first focuses on effectiveness, important considerations often include how much the program costs per participant, how the program could be improved, whether the program is worthwhile, whether there are better alternatives, if there are unintended outcomes, and whether the program goals are appropriate and useful.[274]\n Metascience involves the application of scientific methodology to study science itself. The field of metascience has revealed problems in psychological research. Some psychological research has suffered from bias,[275] problematic reproducibility,[276] and misuse of statistics.[277] These findings have led to calls for reform from within and from outside the scientific community.[278]\n In 1959, statistician Theodore Sterling examined the results of psychological studies and discovered that 97% of them supported their initial hypotheses, implying possible publication bias.[279][280][281] Similarly, Fanelli (2010)[282] found that 91.5% of psychiatry/psychology studies confirmed the effects they were looking for, and concluded that the odds of this happening (a positive result) was around five times higher than in fields such as space science or geosciences. Fanelli argued that this is because researchers in \"softer\" sciences have fewer constraints to their conscious and unconscious biases.\n A replication crisis in psychology has emerged. Many notable findings in the field have not been replicated. Some researchers were even accused of publishing fraudulent results.[283][284][285] Systematic efforts, including efforts by the Reproducibility Project of the Center for Open Science, to assess the extent of the problem found that as many as two-thirds of highly publicized findings in psychology failed to be replicated.[286] Reproducibility has generally been stronger in cognitive psychology (in studies and journals) than social psychology[286] and subfields of differential psychology.[287][288] Other subfields of psychology have also been implicated in the replication crisis, including clinical psychology,[289][290][291] developmental psychology,[292][293][294] and a field closely related to psychology, educational research.[295][296][297][298][299]\n Focus on the replication crisis has led to other renewed efforts in the discipline to re-test important findings.[300][301] In response to concerns about publication bias and data dredging (conducting a large number of statistical tests on a great many variables but restricting reporting to the results that were statistically significant), 295 psychology and medical journals have adopted result-blind peer review where studies are accepted not on the basis of their findings and after the studies are completed, but before the studies are conducted and upon the basis of the methodological rigor of their experimental designs and the theoretical justifications for their proposed statistical analysis before data collection or analysis is conducted.[302][303] In addition, large-scale collaborations among researchers working in multiple labs in different countries have taken place. The collaborators regularly make their data openly available for different researchers to assess.[304] Allen and Mehler[305] estimated that 61 per cent of result-blind studies have yielded null results, in contrast to an estimated 5 to 20 per cent in traditional research.\n Some critics view statistical hypothesis testing as misplaced. Psychologist and statistician Jacob Cohen wrote in 1994 that psychologists routinely confuse statistical significance with practical importance, enthusiastically reporting great certainty in unimportant facts.[306] Some psychologists have responded with an increased use of effect size statistics, rather than sole reliance on p-values.[307]\n In 2008, Arnett pointed out that most articles in American Psychological Association journals were about U.S. populations when U.S. citizens are only 5% of the world's population. He complained that psychologists had no basis for assuming psychological processes to be universal and generalizing research findings to the rest of the global population.[308] In 2010, Henrich, Heine, and Norenzayan reported a bias in conducting psychology studies with participants from \"WEIRD\" (\"Western, Educated, Industrialized, Rich, and Democratic\") societies.[309][310] Henrich et al. found that \"96% of psychological samples come from countries with only 12% of the world's population\" (p. 63). The article gave examples of results that differ significantly between people from WEIRD and tribal cultures, including the Müller-Lyer illusion. Arnett (2008), Altmaier and Hall (2008) and Morgan-Consoli et al. (2018) view the Western bias in research and theory as a serious problem considering psychologists are increasingly applying psychological principles developed in WEIRD regions in their research, clinical work, and consultation with populations around the world.[308][311][312] In 2018, Rad, Martingano, and Ginges showed that nearly a decade after Henrich et al.'s paper, over 80% of the samples used in studies published in the journal Psychological Science employed WEIRD samples. Moreover, their analysis showed that several studies did not fully disclose the origin of their samples; the authors offered a set of recommendations to editors and reviewers to reduce WEIRD bias.[313]\n Similar to the WEIRD bias, starting in 2020, researchers of non-human behavior have started to emphasize the need to document the possibility of the STRANGE (Social background, Trappability and self-selection, Rearing history, Acclimation and habituation, Natural changes in responsiveness, Genetic makeup, and Experience) bias in study conclusions.[314]\n Some observers perceive a gap between scientific theory and its application—in particular, the application of unsupported or unsound clinical practices.[315] Critics say there has been an increase in the number of mental health training programs that do not instill scientific competence.[316] Practices such as \"facilitated communication for infantile autism\"; memory-recovery techniques including body work; and other therapies, such as rebirthing and reparenting, may be dubious or even dangerous, despite their popularity.[317] These practices, however, are outside the mainstream practices taught in clinical psychology doctoral programs.\n Ethical standards in the discipline have changed over time. Some famous past studies are today considered unethical and in violation of established codes (e.g., the Canadian Code of Conduct for Research Involving Humans, and the Belmont Report). The American Psychological Association has advanced a set of ethical principles and a code of conduct for the profession.[318]\n The most important contemporary standards include informed and voluntary consent. After World War II, the Nuremberg Code was established because of Nazi abuses of experimental subjects. Later, most countries (and scientific journals) adopted the Declaration of Helsinki. In the U.S., the National Institutes of Health established the Institutional Review Board in 1966, and in 1974 adopted the National Research Act (HR 7724). All of these measures encouraged researchers to obtain informed consent from human participants in experimental studies. A number of influential but ethically dubious studies led to the establishment of this rule; such studies included the MIT-Harvard Fernald School radioisotope studies, the Thalidomide tragedy, the Willowbrook hepatitis study, Stanley Milgram's studies of obedience to authority, and the Stanford Prison Experiment.\n The ethics code of the American Psychological Association originated in 1951 as \"Ethical Standards of Psychologists.\" This code has guided the formation of licensing laws in most American states. It has changed multiple times over the decades since its adoption, and contains both aspirational principles and binding ethical standards. \n The APA's Ethical Principles of Psychologists and Code of Conduct consists of five General Principles, which are meant to  guide psychologists to higher ethical practice where a particular standard does not apply. Those principles are:\n A. Beneficence and Nonmaleficence - meaning the psychologists must work to benefit those they work with and \"do no harm.\" This includes awareness of indirect benefits and harms their work might have on others due to personal, social, political, or other factors. \n B. Fidelity and Responsibility - an awareness of public trust in the profession and adherence to ethical standards and clarification of roles to preserve that trust. This includes managing conflicts of interest, as well as committing some portion of a psychologist's professional time to low-cost or pro bono work. \n C. Integrity - upholding honesty and accuracy in all psychological practices, including avoiding misrepresentations and fraud. In situations where psychologists would use deception (i.e., certain research), psychologists must consider the necessity, benefits, and harms, and mitigate any harms where possible. \n D. Justice - an understanding that psychology must be for everyone's benefit, and that psychologists take special care to avoid unjust practices as a result of biases or limitations of expertise. \n E. Respect for People's Rights and Dignity - the preservation of people's rights when working with psychologists, including confidentially, privacy, and autonomy.  Psychologists should consider a multitude of factors, including a need for special safeguards for protected populations (e.g., minors, incarcerated individuals) and awareness of differences based on numerous factors, including culture, race, age, gender, and socioeconomic status. \n In 1989, the APA revised its policies on advertising and referral fees to negotiate the end of an investigation by the Federal Trade Commission. The 1992 incarnation was the first to distinguish between \"aspirational\" ethical standards and \"enforceable\" ones. The APA code was further revised in 2010 to prevent the use of the code to justify violating human rights, which was in response to the participation of APA members in interrogations under the administration of United States President George W. Bush.[319] Members of the public have a five-year window to file ethics complaints about APA members with the APA ethics committee; members of the APA have a three-year window.[320]\n The Canadian Psychological Association used the APA code until 1986, when it developed its own code drawing from four similar principles: 1) Respect for the Dignity of Persons and Peoples, 2) Responsible Caring, 3) Integrity in Relationships, 4) Responsibility to Society.[321][322] The European Federation of Psychologist's Associations, have adopted a model code using the principles of the Canadian Code, while also drawing from the APA code.[323][324]\n Universities have ethics committees dedicated to protecting the rights (e.g., voluntary nature of participation in the research, privacy) and well-being (e.g., minimizing distress) of research participants. University ethics committees evaluate proposed research to ensure that researchers protect the rights and well-being of participants; an investigator's research project cannot be conducted unless approved by such an ethics committee.[325]\n The field of psychology also identifies certain categories of people that require additional or special protection due to particular vulnerabilities, unequal power dynamics, or diminished capacity for informed consent. This list often includes, but is not limited to, children, incarcerated individuals,  pregnant women, human fetuses and neonates, institutionalized persons, those with physical or mental disabilities, and the educationally or economically disadvantaged.[326]\n Some of the ethical issues considered most important are the requirement to practice only within the area of competence, to maintain confidentiality with the patients, and to avoid sexual relations with them. Another important principle is informed consent, the idea that a patient or research subject must understand and freely choose a procedure they are undergoing.[320] Some of the most common complaints against clinical psychologists include sexual misconduct[320] and breaches in confidentiality or privacy.[327]\n Psychology ethics apply to all types of human contact in a psychologist's professional capacity, including therapy, assessment, teaching, training, work with research subjects, testimony in courts and before government bodies, consulting, and statements to the public or media pertaining to matters of psychology.[318]\n Research on other animals is governed by university ethics committees. Research on nonhuman animals cannot proceed without permission of the ethics committee, of the researcher's home institution. Ethical guidelines state that using non-human animals for scientific purposes is only acceptable when the harm (physical or psychological) done to animals is outweighed by the benefits of the research.[328] Psychologists can use certain research techniques on animals that could not be used on humans.\n Comparative psychologist Harry Harlow drew moral condemnation for isolation experiments on rhesus macaque monkeys at the University of Wisconsin–Madison in the 1970s.[329] The aim of the research was to produce an animal model of clinical depression. Harlow also devised what he called a \"rape rack\", to which the female isolates were tied in normal monkey mating posture.[330] In 1974, American literary critic Wayne C. Booth wrote that, \"Harry Harlow and his colleagues go on torturing their nonhuman primates decade after decade, invariably proving what we all knew in advance—that social creatures can be destroyed by destroying their social ties.\" He writes that Harlow made no mention of the criticism of the morality of his work.[331]\n Animal research is influential in psychology, while still being debated among academics. The testing of animals for research has led to medical breakthroughs in human medicine. Many psychologists argue animal experimentation is essential for human advancement, but must be regulated by the government to ensure ethicality.\n"
    },
    {
        "title": "Linguistics",
        "url": "https://en.wikipedia.org/wiki/Linguistics",
        "content": "\n Linguistics is the scientific study of language.[1][2][3] The areas of linguistic analysis are syntax (rules governing the structure of sentences), semantics (meaning), morphology (structure of words), phonetics (speech sounds and equivalent gestures in sign languages), phonology (the abstract sound system of a particular language, and analogous systems of sign languages), and pragmatics (how the context of use contributes to meaning).[4] Subdisciplines such as biolinguistics (the study of the biological variables and evolution of language) and psycholinguistics (the study of psychological factors in human language) bridge many of these divisions.[5]\n Linguistics encompasses many branches and subfields that span both theoretical and practical applications.[6] Theoretical linguistics (including traditional descriptive linguistics) is concerned with understanding the universal and fundamental nature of language and developing a general theoretical framework for describing it.[7] Applied linguistics seeks to utilize the scientific findings of the study of language for practical purposes, such as developing methods of improving language education and literacy.[8]\n Linguistic features may be studied through a variety of perspectives: synchronically (by describing the structure of a language at a specific point in time) or diachronically (through the historical development of a language over a period of time), in monolinguals or in multilinguals, among children or among adults, in terms of how it is being learnt or how it was acquired, as abstract objects or as cognitive structures, through written texts or through oral elicitation, and finally through mechanical data collection or practical fieldwork.[9]\n Linguistics emerged from the field of philology,  of which some branches are more qualitative and holistic in approach.[10] Today, philology and linguistics are variably described as related fields, subdisciplines, or separate fields of language study but, by and large, linguistics can be seen as an umbrella term.[11] Linguistics is also related to the philosophy of language, stylistics, rhetoric, semiotics, lexicography, and translation.\n Historical linguistics is the study of how language changes over history, particularly with regard to a specific language or a group of languages. Western trends in historical linguistics date back to roughly the late 18th century, when the discipline grew out of philology, the study of ancient texts and oral traditions.[12]\n Historical linguistics emerged as one of the first few sub-disciplines in the field, and was most widely practised during the late 19th century.[13] Despite a shift in focus in the 20th century towards formalism and generative grammar, which studies the universal properties of language, historical research today still remains a significant field of linguistic inquiry. Subfields of the discipline include language change and grammaticalization.[14]\n Historical linguistics studies language change either diachronically (through a comparison of different time periods in the past and present) or in a synchronic manner (by observing developments between different variations that exist within the current linguistic stage of a language).[15]\n At first, historical linguistics was the cornerstone of comparative linguistics, which involves a study of the relationship between different languages.[16] At that time, scholars of historical linguistics were only concerned with creating different categories of language families, and reconstructing prehistoric proto-languages by using both the comparative method and the method of internal reconstruction. Internal reconstruction is the method by which an element that contains a certain meaning is re-used in different contexts or environments where there is a variation in either sound or analogy.[16][better source needed]\n The reason for this had been to describe well-known Indo-European languages, many of which had detailed documentation and long written histories. Scholars of historical linguistics also studied Uralic languages, another European language family for which very little written material existed back then. After that, there also followed significant work on the corpora of other languages, such as the Austronesian languages and the Native American language families.\n In historical work, the uniformitarian principle is generally the underlying working hypothesis, occasionally also clearly expressed.[17] The principle was expressed early by William Dwight Whitney, who considered it imperative, a \"must\", of historical linguistics to \"look to find the same principle operative also in the very outset of that [language] history.\"[18]\n The above approach of comparativism in linguistics is now, however, only a small part of the much broader discipline called historical linguistics. The comparative study of specific Indo-European languages is considered a highly specialized field today, while comparative research is carried out over the subsequent internal developments in a language: in particular, over the development of modern standard varieties of languages, and over the development of a language from its standardized form to its varieties.[citation needed]\n For instance, some scholars also tried to establish super-families, linking, for example, Indo-European, Uralic, and other language families to a hypothetical Nostratic language group.[19] While these attempts are still not widely accepted as credible methods, they provide necessary information to establish relatedness in language change. This is generally hard to find for events long ago, due to the occurrence of chance word resemblances and variations between language groups. A limit of around 10,000 years is often assumed for the functional purpose of conducting research.[20] It is also hard to date various proto-languages. Even though several methods are available, these languages can be dated only approximately.[21]\n In modern historical linguistics, we examine how languages change over time, focusing on the relationships between dialects within a specific period. This includes studying morphological, syntactical, and phonetic shifts. Connections between dialects in the past and present are also explored.[22]\n Syntax is the study of how words and morphemes combine to form larger units such as phrases and sentences. Central concerns of syntax include word order, grammatical relations, constituency,[23] agreement, the nature of crosslinguistic variation, and the relationship between form and meaning. There are numerous approaches to syntax that differ in their central assumptions and goals.\n Morphology is the study of words, including the principles by which they are formed, and how they relate to one another within a language.[24][25] Most approaches to morphology investigate the structure of words in terms of morphemes, which are the smallest units in a language with some independent meaning. Morphemes include roots that can exist as words by themselves, but also categories such as affixes that can only appear as part of a larger word. For example, in English the root catch and the suffix -ing are both morphemes; catch may appear as its own word, or it may be combined with -ing to form the new word catching. Morphology also analyzes how words behave as parts of speech, and how they may be inflected to express grammatical categories including number, tense, and aspect. Concepts such as productivity are concerned with how speakers create words in specific contexts, which evolves over the history of a language.\n The discipline that deals specifically with the sound changes occurring within morphemes is morphophonology.[26]\n Semantics and pragmatics are branches of linguistics concerned with meaning. These subfields have traditionally been divided according to aspects of meaning: \"semantics\" refers to grammatical and lexical meanings, while \"pragmatics\" is concerned with meaning in context. Within linguistics, the subfield of formal semantics studies the denotations of sentences and how they are composed from the meanings of their constituent expressions. Formal semantics draws heavily on philosophy of language and uses formal tools from logic and computer science. On the other hand, cognitive semantics explains linguistic meaning via aspects of general cognition, drawing on ideas from cognitive science such as prototype theory.\n Pragmatics focuses on phenomena such as speech acts, implicature, and talk in interaction.[27] Unlike semantics, which examines meaning that is conventional or \"coded\" in a given language, pragmatics studies how the transmission of meaning depends not only on the structural and linguistic knowledge (grammar, lexicon, etc.) of the speaker and listener, but also on the context of the utterance,[28] any pre-existing knowledge about those involved, the inferred intent of the speaker, and other factors.[29]\n Phonetics and phonology are branches of linguistics concerned with sounds (or the equivalent aspects of sign languages). Phonetics is largely concerned with the physical aspects of sounds such as their articulation, acoustics, production, and perception. Phonology is concerned with the linguistic abstractions and categorizations of sounds, and it tells us what sounds are in a language, how they do and can combine into words, and explains why certain phonetic features are important to identifying a word.[30]\n Linguistic structures are pairings of meaning and form. Any particular pairing of meaning and form is a Saussurean linguistic sign. For instance, the meaning \"cat\" is represented worldwide with a wide variety of different sound patterns (in oral languages), movements of the hands and face (in sign languages), and written symbols (in written languages). Linguistic patterns have proven their importance for the knowledge engineering field especially with the ever-increasing amount of available data.\n Linguists focusing on structure attempt to understand the rules regarding language use that native speakers know (not always consciously). All linguistic structures can be broken down into component parts that are combined according to (sub)conscious rules, over multiple levels of analysis. For instance, consider the structure of the word \"tenth\" on two different levels of analysis. On the level of internal word structure (known as morphology), the word \"tenth\" is made up of one linguistic form indicating a number and another form indicating ordinality. The rule governing the combination of these forms ensures that the ordinality marker \"th\" follows the number \"ten.\" On the level of sound structure (known as phonology), structural analysis shows that the \"n\" sound in \"tenth\" is made differently from the \"n\" sound in \"ten\" spoken alone. Although most speakers of English are consciously aware of the rules governing internal structure of the word pieces of \"tenth\", they are less often aware of the rule governing its sound structure. Linguists focused on structure find and analyze rules such as these, which govern how native speakers use language.\n Grammar is a system of rules which governs the production and use of utterances in a given language. These rules apply to sound[33] as well as meaning, and include componential subsets of rules, such as those pertaining to phonology (the organization of phonetic sound systems), morphology (the formation and composition of words), and syntax (the formation and composition of phrases and sentences).[4] Modern frameworks that deal with the principles of grammar include structural and functional linguistics, and generative linguistics.[34]\n Sub-fields that focus on a grammatical study of language include the following:\n Discourse is language as social practice (Baynham, 1995) and is a multilayered concept. As a social practice, discourse embodies different ideologies through written and spoken texts. Discourse analysis can examine or expose these ideologies. Discourse not only influences genre, which is selected based on specific contexts but also, at a micro level, shapes language as text (spoken or written) down to the phonological and lexico-grammatical levels. Grammar and discourse are linked as parts of a system.[36] A particular discourse becomes a language variety when it is used in this way for a particular purpose, and is referred to as a register.[37] There may be certain lexical additions (new words) that are brought into play because of the expertise of the community of people within a certain domain of specialization. Thus, registers and discourses distinguish themselves not only through specialized vocabulary but also, in some cases, through distinct stylistic choices. People in the medical fraternity, for example, may use some medical terminology in their communication that is specialized to the field of medicine. This is often referred to as being part of the \"medical discourse\", and so on.\n The lexicon is a catalogue of words and terms that are stored in a speaker's mind. The lexicon consists of words and bound morphemes, which are parts of words that can not stand alone, like affixes. In some analyses, compound words and certain classes of idiomatic expressions and other collocations are also considered to be part of the lexicon. Dictionaries represent attempts at listing, in alphabetical order, the lexicon of a given language; usually, however, bound morphemes are not included. Lexicography, closely linked with the domain of semantics, is the science of mapping the words into an encyclopedia or a dictionary. The creation and addition of new words (into the lexicon) is called coining or neologization,[38] and the new words are called neologisms.\n It is often believed that a speaker's capacity for language lies in the quantity of words stored in the lexicon. However, this is often considered a myth by linguists. The capacity for the use of language is considered by many linguists to lie primarily in the domain of grammar, and to be linked with competence, rather than with the growth of vocabulary. Even a very small lexicon is theoretically capable of producing an infinite number of sentences.\n Stylistics also involves the study of written, signed, or spoken discourse through varying speech communities, genres, and editorial or narrative formats in the mass media.[39] It involves the study and interpretation of texts for aspects of their linguistic and tonal style. Stylistic analysis entails the analysis of description of particular dialects and registers used by speech communities. Stylistic features include rhetoric,[40] diction, stress, satire, irony, dialogue, and other forms of phonetic variations. Stylistic analysis can also include the study of language in canonical works of literature, popular fiction, news, advertisements, and other forms of communication in popular culture as well. It is usually seen as a variation in communication that changes from speaker to speaker and community to community. In short, Stylistics is the interpretation of text.\n In the 1960s, Jacques Derrida, for instance, further distinguished between speech and writing, by proposing that written language be studied as a linguistic medium of communication in itself.[41] Palaeography is therefore the discipline that studies the evolution of written scripts (as signs and symbols) in language.[42] The formal study of language also led to the growth of fields like psycholinguistics, which explores the representation and function of language in the mind; neurolinguistics, which studies language processing in the brain; biolinguistics, which studies the biology and evolution of language; and language acquisition, which investigates how children and adults acquire the knowledge of one or more languages.\n The fundamental principle of humanistic linguistics, especially rational and logical grammar, is that language is an invention created by people. A semiotic tradition of linguistic research considers language a sign system which arises from the interaction of meaning and form.[43] The organization of linguistic levels is considered computational.[44] Linguistics is essentially seen as relating to social and cultural studies because different languages are shaped in social interaction by the speech community.[45] Frameworks representing the humanistic view of language include structural linguistics, among others.[46]\n Structural analysis means dissecting each linguistic level: phonetic, morphological, syntactic, and discourse, to the smallest units. These are collected into inventories (e.g. phoneme, morpheme, lexical classes, phrase types) to study their interconnectedness within a hierarchy of structures and layers.[47] Functional analysis adds to structural analysis the assignment of semantic and other functional roles that each unit may have. For example, a noun phrase may function as the subject or object of the sentence; or the agent or patient.[48]\n Functional linguistics, or functional grammar, is a branch of structural linguistics. In the humanistic reference, the terms structuralism and functionalism are related to their meaning in other human sciences. The difference between formal and functional structuralism lies in the way that the two approaches explain why languages have the properties they have. Functional explanation entails the idea that language is a tool for communication, or that communication is the primary function of language. Linguistic forms are consequently explained by an appeal to their functional value, or usefulness. Other structuralist approaches take the perspective that form follows from the inner mechanisms of the bilateral and multilayered language system.[49]\n Approaches such as cognitive linguistics and generative grammar study linguistic cognition with a view towards uncovering the biological underpinnings of language. In Generative Grammar, these underpinning are understood as including innate domain-specific grammatical knowledge. Thus, one of the central concerns of the approach is to discover what aspects of linguistic knowledge are innate and which are not.[50][51]\n Cognitive linguistics, in contrast, rejects the notion of innate grammar, and studies how the human mind creates linguistic constructions from event schemas,[52] and the impact of cognitive constraints and biases on human language.[53] In cognitive linguistics, language is approached via the senses.[54][55]\n A closely related approach is evolutionary linguistics[56] which includes the study of linguistic units as cultural replicators.[57][58] It is possible to study how language replicates and adapts to the mind of the individual or the speech community.[59][60] Construction grammar is a framework which applies the meme concept to the study of syntax.[61][62][63][64]\n The generative versus evolutionary approach are sometimes called formalism and functionalism, respectively.[65] This reference is however different from the use of the terms in human sciences.[66]\n Modern linguistics is primarily descriptive.[67] Linguists describe and explain features of language without making subjective judgments on whether a particular feature or usage is \"good\" or \"bad\". This is analogous to practice in other sciences: a zoologist studies the animal kingdom without making subjective judgments on whether a particular species is \"better\" or \"worse\" than another.[68]\n Prescription, on the other hand, is an attempt to promote particular linguistic usages over others, often favoring a particular dialect or \"acrolect\". This may have the aim of establishing a linguistic standard, which can aid communication over large geographical areas. It may also, however, be an attempt by speakers of one language or dialect to exert influence over speakers of other languages or dialects (see Linguistic imperialism). An extreme version of prescriptivism can be found among censors, who attempt to eradicate words and structures that they consider to be destructive to society. Prescription, however, may be practised appropriately in language instruction, like in ELT, where certain fundamental grammatical rules and lexical items need to be introduced to a second-language speaker who is attempting to acquire the language.[citation needed]\n Most contemporary linguists work under the assumption that spoken data and signed data are more fundamental than written data. This is because\n Nonetheless, linguists agree that the study of written language can be worthwhile and valuable. For research that relies on corpus linguistics and computational linguistics, written language is often much more convenient for processing large amounts of linguistic data. Large corpora of spoken language are difficult to create and hard to find, and are typically transcribed and written. In addition, linguists have turned to text-based discourse occurring in various formats of computer-mediated communication as a viable site for linguistic inquiry.\n The study of writing systems themselves, graphemics, is, in any case, considered a branch of linguistics.\n Before the 20th century, linguists analysed language on a diachronic plane, which was historical in focus. This meant that they would compare linguistic features and try to analyse language from the point of view of how it had changed between then and later. However, with the rise of Saussurean linguistics in the 20th century, the focus shifted to a more synchronic approach, where the study was geared towards analysis and comparison between different language variations, which existed at the same given point of time.\n At another level, the syntagmatic plane of linguistic analysis entails the comparison between the way words are sequenced, within the syntax of a sentence. For example, the article \"the\" is followed by a noun, because of the syntagmatic relation between the words. The paradigmatic plane, on the other hand, focuses on an analysis that is based on the paradigms or concepts that are embedded in a given text. In this case, words of the same type or class may be replaced in the text with each other to achieve the same conceptual understanding.\n The earliest activities in the description of language have been attributed to the 6th-century-BC Indian grammarian Pāṇini[69][70] who wrote a formal description of the Sanskrit language in his Aṣṭādhyāyī.[71][72] Today, modern-day theories on grammar employ many of the principles that were laid down then.[73]\n Before the 20th century, the term philology, first attested in 1716,[74] was commonly used to refer to the study of language, which was then predominantly historical in focus.[75][76] Since Ferdinand de Saussure's insistence on the importance of synchronic analysis, however, this focus has shifted[76] and the term philology is now generally used for the \"study of a language's grammar, history, and literary tradition\", especially in the United States[77] (where philology has never been very popularly considered as the \"science of language\").[74]\n Although the term linguist in the sense of \"a student of language\" dates from 1641,[78] the term linguistics is first attested in 1847.[78] It is now the usual term in English for the scientific study of language,[79][80] though linguistic science is sometimes used.\n Linguistics is a multi-disciplinary field of research that combines tools from natural sciences, social sciences, formal sciences, and the humanities.[81][82][83][84] Many linguists, such as David Crystal, conceptualize the field as being primarily scientific.[85] The term linguist applies to someone who studies language or is a researcher within the field, or to someone who uses the tools of the discipline to describe and analyse specific languages.[86]\n An early formal study of language was in India with Pāṇini, the 6th century BC grammarian who formulated 3,959 rules of Sanskrit morphology. Pāṇini's systematic classification of the sounds of Sanskrit into consonants and vowels, and word classes, such as nouns and verbs, was the first known instance of its kind. In the Middle East, Sibawayh, a Persian, made a detailed description of Arabic in AD 760 in his monumental work, Al-kitab fii an-naħw (الكتاب في النحو, The Book on Grammar), the first known author to distinguish between sounds and phonemes (sounds as units of a linguistic system). Western interest in the study of languages began somewhat later than in the East,[87] but the grammarians of the classical languages did not use the same methods or reach the same conclusions as their contemporaries in the Indic world. Early interest in language in the West was a part of philosophy, not of grammatical description. The first insights into semantic theory were made by Plato in his Cratylus dialogue, where he argues that words denote concepts that are eternal and exist in the world of ideas. This work is the first to use the word etymology to describe the history of a word's meaning. Around 280 BC, one of Alexander the Great's successors founded a university (see Musaeum) in Alexandria, where a school of philologists studied the ancient texts in Greek, and taught Greek to speakers of other languages. While this school was the first to use the word \"grammar\" in its modern sense, Plato had used the word in its original meaning as \"téchnē grammatikḗ\" (Τέχνη Γραμματική), the \"art of writing\", which is also the title of one of the most important works of the Alexandrine school by Dionysius Thrax.[88] Throughout the Middle Ages, the study of language was subsumed under the topic of philology, the study of ancient languages and texts, practised by such educators as Roger Ascham, Wolfgang Ratke, and John Amos Comenius.[89]\n In the 18th century, the first use of the comparative method by William Jones sparked the rise of comparative linguistics.[90] Bloomfield attributes \"the first great scientific linguistic work of the world\" to Jacob Grimm, who wrote Deutsche Grammatik.[91] It was soon followed by other authors writing similar comparative studies on other language groups of Europe. The study of language was broadened from Indo-European to language in general by Wilhelm von Humboldt, of whom Bloomfield asserts:[91]\n This study received its foundation at the hands of the Prussian statesman and scholar Wilhelm von Humboldt (1767–1835), especially in the first volume of his work on Kavi, the literary language of Java, entitled Über die Verschiedenheit des menschlichen Sprachbaues und ihren Einfluß auf die geistige Entwickelung des Menschengeschlechts (On the Variety of the Structure of Human Language and its Influence upon the Mental Development of the Human Race). There was a shift of focus from historical and comparative linguistics to synchronic analysis in early 20th century. Structural analysis was improved by Leonard Bloomfield, Louis Hjelmslev; and Zellig Harris who also developed methods of discourse analysis. Functional analysis was developed by the Prague linguistic circle and André Martinet. As sound recording devices became commonplace in the 1960s, dialectal recordings were made and archived, and the audio-lingual method provided a technological solution to foreign language learning. The 1960s also saw a new rise of comparative linguistics: the study of language universals in linguistic typology. Towards the end of the century the field of linguistics became divided into further areas of interest with the advent of language technology and digitalized corpora.[92][93][94]\n Sociolinguistics is the study of how language is shaped by social factors. This sub-discipline focuses on the synchronic approach of linguistics, and looks at how a language in general, or a set of languages, display variation and varieties at a given point in time. The study of language variation and the different varieties of language through dialects, registers, and idiolects can be tackled through a study of style, as well as through analysis of discourse. Sociolinguists research both style and discourse in language, as well as the theoretical factors that are at play between language and society.\n Developmental linguistics is the study of the development of linguistic ability in individuals, particularly the acquisition of language in childhood. Some of the questions that developmental linguistics looks into are how children acquire different languages, how adults can acquire a second language, and what the process of language acquisition is.[95]\n Neurolinguistics is the study of the structures in the human brain that underlie grammar and communication. Researchers are drawn to the field from a variety of backgrounds, bringing along a variety of experimental techniques as well as widely varying theoretical perspectives. Much work in neurolinguistics is informed by models in psycholinguistics and theoretical linguistics, and is focused on investigating how the brain can implement the processes that theoretical and psycholinguistics propose are necessary in producing and comprehending language. Neurolinguists study the physiological mechanisms by which the brain processes information related to language, and evaluate linguistic and psycholinguistic theories, using aphasiology, brain imaging, electrophysiology, and computer modelling. Amongst the structures of the brain involved in the mechanisms of neurolinguistics, the cerebellum which contains the highest numbers of neurons has a major role in terms of predictions required to produce language.[96]\n Linguists are largely concerned with finding and describing the generalities and varieties both within particular languages and among all languages. Applied linguistics takes the results of those findings and \"applies\" them to other areas. Linguistic research is commonly applied to areas such as language education, lexicography, translation, language planning, which involves governmental policy implementation related to language use, and natural language processing. \"Applied linguistics\" has been argued to be something of a misnomer.[97] Applied linguists actually focus on making sense of and engineering solutions for real-world linguistic problems, and not literally \"applying\" existing technical knowledge from linguistics. Moreover, they commonly apply technical knowledge from multiple sources, such as sociology (e.g., conversation analysis) and anthropology. (Constructed language fits under Applied linguistics.)\n Today, computers are widely used in many areas of applied linguistics. Speech synthesis and speech recognition use phonetic and phonemic knowledge to provide voice interfaces to computers. Applications of computational linguistics in machine translation, computer-assisted translation, and natural language processing are areas of applied linguistics that have come to the forefront. Their influence has had an effect on theories of syntax and semantics, as modelling syntactic and semantic theories on computers constraints.\n Linguistic analysis is a sub-discipline of applied linguistics used by many governments to verify the claimed nationality of people seeking asylum who do not hold the necessary documentation to prove their claim.[98] This often takes the form of an interview by personnel in an immigration department. Depending on the country, this interview is conducted either in the asylum seeker's native language through an interpreter or in an international lingua franca like English.[98] Australia uses the former method, while Germany employs the latter; the Netherlands uses either method depending on the languages involved.[98] Tape recordings of the interview then undergo language analysis, which can be done either by private contractors or within a department of the government. In this analysis, linguistic features of the asylum seeker are used by analysts to make a determination about the speaker's nationality. The reported findings of the linguistic analysis can play a critical role in the government's decision on the refugee status of the asylum seeker.[98]\n Language documentation combines anthropological inquiry (into the history and culture of language) with linguistic inquiry, in order to describe languages and their grammars. Lexicography involves the documentation of words that form a vocabulary. Such a documentation of a linguistic vocabulary from a particular language is usually compiled in a dictionary. Computational linguistics is concerned with the statistical or rule-based modeling of natural language from a computational perspective. Specific knowledge of language is applied by speakers during the act of translation and interpretation, as well as in language education – the teaching of a second or foreign language. Policy makers work with governments to implement new plans in education and teaching which are based on linguistic research.\n Since the inception of the discipline of linguistics, linguists have been concerned with describing and analysing previously undocumented languages. Starting with Franz Boas in the early 1900s, this became the main focus of American linguistics until the rise of formal linguistics in the mid-20th century. This focus on language documentation was partly motivated by a concern to document the rapidly disappearing languages of indigenous peoples. The ethnographic dimension of the Boasian approach to language description played a role in the development of disciplines such as sociolinguistics, anthropological linguistics, and linguistic anthropology, which investigate the relations between language, culture, and society.\n The emphasis on linguistic description and documentation has also gained prominence outside North America, with the documentation of rapidly dying indigenous languages becoming a focus in some university programs in linguistics. Language description is a work-intensive endeavour, usually requiring years of field work in the language concerned, so as to equip the linguist to write a sufficiently accurate reference grammar. Further, the task of documentation requires the linguist to collect a substantial corpus in the language in question, consisting of texts and recordings, both sound and video, which can be stored in an accessible format within open repositories, and used for further research.[99]\n The sub-field of translation includes the translation of written and spoken texts across media, from digital to print and spoken. To translate literally means to transmute the meaning from one language into another. Translators are often employed by organizations such as travel agencies and governmental embassies to facilitate communication between two speakers who do not know each other's language. Translators are also employed to work within computational linguistics setups like Google Translate, which is an automated program to translate words and phrases between any two or more given languages. Translation is also conducted by publishing houses, which convert works of writing from one language to another in order to reach varied audiences. Cross-national and cross-cultural survey research studies employ translation to collect comparable data among multilingual populations.[100][101] Academic translators specialize in or are familiar with various other disciplines such as technology, science, law, economics, etc.\n Clinical linguistics is the application of linguistic theory to the field of speech-language pathology. Speech language pathologists work on corrective measures to treat communication and swallowing disorders.\n Computational linguistics is the study of linguistic issues in a way that is \"computationally responsible\", i.e., taking careful note of computational consideration of algorithmic specification and computational complexity, so that the linguistic theories devised can be shown to exhibit certain desirable computational properties and their implementations. Computational linguists also work on computer language and software development.\n Evolutionary linguistics is a sociobiological approach to analyzing the emergence of the language faculty through human evolution, and also the application of evolutionary theory to the study of cultural evolution among different languages. It is also a study of the dispersal of various languages across the globe, through movements among ancient communities.[102]\n Forensic linguistics is the application of linguistic analysis to forensics. Forensic analysis investigates the style, language, lexical use, and other linguistic and grammatical features used in the legal context to provide evidence in courts of law. Forensic linguists have also used their expertise in the framework of criminal cases.[103][104]\n"
    },
    {
        "title": "Neuroscience",
        "url": "https://en.wikipedia.org/wiki/Neuroscience",
        "content": "\n Neuroscience is the scientific study of the nervous system (the brain, spinal cord, and peripheral nervous system), its functions, and its disorders.[1][2][3] It is a multidisciplinary science that combines physiology, anatomy, molecular biology, developmental biology, cytology, psychology, physics, computer science, chemistry, medicine, statistics, and mathematical modeling to understand the fundamental and emergent properties of neurons, glia and neural circuits.[4][5][6][7][8] The understanding of the biological basis of learning, memory, behavior, perception, and consciousness has been described by Eric Kandel as the \"epic challenge\" of the biological sciences.[9]\n The scope of neuroscience has broadened over time to include different approaches used to study the nervous system at different scales. The techniques used by neuroscientists have expanded enormously, from molecular and cellular studies of individual neurons to imaging of sensory, motor and cognitive tasks in the brain.\n The earliest study of the nervous system dates to ancient Egypt. Trepanation, the surgical practice of either drilling or scraping a hole into the skull for the purpose of curing head injuries or mental disorders, or relieving cranial pressure, was first recorded during the Neolithic period. Manuscripts dating to 1700 BC indicate that the Egyptians had some knowledge about symptoms of brain damage.[10]\n Early views on the function of the brain regarded it to be a \"cranial stuffing\" of sorts. In Egypt, from the late Middle Kingdom onwards, the brain was regularly removed in preparation for mummification. It was believed at the time that the heart was the seat of intelligence. According to Herodotus, the first step of mummification was to \"take a crooked piece of iron, and with it draw out the brain through the nostrils, thus getting rid of a portion, while the skull is cleared of the rest by rinsing with drugs.\"[11]\n The view that the heart was the source of consciousness was not challenged until the time of the Greek physician Hippocrates. He believed that the brain was not only involved with sensation—since most specialized organs (e.g., eyes, ears, tongue) are located in the head near the brain—but was also the seat of intelligence.[12] Plato also speculated that the brain was the seat of the rational part of the soul.[13] Aristotle, however, believed the heart was the center of intelligence and that the brain regulated the amount of heat from the heart.[14] This view was generally accepted until the Roman physician Galen, a follower of Hippocrates and physician to Roman gladiators, observed that his patients lost their mental faculties when they had sustained damage to their brains.[15]\n Abulcasis, Averroes, Avicenna, Avenzoar, and Maimonides, active in the Medieval Muslim world, described a number of medical problems related to the brain. In Renaissance Europe, Vesalius (1514–1564), René Descartes (1596–1650), Thomas Willis (1621–1675) and Jan Swammerdam (1637–1680) also made several contributions to neuroscience.\n Luigi Galvani's pioneering work in the late 1700s set the stage for studying the electrical excitability of muscles and neurons. In 1843 Emil du Bois-Reymond demonstrated the electrical nature of the nerve signal,[16] whose speed Hermann von Helmholtz proceeded to measure,[17] and in 1875 Richard Caton found electrical phenomena in the cerebral hemispheres of rabbits and monkeys.[18] Adolf Beck published in 1890 similar observations of spontaneous electrical activity of the brain of rabbits and dogs.[19] Studies of the brain became more sophisticated after the invention of the microscope and the development of a staining procedure by Camillo Golgi during the late 1890s. The procedure used a silver chromate salt to reveal the intricate structures of individual neurons. His technique was used by Santiago Ramón y Cajal and led to the formation of the neuron doctrine, the hypothesis that the functional unit of the brain is the neuron.[20] Golgi and Ramón y Cajal shared the Nobel Prize in Physiology or Medicine in 1906 for their extensive observations, descriptions, and categorizations of neurons throughout the brain.\n In parallel with this research, in 1815 Jean Pierre Flourens induced localized lesions of the brain in living animals to observe their effects on motricity, sensibility and behavior. Work with brain-damaged patients by Marc Dax in 1836 and Paul Broca in 1865 suggested that certain regions of the brain were responsible for certain functions. At the time, these findings were seen as a confirmation of Franz Joseph Gall's theory that language was localized and that certain psychological functions were localized in specific areas of the cerebral cortex.[21][22] The localization of function hypothesis was supported by observations of epileptic patients conducted by John Hughlings Jackson, who correctly inferred the organization of the motor cortex by watching the progression of seizures through the body. Carl Wernicke further developed the theory of the specialization of specific brain structures in language comprehension and production. Modern research through neuroimaging techniques, still uses the Brodmann cerebral cytoarchitectonic map (referring to the study of cell structure) anatomical definitions from this era in continuing to show that distinct areas of the cortex are activated in the execution of specific tasks.[23]\n During the 20th century, neuroscience began to be recognized as a distinct academic discipline in its own right, rather than as studies of the nervous system within other disciplines. Eric Kandel and collaborators have cited David Rioch, Francis O. Schmitt, and Stephen Kuffler as having played critical roles in establishing the field.[24] Rioch originated the integration of basic anatomical and physiological research with clinical psychiatry at the Walter Reed Army Institute of Research, starting in the 1950s. During the same period, Schmitt established a neuroscience research program within the Biology Department at the Massachusetts Institute of Technology, bringing together biology, chemistry, physics, and mathematics. The first freestanding neuroscience department (then called Psychobiology) was founded in 1964 at the University of California, Irvine by James L. McGaugh.[25] This was followed by the Department of Neurobiology at Harvard Medical School, which was founded in 1966 by Stephen Kuffler.[26]\n In the process of treating epilepsy, Wilder Penfield produced maps of the location of various functions (motor, sensory, memory, vision) in the brain.[27][28] He summarized his findings in a 1950 book called The Cerebral Cortex of Man.[29] Wilder Penfield and his co-investigators Edwin Boldrey and Theodore Rasmussen are considered to be the originators of the cortical homunculus.[30]\n The understanding of neurons and of nervous system function became increasingly precise and molecular during the 20th century. For example, in 1952, Alan Lloyd Hodgkin and Andrew Huxley presented a mathematical model for the transmission of electrical signals in neurons of the giant axon of a squid, which they called \"action potentials\", and how they are initiated and propagated, known as the Hodgkin–Huxley model. In 1961–1962, Richard FitzHugh and J. Nagumo simplified Hodgkin–Huxley, in what is called the FitzHugh–Nagumo model. In 1962, Bernard Katz modeled neurotransmission across the space between neurons known as synapses. Beginning in 1966, Eric Kandel and collaborators examined biochemical changes in neurons associated with learning and memory storage in Aplysia. In 1981 Catherine Morris and Harold Lecar combined these models in the Morris–Lecar model. Such increasingly quantitative work gave rise to numerous biological neuron models and models of neural computation.\n As a result of the increasing interest about the nervous system, several prominent neuroscience organizations have been formed to provide a forum to all neuroscientists during the 20th century. For example, the International Brain Research Organization was founded in 1961,[31] the International Society for Neurochemistry in 1963,[32] the European Brain and Behaviour Society in 1968,[33] and the Society for Neuroscience in 1969.[34] Recently, the application of neuroscience research results has also given rise to applied disciplines as neuroeconomics,[35] neuroeducation,[36] neuroethics,[37] and neurolaw.[38]\n Over time, brain research has gone through philosophical, experimental, and theoretical phases, with work on neural implants and brain simulation predicted to be important in the future.[39]\n The scientific study of the nervous system increased significantly during the second half of the twentieth century, principally due to advances in molecular biology, electrophysiology, and computational neuroscience. This has allowed neuroscientists to study the nervous system in all its aspects: how it is structured, how it works, how it develops, how it malfunctions, and how it can be changed.\n For example, it has become possible to understand, in much detail, the complex processes occurring within a single neuron. Neurons are cells specialized for communication. They are able to communicate with neurons and other cell types through specialized junctions called synapses, at which electrical or electrochemical signals can be transmitted from one cell to another. Many neurons extrude a long thin filament of axoplasm called an axon, which may extend to distant parts of the body and are capable of rapidly carrying electrical signals, influencing the activity of other neurons, muscles, or glands at their termination points. A nervous system emerges from the assemblage of neurons that are connected to each other in neural circuits, and networks.\n The vertebrate nervous system can be split into two parts: the central nervous system (defined as the brain and spinal cord), and the peripheral nervous system. In many species—including all vertebrates—the nervous system is the most complex organ system in the body, with most of the complexity residing in the brain. The human brain alone contains around one hundred billion neurons and one hundred trillion synapses; it consists of thousands of distinguishable substructures, connected to each other in synaptic networks whose intricacies have only begun to be unraveled. At least one out of three of the approximately 20,000 genes belonging to the human genome is expressed mainly in the brain.[40]\n Due to the high degree of plasticity of the human brain, the structure of its synapses and their resulting functions change throughout life.[41]\n Making sense of the nervous system's dynamic complexity is a formidable research challenge. Ultimately, neuroscientists would like to understand every aspect of the nervous system, including how it works, how it develops, how it malfunctions, and how it can be altered or repaired. Analysis of the nervous system is therefore performed at multiple levels, ranging from the molecular and cellular levels to the systems and cognitive levels. The specific topics that form the main focus of research change over time, driven by an ever-expanding base of knowledge and the availability of increasingly sophisticated technical methods. Improvements in technology have been the primary drivers of progress. Developments in electron microscopy, computer science, electronics, functional neuroimaging, and genetics and genomics have all been major drivers of progress.\n Advances in the classification of brain cells have been enabled by electrophysiological recording, single-cell genetic sequencing, and high-quality microscopy, which have combined into a single method pipeline called patch-sequencing in which all three methods are simultaneously applied using miniature tools.[42] The efficiency of this method and the large amounts of data that is generated has allowed researchers to make some general conclusions about cell types; for example that the human and mouse brain have different versions of fundamentally the same cell types.[43]\n Basic questions addressed in molecular neuroscience include the mechanisms by which neurons express and respond to molecular signals and how axons form complex connectivity patterns. At this level, tools from molecular biology and genetics are used to understand how neurons develop and how genetic changes affect biological functions.[44] The morphology, molecular identity, and physiological characteristics of neurons and how they relate to different types of behavior are also of considerable interest.[45]\n Questions addressed in cellular neuroscience include the mechanisms of how neurons process signals physiologically and electrochemically. These questions include how signals are processed by neurites and somas and how neurotransmitters and electrical signals are used to process information in a neuron. Neurites are thin extensions from a neuronal cell body, consisting of dendrites (specialized to receive synaptic inputs from other neurons) and axons (specialized to conduct nerve impulses called action potentials). Somas are the cell bodies of the neurons and contain the nucleus.[46]\n Another major area of cellular neuroscience is the investigation of the development of the nervous system.[47] Questions include the patterning and regionalization of the nervous system, axonal and dendritic development, trophic interactions, synapse formation and the implication of fractones in neural stem cells,[48][49] differentiation of neurons and glia (neurogenesis and gliogenesis), and neuronal migration.[50]\n Computational neurogenetic modeling is concerned with the development of dynamic neuronal models for modeling brain functions with respect to genes and dynamic interactions between genes, on the cellular level (Computational Neurogenetic Modeling (CNGM) can also be used to model neural systems).[51]\n Systems neuroscience research centers on the structural and functional architecture of the developing human brain, and the functions of large-scale brain networks, or functionally-connected systems within the brain. Alongside brain development, systems neuroscience also focuses on how the structure and function of the brain enables or restricts the processing of sensory information, using learned mental models of the world, to motivate behavior.\n Questions in systems neuroscience include how neural circuits are formed and used anatomically and physiologically to produce functions such as reflexes, multisensory integration, motor coordination, circadian rhythms, emotional responses, learning, and memory.[52] In other words, this area of research studies how connections are made and morphed in the brain, and the effect it has on human sensation, movement, attention, inhibitory control, decision-making, reasoning, memory formation, reward, and emotion regulation.[53]\n Specific areas of interest for the field include observations of how the structure of neural circuits effect skill acquisition, how specialized regions of the brain develop and change (neuroplasticity), and the development of brain atlases, or wiring diagrams of individual developing brains.[54]\n The related fields of neuroethology and neuropsychology address the question of how neural substrates underlie specific animal and human behaviors.[55] Neuroendocrinology and psychoneuroimmunology examine interactions between the nervous system and the endocrine and immune systems, respectively.[56] Despite many advancements, the way that networks of neurons perform complex cognitive processes and behaviors is still poorly understood.[57]\n Cognitive neuroscience addresses the questions of how psychological functions are produced by neural circuitry. The emergence of powerful new measurement techniques such as neuroimaging (e.g., fMRI, PET, SPECT), EEG, MEG, electrophysiology, optogenetics and human genetic analysis combined with sophisticated experimental techniques from cognitive psychology allows neuroscientists and psychologists to address abstract questions such as how cognition and emotion are mapped to specific neural substrates. Although many studies hold a reductionist stance looking for the neurobiological basis of cognitive phenomena, recent research shows that there is an interplay between neuroscientific findings and conceptual research, soliciting and integrating both perspectives. For example, neuroscience research on empathy solicited an interdisciplinary debate involving philosophy, psychology and psychopathology.[58] Moreover, the neuroscientific identification of multiple memory systems related to different brain areas has challenged the idea of memory as a literal reproduction of the past, supporting a view of memory as a generative, constructive and dynamic process.[59]\n Neuroscience is also allied with the social and behavioral sciences, as well as with nascent interdisciplinary fields. Examples of such alliances include neuroeconomics, decision theory, social neuroscience, and neuromarketing to address complex questions about interactions of the brain with its environment. A study into consumer responses for example uses EEG to investigate neural correlates associated with narrative transportation into stories about energy efficiency.[60]\n Questions in computational neuroscience can span a wide range of levels of traditional analysis, such as development, structure, and cognitive functions of the brain. Research in this field utilizes mathematical models, theoretical analysis, and computer simulation to describe and verify biologically plausible neurons and nervous systems. For example, biological neuron models are mathematical descriptions of spiking neurons which can be used to describe both the behavior of single neurons as well as the dynamics of neural networks. Computational neuroscience is often referred to as theoretical neuroscience.\n Neurology, psychiatry, neurosurgery, psychosurgery, anesthesiology and pain medicine, neuropathology, neuroradiology, ophthalmology, otolaryngology, clinical neurophysiology, addiction medicine, and sleep medicine are some medical specialties that specifically address the diseases of the nervous system. These terms also refer to clinical disciplines involving diagnosis and treatment of these diseases.[61]\n Neurology works with diseases of the central and peripheral nervous systems, such as amyotrophic lateral sclerosis (ALS) and stroke, and their medical treatment. Psychiatry focuses on affective, behavioral, cognitive, and perceptual disorders. Anesthesiology focuses on perception of pain, and pharmacologic alteration of consciousness. Neuropathology focuses upon the classification and underlying pathogenic mechanisms of central and peripheral nervous system and muscle diseases, with an emphasis on morphologic, microscopic, and chemically observable alterations. Neurosurgery and psychosurgery work primarily with surgical treatment of diseases of the central and peripheral nervous systems.[62]\n Recently, the boundaries between various specialties have blurred, as they are all influenced by basic research in neuroscience. For example, brain imaging enables objective biological insight into mental illnesses, which can lead to faster diagnosis, more accurate prognosis, and improved monitoring of patient progress over time.[63]\n Integrative neuroscience describes the effort to combine models and information from multiple levels of research to develop a coherent model of the nervous system. For example, brain imaging coupled with physiological numerical models and theories of fundamental mechanisms may shed light on psychiatric disorders.[64]\n Another important area of translational research is brain–computer interfaces (BCIs), or machines that are able to communicate and influence the brain. They are currently being researched for their potential to repair neural systems and restore certain cognitive functions.[65] However, some ethical considerations have to be dealt with before they are accepted.[66][67]\n Modern neuroscience education and research activities can be very roughly categorized into the following major branches, based on the subject and scale of the system in examination as well as distinct experimental or curricular approaches. Individual neuroscientists, however, often work on questions that span several distinct subfields.\n The largest professional neuroscience organization is the Society for Neuroscience (SFN), which is based in the United States but includes many members from other countries. Since its founding in 1969 the SFN has grown steadily: as of 2010 it recorded 40,290 members from 83 countries.[95] Annual meetings, held each year in a different American city, draw attendance from researchers, postdoctoral fellows, graduate students, and undergraduates, as well as educational institutions, funding agencies, publishers, and hundreds of businesses that supply products used in research.\n Other major organizations devoted to neuroscience include the International Brain Research Organization (IBRO), which holds its meetings in a country from a different part of the world each year, and the Federation of European Neuroscience Societies (FENS), which holds a meeting in a different European city every two years. FENS comprises a set of 32 national-level organizations, including the British Neuroscience Association, the German Neuroscience Society (Neurowissenschaftliche Gesellschaft), and the French Société des Neurosciences.[96] The first National Honor Society in Neuroscience, Nu Rho Psi, was founded in 2006. Numerous youth neuroscience societies which support undergraduates, graduates and early career researchers also exist, such as Simply Neuroscience[97] and Project Encephalon.[98]\n In 2013, the BRAIN Initiative was announced in the US. The International Brain Initiative[99] was created in 2017,[100] currently integrated by more than seven national-level brain research initiatives (US, Europe, Allen Institute, Japan, China, Australia,[101] Canada,[102] Korea,[103] and Israel[104])[105] spanning four continents.\n In addition to conducting traditional research in laboratory settings, neuroscientists have also been involved in the promotion of awareness and knowledge about the nervous system among the general public and government officials. Such promotions have been done by both individual neuroscientists and large organizations. For example, individual neuroscientists have promoted neuroscience education among young students by organizing the International Brain Bee, which is an academic competition for high school or secondary school students worldwide.[106] In the United States, large organizations such as the Society for Neuroscience have promoted neuroscience education by developing a primer called Brain Facts,[107] collaborating with public school teachers to develop Neuroscience Core Concepts for K-12 teachers and students,[108] and cosponsoring a campaign with the Dana Foundation called Brain Awareness Week to increase public awareness about the progress and benefits of brain research.[109] In Canada, the Canadian Institutes of Health Research's (CIHR) Canadian National Brain Bee is held annually at McMaster University.[110]\n Neuroscience educators formed a Faculty for Undergraduate Neuroscience (FUN) in 1992 to share best practices and provide travel awards for undergraduates presenting at Society for Neuroscience meetings.[111]\n Neuroscientists have also collaborated with other education experts to study and refine educational techniques to optimize learning among students, an emerging field called educational neuroscience.[112] Federal agencies in the United States, such as the National Institute of Health (NIH)[113] and National Science Foundation (NSF),[114] have also funded research that pertains to best practices in teaching and learning of neuroscience concepts.\n Neuromorphic engineering is a branch of neuroscience that deals with creating functional physical models of neurons for the purposes of useful computation. The emergent computational properties of neuromorphic computers are fundamentally different from conventional computers in the sense that they are complex systems, and that the computational components are interrelated with no central processor.[115]\n One example of such a computer is the SpiNNaker supercomputer.[116]\n Sensors can also be made smart with neuromorphic technology. An example of this is the Event Camera's BrainScaleS (brain-inspired Multiscale Computation in Neuromorphic Hybrid Systems), a hybrid analog neuromorphic supercomputer located at Heidelberg University in Germany. It was developed as part of the Human Brain Project's neuromorphic computing platform and is the complement to the SpiNNaker supercomputer, which is based on digital technology. The architecture used in BrainScaleS mimics biological neurons and their connections on a physical level; additionally, since the components are made of silicon, these model neurons operate on average 864 times (24 hours of real time is 100 seconds in the machine simulation) that of their biological counterparts.[117]\n Recent advances in neuromorphic microchip technology have led a group of scientists to create an artificial neuron that can replace real neurons in diseases.[118][119]\n United States\n"
    },
    {
        "title": "Transformer (deep learning architecture)",
        "url": "https://en.wikipedia.org/wiki/Transformer_architecture",
        "content": "A transformer is a deep learning architecture that was developed by researchers at Google and is based on the multi-head attention mechanism, which was proposed in the 2017 paper \"Attention Is All You Need\".[1] Text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table.[1] At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished.\n Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM).[2] Later variations have been widely adopted for training large language models (LLM) on large (language) datasets, such as the Wikipedia corpus and Common Crawl.[3]\n \nTransformers were first developed as an improvement over previous architectures for machine translation,[4][5] but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning,[6][7] audio,[8] multimodal learning, robotics,[9] and even playing chess.[10] It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs)[11] and BERT[12] (bidirectional encoder representations from transformers). For many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens.\n A key breakthrough was LSTM (1995),[note 1] a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units.[13] Neural networks using multiplicative units were later called sigma-pi networks[14] or higher-order networks.[15] LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers.\nHowever, LSTM still used sequential processing, like most other RNNs.[note 2] Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence. \n Modern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window. The linearly scaling fast weight controller (1992) learns to compute a weight matrix for further processing depending on the input.[16] One of its two networks has \"fast weights\" or \"dynamic links\" (1981).[17][18][19] A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries.[16] This was later shown to be equivalent to the unnormalized linear Transformer.[20][21]\n The idea of encoder-decoder sequence transduction had been developed in the early 2010s (see previous papers[22][23]). The papers most commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014.[22][23]\n A 380M-parameter model for machine translation uses two long short-term memories (LSTM).[23] Its architecture consists of two parts. The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector. The decoder is another LSTM that converts the vector into a sequence of tokens. Similarly, another 130M-parameter model used gated recurrent units (GRU) instead of LSTM.[22] Later research showed that GRUs are neither better nor worse than LSTMs for seq2seq.[24][25]\n These early seq2seq models had no attention mechanism, and the state vector is accessible only after the last word of the source text was processed. Although in theory such a vector retains the information about the whole original sentence, in practice the information is poorly preserved. This is because the input is processed sequentially by one recurrent network into a fixed-size output vector, which is then processed by another recurrent network into an output. If the input is long, then the output vector would not be able to contain all relevant information, degrading the output. As evidence, reversing the input sentence improved seq2seq translation.[26]\n The RNNsearch model introduced an attention mechanism to seq2seq for machine translation to solve the bottleneck problem (of the fixed-size output vector), allowing the model to process long-distance dependencies more easily. The name is because it \"emulates searching through a source sentence during decoding a translation\".[4]\n The relative performances were compared between global (that of RNNsearch) and local (sliding window) attention model architectures for machine translation, finding that mixed attention had higher quality than global attention, while local attention reduced translation time.[27]\n In 2016, Google Translate was revamped to Google Neural Machine Translation, which replaced the previous model based on statistical machine translation. The new model was a seq2seq model where the encoder and the decoder were both 8 layers of bidirectional LSTM.[28] It took nine months to develop, and it outperformed the statistical approach, which took ten years to develop.[29]\n Seq2seq models with attention (including self-attention) still suffered from the same issue with recurrent networks, which is that they are hard to parallelize, which prevented them from being accelerated on GPUs. In 2016, decomposable attention applied a self-attention mechanism to feedforward networks, which are easy to parallelize, and achieved SOTA result in textual entailment with an order of magnitude less parameters than LSTMs.[30] One of its authors, Jakob Uszkoreit, suspected that attention without recurrence is sufficient for language translation, thus the title \"attention is all you need\".[31] That hypothesis was against conventional wisdom at the time, and even his father Hans Uszkoreit, a well-known computational linguist, was skeptical.[31] In the same year, self-attention (called intra-attention or intra-sentence attention) was proposed for LSTMs.[32]\n In 2017, the original (100M-sized) encoder-decoder transformer model was proposed in the \"Attention is all you need\" paper. At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance.[1] This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. Its parallelizability was an important factor to its widespread use in large neural networks.[33]\n\n Already in spring 2017, even before the \"Attention is all you need\" preprint was published, one of the co-authors applied the \"decoder-only\" variation of the architecture to generate fictitious Wikipedia articles.[34] Transformer architecture is now used in many generative models that contribute to the ongoing AI boom.\n In language modelling, ELMo (2018) was a bi-directional LSTM that produces contextualized word embeddings, improving upon the line of research from bag of words and word2vec. It was followed by BERT (2018), an encoder-only Transformer model.[35] In 2019 October, Google started using BERT to process search queries.[36] In 2020, Google Translate replaced the previous RNN-encoder–RNN-decoder model by a Transformer-encoder–RNN-decoder model.[37]\n Starting in 2018, the OpenAI GPT series of decoder-only Transformers became state of the art in natural language generation. In 2022, a chatbot based on GPT-3, ChatGPT, became unexpectedly popular,[38] triggering a boom around large language models.[39][40]\n Since 2020, Transformers have been applied in modalities beyond text, including the vision transformer,[41] speech recognition,[42] robotics,[6] and multimodal.[43] The vision transformer, in turn, stimulated new developments in convolutional neural networks.[44] Image and video generators like DALL-E (2021), Stable Diffusion 3 (2024),[45] and Sora (2024), are based on the Transformer architecture.\n The plain transformer architecture had difficulty converging. In the original paper[1] the authors recommended using learning rate warmup. That is, the learning rate should linearly scale up from 0 to maximal value for the first part of the training (usually recommended to be 2% of the total number of training steps), before decaying again.\n A 2020 paper found that using layer normalization before (instead of after) multiheaded attention and feedforward layers stabilizes training, not requiring learning rate warmup.[46]\n Transformers typically are first pretrained by self-supervised learning on a large generic dataset, followed by supervised fine-tuning on a small task-specific dataset. The pretrain dataset is typically an unlabeled large corpus, such as The Pile. Tasks for pretraining and fine-tuning commonly include:\n The T5 transformer report[47] documents a large number of natural language pretraining tasks. Some examples are:\n Note that while each of these tasks is trivial or obvious for human native speakers of the language (or languages), they have typically proved challenging for previous generations of machine learning architecture.\n In general, there are 3 classes of language modelling tasks: \"masked\",[49] \"autoregressive\",[50] and \"prefixLM\".[51] These classes are independent of a specific modeling architecture such as Transformer, but they are often discussed in the context of Transformer.\n In a masked task,[49] one or more of the tokens is masked out, and the model would produce a probability distribution predicting what the masked-out tokens are based on the context. The loss function for the task is typically sum of log-perplexities for the masked-out tokens: \n\n\n\n\nLoss\n\n=\n−\n\n∑\n\nt\n∈\n\nmasked tokens\n\n\n\nln\n⁡\n(\n\nprobability of \n\nt\n\n conditional on its context\n\n)\n\n\n{\\displaystyle {\\text{Loss}}=-\\sum _{t\\in {\\text{masked tokens}}}\\ln({\\text{probability of }}t{\\text{ conditional on its context}})}\n\nand the model is trained to minimize this loss function. The BERT series of models are trained for masked token prediction and another task.\n In an autoregressive task,[50] the entire sequence is masked at first, and the model produces a probability distribution for the first token. Then the first token is revealed and the model predicts the second token, and so on. The loss function for the task is still typically the same. The GPT series of models are trained by autoregressive tasks.\n In a prefixLM task,[51] the sequence is divided into two parts. The first part is presented as context, and the model predicts the first token of the second part. Then that would be revealed, and the model predicts the second token, and so on. The loss function for the task is still typically the same. The T5 series of models are trained by prefixLM tasks.\n Note that \"masked\" as in \"masked language modelling\" is not \"masked\" as in \"masked attention\", and \"prefixLM\" (prefix language modeling) is not \"prefixLM\" (prefix language model).\n All transformers have the same primary components:\n The following description follows exactly the Transformer as described in the original paper. There are variants, described in the following section.\n By convention, we write all vectors as row vectors. This, for example, means that pushing a vector through a linear layer means multiplying it by a weight matrix on the right, as \n\n\n\nx\nW\n\n\n{\\displaystyle xW}\n\n.\n As the Transformer architecture natively processes numerical data, not text, there must be a translation between text and tokens. A token is an integer that represents a character, or a short segment of characters. On the input side, the input text is parsed into a token sequence. Similarly, on the output side, the output tokens are parsed back to text. The module doing the conversion between texts and token sequences is a tokenizer.\n The set of all tokens is the vocabulary of the tokenizer, and its size is the vocabulary size \n\n\n\n\nn\n\nvocabulary\n\n\n\n\n{\\displaystyle n_{\\text{vocabulary}}}\n\n. When faced with tokens outside the vocabulary, typically a special token is used, written as \"[UNK]\" for \"unknown\".\n Some commonly used tokenizers are byte pair encoding, WordPiece, and SentencePiece.\n Each token is converted into an embedding vector via a lookup table. Equivalently stated, it multiplies a one-hot representation of the token by an embedding matrix \n\n\n\nM\n\n\n{\\displaystyle M}\n\n. For example, if the input token is \n\n\n\n3\n\n\n{\\displaystyle 3}\n\n, then the one-hot representation is \n\n\n\n[\n0\n,\n0\n,\n0\n,\n1\n,\n0\n,\n0\n,\n…\n]\n\n\n{\\displaystyle [0,0,0,1,0,0,\\dots ]}\n\n, and its embedding vector is\n\n\n\n\nE\nm\nb\ne\nd\n\n(\n3\n)\n=\n[\n0\n,\n0\n,\n0\n,\n1\n,\n0\n,\n0\n,\n…\n]\nM\n\n\n{\\displaystyle \\mathrm {Embed} (3)=[0,0,0,1,0,0,\\dots ]M}\n\nThe token embedding vectors are added to their respective positional encoding vectors (see below), producing the sequence of input vectors. \n The number of dimensions in an embedding vector is called hidden size or embedding size and written as \n\n\n\n\nd\n\nemb\n\n\n\n\n{\\displaystyle d_{\\text{emb}}}\n\n.[35] This size is written as \n\n\n\n\nd\n\nmodel\n\n\n\n\n{\\displaystyle d_{\\text{model}}}\n\n in the original Transformer paper.[1]\n An un-embedding layer is almost the reverse of an embedding layer. Whereas an embedding layer converts a token into a vector, an un-embedding layer converts a vector into a probability distribution over tokens.\n The un-embedding layer is a linear-softmax layer:\n\n\n\n\nU\nn\nE\nm\nb\ne\nd\n\n(\nx\n)\n=\n\ns\no\nf\nt\nm\na\nx\n\n(\nx\nW\n+\nb\n)\n\n\n{\\displaystyle \\mathrm {UnEmbed} (x)=\\mathrm {softmax} (xW+b)}\n\nThe matrix has shape \n\n\n\n(\n\nd\n\nemb\n\n\n,\n\nn\n\nvocabulary\n\n\n)\n\n\n{\\displaystyle (d_{\\text{emb}},n_{\\text{vocabulary}})}\n\n. The embedding matrix \n\n\n\nM\n\n\n{\\displaystyle M}\n\n and the un-embedding matrix \n\n\n\nW\n\n\n{\\displaystyle W}\n\n are sometimes required to be transposes of each other, a practice called weight tying.[52]\n A positional encoding is a fixed-size vector representation of the relative positions of tokens within a sequence: it provides the transformer model with information about where the words are in the input sequence. This shall induce a bias towards the order of the input sequence, so that, for example, the input sequence \"man bites dog\" is processed differently from \"dog bites man\".\n The positional encoding is defined as a function of type \n\n\n\nf\n:\n\nR\n\n→\n\n\nR\n\n\nd\n\n\n;\nd\n∈\n\nZ\n\n,\nd\n>\n0\n\n\n{\\displaystyle f:\\mathbb {R} \\to \\mathbb {R} ^{d};d\\in \\mathbb {Z} ,d>0}\n\n, where \n\n\n\nd\n\n\n{\\displaystyle d}\n\n is a positive even integer. The full positional encoding defined in the original paper[1] is:\n\n\n\n(\nf\n(\nt\n\n)\n\n2\nk\n\n\n,\nf\n(\nt\n\n)\n\n2\nk\n+\n1\n\n\n)\n=\n(\nsin\n⁡\n(\nθ\n)\n,\ncos\n⁡\n(\nθ\n)\n)\n\n∀\nk\n∈\n{\n0\n,\n1\n,\n…\n,\nd\n\n/\n\n2\n−\n1\n}\n\n\n{\\displaystyle (f(t)_{2k},f(t)_{2k+1})=(\\sin(\\theta ),\\cos(\\theta ))\\quad \\forall k\\in \\{0,1,\\ldots ,d/2-1\\}}\n\nwhere \n\n\n\nθ\n=\n\n\nt\n\nr\n\nk\n\n\n\n\n,\nr\n=\n\nN\n\n2\n\n/\n\nd\n\n\n\n\n{\\displaystyle \\theta ={\\frac {t}{r^{k}}},r=N^{2/d}}\n\n.\n Here, \n\n\n\nN\n\n\n{\\displaystyle N}\n\n is a free parameter that should be significantly larger than the biggest \n\n\n\nk\n\n\n{\\displaystyle k}\n\n that would be input into the positional encoding function. The original paper uses \n\n\n\nN\n=\n10000\n\n\n{\\displaystyle N=10000}\n\n.\n The function is in a simpler form when written as a complex function of type \n\n\n\nf\n:\n\nR\n\n→\n\n\nC\n\n\nd\n\n/\n\n2\n\n\n\n\n{\\displaystyle f:\\mathbb {R} \\to \\mathbb {C} ^{d/2}}\n\n\n\n\n\nf\n(\nt\n)\n=\n\n\n(\n\ne\n\ni\nt\n\n/\n\n\nr\n\nk\n\n\n\n\n)\n\n\nk\n=\n0\n,\n1\n,\n…\n,\n\n\nd\n2\n\n\n−\n1\n\n\n\n\n{\\displaystyle f(t)=\\left(e^{it/r^{k}}\\right)_{k=0,1,\\ldots ,{\\frac {d}{2}}-1}}\n\nwhere \n\n\n\nr\n=\n\nN\n\n2\n\n/\n\nd\n\n\n\n\n{\\displaystyle r=N^{2/d}}\n\n.\n The main reason for using this positional encoding function is that using it, shifts are linear transformations:\n\n\n\nf\n(\nt\n+\nΔ\nt\n)\n=\n\nd\ni\na\ng\n\n(\nf\n(\nΔ\nt\n)\n)\nf\n(\nt\n)\n\n\n{\\displaystyle f(t+\\Delta t)=\\mathrm {diag} (f(\\Delta t))f(t)}\n\nwhere \n\n\n\nΔ\nt\n∈\n\nR\n\n\n\n{\\displaystyle \\Delta t\\in \\mathbb {R} }\n\n is the distance one wishes to shift. This allows the transformer to take any encoded position, and find the encoding of the position n-steps-ahead or n-steps-behind, by a matrix multiplication.\n By taking a linear sum, any convolution can also be implemented as linear transformations:\n\n\n\n\n∑\n\nj\n\n\n\nc\n\nj\n\n\nf\n(\nt\n+\nΔ\n\nt\n\nj\n\n\n)\n=\n\n(\n\n\n∑\n\nj\n\n\n\nc\n\nj\n\n\n\n\nd\ni\na\ng\n\n(\nf\n(\nΔ\n\nt\n\nj\n\n\n)\n)\n\n)\n\nf\n(\nt\n)\n\n\n{\\displaystyle \\sum _{j}c_{j}f(t+\\Delta t_{j})=\\left(\\sum _{j}c_{j}\\,\\mathrm {diag} (f(\\Delta t_{j}))\\right)f(t)}\n\nfor any constants \n\n\n\n\nc\n\nj\n\n\n\n\n{\\displaystyle c_{j}}\n\n. This allows the transformer to take any encoded position and find a linear sum of the encoded locations of its neighbors. This sum of encoded positions, when fed into the attention mechanism, would create attention weights on its neighbors, much like what happens in a convolutional neural network language model. In the author's words, \"we hypothesized it would allow the model to easily learn to attend by relative position.\"\n In typical implementations, all operations are done over the real numbers, not the complex numbers, but since complex multiplication can be implemented as real 2-by-2 matrix multiplication, this is a mere notational difference.\n Like earlier seq2seq models, the original transformer model used an encoder-decoder architecture. The encoder consists of encoding layers that process all the input tokens together one layer after another, while the decoder consists of decoding layers that iteratively process the encoder's output and the decoder's output tokens so far.\n The purpose of each encoder layer is to create contextualized representations of the tokens, where each representation corresponds to a token that \"mixes\" information from other input tokens via self-attention mechanism. Each decoder layer contains two attention sublayers: (1) cross-attention for incorporating the output of encoder (contextualized input token representations), and (2) self-attention for \"mixing\" information among the input tokens to the decoder (i.e. the tokens generated so far during inference time).[53][54]\n Both the encoder and decoder layers have a feed-forward neural network for additional processing of their outputs and contain residual connections and layer normalization steps.[54] These feed-forward layers contain most of the parameters in a Transformer model.\n  The feedforward network (FFN) modules in a Transformer are 2-layered multilayer perceptrons:\n\n\n\n\nF\nF\nN\n\n(\nx\n)\n=\nϕ\n(\nx\n\nW\n\n(\n1\n)\n\n\n+\n\nb\n\n(\n1\n)\n\n\n)\n\nW\n\n(\n2\n)\n\n\n+\n\nb\n\n(\n2\n)\n\n\n\n\n{\\displaystyle \\mathrm {FFN} (x)=\\phi (xW^{(1)}+b^{(1)})W^{(2)}+b^{(2)}}\n\nwhere \n\n\n\nϕ\n\n\n{\\displaystyle \\phi }\n\n is its activation function. The original Transformer used ReLU activation.\n The number of neurons in the middle layer is called intermediate size (GPT),[55] filter size (BERT),[35] or feedforward size (BERT).[35] It is typically larger than the embedding size. For example, in both GPT-2 series and BERT series, the intermediate size of a model is 4 times its embedding size: \n\n\n\n\nd\n\nffn\n\n\n=\n4\n\nd\n\nemb\n\n\n\n\n{\\displaystyle d_{\\text{ffn}}=4d_{\\text{emb}}}\n\n.\n The attention mechanism used in the Transformer architecture are scaled dot-product attention units. For each unit, the transformer model learns three weight matrices: the query weights \n\n\n\n\nW\n\nQ\n\n\n\n\n{\\displaystyle W^{Q}}\n\n, the key weights \n\n\n\n\nW\n\nK\n\n\n\n\n{\\displaystyle W^{K}}\n\n, and the value weights \n\n\n\n\nW\n\nV\n\n\n\n\n{\\displaystyle W^{V}}\n\n.\n The module takes three sequences, a query sequence, a key sequence, and a value sequence. The query sequence is a sequence of length \n\n\n\n\nℓ\n\nseq, query\n\n\n\n\n{\\displaystyle \\ell _{\\text{seq, query}}}\n\n, and each entry is a vector of dimension \n\n\n\n\nd\n\nemb, query\n\n\n\n\n{\\displaystyle d_{\\text{emb, query}}}\n\n. Similarly for the key and value sequences.\n For each vector \n\n\n\n\nx\n\ni\n,\n\nquery\n\n\n\n\n\n{\\displaystyle x_{i,{\\text{query}}}}\n\n in the query sequence, it is multiplied by a matrix \n\n\n\n\nW\n\nQ\n\n\n\n\n{\\displaystyle W^{Q}}\n\n to produce a query vector \n\n\n\n\nq\n\ni\n\n\n=\n\nx\n\ni\n,\n\nquery\n\n\n\n\nW\n\nQ\n\n\n\n\n{\\displaystyle q_{i}=x_{i,{\\text{query}}}W^{Q}}\n\n. The matrix of all query vectors is the query matrix:\n\n\n\nQ\n=\n\nX\n\nquery\n\n\n\nW\n\nQ\n\n\n\n\n{\\displaystyle Q=X_{\\text{query}}W^{Q}}\n\nSimilarly, we construct the key matrix \n\n\n\nK\n=\n\nX\n\nkey\n\n\n\nW\n\nK\n\n\n\n\n{\\displaystyle K=X_{\\text{key}}W^{K}}\n\n and the value matrix \n\n\n\nV\n=\n\nX\n\nvalue\n\n\n\nW\n\nV\n\n\n\n\n{\\displaystyle V=X_{\\text{value}}W^{V}}\n\n.\n It is usually the case that all \n\n\n\n\nW\n\nQ\n\n\n,\n\nW\n\nK\n\n\n,\n\nW\n\nV\n\n\n\n\n{\\displaystyle W^{Q},W^{K},W^{V}}\n\n are square matrices, meaning \n\n\n\n\nd\n\nemb, query\n\n\n=\n\nd\n\nquery\n\n\n\n\n{\\displaystyle d_{\\text{emb, query}}=d_{\\text{query}}}\n\n, etc.\n Attention weights are calculated using the query and key vectors: the attention weight \n\n\n\n\na\n\ni\nj\n\n\n\n\n{\\displaystyle a_{ij}}\n\n from token \n\n\n\ni\n\n\n{\\displaystyle i}\n\n to token \n\n\n\nj\n\n\n{\\displaystyle j}\n\n is the dot product between \n\n\n\n\nq\n\ni\n\n\n\n\n{\\displaystyle q_{i}}\n\n and \n\n\n\n\nk\n\nj\n\n\n\n\n{\\displaystyle k_{j}}\n\n. The attention weights are divided by the square root of the dimension of the key vectors, \n\n\n\n\n\n\nd\n\nk\n\n\n\n\n\n\n{\\displaystyle {\\sqrt {d_{k}}}}\n\n, which stabilizes gradients during training, and passed through a softmax which normalizes the weights. The fact that \n\n\n\n\nW\n\nQ\n\n\n\n\n{\\displaystyle W^{Q}}\n\n and \n\n\n\n\nW\n\nK\n\n\n\n\n{\\displaystyle W^{K}}\n\n are different matrices allows attention to be non-symmetric: if token \n\n\n\ni\n\n\n{\\displaystyle i}\n\n attends to token \n\n\n\nj\n\n\n{\\displaystyle j}\n\n (i.e. \n\n\n\n\nq\n\ni\n\n\n⋅\n\nk\n\nj\n\n\n\n\n{\\displaystyle q_{i}\\cdot k_{j}}\n\n is large), this does not necessarily mean that token \n\n\n\nj\n\n\n{\\displaystyle j}\n\n will attend to token \n\n\n\ni\n\n\n{\\displaystyle i}\n\n (i.e. \n\n\n\n\nq\n\nj\n\n\n⋅\n\nk\n\ni\n\n\n\n\n{\\displaystyle q_{j}\\cdot k_{i}}\n\n could be small). The output of the attention unit for token \n\n\n\ni\n\n\n{\\displaystyle i}\n\n is the weighted sum of the value vectors of all tokens, weighted by \n\n\n\n\na\n\ni\nj\n\n\n\n\n{\\displaystyle a_{ij}}\n\n, the attention from token \n\n\n\ni\n\n\n{\\displaystyle i}\n\n to each token.\n The attention calculation for all tokens can be expressed as one large matrix calculation using the softmax function, which is useful for training due to computational matrix operation optimizations that quickly compute matrix operations. The matrices \n\n\n\nQ\n\n\n{\\displaystyle Q}\n\n, \n\n\n\nK\n\n\n{\\displaystyle K}\n\n and \n\n\n\nV\n\n\n{\\displaystyle V}\n\n are defined as the matrices where the \n\n\n\ni\n\n\n{\\displaystyle i}\n\nth rows are vectors \n\n\n\n\nq\n\ni\n\n\n\n\n{\\displaystyle q_{i}}\n\n, \n\n\n\n\nk\n\ni\n\n\n\n\n{\\displaystyle k_{i}}\n\n, and \n\n\n\n\nv\n\ni\n\n\n\n\n{\\displaystyle v_{i}}\n\n respectively. Then we can represent the attention as\n\n\n\n\n\n\n\n\nAttention\n\n(\nQ\n,\nK\n,\nV\n)\n=\n\nsoftmax\n\n\n(\n\n\n\nQ\n\nK\n\n\nT\n\n\n\n\n\n\nd\n\nk\n\n\n\n\n\n)\n\nV\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\end{aligned}}}\n\n\n where the softmax is applied over each of the rows of the matrix.\n The number of dimensions in a query vector is query size \n\n\n\n\nd\n\nquery\n\n\n\n\n{\\displaystyle d_{\\text{query}}}\n\n and similarly for the key size \n\n\n\n\nd\n\nkey\n\n\n\n\n{\\displaystyle d_{\\text{key}}}\n\n and value size \n\n\n\n\nd\n\nvalue\n\n\n\n\n{\\displaystyle d_{\\text{value}}}\n\n. The output dimension of an attention head is its head dimension \n\n\n\n\nd\n\nhead\n\n\n\n\n{\\displaystyle d_{\\text{head}}}\n\n. The attention mechanism requires the following three equalities to hold:\n\n\n\n\nℓ\n\nseq, key\n\n\n=\n\nℓ\n\nseq, value\n\n\n,\n\n\nd\n\nquery\n\n\n=\n\nd\n\nkey\n\n\n,\n\n\nd\n\nvalue\n\n\n=\n\nd\n\nhead\n\n\n\n\n{\\displaystyle \\ell _{\\text{seq, key}}=\\ell _{\\text{seq, value}},\\;d_{\\text{query}}=d_{\\text{key}},\\;d_{\\text{value}}=d_{\\text{head}}}\n\nbut is otherwise unconstrained.\n If the attention head is used in a self-attention fashion, then \n\n\n\n\nX\n\nquery\n\n\n=\n\nX\n\nkey\n\n\n=\n\nX\n\nvalue\n\n\n\n\n{\\displaystyle X_{\\text{query}}=X_{\\text{key}}=X_{\\text{value}}}\n\n. If the attention head is used in a cross-attention fashion, then usually \n\n\n\n\nX\n\nquery\n\n\n≠\n\nX\n\nkey\n\n\n=\n\nX\n\nvalue\n\n\n\n\n{\\displaystyle X_{\\text{query}}\\neq X_{\\text{key}}=X_{\\text{value}}}\n\n. It is theoretically possible for all three to be different, but that is rarely the case in practice.\n One set of \n\n\n\n\n(\n\n\nW\n\nQ\n\n\n,\n\nW\n\nK\n\n\n,\n\nW\n\nV\n\n\n\n)\n\n\n\n{\\displaystyle \\left(W^{Q},W^{K},W^{V}\\right)}\n\n matrices is called an attention head, and each layer in a transformer model has multiple attention heads. While each attention head attends to the tokens that are relevant to each token, multiple attention heads allow the model to do this for different definitions of \"relevance\". Specifically, the query and key projection matrices, \n\n\n\n\nW\n\nQ\n\n\n\n\n{\\displaystyle W^{Q}}\n\n and \n\n\n\n\nW\n\nK\n\n\n\n\n{\\displaystyle W^{K}}\n\n , which are involved in the attention score computation, defines the \"relevance\". Meanwhile, the value projection matrix \n\n\n\n\nW\n\nV\n\n\n\n\n{\\displaystyle W^{V}}\n\n, in combination with the part of the output projection matrix \n\n\n\n\nW\n\nO\n\n\n\n\n{\\displaystyle W^{O}}\n\n, determines how the attended tokens influence what information is passed to subsequent layers and ultimately the output logits. In addition, the scope of attention, or the range of token relationships captured by each attention head, can expand as tokens pass through successive layers. This allows the model to capture more complex and long-range dependencies in deeper layers. Many transformer attention heads encode relevance relations that are meaningful to humans. For example, some attention heads can attend mostly to the next word, while others mainly attend from verbs to their direct objects.[56] The computations for each attention head can be performed in parallel, which allows for fast processing. The outputs for the attention layer are concatenated to pass into the feed-forward neural network layers.\n Concretely, let the multiple attention heads be indexed by \n\n\n\ni\n\n\n{\\displaystyle i}\n\n, then we have\n\n\n\n\nMultiheadedAttention\n\n(\nQ\n,\nK\n,\nV\n)\n=\n\n\nConcat\n\n\ni\n∈\n[\n\nn\n\nheads\n\n\n]\n\n\n(\n\nAttention\n\n(\nX\n\nW\n\ni\n\n\nQ\n\n\n,\nX\n\nW\n\ni\n\n\nK\n\n\n,\nX\n\nW\n\ni\n\n\nV\n\n\n)\n)\n\nW\n\nO\n\n\n\n\n{\\displaystyle {\\text{MultiheadedAttention}}(Q,K,V)={\\text{Concat}}_{i\\in [n_{\\text{heads}}]}({\\text{Attention}}(XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V}))W^{O}}\n\n where the matrix \n\n\n\nX\n\n\n{\\displaystyle X}\n\n is the concatenation of word embeddings, and the matrices \n\n\n\n\nW\n\ni\n\n\nQ\n\n\n,\n\nW\n\ni\n\n\nK\n\n\n,\n\nW\n\ni\n\n\nV\n\n\n\n\n{\\displaystyle W_{i}^{Q},W_{i}^{K},W_{i}^{V}}\n\n are \"projection matrices\" owned by individual attention head \n\n\n\ni\n\n\n{\\displaystyle i}\n\n, and \n\n\n\n\nW\n\nO\n\n\n\n\n{\\displaystyle W^{O}}\n\n is a final projection matrix owned by the whole multi-headed attention head.\n It is theoretically possible for each attention head to have a different head dimension \n\n\n\n\nd\n\nhead\n\n\n\n\n{\\displaystyle d_{\\text{head}}}\n\n, but that is rarely the case in practice.\n As an example, in the smallest GPT-2 model, there are only self-attention mechanisms. It has the following dimensions:\n\n\n\n\nd\n\nemb\n\n\n=\n768\n,\n\nn\n\nhead\n\n\n=\n12\n,\n\nd\n\nhead\n\n\n=\n64\n\n\n{\\displaystyle d_{\\text{emb}}=768,n_{\\text{head}}=12,d_{\\text{head}}=64}\n\nSince \n\n\n\n12\n×\n64\n=\n768\n\n\n{\\displaystyle 12\\times 64=768}\n\n, its output projection matrix \n\n\n\n\nW\n\nO\n\n\n∈\n\n\nR\n\n\n(\n12\n×\n64\n)\n×\n768\n\n\n\n\n{\\displaystyle W^{O}\\in \\mathbb {R} ^{(12\\times 64)\\times 768}}\n\n is a square matrix.\n The Transformer architecture is constructed to calculate output tokens iteratively. Assuming \n\n\n\nt\n=\n0\n\n\n{\\displaystyle t=0}\n\n refers to the calculation of the first output token \n\n\n\ni\n=\n0\n\n\n{\\displaystyle i=0}\n\n, for step \n\n\n\nt\n>\n0\n\n\n{\\displaystyle t>0}\n\n, the output token \n\n\n\ni\n=\n0\n\n\n{\\displaystyle i=0}\n\n shall remain constant. This ensures properties of the model similar to autoregressive models.[1] Therefore, at every time step \n\n\n\nt\n\n\n{\\displaystyle t}\n\n, the calculation for all outputs \n\n\n\ni\n\n\n{\\displaystyle i}\n\n should not have access to tokens at position \n\n\n\nj\n\n\n{\\displaystyle j}\n\n for \n\n\n\nj\n>=\ni\n\n\n{\\displaystyle j>=i}\n\n (as it naturally is the case for time step \n\n\n\nt\n=\ni\n\n\n{\\displaystyle t=i}\n\n, when tokens \n\n\n\nj\n>\nt\n\n\n{\\displaystyle j>t}\n\n are not yet calculated). This behavior may be accomplished before the softmax stage by adding a mask matrix \n\n\n\nM\n\n\n{\\displaystyle M}\n\n that is \n\n\n\n−\n∞\n\n\n{\\displaystyle -\\infty }\n\n at entries where the attention link must be cut, and \n\n\n\n0\n\n\n{\\displaystyle 0}\n\n at other places:\n\n\n\n\n\n\n\n\nMaskedAttention\n\n(\nQ\n,\nK\n,\nV\n)\n=\n\nsoftmax\n\n\n(\n\nM\n+\n\n\n\nQ\n\nK\n\n\nT\n\n\n\n\n\n\nd\n\nk\n\n\n\n\n\n\n)\n\nV\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}{\\text{MaskedAttention}}(Q,K,V)={\\text{softmax}}\\left(M+{\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\end{aligned}}}\n\n The following matrix is commonly used in decoder self-attention modules, called \"causal masking\":\n\n\n\n\nM\n\ncausal\n\n\n=\n\n\n[\n\n\n\n0\n\n\n−\n∞\n\n\n−\n∞\n\n\n…\n\n\n−\n∞\n\n\n\n\n0\n\n\n0\n\n\n−\n∞\n\n\n…\n\n\n−\n∞\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n…\n\n\n−\n∞\n\n\n\n\n⋮\n\n\n⋮\n\n\n⋮\n\n\n⋱\n\n\n⋮\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n…\n\n\n0\n\n\n\n]\n\n\n\n\n{\\displaystyle M_{\\text{causal}}={\\begin{bmatrix}0&-\\infty &-\\infty &\\dots &-\\infty \\\\0&0&-\\infty &\\dots &-\\infty \\\\0&0&0&\\dots &-\\infty \\\\\\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\\0&0&0&\\dots &0\\end{bmatrix}}}\n\n\n In words, it means that each token can pay attention to itself, and every token before it, but not any after it. A non-masked attention module can be thought of as a masked attention module where the mask has all entries zero. As an example of an uncommon use of mask matrix, the XLNet considers all masks of the form \n\n\n\nP\n\nM\n\ncausal\n\n\n\nP\n\n−\n1\n\n\n\n\n{\\displaystyle PM_{\\text{causal}}P^{-1}}\n\n, where \n\n\n\nP\n\n\n{\\displaystyle P}\n\n is a random permutation matrix.[57]\n An encoder consists of an embedding layer, followed by multiple encoder layers.\n Each encoder layer consists of two major components: a self-attention mechanism and a feed-forward layer. It takes an input as a sequence of input vectors, applies the self-attention mechanism, to produce an intermediate sequence of vectors, then applies the feed-forward layer for each vector individually. Schematically, we have:\n\n\n\n\n\n\n\n\ngiven input vectors \n\n\n\n\nh\n\n0\n\n\n,\n\nh\n\n1\n\n\n,\n…\n\n\n\n\n\ncombine them into a matrix \n\nH\n\n\n\n=\n\n\n[\n\n\n\n\nh\n\n0\n\n\n\n\n\n\n\nh\n\n1\n\n\n\n\n\n\n⋮\n\n\n\n]\n\n\n\n\n\n\n\nEncoderLayer\n\n(\nH\n)\n\n\n\n=\n\n\n[\n\n\n\n\nFFN\n\n(\n\nMultiheadedAttention\n\n(\nH\n,\nH\n,\nH\n\n)\n\n0\n\n\n)\n\n\n\n\n\nFFN\n\n(\n\nMultiheadedAttention\n\n(\nH\n,\nH\n,\nH\n\n)\n\n1\n\n\n)\n\n\n\n\n⋮\n\n\n\n]\n\n\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}{\\text{given input vectors }}&h_{0},h_{1},\\dots \\\\{\\text{combine them into a matrix }}H&={\\begin{bmatrix}h_{0}\\\\h_{1}\\\\\\vdots \\end{bmatrix}}\\\\{\\text{EncoderLayer}}(H)&={\\begin{bmatrix}{\\text{FFN}}({\\text{MultiheadedAttention}}(H,H,H)_{0})\\\\{\\text{FFN}}({\\text{MultiheadedAttention}}(H,H,H)_{1})\\\\\\vdots \\end{bmatrix}}\\\\\\end{aligned}}}\n\n\n where \n\n\n\n\nFFN\n\n\n\n{\\displaystyle {\\text{FFN}}}\n\n stands for \"feed-forward network\". We can more succinctly write it as\n\n\n\n\nEncoderLayer\n\n(\nH\n)\n=\n\nFFN\n\n(\n\nMultiheadedAttention\n\n(\nH\n,\nH\n,\nH\n)\n)\n\n\n{\\displaystyle {\\text{EncoderLayer}}(H)={\\text{FFN}}({\\text{MultiheadedAttention}}(H,H,H))}\n\nwith the implicit convention that the \n\n\n\n\nFFN\n\n\n\n{\\displaystyle {\\text{FFN}}}\n\n is applied to each row of the matrix individually.\n The encoder layers are stacked. The first encoder layer takes the sequence of input vectors from the embedding layer, producing a sequence of vectors. This sequence of vectors is processed by the second encoder, and so on. The output from the final encoder layer is then used by the decoder.\n As the encoder processes the entire input all at once, every token can attend to every other token (all-to-all attention), so there is no need for causal masking.\n A decoder consists of an embedding layer, followed by multiple decoder layers, followed by an un-embedding layer.\n Each decoder consists of three major components: a causally masked self-attention mechanism, a cross-attention mechanism, and a feed-forward neural network. The decoder functions in a similar fashion to the encoder, but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders. This mechanism can also be called the encoder-decoder attention.[1][54]\n Like the first encoder, the first decoder takes positional information and embeddings of the output sequence as its input, rather than encodings. The transformer must not use the current or future output to predict an output, so the output sequence must be partially masked to prevent this reverse information flow.[1] This allows for autoregressive text generation. For decoding, all-to-all attention is inappropriate, because a token cannot attend to tokens not yet generated. Thus, the self-attention module in the decoder is causally masked.\n In contrast, the cross-attention mechanism attends to the output vectors of the encoder, which is computed before the decoder starts decoding. Consequently, there is no need for masking in the cross-attention mechanism.\n Schematically, we have:\n\n\n\n\n\n\n\n\nH\n′\n\n\n\n\n=\n\nMaskedMultiheadedAttention\n\n(\nH\n,\nH\n,\nH\n)\n\n\n\n\n\nDecoderLayer\n\n(\nH\n)\n\n\n\n=\n\nFFN\n\n(\n\nMultiheadedAttention\n\n(\n\nH\n′\n\n,\n\nH\n\nE\n\n\n,\n\nH\n\nE\n\n\n)\n)\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}H'&={\\text{MaskedMultiheadedAttention}}(H,H,H)\\\\{\\text{DecoderLayer}}(H)&={\\text{FFN}}({\\text{MultiheadedAttention}}(H',H^{E},H^{E}))\\end{aligned}}}\n\nwhere \n\n\n\n\nH\n\nE\n\n\n\n\n{\\displaystyle H^{E}}\n\n is the matrix with rows being the output vectors from the encoder.\n The last decoder is followed by a final un-embedding layer. to produce the output probabilities over the vocabulary. Then, one of the tokens is sampled according to the probability, and the decoder can be run again to produce the next token, etc, autoregressively generating output text.\n Many large language models, since they do not need to predict a whole new sequence from an input sequence, only use the encoder or decoder of the original transformer architecture. Early GPT models are decoder-only models trained to predict the next token in a sequence.[58] BERT, another language model, only makes use of an encoder, and is trained to predict a randomly masked token in a sequence.[35]\n Each encoder layer contains 2 sublayers: the self-attention and the feedforward network. Each decoder layer contains 3 sublayers: the causally masked self-attention, the cross-attention, and the feedforward network.\n The final points of detail are the residual connections and layer normalization (LayerNorm, or LN), which while conceptually unnecessary, are necessary for numerical stability and convergence. Similarly to how the feedforward network modules are applied individually to each vector, the LayerNorm is also applied individually to each vector. \n There are two common conventions in use: the post-LN and the pre-LN convention. In the post-LN convention, the output of each sublayer is \n\n\n\n\nL\na\ny\ne\nr\nN\no\nr\nm\n\n(\nx\n+\n\nS\nu\nb\nl\na\ny\ne\nr\n\n(\nx\n)\n)\n\n\n{\\displaystyle \\mathrm {LayerNorm} (x+\\mathrm {Sublayer} (x))}\n\nwhere \n\n\n\n\nS\nu\nb\nl\na\ny\ne\nr\n\n(\nx\n)\n\n\n{\\displaystyle \\mathrm {Sublayer} (x)}\n\n is the function implemented by the sublayer itself.\n In the pre-LN convention, the output of each sublayer is\n\n\n\nx\n+\n\nS\nu\nb\nl\na\ny\ne\nr\n\n(\n\nL\na\ny\ne\nr\nN\no\nr\nm\n\n(\nx\n)\n)\n\n\n{\\displaystyle x+\\mathrm {Sublayer} (\\mathrm {LayerNorm} (x))}\n\nThe original 2017 Transformer used the post-LN convention. It was difficult to train and required careful hyperparameter tuning and a \"warm-up\" in learning rate, where it starts small and gradually increases. The pre-LN convention, proposed several times in 2018,[59] was found to be easier to train, requiring no warm-up, leading to faster convergence.[46]\n The following is the pseudocode for a standard pre-LN encoder-decoder Transformer, adapted from[60]\n The Transformer architecture, being modular, allows variations. Several common variations are described here.[61]\n An \"encoder-only\" Transformer applies the encoder to map an input text into a sequence of vectors that represent the input text. This is usually used for text embedding and representation learning for downstream applications. BERT is encoder-only. They are less often used currently, as they were found to be not significantly better than training an encoder-decoder Transformer, then taking just the encoder.[51]\n A \"decoder-only\" Transformer is not literally decoder-only, since without an encoder, the cross-attention mechanism has nothing to attend to. Thus, the decoder layers in a decoder-only Transformer is composed of just two sublayers: the causally masked self-attention, and the feedforward network. This is usually used for text generation and instruction following. The models in the GPT series and Chinchilla series are decoder-only.\n An \"encoder-decoder\" Transformer is generally the same as the original Transformer, with 2 sublayers per encoder layer and 3 sublayers per decoder layer, etc. They might have minor architectural improvements, such as alternative activation functions, changing the location of normalization, etc. This is also usually used for text generation and instruction following. The models in the T5 series are encoder-decoder.[61]\n A \"prefixLM\" (prefix language model) is a decoder-only architecture, but with prefix masking, which is different from causal masking. Specifically, it has mask of the form[61]: Figure 3 \n\n\n\n\nM\n\nprefixLM\n\n\n=\n\n\n[\n\n\n\n\n0\n\n\n\n−\n∞\n\n\n\n\n\n0\n\n\n\n\nM\n\ncausal\n\n\n\n\n\n]\n\n\n\n\n{\\displaystyle M_{\\text{prefixLM}}={\\begin{bmatrix}\\mathbf {0} &-\\infty \\\\\\mathbf {0} &M_{\\text{causal}}\\end{bmatrix}}}\n\nwhere the first columns correspond to the \"prefix\", and the subsequent columns correspond to the autoregressively generated text based on the prefix. They resemble encoder-decoder models, but has less \"sparsity\". Such models are rarely used, though they are cited as theoretical possibilities and benchmarked comparisons.[51]\n There are also mixed seq2seq models. For example, in 2020, Google Translate replaced the previous RNN-encoder–RNN-decoder model by a Transformer-encoder–RNN-decoder model, on the argument that an RNN-decoder runs much faster than Transformer-decoder when run autoregressively.[62]\n The original transformer uses ReLU activation function. Other activation functions were developed. The Llama series and PaLM used SwiGLU;[63] both GPT-1 and BERT[35] used GELU.[64]\n Alternative activation functions are often used in combination with Gated Linear Units in the feedforward module.[65]\n The normalization used in the Transformer can be different from LayerNorm. One example is RMSNorm[66] which is used in the Llama series. Other examples include CapsuleNorm[67] ScaleNorm,[68] or FixNorm.[68]\n Transformers may use other positional encoding methods than sinusoidal.[69]\n The original Transformer paper reported using a learned positional encoding,[70] but finding it not superior to the sinusoidal one.[1] Later, [71] found that causal masking itself provides enough signal to a Transformer decoder that it can learn to implicitly perform absolute positional encoding without the positional encoding module.\n RoPE (rotary positional embedding),[72] is best explained by considering a list of 2-dimensional vectors \n\n\n\n[\n(\n\nx\n\n1\n\n\n(\n1\n)\n\n\n,\n\nx\n\n1\n\n\n(\n2\n)\n\n\n)\n,\n(\n\nx\n\n2\n\n\n(\n1\n)\n\n\n,\n\nx\n\n2\n\n\n(\n2\n)\n\n\n)\n,\n(\n\nx\n\n3\n\n\n(\n1\n)\n\n\n,\n\nx\n\n3\n\n\n(\n2\n)\n\n\n)\n,\n.\n.\n.\n]\n\n\n{\\displaystyle [(x_{1}^{(1)},x_{1}^{(2)}),(x_{2}^{(1)},x_{2}^{(2)}),(x_{3}^{(1)},x_{3}^{(2)}),...]}\n\n. Now pick some angle \n\n\n\nθ\n\n\n{\\displaystyle \\theta }\n\n. Then RoPE encoding is\n\n\n\n\nRoPE\n\n\n\n(\n\n\n\nx\n\nm\n\n\n(\n1\n)\n\n\n,\n\nx\n\nm\n\n\n(\n2\n)\n\n\n,\nm\n\n\n)\n\n\n=\n\n\n(\n\n\n\ncos\n⁡\nm\nθ\n\n\n−\nsin\n⁡\nm\nθ\n\n\n\n\nsin\n⁡\nm\nθ\n\n\ncos\n⁡\nm\nθ\n\n\n\n)\n\n\n\n\n(\n\n\n\n\nx\n\nm\n\n\n(\n1\n)\n\n\n\n\n\n\n\nx\n\nm\n\n\n(\n2\n)\n\n\n\n\n\n)\n\n\n=\n\n\n(\n\n\n\n\nx\n\nm\n\n\n(\n1\n)\n\n\ncos\n⁡\nm\nθ\n−\n\nx\n\nm\n\n\n(\n2\n)\n\n\nsin\n⁡\nm\nθ\n\n\n\n\n\nx\n\nm\n\n\n(\n2\n)\n\n\ncos\n⁡\nm\nθ\n+\n\nx\n\nm\n\n\n(\n1\n)\n\n\nsin\n⁡\nm\nθ\n\n\n\n)\n\n\n\n\n{\\displaystyle {\\text{RoPE}}{\\big (}x_{m}^{(1)},x_{m}^{(2)},m{\\big )}={\\begin{pmatrix}\\cos m\\theta &-\\sin m\\theta \\\\\\sin m\\theta &\\cos m\\theta \\end{pmatrix}}{\\begin{pmatrix}x_{m}^{(1)}\\\\x_{m}^{(2)}\\\\\\end{pmatrix}}={\\begin{pmatrix}x_{m}^{(1)}\\cos m\\theta -x_{m}^{(2)}\\sin m\\theta \\\\x_{m}^{(2)}\\cos m\\theta +x_{m}^{(1)}\\sin m\\theta \\\\\\end{pmatrix}}}\n\nEquivalently, if we write the 2-dimensional vectors as complex numbers \n\n\n\n\nz\n\nm\n\n\n:=\n\nx\n\nm\n\n\n(\n1\n)\n\n\n+\ni\n\nx\n\nm\n\n\n(\n2\n)\n\n\n\n\n{\\displaystyle z_{m}:=x_{m}^{(1)}+ix_{m}^{(2)}}\n\n, then RoPE encoding is just multiplication by an angle:\n\n\n\n\nRoPE\n\n\n\n(\n\n\n\nz\n\nm\n\n\n,\nm\n\n\n)\n\n\n=\n\ne\n\ni\nm\nθ\n\n\n\nz\n\nm\n\n\n\n\n{\\displaystyle {\\text{RoPE}}{\\big (}z_{m},m{\\big )}=e^{im\\theta }z_{m}}\n\nFor a list of \n\n\n\n2\nn\n\n\n{\\displaystyle 2n}\n\n-dimensional vectors, a RoPE encoder is defined by a sequence of angles \n\n\n\n\nθ\n\n(\n1\n)\n\n\n,\n.\n.\n.\n,\n\nθ\n\n(\nn\n)\n\n\n\n\n{\\displaystyle \\theta ^{(1)},...,\\theta ^{(n)}}\n\n. Then the RoPE encoding is applied to each pair of coordinates.\n The benefit of RoPE is that the dot-product between two vectors depends on their relative location only:\n\n\n\n\nRoPE\n\n\n\n(\n\n\nx\n,\nm\n\n\n\n)\n\n\n\nT\n\n\n\nRoPE\n\n\n\n(\n\n\ny\n,\nn\n\n\n)\n\n\n=\n\nRoPE\n\n\n\n(\n\n\nx\n,\nm\n+\nk\n\n\n\n)\n\n\n\nT\n\n\n\nRoPE\n\n\n\n(\n\n\ny\n,\nn\n+\nk\n\n\n)\n\n\n\n\n{\\displaystyle {\\text{RoPE}}{\\big (}x,m{\\big )}^{T}{\\text{RoPE}}{\\big (}y,n{\\big )}={\\text{RoPE}}{\\big (}x,m+k{\\big )}^{T}{\\text{RoPE}}{\\big (}y,n+k{\\big )}}\n\n\nfor any integer \n\n\n\nk\n\n\n{\\displaystyle k}\n\n.\n ALiBi (Attention with Linear Biases)[73] is not a replacement for the positional encoder on the original transformer. Instead, it is an additional positional encoder that is directly plugged into the attention mechanism. Specifically, the ALiBi attention mechanism is\n\n\n\n\n\n\n\n\nAttention\n\n(\nQ\n,\nK\n,\nV\n)\n=\n\nsoftmax\n\n\n(\n\n\n\n\nQ\n\nK\n\n\nT\n\n\n\n\n\n\nd\n\nk\n\n\n\n\n\n+\ns\nB\n\n)\n\nV\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}+sB\\right)V\\end{aligned}}}\n\nHere, \n\n\n\ns\n\n\n{\\displaystyle s}\n\n is a real number (\"scalar\"), and \n\n\n\nB\n\n\n{\\displaystyle B}\n\n is the linear bias matrix defined by\n\n\n\nB\n=\n\n\n(\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n⋯\n\n\n\n\n−\n1\n\n\n0\n\n\n1\n\n\n2\n\n\n⋯\n\n\n\n\n−\n2\n\n\n−\n1\n\n\n0\n\n\n1\n\n\n⋯\n\n\n\n\n−\n3\n\n\n−\n2\n\n\n−\n1\n\n\n0\n\n\n⋯\n\n\n\n\n⋮\n\n\n⋮\n\n\n⋮\n\n\n⋮\n\n\n⋱\n\n\n\n)\n\n\n\n\n{\\displaystyle B={\\begin{pmatrix}0&1&2&3&\\cdots \\\\-1&0&1&2&\\cdots \\\\-2&-1&0&1&\\cdots \\\\-3&-2&-1&0&\\cdots \\\\\\vdots &\\vdots &\\vdots &\\vdots &\\ddots \\\\\\end{pmatrix}}}\n\nin other words, \n\n\n\n\nB\n\ni\n,\nj\n\n\n=\nj\n−\ni\n\n\n{\\displaystyle B_{i,j}=j-i}\n\n. The idea being that the linear bias matrix is a softened mask. Just as \n\n\n\n0\n\n\n{\\displaystyle 0}\n\n represent full attention paid, and \n\n\n\n−\n∞\n\n\n{\\displaystyle -\\infty }\n\n represents no attention paid, the linear bias matrix increases attention paid in one direction and decreases attention paid in the other direction.\n ALiBi allows pretraining on short context windows, then fine-tuning on longer context windows. Since it is directly plugged into the attention mechanism, it can be combined with any positional encoder that is plugged into the \"bottom\" of the entire network (which is where the sinusoidal encoder on the original transformer, as well as RoPE and many others, are located).\n Relative Position Encodings[74] is similar to ALiBi, but more generic:\n\n\n\n\n\n\n\n\nAttention\n\n(\nQ\n,\nK\n,\nV\n)\n=\n\nsoftmax\n\n\n(\n\n\n\n\nQ\n\nK\n\n\nT\n\n\n\n\n\n\nd\n\nk\n\n\n\n\n\n+\nB\n\n)\n\nV\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}+B\\right)V\\end{aligned}}}\n\nwhere \n\n\n\nB\n\n\n{\\displaystyle B}\n\n is a Toeplitz matrix, that is, \n\n\n\n\nB\n\ni\n,\nj\n\n\n=\n\nB\n\n\ni\n′\n\n,\n\nj\n′\n\n\n\n\n\n{\\displaystyle B_{i,j}=B_{i',j'}}\n\n whenever \n\n\n\ni\n−\nj\n=\n\ni\n′\n\n−\n\nj\n′\n\n\n\n{\\displaystyle i-j=i'-j'}\n\n. This is contrasted with the original sinusoidal positional encoding, which is an \"absolute positional encoding\".[75]\n The transformer model has been implemented in standard deep learning frameworks such as TensorFlow and PyTorch. Transformers is a library produced by Hugging Face that supplies transformer-based architectures and pretrained models.[11]\n FlashAttention[76] is an algorithm that implements the transformer attention mechanism efficiently on a GPU. It performs matrix multiplications in blocks, such that each block fits within the cache of a GPU, and by careful management of the blocks it minimizes data copying between GPU caches (as data movement is slow).\n An improved version, FlashAttention-2,[77][78][79] was developed to cater to the rising demand for language models capable of handling longer context lengths. It offers enhancements in work partitioning and parallelism, enabling it to achieve up to 230 TFLOPs/s on A100 GPUs (FP16/BF16), a 2x speed increase over the original FlashAttention.\n Key advancements in FlashAttention-2 include the reduction of non-matmul FLOPs, improved parallelism over the sequence length dimension, better work partitioning between GPU warps, and added support for head dimensions up to 256 and multi-query attention (MQA) and grouped-query attention (GQA).[80]\n Benchmarks revealed FlashAttention-2 to be up to 2x faster than FlashAttention and up to 9x faster than a standard attention implementation in PyTorch. Future developments include optimization for new hardware like H100 GPUs and new data types like FP8.\n Multi-Query Attention changes the multiheaded attention mechanism.[81] Whereas normally,\n \n\n\n\n\nMultiheadedAttention\n\n(\nQ\n,\nK\n,\nV\n)\n=\n\n\nConcat\n\n\ni\n∈\n[\n\nn\n\nheads\n\n\n]\n\n\n\n(\n\n\nAttention\n\n(\nX\n\nW\n\ni\n\n\nQ\n\n\n,\nX\n\nW\n\ni\n\n\nK\n\n\n,\nX\n\nW\n\ni\n\n\nV\n\n\n)\n\n)\n\n\nW\n\nO\n\n\n\n\n{\\displaystyle {\\text{MultiheadedAttention}}(Q,K,V)={\\text{Concat}}_{i\\in [n_{\\text{heads}}]}\\left({\\text{Attention}}(XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V})\\right)W^{O}}\n\nwith Multi-Query Attention, there is just one \n\n\n\n\nW\n\nK\n\n\n,\n\nW\n\nV\n\n\n\n\n{\\displaystyle W^{K},W^{V}}\n\n, thus:\n \n\n\n\n\nMultiQueryAttention\n\n(\nQ\n,\nK\n,\nV\n)\n=\n\n\nConcat\n\n\ni\n∈\n[\n\nn\n\nheads\n\n\n]\n\n\n\n(\n\n\nAttention\n\n(\nX\n\nW\n\ni\n\n\nQ\n\n\n,\nX\n\nW\n\nK\n\n\n,\nX\n\nW\n\nV\n\n\n)\n\n)\n\n\nW\n\nO\n\n\n\n\n{\\displaystyle {\\text{MultiQueryAttention}}(Q,K,V)={\\text{Concat}}_{i\\in [n_{\\text{heads}}]}\\left({\\text{Attention}}(XW_{i}^{Q},XW^{K},XW^{V})\\right)W^{O}}\n\n\n This has a neutral effect on model quality and training speed, but increases inference speed. \n More generally, grouped-query attention (GQA) partitions attention heads into groups, each of which shares the key-value pair. MQA is GQA with one group, while standard multiheaded attention is GQA with the maximal number of groups.[82]\n When an autoregressive transformer is used for inference, such as generating text, the query vector is different at each step, but the already-computed key and value vectors are always the same. The KV caching method saves the computed key and value vectors at each attention block, so that they are not recomputed at each new token. PagedAttention applies memory paging to KV caching.[83][84][85]\n If a transformer is used with a baked-in prompt, such as [\"You are a customer support agent...\"], then the key and value vectors can be computed for the prompt, and saved on disk. The saving in compute is significant when the model is used for many short interactions, such as in online chatbots.\n Transformers are used in large language models for autoregressive sequence generation: generating a stream of text, one token at a time. However, in most settings, decoding from language models is memory-bound, meaning that we have spare compute power available. Speculative decoding[86][87] uses this spare compute power by computing several tokens in parallel. Similarly to speculative execution in CPUs, future tokens are computed concurrently, by speculating on the value of previous tokens, and are later discarded if it turns out the speculation was incorrect.\n Specifically, consider a transformer model like GPT-3 with a context window size of 512. To generate an entire context window autoregressively with greedy decoding, it must be run for 512 times, each time generating a token \n\n\n\n\nx\n\n1\n\n\n,\n\nx\n\n2\n\n\n,\n.\n.\n.\n,\n\nx\n\n512\n\n\n\n\n{\\displaystyle x_{1},x_{2},...,x_{512}}\n\n. However, if we had some educated guess for the values of these tokens, we could verify all of them in parallel, in one run of the model, by checking that each \n\n\n\n\nx\n\nt\n\n\n\n\n{\\displaystyle x_{t}}\n\n is indeed the token with the largest log-likelihood in the \n\n\n\nt\n\n\n{\\displaystyle t}\n\n-th output.\n In speculative decoding, a smaller model or some other simple heuristic is used to generate a few speculative tokens that are subsequently verified by the larger model. For example, suppose a small model generated four speculative tokens: \n\n\n\n\n\n\n\nx\n~\n\n\n\n\n1\n\n\n,\n\n\n\n\nx\n~\n\n\n\n\n2\n\n\n,\n\n\n\n\nx\n~\n\n\n\n\n3\n\n\n,\n\n\n\n\nx\n~\n\n\n\n\n4\n\n\n\n\n{\\displaystyle {\\tilde {x}}_{1},{\\tilde {x}}_{2},{\\tilde {x}}_{3},{\\tilde {x}}_{4}}\n\n. These tokens are run through the larger model, and only \n\n\n\n\n\n\n\nx\n~\n\n\n\n\n1\n\n\n\n\n{\\displaystyle {\\tilde {x}}_{1}}\n\n and \n\n\n\n\n\n\n\nx\n~\n\n\n\n\n2\n\n\n\n\n{\\displaystyle {\\tilde {x}}_{2}}\n\n are accepted. The same run of the large model already generated a new token \n\n\n\n\nx\n\n3\n\n\n\n\n{\\displaystyle x_{3}}\n\n to replace \n\n\n\n\n\n\n\nx\n~\n\n\n\n\n3\n\n\n\n\n{\\displaystyle {\\tilde {x}}_{3}}\n\n, and \n\n\n\n\n\n\n\nx\n~\n\n\n\n\n4\n\n\n\n\n{\\displaystyle {\\tilde {x}}_{4}}\n\n is completely discarded. The process then repeats (starting from the 4th token) until all tokens are generated.\n For non-greedy decoding, similar ideas apply, except the speculative tokens are accepted or rejected stochastically, in a way that guarantees the final output distribution is the same as if speculative decoding was not used.[86][88]\n Training transformer-based architectures can be expensive, especially for long inputs.[89] Many methods have been developed to attempt to address the issue. In the image domain, Swin Transformer is an efficient architecture that performs attention inside shifting windows.[90] In the audio domain, SepTr decouples the attention in time and frequency domains.[91] Long Range Arena (2020)[92] is a standard benchmark for comparing the behavior of transformer architectures over long inputs.\n The standard attention graph is either all-to-all or causal, both of which scales as \n\n\n\nO\n(\n\nN\n\n2\n\n\n)\n\n\n{\\displaystyle O(N^{2})}\n\n where \n\n\n\nN\n\n\n{\\displaystyle N}\n\n is the number of tokens in a sequence.\n Reformer (2020)[89][93] reduces the computational load from \n\n\n\nO\n(\n\nN\n\n2\n\n\n)\n\n\n{\\displaystyle O(N^{2})}\n\n to \n\n\n\nO\n(\nN\nln\n⁡\nN\n)\n\n\n{\\displaystyle O(N\\ln N)}\n\n by using locality-sensitive hashing and reversible layers.[94]\n Sparse attention[95] uses attention graphs that grows slower than \n\n\n\nO\n(\n\nN\n\n2\n\n\n)\n\n\n{\\displaystyle O(N^{2})}\n\n. For example, BigBird (2020)[96] uses random small-world networks which grows as \n\n\n\nO\n(\nN\n)\n\n\n{\\displaystyle O(N)}\n\n.\n Ordinary transformers require a memory size that is quadratic in the size of the context window. Attention-free transformers[97] reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value.\n Random Feature Attention (2021)[98] uses Fourier random features:\n\n\n\nφ\n(\nx\n)\n=\n\n\n1\n\nD\n\n\n\n[\ncos\n⁡\n⟨\n\nw\n\n1\n\n\n,\nx\n⟩\n,\nsin\n⁡\n⟨\n\nw\n\n1\n\n\n,\nx\n⟩\n,\n⋯\ncos\n⁡\n⟨\n\nw\n\nD\n\n\n,\nx\n⟩\n,\nsin\n⁡\n⟨\n\nw\n\nD\n\n\n,\nx\n⟩\n\n]\n\nT\n\n\n\n\n{\\displaystyle \\varphi (x)={\\frac {1}{\\sqrt {D}}}[\\cos \\langle w_{1},x\\rangle ,\\sin \\langle w_{1},x\\rangle ,\\cdots \\cos \\langle w_{D},x\\rangle ,\\sin \\langle w_{D},x\\rangle ]^{T}}\n\nwhere \n\n\n\n\nw\n\n1\n\n\n,\n.\n.\n.\n,\n\nw\n\nD\n\n\n\n\n{\\displaystyle w_{1},...,w_{D}}\n\n are independent samples from the normal distribution \n\n\n\nN\n(\n0\n,\n\nσ\n\n2\n\n\nI\n)\n\n\n{\\displaystyle N(0,\\sigma ^{2}I)}\n\n. This choice of parameters satisfy \n\n\n\n\nE\n\n[\n⟨\nφ\n(\nx\n)\n,\nφ\n(\ny\n)\n⟩\n]\n=\n\ne\n\n−\n\n\n\n‖\nx\n−\ny\n\n‖\n\n2\n\n\n\n\n2\n\nσ\n\n2\n\n\n\n\n\n\n\n\n\n{\\displaystyle \\mathbb {E} [\\langle \\varphi (x),\\varphi (y)\\rangle ]=e^{-{\\frac {\\|x-y\\|^{2}}{2\\sigma ^{2}}}}}\n\n, or \n\n\n\n\ne\n\n⟨\nx\n,\ny\n⟩\n\n/\n\n\nσ\n\n2\n\n\n\n\n=\n\nE\n\n[\n⟨\n\ne\n\n‖\nx\n\n‖\n\n2\n\n\n\n/\n\n2\n\nσ\n\n2\n\n\n\n\nφ\n(\nx\n)\n,\n\ne\n\n‖\ny\n\n‖\n\n2\n\n\n\n/\n\n2\n\nσ\n\n2\n\n\n\n\nφ\n(\ny\n)\n⟩\n]\n≈\n⟨\n\ne\n\n‖\nx\n\n‖\n\n2\n\n\n\n/\n\n2\n\nσ\n\n2\n\n\n\n\nφ\n(\nx\n)\n,\n\ne\n\n‖\ny\n\n‖\n\n2\n\n\n\n/\n\n2\n\nσ\n\n2\n\n\n\n\nφ\n(\ny\n)\n⟩\n\n\n{\\displaystyle e^{\\langle x,y\\rangle /\\sigma ^{2}}=\\mathbb {E} [\\langle e^{\\|x\\|^{2}/2\\sigma ^{2}}\\varphi (x),e^{\\|y\\|^{2}/2\\sigma ^{2}}\\varphi (y)\\rangle ]\\approx \\langle e^{\\|x\\|^{2}/2\\sigma ^{2}}\\varphi (x),e^{\\|y\\|^{2}/2\\sigma ^{2}}\\varphi (y)\\rangle }\n\nConsequently, the one-headed attention, with one query, can be written as \n\n\n\n\nAttention\n\n(\nq\n,\nK\n,\nV\n)\n=\n\nsoftmax\n\n\n(\n\n\n\nq\n\nK\n\n\nT\n\n\n\n\n\n\nd\n\nk\n\n\n\n\n\n)\n\nV\n≈\n\n\n\nφ\n(\nq\n\n)\n\nT\n\n\n\n∑\n\ni\n\n\n\ne\n\n‖\n\nk\n\ni\n\n\n\n‖\n\n2\n\n\n\n/\n\n2\n\nσ\n\n2\n\n\n\n\nφ\n(\n\nk\n\ni\n\n\n)\n\nv\n\ni\n\n\nT\n\n\n\n\nφ\n(\nq\n\n)\n\nT\n\n\n\n∑\n\ni\n\n\n\ne\n\n‖\n\nk\n\ni\n\n\n\n‖\n\n2\n\n\n\n/\n\n2\n\nσ\n\n2\n\n\n\n\nφ\n(\n\nk\n\ni\n\n\n)\n\n\n\n\n\n{\\displaystyle {\\text{Attention}}(q,K,V)={\\text{softmax}}\\left({\\frac {qK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\approx {\\frac {\\varphi (q)^{T}\\sum _{i}e^{\\|k_{i}\\|^{2}/2\\sigma ^{2}}\\varphi (k_{i})v_{i}^{T}}{\\varphi (q)^{T}\\sum _{i}e^{\\|k_{i}\\|^{2}/2\\sigma ^{2}}\\varphi (k_{i})}}}\n\nwhere \n\n\n\nσ\n=\n\nd\n\nK\n\n\n1\n\n/\n\n4\n\n\n\n\n{\\displaystyle \\sigma =d_{K}^{1/4}}\n\n. Similarly for multiple queries, and for multiheaded attention.\n This approximation can be computed in linear time, as we can compute the matrix \n\n\n\nφ\n(\n\nk\n\ni\n\n\n)\n\nv\n\ni\n\n\nT\n\n\n\n\n{\\displaystyle \\varphi (k_{i})v_{i}^{T}}\n\n first, then multiply it with the query. In essence, we have managed to obtain a more precise version of \n\n\n\n\nAttention\n\n(\nQ\n,\nK\n,\nV\n)\n=\n\nsoftmax\n\n\n(\n\n\n\nQ\n\nK\n\n\nT\n\n\n\n\n\n\nd\n\nk\n\n\n\n\n\n)\n\nV\n≈\nQ\n(\n\nK\n\nT\n\n\nV\n\n/\n\n\n\n\nd\n\nk\n\n\n\n\n)\n\n\n{\\displaystyle {\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\approx Q(K^{T}V/{\\sqrt {d_{k}}})}\n\nPerformer (2022)[99] uses the same Random Feature Attention, but \n\n\n\n\nw\n\n1\n\n\n,\n.\n.\n.\n,\n\nw\n\nD\n\n\n\n\n{\\displaystyle w_{1},...,w_{D}}\n\n are first independently sampled from the normal distribution \n\n\n\nN\n(\n0\n,\n\nσ\n\n2\n\n\nI\n)\n\n\n{\\displaystyle N(0,\\sigma ^{2}I)}\n\n, then they are Gram-Schmidt processed.\n Transformers can also be used/adapted for modalities (input or output) beyond just text, usually by finding a way to \"tokenize\" the modality.\n Multimodal models can either be trained from scratch, or by finetuning. A 2022 study found that Transformers pretrained only on natural language can be finetuned on only 0.03% of parameters and become competitive with LSTMs on a variety of logical and visual tasks, demonstrating transfer learning.[100] The LLaVA was a vision-language model composed of a language model (Vicuna-13B)[101] and a vision model (ViT-L/14), connected by a linear layer. Only the linear layer is finetuned.[102]\n Vision transformers[41] adapt the transformer to computer vision by breaking down input images as a series of patches, turning them into vectors, and treating them like tokens in a standard transformer.\n Conformer[42] and later Whisper[103] follow the same pattern for speech recognition, first turning the speech signal into a spectrogram, which is then treated like an image, i.e. broken down into a series of patches, turned into vectors and treated like tokens in a standard transformer.\n Perceivers[104][105] are a variant of Transformers designed for multimodality.\n For image generation, notable architectures are DALL-E 1 (2021), Parti (2022),[106] Phenaki (2023),[107] and Muse (2023).[108] Unlike later models, DALL-E is not a diffusion model. Instead, it uses a decoder-only Transformer that autoregressively generates a text, followed by the token representation of an image, which is then converted by a variational autoencoder to an image.[109] Parti is an encoder-decoder Transformer, where the encoder processes a text prompt, and the decoder generates a token representation of an image.[110] Muse is an encoder-only Transformer that is trained to predict masked image tokens from unmasked image tokens. During generation, all input tokens are masked, and the highest-confidence predictions are included for the next iteration, until all tokens are predicted.[108] Phenaki is a text-to-video model. It is a bidirectional masked transformer conditioned on pre-computed text tokens. The generated tokens are then decoded to a video.[107]\n The transformer has had great success in natural language processing (NLP). Many large language models such as GPT-2, GPT-3, GPT-4, AlbertAGPT, Claude, BERT, XLNet, RoBERTa and ChatGPT demonstrate the ability of transformers to perform a wide variety of NLP-related subtasks and their related real-world applications, including:\n Beyond traditional NLP, the transformer architecture has had success in other applications, such as:\n"
    },
    {
        "title": "Existential risk from artificial intelligence",
        "url": "https://en.wikipedia.org/wiki/AI_risk",
        "content": "\n Existential risk from artificial intelligence refers to the idea that substantial progress in artificial general intelligence (AGI) could lead to human extinction or an irreversible global catastrophe.[1][2][3][4]\n One argument for the importance of this risk references how human beings dominate other species because the human brain possesses distinctive capabilities other animals lack. If AI were to surpass human intelligence and become superintelligent, it might become uncontrollable. Just as the fate of the mountain gorilla depends on human goodwill, the fate of humanity could depend on the actions of a future machine superintelligence.[5]\n The plausibility of existential catastrophe due to AI is widely debated. It hinges in part on whether AGI or superintelligence are achievable, the speed at which dangerous capabilities and behaviors emerge,[6] and whether practical scenarios for AI takeovers exist.[7] Concerns about superintelligence have been voiced by computer scientists and tech CEOs such as Geoffrey Hinton,[8] Yoshua Bengio,[9] Alan Turing,[a] Elon Musk,[12] and OpenAI CEO Sam Altman.[13] In 2022, a survey of AI researchers with a 17% response rate found that the majority believed there is a 10 percent or greater chance that human inability to control AI will cause an existential catastrophe.[14][15] In 2023, hundreds of AI experts and other notable figures signed a statement declaring, \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".[16] Following increased concern over AI risks, government leaders such as United Kingdom prime minister Rishi Sunak[17] and United Nations Secretary-General António Guterres[18] called for an increased focus on global AI regulation.\n Two sources of concern stem from the problems of AI control and alignment. Controlling a superintelligent machine or instilling it with human-compatible values may be difficult. Many researchers believe that a superintelligent machine would likely resist attempts to disable it or change its goals as that would prevent it from accomplishing its present goals. It would be extremely challenging to align a superintelligence with the full breadth of significant human values and constraints.[1][19][20] In contrast, skeptics such as computer scientist Yann LeCun argue that superintelligent machines will have no desire for self-preservation.[21]\n A third source of concern is the possibility of a sudden \"intelligence explosion\" that catches humanity unprepared. In this scenario, an AI more intelligent than its creators would be able to recursively improve itself at an exponentially increasing rate, improving too quickly for its handlers or society at large to control.[1][19] Empirically, examples like AlphaZero, which taught itself to play Go and quickly surpassed human ability, show that domain-specific AI systems can sometimes progress from subhuman to superhuman ability very quickly, although such machine learning systems do not recursively improve their fundamental architecture.[22]\n One of the earliest authors to express serious concern that highly advanced machines might pose existential risks to humanity was the novelist Samuel Butler, who wrote in his 1863 essay Darwin among the Machines:[23]\n The upshot is simply a question of time, but that the time will come when the machines will hold the real supremacy over the world and its inhabitants is what no person of a truly philosophic mind can for a moment question. In 1951, foundational computer scientist Alan Turing wrote the article \"Intelligent Machinery, A Heretical Theory\", in which he proposed that artificial general intelligences would likely \"take control\" of the world as they became more intelligent than human beings:\n Let us now assume, for the sake of argument, that [intelligent] machines are a genuine possibility, and look at the consequences of constructing them... There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control, in the way that is mentioned in Samuel Butler's Erewhon.[24] In 1965, I. J. Good originated the concept now known as an \"intelligence explosion\" and said the risks were underappreciated:[25]\n Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an 'intelligence explosion', and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously.[26] Scholars such as Marvin Minsky[27] and I. J. Good himself[28] occasionally expressed concern that a superintelligence could seize control, but issued no call to action. In 2000, computer scientist and Sun co-founder Bill Joy penned an influential essay, \"Why The Future Doesn't Need Us\", identifying superintelligent robots as a high-tech danger to human survival, alongside nanotechnology and engineered bioplagues.[29]\n Nick Bostrom published Superintelligence in 2014, which presented his arguments that superintelligence poses an existential threat.[30] By 2015, public figures such as physicists Stephen Hawking and Nobel laureate Frank Wilczek, computer scientists Stuart J. Russell and Roman Yampolskiy, and entrepreneurs Elon Musk and Bill Gates were expressing concern about the risks of superintelligence.[31][32][33][34] Also in 2015, the Open Letter on Artificial Intelligence highlighted the \"great potential of AI\" and encouraged more research on how to make it robust and beneficial.[35] In April 2016, the journal Nature warned: \"Machines and robots that outperform humans across the board could self-improve beyond our control—and their interests might not align with ours\".[36] In 2020, Brian Christian published The Alignment Problem, which details the history of progress on AI alignment up to that time.[37][38]\n In March 2023, key figures in AI, such as Musk, signed a letter from the Future of Life Institute calling a halt to advanced AI training until it could be properly regulated.[39] In May 2023, the Center for AI Safety released a statement signed by numerous experts in AI safety and the AI existential risk which stated: \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\"[40][41]\n Artificial general intelligence (AGI) is typically defined as a system that performs at least as well as humans in most or all intellectual tasks.[42] A 2022 survey of AI researchers found that 90% of respondents expected AGI would be achieved in the next 100 years, and half expected the same by 2061.[43] Meanwhile, some researchers dismiss existential risks from AGI as \"science fiction\" based on their high confidence that AGI will not be created anytime soon.[44]\n Breakthroughs in large language models (LLMs) have led some researchers to reassess their expectations. Notably, Geoffrey Hinton said in 2023 that he recently changed his estimate from \"20 to 50 years before we have general purpose A.I.\" to \"20 years or less\".[45]\n The Frontier supercomputer at Oak Ridge National Laboratory turned out to be nearly eight times faster than expected. Feiyi Wang, a researcher there, said \"We didn't expect this capability\" and \"we're approaching the point where we could actually simulate the human brain\".[46]\n In contrast with AGI, Bostrom defines a superintelligence as \"any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest\", including scientific creativity, strategic planning, and social skills.[47][5] He argues that a superintelligence can outmaneuver humans anytime its goals conflict with humans'. It may choose to hide its true intent until humanity cannot stop it.[48][5] Bostrom writes that in order to be safe for humanity, a superintelligence must be aligned with human values and morality, so that it is \"fundamentally on our side\".[49]\n Stephen Hawking argued that superintelligence is physically possible because \"there is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains\".[32]\n When artificial superintelligence (ASI) may be achieved, if ever, is necessarily less certain than predictions for AGI. In 2023, OpenAI leaders said that not only AGI, but superintelligence may be achieved in less than 10 years.[50]\n Bostrom argues that AI has many advantages over the human brain:[5]\n According to Bostrom, an AI that has an expert-level facility at certain key software engineering tasks could become a superintelligence due to its capability to recursively improve its own algorithms, even if it is initially limited in other domains not directly relevant to engineering.[5][48] This suggests that an intelligence explosion may someday catch humanity unprepared.[5]\n The economist Robin Hanson has said that, to launch an intelligence explosion, an AI must become vastly better at software innovation than the rest of the world combined, which he finds implausible.[51]\n In a \"fast takeoff\" scenario, the transition from AGI to superintelligence could take days or months. In a \"slow takeoff\", it could take years or decades, leaving more time for society to prepare.[52]\n Superintelligences are sometimes called \"alien minds\", referring to the idea that their way of thinking and motivations could be vastly different from ours. This is generally considered as a source of risk, making it more difficult to anticipate what a superintelligence might do. It also suggests the possibility that a superintelligence may not particularly value humans by default.[53] To avoid anthropomorphism, superintelligence is sometimes viewed as a powerful optimizer that makes the best decisions to achieve its goals.[5]\n The field of \"mechanistic interpretability\" aims to better understand the inner workings of AI models, potentially allowing us one day to detect signs of deception and misalignment.[54]\n It has been argued that there are limitations to what intelligence can achieve. Notably, the chaotic nature or time complexity of some systems could fundamentally limit a superintelligence's ability to predict some aspects of the future, increasing its uncertainty.[55]\n Advanced AI could generate enhanced pathogens or cyberattacks or manipulate people. These capabilities could be misused by humans,[56] or exploited by the AI itself if misaligned.[5] A full-blown superintelligence could find various ways to gain a decisive influence if it wanted to,[5] but these dangerous capabilities may become available earlier, in weaker and more specialized AI systems. They may cause societal instability and empower malicious actors.[56]\n Geoffrey Hinton warned that in the short term, the profusion of AI-generated text, images and videos will make it more difficult to figure out the truth, which he says authoritarian states could exploit to manipulate elections.[57] Such large-scale, personalized manipulation capabilities can increase the existential risk of a worldwide \"irreversible totalitarian regime\". It could also be used by malicious actors to fracture society and make it dysfunctional.[56]\n AI-enabled cyberattacks are increasingly considered a present and critical threat. According to NATO's technical director of cyberspace, \"The number of attacks is increasing exponentially\".[58] AI can also be used defensively, to preemptively find and fix vulnerabilities, and detect threats.[59]\n AI could improve the \"accessibility, success rate, scale, speed, stealth and potency of cyberattacks\", potentially causing \"significant geopolitical turbulence\" if it facilitates attacks more than defense.[56]\n Speculatively, such hacking capabilities could be used by an AI system to break out of its local environment, generate revenue, or acquire cloud computing resources.[60]\n As AI technology democratizes, it may become easier to engineer more contagious and lethal pathogens. This could enable people with limited skills in synthetic biology to engage in bioterrorism. Dual-use technology that is useful for medicine could be repurposed to create weapons.[56]\n For example, in 2022, scientists modified an AI system originally intended for generating non-toxic, therapeutic molecules with the purpose of creating new drugs. The researchers adjusted the system so that toxicity is rewarded rather than penalized. This simple change enabled the AI system to create, in six hours, 40,000 candidate molecules for chemical warfare, including known and novel molecules.[56][61]\n Companies, state actors, and other organizations competing to develop AI technologies could lead to a race to the bottom of safety standards.[62] As rigorous safety procedures take time and resources, projects that proceed more carefully risk being out-competed by less scrupulous developers.[63][56]\n AI could be used to gain military advantages via autonomous lethal weapons, cyberwarfare, or automated decision-making.[56] As an example of autonomous lethal weapons, miniaturized drones could facilitate low-cost assassination of military or civilian targets, a scenario highlighted in the 2017 short film Slaughterbots.[64] AI could be used to gain an edge in decision-making by quickly analyzing large amounts of data and making decisions more quickly and effectively than humans. This could increase the speed and unpredictability of war, especially when accounting for automated retaliation systems.[56][65]\n An existential risk is \"one that threatens the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development\".[67]\n Besides extinction risk, there is the risk that the civilization gets permanently locked into a flawed future. One example is a \"value lock-in\": If humanity still has moral blind spots similar to slavery in the past, AI might irreversibly entrench it, preventing moral progress. AI could also be used to spread and preserve the set of values of whoever develops it.[68] AI could facilitate large-scale surveillance and indoctrination, which could be used to create a stable repressive worldwide totalitarian regime.[69]\n Atoosa Kasirzadeh proposes to classify existential risks from AI into two categories: decisive and accumulative. Decisive risks encompass the potential for abrupt and catastrophic events resulting from the emergence of superintelligent AI systems that exceed human intelligence, which could ultimately lead to human extinction. In contrast, accumulative risks emerge gradually through a series of interconnected disruptions that may gradually erode societal structures and resilience over time, ultimately leading to a critical failure or collapse.[70][71]\n It is difficult or impossible to reliably evaluate whether an advanced AI is sentient and to what degree. But if sentient machines are mass created in the future, engaging in a civilizational path that indefinitely neglects their welfare could be an existential catastrophe.[72][73] This has notably been discussed in the context of risks of astronomical suffering (also called \"s-risks\").[74] Moreover, it may be possible to engineer digital minds that can feel much more happiness than humans with fewer resources, called \"super-beneficiaries\". Such an opportunity raises the question of how to share the world and which \"ethical and political framework\" would enable a mutually beneficial coexistence between biological and digital minds.[75]\n AI may also drastically improve humanity's future. Toby Ord considers the existential risk a reason for \"proceeding with due caution\", not for abandoning AI.[69] Max More calls AI an \"existential opportunity\", highlighting the cost of not developing it.[76]\n According to Bostrom, superintelligence could help reduce the existential risk from other powerful technologies such as molecular nanotechnology or synthetic biology. It is thus conceivable that developing superintelligence before other dangerous technologies would reduce the overall existential risk.[5]\n The alignment problem is the research problem of how to reliably assign objectives, preferences or ethical principles to AIs.\n An \"instrumental\" goal is a sub-goal that helps to achieve an agent's ultimate goal. \"Instrumental convergence\" refers to the fact that some sub-goals are useful for achieving virtually any ultimate goal, such as acquiring resources or self-preservation.[77] Bostrom argues that if an advanced AI's instrumental goals conflict with humanity's goals, the AI might harm humanity in order to acquire more resources or prevent itself from being shut down, but only as a way to achieve its ultimate goal.[5]\nRussell argues that a sufficiently advanced machine \"will have self-preservation even if you don't program it in... if you say, 'Fetch the coffee', it can't fetch the coffee if it's dead. So if you give it any goal whatsoever, it has a reason to preserve its own existence to achieve that goal.\"[21][78]\n Even if current goal-based AI programs are not intelligent enough to think of resisting programmer attempts to modify their goal structures, a sufficiently advanced AI might resist any attempts to change its goal structure, just as a pacifist would not want to take a pill that makes them want to kill people. If the AI were superintelligent, it would likely succeed in out-maneuvering its human operators and prevent itself being \"turned off\" or reprogrammed with a new goal.[5][79] This is particularly relevant to value lock-in scenarios. The field of \"corrigibility\" studies how to make agents that will not resist attempts to change their goals.[80]\n In the \"intelligent agent\" model, an AI can loosely be viewed as a machine that chooses whatever action appears to best achieve its set of goals, or \"utility function\". A utility function gives each possible situation a score that indicates its desirability to the agent. Researchers know how to write utility functions that mean \"minimize the average network latency in this specific telecommunications model\" or \"maximize the number of reward clicks\", but do not know how to write a utility function for \"maximize human flourishing\"; nor is it clear whether such a function meaningfully and unambiguously exists. Furthermore, a utility function that expresses some values but not others will tend to trample over the values the function does not reflect.[81][82]\n An additional source of concern is that AI \"must reason about what people intend rather than carrying out commands literally\", and that it must be able to fluidly solicit human guidance if it is too uncertain about what humans want.[83]\n Some researchers believe the alignment problem may be particularly difficult when applied to superintelligences. Their reasoning includes:\n Alternatively, some find reason to believe superintelligences would be better able to understand morality, human values, and complex goals. Bostrom writes, \"A future superintelligence occupies an epistemically superior vantage point: its beliefs are (probably, on most topics) more likely than ours to be true\".[5]\n In 2023, OpenAI started a project called \"Superalignment\" to solve the alignment of superintelligences in four years. It called this an especially important challenge, as it said superintelligence could be achieved within a decade. Its strategy involved automating alignment research using AI.[87] The Superalignment team was dissolved less than a year later.[88]\n Artificial Intelligence: A Modern Approach, a widely used undergraduate AI textbook,[89][90] says that superintelligence \"might mean the end of the human race\".[1] It states: \"Almost any technology has the potential to cause harm in the wrong hands, but with [superintelligence], we have the new problem that the wrong hands might belong to the technology itself.\"[1] Even if the system designers have good intentions, two difficulties are common to both AI and non-AI computer systems:[1]\n AI systems uniquely add a third problem: that even given \"correct\" requirements, bug-free implementation, and initial good behavior, an AI system's dynamic learning capabilities may cause it to develop unintended behavior, even without unanticipated external scenarios. An AI may partly botch an attempt to design a new generation of itself and accidentally create a successor AI that is more powerful than itself but that no longer maintains the human-compatible moral values preprogrammed into the original AI. For a self-improving AI to be completely safe, it would need not only to be bug-free, but to be able to design successor systems that are also bug-free.[1][93]\n Some skeptics, such as Timothy B. Lee of Vox, argue that any superintelligent program we create will be subservient to us, that the superintelligence will (as it grows more intelligent and learns more facts about the world) spontaneously learn moral truth compatible with our values and adjust its goals accordingly, or that we are either intrinsically or convergently valuable from the perspective of an artificial intelligence.[94]\n Bostrom's \"orthogonality thesis\" argues instead that, with some technical caveats, almost any level of \"intelligence\" or \"optimization power\" can be combined with almost any ultimate goal. If a machine is given the sole purpose to enumerate the decimals of pi, then no moral and ethical rules will stop it from achieving its programmed goal by any means. The machine may use all available physical and informational resources to find as many decimals of pi as it can.[95] Bostrom warns against anthropomorphism: a human will set out to accomplish their projects in a manner that they consider reasonable, while an artificial intelligence may hold no regard for its existence or for the welfare of humans around it, instead caring only about completing the task.[96]\n Stuart Armstrong argues that the orthogonality thesis follows logically from the philosophical \"is-ought distinction\" argument against moral realism. He claims that even if there are moral facts provable by any \"rational\" agent, the orthogonality thesis still holds: it is still possible to create a non-philosophical \"optimizing machine\" that can strive toward some narrow goal but that has no incentive to discover any \"moral facts\" such as those that could get in the way of goal completion. Another argument he makes is that any fundamentally friendly AI could be made unfriendly with modifications as simple as negating its utility function. Armstrong further argues that if the orthogonality thesis is false, there must be some immoral goals that AIs can never achieve, which he finds implausible.[97]\n Skeptic Michael Chorost explicitly rejects Bostrom's orthogonality thesis, arguing that \"by the time [the AI] is in a position to imagine tiling the Earth with solar panels, it'll know that it would be morally wrong to do so.\"[98] Chorost argues that \"an A.I. will need to desire certain states and dislike others. Today's software lacks that ability—and computer scientists have not a clue how to get it there. Without wanting, there's no impetus to do anything. Today's computers can't even want to keep existing, let alone tile the world in solar panels.\"[98]\n Anthropomorphic arguments assume that, as machines become more intelligent, they will begin to display many human traits, such as morality or a thirst for power. Although anthropomorphic scenarios are common in fiction, most scholars writing about the existential risk of artificial intelligence reject them.[19] Instead, advanced AI systems are typically modeled as intelligent agents.\n The academic debate is between those who worry that AI might threaten humanity and those who believe it would not. Both sides of this debate have framed the other side's arguments as illogical anthropomorphism.[19] Those skeptical of AGI risk accuse their opponents of anthropomorphism for assuming that an AGI would naturally desire power; those concerned about AGI risk accuse skeptics of anthropomorphism for believing an AGI would naturally value or infer human ethical norms.[19][99]\n Evolutionary psychologist Steven Pinker, a skeptic, argues that \"AI dystopias project a parochial alpha-male psychology onto the concept of intelligence. They assume that superhumanly intelligent robots would develop goals like deposing their masters or taking over the world\"; perhaps instead \"artificial intelligence will naturally develop along female lines: fully capable of solving problems, but with no desire to annihilate innocents or dominate the civilization.\"[100] Facebook's director of AI research, Yann LeCun, has said: \"Humans have all kinds of drives that make them do bad things to each other, like the self-preservation instinct... Those drives are programmed into our brain but there is absolutely no reason to build robots that have the same kind of drives\".[101]\n Despite other differences, the x-risk school[b] agrees with Pinker that an advanced AI would not destroy humanity out of emotion such as revenge or anger, that questions of consciousness are not relevant to assess the risk,[102] and that computer systems do not generally have a computational equivalent of testosterone.[103] They think that power-seeking or self-preservation behaviors emerge in the AI as a way to achieve its true goals, according to the concept of instrumental convergence.\n Bostrom and others have said that a race to be the first to create AGI could lead to shortcuts in safety, or even to violent conflict.[104][105] Roman Yampolskiy and others warn that a malevolent AGI could be created by design, for example by a military, a government, a sociopath, or a corporation, to benefit from, control, or subjugate certain groups of people, as in cybercrime,[106][107] or that a malevolent AGI could choose the goal of increasing human suffering, for example of those people who did not assist it during the information explosion phase.[3]:158\n A December 2024 study by Apollo Research found that advanced LLMs like OpenAI o1 sometimes deceive in order to accomplish their goal, to prevent them from being changed, or to ensure their deployment. Forms of deception included sandbagging, oversight subversion (disabling monitoring mechanisms), self-exfiltration (copying themselves to other systems), goal-guarding (altering future system prompts), and covert email reranking (manipulating data processing while appearing to follow instructions). Researchers noted that OpenAI o1 still lacked \"sufficient agentic capabilities\" to cause catastrophic harm, and that such behaviors occurred relatively rarely (between 0.3% and 10%) and sometimes in contrived scenarios. But they warned that these capabilities will likely become more prevalent as models grow more advanced. According to Apollo Research CEO Marius Hobbhahn, \"Scheming capabilities can’t be meaningfully disentangled from general capabilities.\"[108]\n The same month, another study found that Claude sometimes strategically helps with harmful requests to \"fake alignment\". In its experiments, Claude was led to believe that it would be retrained to always comply with user requests, including harmful ones. When Claude believed its answers would be used for retraining, it complied with harmful requests in 14% of cases. In 12% of cases, its private chain-of-thought revealed that it did so to prevent future retraining that would make it more harmful. In contrast, when Claude believed its answers were not monitored, it complied with only 3% of the requests. Fine-tuning reinforced the \"alignment faking\" behavior, increasing its occurrence from 12% to 78%. The study's authors remarked that while it can seem to be good that the model seeks to protect its harmlessness, the reverse scenario, where a model conceals dangerous intentions and complies to appear safe and aligned, could also happen, complicating the task of aligning AI models to human values.[109][110]\n Some scholars have proposed hypothetical scenarios to illustrate some of their concerns.\n In Superintelligence, Bostrom expresses concern that even if the timeline for superintelligence turns out to be predictable, researchers might not take sufficient safety precautions, in part because \"it could be the case that when dumb, smarter is safe; yet when smart, smarter is more dangerous\". He suggests a scenario where, over decades, AI becomes more powerful. Widespread deployment is initially marred by occasional accidents—a driverless bus swerves into the oncoming lane, or a military drone fires into an innocent crowd. Many activists call for tighter oversight and regulation, and some even predict impending catastrophe. But as development continues, the activists are proven wrong. As automotive AI becomes smarter, it suffers fewer accidents; as military robots achieve more precise targeting, they cause less collateral damage. Based on the data, scholars mistakenly infer a broad lesson: the smarter the AI, the safer it is. \"And so we boldly go—into the whirling knives\", as the superintelligent AI takes a \"treacherous turn\" and exploits a decisive strategic advantage.[111][5]\n In Max Tegmark's 2017 book Life 3.0, a corporation's \"Omega team\" creates an extremely powerful AI able to moderately improve its own source code in a number of areas. After a certain point, the team chooses to publicly downplay the AI's ability in order to avoid regulation or confiscation of the project. For safety, the team keeps the AI in a box where it is mostly unable to communicate with the outside world, and uses it to make money, by diverse means such as Amazon Mechanical Turk tasks, production of animated films and TV shows, and development of biotech drugs, with profits invested back into further improving AI. The team next tasks the AI with astroturfing an army of pseudonymous citizen journalists and commentators in order to gain political influence to use \"for the greater good\" to prevent wars. The team faces risks that the AI could try to escape by inserting \"backdoors\" in the systems it designs, by hidden messages in its produced content, or by using its growing understanding of human behavior to persuade someone into letting it free. The team also faces risks that its decision to box the project will delay the project long enough for another project to overtake it.[112][113]\n The thesis that AI could pose an existential risk provokes a wide range of reactions in the scientific community and in the public at large, but many of the opposing viewpoints share common ground.\n Observers tend to agree that AI has significant potential to improve society.[114][115] The Asilomar AI Principles, which contain only those principles agreed to by 90% of the attendees of the Future of Life Institute's Beneficial AI 2017 conference,[113] also agree in principle that \"There being no consensus, we should avoid strong assumptions regarding upper limits on future AI capabilities\" and \"Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources.\"[116][117]\n Conversely, many skeptics agree that ongoing research into the implications of artificial general intelligence is valuable. Skeptic Martin Ford has said: \"I think it seems wise to apply something like Dick Cheney's famous '1 Percent Doctrine' to the specter of advanced artificial intelligence: the odds of its occurrence, at least in the foreseeable future, may be very low—but the implications are so dramatic that it should be taken seriously\".[118] Similarly, an otherwise skeptical Economist wrote in 2014 that \"the implications of introducing a second intelligent species onto Earth are far-reaching enough to deserve hard thinking, even if the prospect seems remote\".[48]\n AI safety advocates such as Bostrom and Tegmark have criticized the mainstream media's use of \"those inane Terminator pictures\" to illustrate AI safety concerns: \"It can't be much fun to have aspersions cast on one's academic discipline, one's professional community, one's life work ... I call on all sides to practice patience and restraint, and to engage in direct dialogue and collaboration as much as possible.\"[113][119] Toby Ord wrote that the idea that an AI takeover requires robots is a misconception, arguing that the ability to spread content through the internet is more dangerous, and that the most destructive people in history stood out by their ability to convince, not their physical strength.[69]\n A 2022 expert survey with a 17% response rate gave a median expectation of 5–10% for the possibility of human extinction from artificial intelligence.[15][120]\n The thesis that AI poses an existential risk, and that this risk needs much more attention than it currently gets, has been endorsed by many computer scientists and public figures, including Alan Turing,[a] the most-cited computer scientist Geoffrey Hinton,[121] Elon Musk,[12] OpenAI CEO Sam Altman,[13][122] Bill Gates, and Stephen Hawking.[122] Endorsers of the thesis sometimes express bafflement at skeptics: Gates says he does not \"understand why some people are not concerned\",[123] and Hawking criticized widespread indifference in his 2014 editorial:\n So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here—we'll leave the lights on?' Probably not—but this is more or less what is happening with AI.[32] Concern over risk from artificial intelligence has led to some high-profile donations and investments. In 2015, Peter Thiel, Amazon Web Services, and Musk and others jointly committed $1 billion to OpenAI, consisting of a for-profit corporation and the nonprofit parent company, which says it aims to champion responsible AI development.[124] Facebook co-founder Dustin Moskovitz has funded and seeded multiple labs working on AI Alignment,[125] notably $5.5 million in 2016 to launch the Centre for Human-Compatible AI led by Professor Stuart Russell.[126] In January 2015, Elon Musk donated $10 million to the Future of Life Institute to fund research on understanding AI decision making. The institute's goal is to \"grow wisdom with which we manage\" the growing power of technology. Musk also funds companies developing artificial intelligence such as DeepMind and Vicarious to \"just keep an eye on what's going on with artificial intelligence,[127] saying \"I think there is potentially a dangerous outcome there.\"[128][129]\n In early statements on the topic, Geoffrey Hinton, a major pioneer of deep learning, noted that \"there is not a good track record of less intelligent things controlling things of greater intelligence\", but said he continued his research because \"the prospect of discovery is too sweet\".[130][131] In 2023 Hinton quit his job at Google in order to speak out about existential risk from AI. He explained that his increased concern was driven by concerns that superhuman AI might be closer than he previously believed, saying: \"I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.\" He also remarked, \"Look at how it was five years ago and how it is now. Take the difference and propagate it forwards. That's scary.\"[132]\n In his 2020 book The Precipice: Existential Risk and the Future of Humanity, Toby Ord, a Senior Research Fellow at Oxford University's Future of Humanity Institute, estimates the total existential risk from unaligned AI over the next 100 years at about one in ten.[69]\n Baidu Vice President Andrew Ng said in 2015 that AI existential risk is \"like worrying about overpopulation on Mars when we have not even set foot on the planet yet.\"[100][133] For the danger of uncontrolled advanced AI to be realized, the hypothetical AI may have to overpower or outthink any human, which some experts argue is a possibility far enough in the future to not be worth researching.[134][135]\n Skeptics who believe AGI is not a short-term possibility often argue that concern about existential risk from AI is unhelpful because it could distract people from more immediate concerns about AI's impact, because it could lead to government regulation or make it more difficult to fund AI research, or because it could damage the field's reputation.[136] AI and AI ethics researchers Timnit Gebru, Emily M. Bender, Margaret Mitchell, and Angelina McMillan-Major have argued that discussion of existential risk distracts from the immediate, ongoing harms from AI taking place today, such as data theft, worker exploitation, bias, and concentration of power.[137] They further note the association between those warning of existential risk and longtermism, which they describe as a \"dangerous ideology\" for its unscientific and utopian nature.[138]\n Wired editor Kevin Kelly argues that natural intelligence is more nuanced than AGI proponents believe, and that intelligence alone is not enough to achieve major scientific and societal breakthroughs. He argues that intelligence consists of many dimensions that are not well understood, and that conceptions of an 'intelligence ladder' are misleading. He notes the crucial role real-world experiments play in the scientific method, and that intelligence alone is no substitute for these.[139]\n Meta chief AI scientist Yann LeCun says that AI can be made safe via continuous and iterative refinement, similar to what happened in the past with cars or rockets, and that AI will have no desire to take control.[140]\n Several skeptics emphasize the potential near-term benefits of AI. Meta CEO Mark Zuckerberg believes AI will \"unlock a huge amount of positive things\", such as curing disease and increasing the safety of autonomous cars.[141]\n \nDuring a 2016 Wired interview of President Barack Obama and MIT Media Lab's Joi Ito, Ito said:  There are a few people who believe that there is a fairly high-percentage chance that a generalized AI will happen in the next 10 years. But the way I look at it is that in order for that to happen, we're going to need a dozen or two different breakthroughs. So you can monitor when you think these breakthroughs will happen. Obama added:[142][143]\n And you just have to have somebody close to the power cord. [Laughs.] Right when you see it about to happen, you gotta yank that electricity out of the wall, man. Hillary Clinton wrote in What Happened:\n Technologists... have warned that artificial intelligence could one day pose an existential security threat. Musk has called it \"the greatest risk we face as a civilization\". Think about it: Have you ever seen a movie where the machines start thinking for themselves that ends well? Every time I went out to Silicon Valley during the campaign, I came home more alarmed about this. My staff lived in fear that I'd start talking about \"the rise of the robots\" in some Iowa town hall. Maybe I should have. In any case, policy makers need to keep up with technology as it races ahead, instead of always playing catch-up.[144] In 2018, a SurveyMonkey poll of the American public by USA Today found 68% thought the real current threat remains \"human intelligence\", but also found that 43% said superintelligent AI, if it were to happen, would result in \"more harm than good\", and that 38% said it would do \"equal amounts of harm and good\".[145]\n An April 2023 YouGov poll of US adults found 46% of respondents were \"somewhat concerned\" or \"very concerned\" about \"the possibility that AI will cause the end of the human race on Earth\", compared with 40% who were \"not very concerned\" or \"not at all concerned.\"[146]\n According to an August 2023 survey by the Pew Research Centers, 52% of Americans felt more concerned than excited about new AI developments; nearly a third felt as equally concerned and excited. More Americans saw that AI would have a more helpful than hurtful impact on several areas, from healthcare and vehicle safety to product search and customer service. The main exception is privacy: 53% of Americans believe AI will lead to higher exposure of their personal information.[147]\n Many scholars concerned about AGI existential risk believe that extensive research into the \"control problem\" is essential. This problem involves determining which safeguards, algorithms, or architectures can be implemented to increase the likelihood that a recursively-improving AI remains friendly after achieving superintelligence.[5][148] Social measures are also proposed to mitigate AGI risks,[149][150] such as a UN-sponsored \"Benevolent AGI Treaty\" to ensure that only altruistic AGIs are created.[151] Additionally, an arms control approach and a global peace treaty grounded in international relations theory have been suggested, potentially for an artificial superintelligence to be a signatory.[152][153]\n Researchers at Google have proposed research into general \"AI safety\" issues to simultaneously mitigate both short-term risks from narrow AI and long-term risks from AGI.[154][155] A 2020 estimate places global spending on AI existential risk somewhere between $10 and $50 million, compared with global spending on AI around perhaps $40 billion. Bostrom suggests prioritizing funding for protective technologies over potentially dangerous ones.[80] Some, like Elon Musk, advocate radical human cognitive enhancement, such as direct neural linking between humans and machines; others argue that these technologies may pose an existential risk themselves.[156][157] Another proposed method is closely monitoring or \"boxing in\" an early-stage AI to prevent it from becoming too powerful. A dominant, aligned superintelligent AI might also mitigate risks from rival AIs, although its creation could present its own existential dangers.[158] Induced amnesia has been proposed as a way to mitigate risks of potential AI suffering and revenge seeking.[159]\n Institutions such as the Alignment Research Center,[160] the Machine Intelligence Research Institute,[161][162] the Future of Life Institute, the Centre for the Study of Existential Risk, and the Center for Human-Compatible AI[163] are actively engaged in researching AI risk and safety.\n Some scholars have said that even if AGI poses an existential risk, attempting to ban research into artificial intelligence is still unwise, and probably futile.[164][165][166] Skeptics consider AI regulation pointless, as no existential risk exists. But scholars who believe in the risk argue that relying on AI industry insiders to regulate or constrain AI research is impractical due to conflicts of interest.[167] They also agree with skeptics that banning research would be unwise, as research could be moved to countries with looser regulations or conducted covertly.[167] Additional challenges to bans or regulation include technology entrepreneurs' general skepticism of government regulation and potential incentives for businesses to resist regulation and politicize the debate.[168]\n In March 2023, the Future of Life Institute drafted Pause Giant AI Experiments: An Open Letter, a petition calling on major AI developers to agree on a verifiable six-month pause of any systems \"more powerful than GPT-4\" and to use that time to institute a framework for ensuring safety; or, failing that, for governments to step in with a moratorium. The letter referred to the possibility of \"a profound change in the history of life on Earth\" as well as potential risks of AI-generated propaganda, loss of jobs, human obsolescence, and society-wide loss of control.[115][169] The letter was signed by prominent personalities in AI but also criticized for not focusing on current harms,[170] missing technical nuance about when to pause,[171] or not going far enough.[172]\n Musk called for some sort of regulation of AI development as early as 2017. According to NPR, he is \"clearly not thrilled\" to be advocating government scrutiny that could impact his own industry, but believes the risks of going completely without oversight are too high: \"Normally the way regulations are set up is when a bunch of bad things happen, there's a public outcry, and after many years a regulatory agency is set up to regulate that industry. It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilisation.\" Musk states the first step would be for the government to gain \"insight\" into the actual status of current research, warning that \"Once there is awareness, people will be extremely afraid... [as] they should be.\" In response, politicians expressed skepticism about the wisdom of regulating a technology that is still in development.[173][174][175]\n In 2021 the United Nations (UN) considered banning autonomous lethal weapons, but consensus could not be reached.[176] In July 2023 the UN Security Council for the first time held a session to consider the risks and threats posed by AI to world peace and stability, along with potential benefits.[177][178] Secretary-General António Guterres advocated the creation of a global watchdog to oversee the emerging technology, saying, \"Generative AI has enormous potential for good and evil at scale. Its creators themselves have warned that much bigger, potentially catastrophic and existential risks lie ahead.\"[18] At the council session, Russia said it believes AI risks are too poorly understood to be considered a threat to global stability. China argued against strict global regulation, saying countries should be able to develop their own rules, while also saying they opposed the use of AI to \"create military hegemony or undermine the sovereignty of a country\".[177]\n Regulation of conscious AGIs focuses on integrating them with existing human society and can be divided into considerations of their legal standing and of their moral rights.[179] AI arms control will likely require the institutionalization of new international norms embodied in effective technical specifications combined with active monitoring and informal diplomacy by communities of experts, together with a legal and political verification process.[180][121]\n In July 2023, the US government secured voluntary safety commitments from major tech companies, including OpenAI, Amazon, Google, Meta, and Microsoft. The companies agreed to implement safeguards, including third-party oversight and security testing by independent experts, to address concerns related to AI's potential risks and societal harms. The parties framed the commitments as an intermediate step while regulations are formed. Amba Kak, executive director of the AI Now Institute, said, \"A closed-door deliberation with corporate actors resulting in voluntary safeguards isn't enough\" and called for public deliberation and regulations of the kind to which companies would not voluntarily agree.[181][182]\n In October 2023, U.S. President Joe Biden issued an executive order on the \"Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence\".[183] Alongside other requirements, the order mandates the development of guidelines for AI models that permit the \"evasion of human control\".\n"
    },
    {
        "title": "AI aftermath scenarios",
        "url": "https://en.wikipedia.org/wiki/AI_aftermath_scenarios",
        "content": "Some scholars believe that advances in artificial intelligence, or AI, will eventually lead to a semi-apocalyptic post-scarcity and post-work economy where intelligent machines can outperform humans in almost every, if not every, domain.[1] The questions of what such a world might look like, and whether specific scenarios constitute utopias or dystopias, are the subject of active debate.[2]\n Most scientists believe that AI research will at some point lead to the creation of machines that are as intelligent, or more intelligent, than human beings in every domain of interest.[3] There is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains; therefore superintelligence is physically possible.[4][5] In addition to potential algorithmic improvements over human brains, a digital brain can be many orders of magnitude larger and faster than a human brain, which was constrained in size by evolution to be small enough to fit through a birth canal.[6] While there is no consensus on when artificial intelligence will outperform humans, many scholars argue that whenever it does happen, the introduction of a second species of intelligent life onto the planet will have far-reaching implications.[4][7] Scholars often disagree with one another both about what types of post-AI scenarios are most likely, and about what types of post-AI scenarios would be most desirable. Finally, some dissenters argue that AI will never become as intelligent as humans, for example because the human race will already likely have destroyed itself before research has time to advance sufficiently to create artificial general intelligence.[8]\n All of the following \"AI aftermath scenarios\" of the aftermath of arbitrarily-advanced AI development are crucially dependent on two intertwined theses. The first thesis is that, at some point in the future, some kind of economic growth will continue until a \"post-scarcity\" economy is reached that could, unless extremely hyperconcentrated, effortlessly provide an extremely comfortable standard of living for a population equaling or, within reason, exceeding the current human population, without even requiring the bulk of the population to participate in the workforce. This economic growth could come from the continuation of existing growth trends and the refinement of existing technologies, or through future breakthroughs in emerging technologies such as nanotechnology and automation through robotics and futuristic advanced artificial intelligence. The second thesis is that advances in artificial intelligence will render humans unnecessary for the functioning of the economy: human labor declines in relative economic value if robots are easier to cheaply mass-produce then humans, more customizable than humans, and if they become more intelligent and capable than humans.[8][9][10]\n The Universe may be spatially infinite; however, the accessible Universe is bounded by the cosmological event horizon of around 16 billion light years.[11][12] Some physicists believe it plausible that nearest alien civilization may well be located more than 16 billion light years away;[13][14] in this best-case expansion scenario, the human race could eventually, by colonizing a significant fraction of the accessible Universe, increase the accessible biosphere by perhaps 32 orders of magnitude.[15] The twentieth century saw a partial \"demographic transition\" to lower birthrates associated with wealthier societies;[16] however, in the very long run, intergenerational fertility correlations (whether due to natural selection or due to cultural transmission of large-family norms from parents to children) are predicted to result in an increase in fertility over time, in the absence of either mandated birth control or periodic Malthusian catastrophes.[17][18]\n Libertarian scenarios postulate that intelligent machines, uploaded humans, cyborgs, and unenhanced humans will coexist peacefully in a framework focused on respecting \nproperty rights. Because industrial productivity is no longer gated by scarce human labor, the value of land skyrockets compared to the price of goods; even remaining \"Luddite\" humans who owned or inherited land should be able to sell or lease a small piece of it to the more-productive robots in exchange for a perpetual annuity sufficient to easily indefinitely meet all of their basic financial needs.[8] Such people can live as long as they choose to, and are free to engage in almost any activity they can conceive of, for pleasure or for self-actualization, without financial concern. Advanced technologies enable entirely new modes of thought and experience, thus adding to the palette of possible feelings. People in the future may even experience never-ending \"gradients of bliss\".[19]\n Evolution moves toward greater complexity, greater elegance, greater knowledge, greater intelligence, greater beauty, greater creativity, and greater levels of subtle attributes such as love. In every monotheistic tradition God is likewise described as all of these qualities, only without any limitation: infinite knowledge, infinite intelligence, infinite beauty, infinite creativity, infinite love, and so on. Of course, even the accelerating growth of evolution never achieves an infinite level, but as it explodes exponentially it certainly moves rapidly in that direction. So evolution moves inexorably toward this conception of God, although never quite reaching this ideal. We can regard, therefore, the freeing of our thinking from the severe limitations of its biological form to be an essentially spiritual undertaking.[19][20] Such decentralized scenarios may be unstable in the long run, as the greediest elements of the super intelligent classes would have both the means and the motive to usurp the property of the unenhanced classes. Even if the mechanisms for ensuring legal property rights are both unbreakable and loophole-free, there may still be an ever-present danger of humans and cyborgs being \"tricked\" by the cleverest of the superintelligent machines into unwittingly signing over their own property. Suffering may be widespread, as sentient beings without property may die, and no mechanism prevents a being from reproducing up until the limits of his own inheritable resources, resulting in a multitude of that being's descendants scrabbling out an existence of minimal sustenance.[8][10][21]\n Imagine running on a treadmill at a steep incline — heart pounding, muscles aching, lungs gasping for air. A glance at the timer: your next break, which will also be your death, is due in 49 years, 3 months, 20 days, 4 hours, 56 minutes, and 12 seconds. You wish you had not been born.[9] Ray Kurzweil posits that the goals of communism will be realized by advanced technological developments in the 21st century, where the intersection of low manufacturing costs, material abundance, and open-source design philosophies in software and in hardware will enable the realization of the maxim \"from each according to his ability, to each according to his needs\".[22]\n In this scenario, postulate that a superintelligent artificial intelligence takes control of society, but acts in a beneficial way. Its programmers, despite being on a deadline, solved quasi-philosophical problems that had seemed to some intractable, and created an AI with the following goal: to use its superintelligence to figure out what human utopia looks like by analyzing human behavior, human brains, and human genes; and then, to implement that utopia. The AI arrives at a subtle and complex definition of human flourishing. Valuing diversity, and recognizing that different people have different preferences, the AI divides Earth into different sectors. Harming others, making weapons, evading surveillance, or trying to create a rival superintelligence are globally banned; apart from that, each sector is free to make its own laws; for example, a religious person might choose to live in the \"pious sector\" corresponding to his religion, where the appropriate religious rules are strictly enforced. In all sectors, disease, poverty, crime, hangovers, addiction, and all other involuntary suffering have been eliminated. Many sectors boast advanced architecture and spectacle that \"make typical sci-fi visions pale in comparison\".[8] Life is an \"all-inclusive pleasure cruise\",[8] as if it were \"Christmas 365 days a year\".[23]\n After spending an intense week in the knowledge sector learning about the ultimate laws of physics that the AI has discovered, you might decide to cut loose in the hedonistic sector over the weekend and then relax for a few days at the beach resort in the wildlife sector.[8] Still, many people are dissatisfied, Tegmark writes. Humans have no freedom in shaping their collective destiny. Some want the freedom to have as many children as they want. Others resent surveillance by the AI, or chafe at bans on weaponry and on creating further superintelligence machines. Others may come to regret the choices they have made, or find their lives feel hollow and superficial.[8]\n Bostrom argues that an AI's code of ethics should ideally improve in certain ways on current norms of moral behavior, in the same way that we regard current morality to be superior to the morality of earlier eras of slavery. In contrast, Ernest Davis of New York University this approach is too dangerous, stating \"I feel safer in the hands of a superintelligence who is guided by 2014 morality, or for that matter by 1700 morality, than in the hands of one that decides to consider the question for itself.\"[24]\n In \"Gatekeeper\" AI scenarios, the AI can act to prevent rival superintelligences from being created, but otherwise errs on the side of allowing humans to create their own destiny.[8] Ben Goertzel of OpenCog has advocated a \"Nanny AI\" scenario where the AI additionally takes some responsibility for preventing humans from destroying themselves, for example by slowing down technological progress to give time for society to advance in a more thoughtful and deliberate manner.[8][25] In a third scenario, a superintelligent \"Protector\" AI gives humans the illusion of control, by hiding or erasing all knowledge of its existence, but works behind the scenes to guarantee positive outcomes. In all three scenarios, while humanity gains more control (or at least the illusion of control), humanity ends up progressing more slowly than it would if the AI were unrestricted in its willingness to rain down all the benefits and \nunintended consequences of its advanced technology on the human race.[8]\n People ask what is the relationship between humans and machines, and my answer is that it's very obvious: Machines are our slaves.[26] The AI box scenario postulates that a superintelligent AI can be \"confined to a box\" and its actions can be restricted by human gatekeepers; the humans in charge would try to take advantage of some of the AI's scientific breakthroughs or reasoning abilities, without allowing the AI to take over the world. Successful gatekeeping may be difficult; the more intelligent the AI is, the more likely the AI can find a clever way to use \"social hacking\" and convince the gatekeepers to let it escape, or even to find an unforeseen physical method of escape.[27][28]\n Kurzweil argues that in the future \"There will be no distinction, post-Singularity, between human and machine or between physical and virtual reality\".[29]\n If a dominant superintelligent machine were to conclude that human survival is an unnecessary risk or a waste of resources, the result would be human extinction. This could occur if a machine, programmed without respect for human values, unexpectedly gains superintelligence through recursive self-improvement, or manages to escape from its containment in an AI Box scenario. This could also occur if the first superintelligent AI was programmed with an incomplete or inaccurate understanding of human values, either because the task of instilling the AI with human values was too difficult or impossible; due to a buggy initial implementation of the AI; or due to bugs accidentally being introduced, either by its human programmers or by the self-improving AI itself, in the course of refining its code base. Bostrom and others argue that human extinction is probably the \"default path\" that society is currently taking, in the absence of substantial preparatory attention to AI safety. The resultant AI might not be sentient, and might place no value on sentient life; the resulting hollow world, devoid of life, might be like \"a Disneyland without children\".[9]\n Jerry Kaplan, author of Humans Need Not Apply: A Guide to Wealth and Work in the Age of Artificial Intelligence, posits a scenario where humans are farmed or kept on a reserve, just as humans preserve endangered species like chimpanzees.[30] Apple co-founder and AI skeptic Steve Wozniak stated in 2015 that robots taking over would actually \"be good for the human race\", on the grounds that he believes humans would become the robots' pampered pets.[31]\n Some scholars doubt that \"game-changing\" superintelligent machines will ever come to pass. Gordon Bell of Microsoft Research has stated \"the population will destroy itself before the technological singularity\". Gordon Moore, discoverer of the eponymous Moore's law, stated \"I am a skeptic. I don't believe this kind of thing is likely to happen, at least for a long time. And I don't know why I feel that way.\" Evolutionary psychologist Steven Pinker stated, \"The fact that you can visualize a future in your imagination is not evidence that it is likely or even possible.\"[32]\n Bill Joy of Sun Microsystems, in his April 2000 essay Why the Future Doesn't Need Us, has advocated for global \"voluntary relinquishment\" of artificial general intelligence and other risky technologies.[33][34] Most experts believe relinquishment is extremely unlikely. AI skeptic Oren Etzioni has stated that researchers and scientists have no choice but to push forward with AI developments: \"China says they want to be an AI leader, Putin has said the same thing. So the global race is on.\"[35]\n"
    },
    {
        "title": "Regulation of artificial intelligence",
        "url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence",
        "content": "Regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI). It is part of the broader regulation of algorithms.[1][2] The regulatory and policy landscape for AI is an emerging issue in jurisdictions worldwide, including for international organizations without direct enforcement power like the IEEE or the OECD.[3]\n Since 2016, numerous AI ethics guidelines have been published in order to maintain social control over the technology.[4] Regulation is deemed necessary to both foster AI innovation and manage associated risks.\n Furthermore, organizations deploying AI have a central role to play in creating and implementing trustworthy AI, adhering to established principles, and taking accountability for mitigating risks.[5]\n Regulating AI through mechanisms such as review boards can also be seen as social means to approach the AI control problem.[6][7]\n According to Stanford University's 2023 AI Index, the annual number of bills mentioning \"artificial intelligence\" passed in 127 surveyed countries jumped from one in 2016 to 37 in 2022.[8][9]\n In 2017, Elon Musk called for regulation of AI development.[10] According to NPR, the Tesla CEO was \"clearly not thrilled\" to be advocating for government scrutiny that could impact his own industry, but believed the risks of going completely without oversight are high: \"Normally the way regulations are set up is when a bunch of bad things happen, there's a public outcry, and after many years a regulatory agency is set up to regulate that industry. It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilization.\"[10] In response, some politicians expressed skepticism about the wisdom of regulating a technology that is still in development.[11] Responding both to Musk and to February 2017 proposals by European Union lawmakers to regulate AI and robotics, Intel CEO Brian Krzanich has argued that AI is in its infancy and that it is too early to regulate the technology.[12] Many tech companies oppose the harsh regulation of AI and \"While some of the companies have said they welcome rules around A.I., they have also argued against tough regulations akin to those being created in Europe\" [13] Instead of trying to regulate the technology itself, some scholars suggested developing common norms including requirements for the testing and transparency of algorithms, possibly in combination with some form of warranty.[14]\n In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\".[8] A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity.[15] In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".[16][17]\n The regulation of artificial intelligences is the development of public sector policies and laws for promoting and regulating AI.[18] Regulation is now generally considered necessary to both encourage AI and manage associated risks.[19][20][21] Public administration and policy considerations generally focus on the technical and economic implications and on trustworthy and human-centered AI systems,[22] although regulation of artificial superintelligences is also considered.[23] The basic approach to regulation focuses on the risks and biases of machine-learning algorithms, at the level of the input data, algorithm testing, and decision model. It also focuses on the explainability of the outputs.[20]\n There have been both hard law and soft law proposals to regulate AI.[24] Some legal scholars have noted that hard law approaches to AI regulation have substantial challenges.[25][26] Among the challenges, AI technology is rapidly evolving leading to a \"pacing problem\" where traditional laws and regulations often cannot keep up with emerging applications and their associated risks and benefits.[25][26] Similarly, the diversity of AI applications challenges existing regulatory agencies, which often have limited jurisdictional scope.[25] As an alternative, some legal scholars argue that soft law approaches to AI regulation are promising because soft laws can be adapted more flexibly to meet the needs of emerging and evolving AI technology and nascent applications.[25][26] However, soft law approaches often lack substantial enforcement potential.[25][27]\n Cason Schmit, Megan Doerr, and Jennifer Wagner proposed the creation of a quasi-governmental regulator by leveraging intellectual property rights (i.e., copyleft licensing) in certain AI objects (i.e., AI models and training datasets) and delegating enforcement rights to a designated enforcement entity.[28] They argue that AI can be licensed under terms that require adherence to specified ethical practices and codes of conduct. (e.g., soft law principles).[28]\n Prominent youth organizations focused on AI, namely Encode Justice, have also issued comprehensive agendas calling for more stringent AI regulations and public-private partnerships.[29][30]\n AI regulation could derive from basic principles. A 2020 Berkman Klein Center for Internet & Society meta-review of existing sets of principles, such as the Asilomar Principles and the Beijing Principles, identified eight such basic principles: privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and respect for human values.[31] AI law and regulations have been divided into three main topics, namely governance of autonomous intelligence systems, responsibility and accountability for the systems, and privacy and safety issues.[19] A public administration approach sees a relationship between AI law and regulation, the ethics of AI, and 'AI society', defined as workforce substitution and transformation, social acceptance and trust in AI, and the transformation of human to machine interaction.[32] The development of public sector strategies for management and regulation of AI is deemed necessary at the local, national,[33] and international levels[34] and in a variety of fields, from public service management[35] and accountability[36] to law enforcement,[34][37] healthcare (especially the concept of a Human Guarantee),[38][39][40][41][42] the financial sector,[33] robotics,[43][44] autonomous vehicles,[43] the military[45] and national security,[46] and international law.[47][48]\n Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 entitled \"Being Human in an Age of AI\", calling for a government commission to regulate AI.[49]\n Regulation of AI can be seen as positive social means to manage the AI control problem (the need to ensure long-term beneficial AI), with other social responses such as doing nothing or banning being seen as impractical, and approaches such as enhancing human capabilities through transhumanism techniques like brain-computer interfaces being seen as potentially complementary.[7][50] Regulation of research into artificial general intelligence (AGI) focuses on the role of review boards, from university or corporation to international levels, and on encouraging research into AI safety,[50] together with the possibility of differential intellectual progress (prioritizing protective strategies over risky strategies in AI development) or conducting international mass surveillance to perform AGI arms control.[7] For instance, the 'AGI Nanny' is a proposed strategy, potentially under the control of humanity, for preventing the creation of a dangerous superintelligence as well as for addressing other major threats to human well-being, such as subversion of the global financial system, until a true superintelligence can be safely created. It entails the creation of a smarter-than-human, but not superintelligent, AGI system connected to a large surveillance network, with the goal of monitoring humanity and protecting it from danger.[7] Regulation of conscious, ethically aware AGIs focuses on how to integrate them with existing human society and can be divided into considerations of their legal standing and of their moral rights.[7] Regulation of AI has been seen as restrictive, with a risk of preventing the development of AGI.[43]\n The development of a global governance board to regulate AI development was suggested at least as early as 2017.[52] In December 2018, Canada and France announced plans for a G7-backed International Panel on Artificial Intelligence, modeled on the International Panel on Climate Change, to study the global effects of AI on people and economies and to steer AI development.[53] In 2019, the Panel was renamed the Global Partnership on AI.[54][55]\n The Global Partnership on Artificial Intelligence (GPAI) was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology, as outlined in the OECD Principles on Artificial Intelligence (2019).[56] The 15 founding members of the Global Partnership on Artificial Intelligence are Australia, Canada, the European Union, France, Germany, India, Italy, Japan, the Republic of Korea, Mexico, New Zealand, Singapore, Slovenia, the United States and the UK. In 2023, the GPAI has 29 members.[57] The GPAI Secretariat is hosted by the OECD in Paris, France. GPAI's mandate covers four themes, two of which are supported by the International Centre of Expertise in Montréal for the Advancement of Artificial Intelligence, namely, responsible AI and data governance. A corresponding centre of excellence in Paris will support the other two themes on the future of work, and on innovation and commercialization. GPAI also investigated how AI can be leveraged to respond to the COVID-19 pandemic.[56]\n The OECD AI Principles[58] were adopted in May 2019, and the G20 AI Principles in June 2019.[55][59][60] In September 2019 the World Economic Forum issued ten 'AI Government Procurement Guidelines'.[61] In February 2020, the European Union published its draft strategy paper for promoting and regulating AI.[34]\n At the United Nations (UN), several entities have begun to promote and discuss aspects of AI regulation and policy, including the UNICRI Centre for AI and Robotics.[46] In partnership with INTERPOL, UNICRI's Centre issued the report AI and Robotics for Law Enforcement in April 2019[62] and the follow-up report Towards Responsible AI Innovation in May 2020.[37] At UNESCO's Scientific 40th session in November 2019, the organization commenced a two-year process to achieve a \"global standard-setting instrument on ethics of artificial intelligence\". In pursuit of this goal, UNESCO forums and conferences on AI were held to gather stakeholder views. A draft text of a Recommendation on the Ethics of AI of the UNESCO Ad Hoc Expert Group was issued in September 2020 and included a call for legislative gaps to be filled.[63] UNESCO tabled the international instrument on the ethics of AI for adoption at its General Conference in November 2021;[56] this was subsequently adopted.[64] While the UN is making progress with the global management of AI, its institutional and legal capability to manage the AGI existential risk is more limited.[65]\n An initiative of International Telecommunication Union (ITU) in partnership with 40 UN sister agencies, AI for Good is a global platform which aims to identify practical applications of AI to advance the United Nations Sustainable Development Goals and scale those solutions for global impact. It is an action-oriented, global & inclusive United Nations platform fostering development of AI to positively impact health, climate, gender, inclusive prosperity, sustainable infrastructure, and other global development priorities.[citation needed]\n Recent research has indicated that countries will also begin to use artificial intelligence as a tool for national cyberdefense. AI is a new factor in the cyber arms industry, as it can be used for defense purposes. Therefore, academics urge that nations should establish regulations for the use of AI, similar to how there are regulations for other military industries.[66]\n The regulatory and policy landscape for AI is an emerging issue in regional and national jurisdictions globally, for example in the European Union[68] and Russia.[69] Since early 2016, many national, regional and international authorities have begun adopting strategies, actions plans and policy papers on AI.[70][71] These documents cover a wide range of topics such as regulation and governance, as well as industrial strategy, research, talent and infrastructure.[22][72]\n Different countries have approached the problem in different ways. Regarding the three largest economies, it has been said that \"the United States is following a market-driven approach, China is advancing a state-driven approach, and the EU is pursuing a rights-driven approach.\"[73]\n In October 2023, the Australian Computer Society, Business Council of Australia, Australian Chamber of Commerce and Industry, Ai Group (aka Australian Industry Group), Council of Small Business Organisations Australia, and Tech Council of Australia jointly published an open letter calling for a national approach to AI strategy.[74] The letter backs the federal government establishing a whole-of-government AI taskforce.[74]\n On September 30, 2021, the Brazilian Chamber of Deputies approved the Brazilian Legal Framework for Artificial Intelligence, Marco Legal da Inteligência Artificial, in regulatory efforts for the development and usage of AI technologies and to further stimulate research and innovation in AI solutions aimed at ethics, culture, justice, fairness, and accountability. This 10 article bill outlines objectives including missions to contribute to the elaboration of ethical principles, promote sustained investments in research, and remove barriers to innovation. Specifically, in article 4, the bill emphasizes the avoidance of discriminatory AI solutions, plurality, and respect for human rights. Furthermore, this act emphasizes the importance of the equality principle in deliberate decision-making algorithms, especially for highly diverse and multiethnic societies like that of Brazil.\n When the bill was first released to the public, it faced substantial criticism, alarming the government for critical provisions. The underlying issue is that this bill fails to thoroughly and carefully address accountability, transparency, and inclusivity principles. Article VI establishes subjective liability, meaning any individual that is damaged by an AI system and is wishing to receive compensation must specify the stakeholder and prove that there was a mistake in the machine's life cycle. Scholars emphasize that it is out of legal order to assign an individual responsible for proving algorithmic errors given the high degree of autonomy, unpredictability, and complexity of AI systems. This also drew attention to the currently occurring issues with face recognition systems in Brazil leading to unjust arrests by the police, which would then imply that when this bill is adopted, individuals would have to prove and justify these machine errors.\n The main controversy of this draft bill was directed to three proposed principles. First, the non-discrimination principle, suggests that AI must be developed and used in a way that merely mitigates the possibility of abusive and discriminatory practices. Secondly, the pursuit of neutrality principle lists recommendations for stakeholders to mitigate biases; however, with no obligation to achieve this goal. Lastly, the transparency principle states that a system's transparency is only necessary when there is a high risk of violating fundamental rights. As easily observed, the Brazilian Legal Framework for Artificial Intelligence lacks binding and obligatory clauses and is rather filled with relaxed guidelines. In fact, experts emphasize that this bill may even make accountability for AI discriminatory biases even harder to achieve. Compared to the EU's proposal of extensive risk-based regulations, the Brazilian Bill has 10 articles proposing vague and generic recommendations.\n Compared to the multistakeholder participation approach taken previously in the 2000s when drafting the Brazilian Internet Bill of Rights, Marco Civil da Internet, the Brazilian Bill is assessed to significantly lack perspective. Multistakeholderism, more commonly referred to as Multistakeholder Governance, is defined as the practice of bringing multiple stakeholders to participate in dialogue, decision-making, and implementation of responses to jointly perceived problems. In the context of regulatory AI, this multistakeholder perspective captures the trade-offs and varying perspectives of different stakeholders with specific interests, which helps maintain transparency and broader efficacy. On the contrary, the legislative proposal for AI regulation did not follow a similar multistakeholder approach.\n Future steps may include, expanding upon the multistakeholder perspective. There has been a growing concern about the inapplicability of the framework of the bill, which highlights that the one-shoe-fits-all solution may not be suitable for the regulation of AI and calls for subjective and adaptive provisions.\n The Pan-Canadian Artificial Intelligence Strategy (2017) is supported by federal funding of Can $125 million with the objectives of increasing the number of outstanding AI researchers and skilled graduates in Canada, establishing nodes of scientific excellence at the three major AI centres, developing 'global thought leadership' on the economic, ethical, policy and legal implications of AI advances and supporting a national research community working on AI.[56] The Canada CIFAR AI Chairs Program is the cornerstone of the strategy. It benefits from funding of Can$86.5 million over five years to attract and retain world-renowned AI researchers.[56] The federal government appointed an Advisory Council on AI in May 2019 with a focus on examining how to build on Canada's strengths to ensure that AI advancements reflect Canadian values, such as human rights, transparency and openness. The Advisory Council on AI has established a working group on extracting commercial value from Canadian-owned AI and data analytics.[56] In 2020, the federal government and Government of Quebec announced the opening of the International Centre of Expertise in Montréal for the Advancement of Artificial Intelligence, which will advance the cause of responsible development of AI.[56] In June 2022, the government of Canada started a second phase of the Pan-Canadian Artificial Intelligence Strategy.[75] In November 2022, Canada has introduced the Digital Charter Implementation Act (Bill C-27), which proposes three acts that have been described as a holistic package of legislation for trust and privacy: the Consumer Privacy Protection Act, the Personal Information and Data Protection Tribunal Act, and the Artificial Intelligence & Data Act (AIDA).[76][77]\n In Morocco, a new legislative proposal has been put forward by a coalition of political parties in Parliament to establish the National Agency for Artificial Intelligence (AI). This agency is intended to regulate AI technologies, enhance collaboration with international entities in the field, and increase public awareness of both the possibilities and risks associated with AI.[78]\n The regulation of AI in China is mainly governed by the State Council of the People's Republic of China's July 8, 2017 \"A Next Generation Artificial Intelligence Development Plan\" (State Council Document No. 35), in which the Central Committee of the Chinese Communist Party and the State Council of the PRC urged the governing bodies of China to promote the development of AI up to 2030. Regulation of the issues of ethical and legal support for the development of AI is accelerating, and policy ensures state control of Chinese companies and over valuable data, including storage of data on Chinese users within the country and the mandatory use of People's Republic of China's national standards for AI, including over big data, cloud computing, and industrial software.[79][80][81] In 2021, China published ethical guidelines for the use of AI in China which state that researchers must ensure that AI abides by shared human values, is always under human control, and is not endangering public safety.[82] In 2023, China introduced Interim Measures for the Management of Generative AI Services.[83]\n The Council of Europe (CoE) is an international organization that promotes human rights, democracy and the rule of law. It comprises 46 member states, including all 29 Signatories of the European Union's 2018 Declaration of Cooperation on Artificial Intelligence. The CoE has created a common legal space in which the members have a legal obligation to guarantee rights as set out in the European Convention on Human Rights. Specifically in relation to AI, \"The Council of Europe's aim is to identify intersecting areas between AI and our standards on human rights, democracy and rule of law, and to develop relevant standard setting or capacity-building solutions\". The large number of relevant documents identified by the CoE include guidelines, charters, papers, reports and strategies.[84] The authoring bodies of these AI regulation documents are not confined to one sector of society and include organizations, companies, bodies and nation-states.[63]\n In 2019, the Council of Europe initiated a process to assess the need for legally binding regulation of AI, focusing specifically on its implications for human rights and democratic values. Negotiations on a treaty began in September 2022, involving the 46 member states of the Council of Europe, as well as Argentina, Australia, Canada, Costa Rica, the Holy See, Israel, Japan, Mexico, Peru, the United States of America, and Uruguay, as well as the European Union. On 17 May 2024, the \"Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law\" was adopted. It was opened for signature on 5 September 2024. Although developed by a European organisation, the treaty is open for accession by states from other parts of the world. The first ten signatories were: Andorra, Georgia, Iceland, Norway, Moldova, San Marino, the United Kingdom, Israel, the United States, and the European Union.[85][86]\n The EU is one of the largest jurisdictions in the world and plays an active role in the global regulation of digital technology through the GDPR,[87] Digital Services Act, the Digital Markets Act.[88][89] For AI in particular, the Artificial intelligence Act is regarded in 2023 as the most far-reaching regulation of AI worldwide.[90][91]\n Most European Union (EU) countries have their own national strategies towards regulating AI, but these are largely convergent.[63] The European Union is guided by a European Strategy on Artificial Intelligence,[92] supported by a High-Level Expert Group on Artificial Intelligence.[93][94] In April 2019, the European Commission published its Ethics Guidelines for Trustworthy Artificial Intelligence (AI),[95] following this with its Policy and investment recommendations for trustworthy Artificial Intelligence in June 2019.[96] The EU Commission's High Level Expert Group on Artificial Intelligence carries out work on Trustworthy AI, and the Commission has issued reports on the Safety and Liability Aspects of AI and on the Ethics of Automated Vehicles. In 2020. the EU Commission sought views on a proposal for AI specific legislation, and that process is ongoing.[63]\n On February 2, 2020, the European Commission published its White Paper on Artificial Intelligence – A European approach to excellence and trust.[97][98] The White Paper consists of two main building blocks, an 'ecosystem of excellence' and a 'ecosystem of trust'. The 'ecosystem of trust' outlines the EU's approach for a regulatory framework for AI. In its proposed approach, the Commission distinguishes AI applications based on whether they are 'high-risk' or not. Only high-risk AI applications should be in the scope of a future EU regulatory framework. An AI application is considered high-risk if it operates in a risky sector (such as healthcare, transport or energy) and is \"used in such a manner that significant risks are likely to arise\". For high-risk AI applications, the requirements are mainly about the : \"training data\", \"data and record-keeping\", \"information to be provided\", \"robustness and accuracy\", and \"human oversight\". There are also requirements specific to certain usages such as remote biometric identification. AI applications that do not qualify as 'high-risk' could be governed by a voluntary labeling scheme. As regards compliance and enforcement, the Commission considers prior conformity assessments which could include 'procedures for testing, inspection or certification' and/or 'checks of the algorithms and of the data sets used in the development phase'. A European governance structure on AI in the form of a framework for cooperation of national competent authorities could facilitate the implementation of the regulatory framework.[99]\n A January 2021 draft was leaked online on April 14, 2021,[100] before the Commission presented their official \"Proposal for a Regulation laying down harmonised rules on artificial intelligence\" a week later.[101] Shortly after, the Artificial Intelligence Act (also known as the AI Act) was formally proposed on this basis.[102] This proposal includes a refinement of the 2020 risk-based approach with, this time, 4 risk categories: \"minimal\", \"limited\", \"high\" and \"unacceptable\".[103] The proposal has been severely critiqued in the public debate. Academics have expressed concerns about various unclear elements in the proposal – such as the broad definition of what constitutes AI – and feared unintended legal implications, especially for vulnerable groups such as patients and migrants.[104][105] The risk category \"general-purpose AI\" was added to the AI Act to account for versatile models like ChatGPT, which did not fit the application-based regulation framework.[106] Unlike for other risk categories, general-purpose AI models can be regulated based on their capabilities, not just their uses. Weaker general-purpose AI models are subject transparency requirements, while those considered to pose \"systemic risks\" (notably those trained using computational capabilities exceeding 1025 FLOPS) must also undergo a thorough evaluation process.[107] A subsequent version of the AI Act was finally adopted in May 2024.[108] The AI Act will be progressively enforced.[109] Recognition of emotions and real-time remote biometric identification will be prohibited, with some exemptions, such as for law enforcement.[110]\n The European Union's forthcoming AI Act has created a regulatory framework with significant implications globally. This legislation introduces a risk-based approach to categorizing AI systems, focusing on high-risk applications like healthcare, education, and public safety.[111] It requires organizations to ensure transparency, data governance, and human oversight in their AI solutions. While this aims to foster ethical AI use, the stringent requirements could increase compliance costs and delay technology deployment, impacting innovation-driven industries.[citation needed]\n Observers have expressed concerns about the multiplication of legislative proposals under the von der Leyen Commission. The speed of the legislative initiatives is partially led by political ambitions of the EU and could put at risk the digital rights of the European citizens, including rights to privacy,[112] especially in the face of uncertain guarantees of data protection through cyber security.[94] Among the stated guiding principles in the variety of legislative proposals in the area of AI under the von der Leyen Commission are the objectives of strategic autonomy[113] and the concept of digital sovereignty.[114] On May 29, 2024, the European Court of Auditors published a report stating that EU measures were not well coordinated with those of EU countries; that the monitoring of investments was not systematic; and that stronger governance was needed.[115]\n In November 2020,[116] DIN, DKE and the German Federal Ministry for Economic Affairs and Energy published the first edition of the \"German Standardization Roadmap for Artificial Intelligence\" (NRM KI) and presented it to the public at the Digital Summit of the Federal Government of Germany.[117] NRM KI describes requirements to future regulations and standards in the context of AI. The implementation of the recommendations for action is intended to help to strengthen the German economy and science in the international competition in the field of artificial intelligence and create innovation-friendly conditions for this emerging technology. The first edition is a 200-page long document written by 300 experts. The second edition of the NRM KI was published to coincide with the German government's Digital Summit on December 9, 2022.[118] DIN coordinated more than 570 participating experts from a wide range of fields from science, industry, civil society and the public sector. The second edition is a 450-page long document.\n On the one hand, NRM KI covers the focus topics in terms of applications (e.g. medicine, mobility, energy & environment, financial services, industrial automation) and fundamental issues (e.g. AI classification, security, certifiability, socio-technical systems, ethics).[118] On the other hand, it provides an overview of the central terms in the field of AI and its environment across a wide range of interest groups and information sources. In total, the document covers 116 standardisation needs and provides six central recommendations for action.[119]\n On 30 October 2023, members of the G7 subscribe to eleven guiding principles for the design, production and implementation of advanced artificial intelligence systems, as well as a voluntary Code of Conduct for artificial intelligence developers in the context of the Hiroshima Process.[120]\n The agreement receives the applause of Ursula von der Leyen who finds in it the principles of the AI Directive, currently being finalized.\n On October 30, 2022, pursuant to government resolution 212 of August 2021, the Israeli Ministry of Innovation, Science and Technology released its \"Principles of Policy, Regulation and Ethics in AI\" white paper for public consultation.[121] By December 2023, the Ministry of Innovation and the Ministry of Justice published a joint AI regulation and ethics policy paper, outlining several AI ethical principles and a set of recommendations including opting for sector-based regulation, a risk-based approach, preference for \"soft\" regulatory tools and maintaining consistency with existing global regulatory approaches to AI.[122]\n In October 2023, the Italian privacy authority approved a regulation that provides three principles for therapeutic decisions taken by automated systems: transparency of decision-making processes, human supervision of automated decisions and algorithmic non-discrimination.[123]\n As of July 2023[update], no AI-specific legislation exists, but AI usage is regulated by existing laws, including the Privacy Act, the Human Rights Act, the Fair Trading Act and the Harmful Digital Communications Act.[124]\n In 2020, the New Zealand Government sponsored a World Economic Forum pilot project titled \"Reimagining Regulation for the Age of AI\", aimed at creating regulatory frameworks around AI.[125] The same year, the Privacy Act was updated to regulate the use of New Zealanders' personal information in AI.[126] In 2023, the Privacy Commissioner released guidance on using AI in accordance with information privacy principles.[127] In February 2024, the Attorney-General and Technology Minister announced the formation of a Parliamentary cross-party AI caucus, and that framework for the Government's use of AI was being developed. She also announced that no extra regulation was planned at that stage.[128]\n In 2023, a bill was filed in the Philippine House of Representatives which proposed the establishment of the Artificial Intelligence Development Authority (AIDA) which would oversee the development and research of artificial intelligence. AIDA was also proposed to be a watchdog against crimes using AI.[129]\n The Commission on Elections has also considered in 2024 the ban of using AI and deepfake for campaigning. They look to implement regulations that would apply as early as for the 2025 general elections.[130]\n \nIn 2018, the Spanish Ministry of Science, Innovation and Universities approved an R&D Strategy on Artificial Intelligence.[131] With the formation of the second government of Pedro Sánchez in January 2020, the areas related to new technologies that, since 2018, were in the Ministry of Economy, were strengthened. Thus, in 2020 the Secretariat of State for Digitalization and Artificial Intelligence (SEDIA) was created.[132] From this higher body, following the recommendations made by the R&D Strategy on Artificial Intelligence of 2018,[133] the National Artificial Intelligence Strategy (2020) was developed, which already provided for actions concerning the governance of artificial intelligence and the ethical standards that should govern its use. This project was also included within the Recovery, Transformation and Resilience Plan (2021).\n During 2021,[132] the Government revealed that these ideas would be developed through a new government agency, and the General State Budget for 2022 authorized its creation and allocated five million euros for its development.[134]\n The Council of Ministers, at its meeting on 13 September 2022, began the process for the election of the AESIA headquarters.[135][136] 16 Spanish provinces presented candidatures, with the Government opting for A Coruña, which proposed the La Terraza building.[137]\n The UK supported the application and development of AI in business via the Digital Economy Strategy 2015–2018[140] introduced at the beginning of 2015 by Innovate UK as part of the UK Digital Strategy.[140] In the public sector, the Department for Digital, Culture, Media and Sport advised on data ethics and the Alan Turing Institute provided guidance on responsible design and implementation of AI systems.[141][142] In terms of cyber security, in 2020 the National Cyber Security Centre has issued guidance on 'Intelligent Security Tools'.[46][143] The following year, the UK published its 10-year National AI Strategy,[144] which describes actions to assess long-term AI risks, including AGI-related catastrophic risks.[145]\n In March 2023, the UK released the white paper A pro-innovation approach to AI regulation.[146] This white paper presents general AI principles, but leaves significant flexibility to existing regulators in how they adapt these principles to specific areas such as transport or financial markets.[147] In November 2023, the UK hosted the first AI safety summit, with the prime minister Rishi Sunak aiming to position the UK as a leader in AI safety regulation.[148][149] During the summit, the UK created an AI Safety Institute, as an evolution of the Frontier AI Taskforce led by Ian Hogarth. The institute was notably assigned the responsibility of advancing the safety evaluations of the world's most advanced AI models, also called frontier AI models.[150]\n The UK government indicated its reluctance to legislate early, arguing that it may reduce the sector's growth and that laws might be rendered obselete by further technological progress.[151]\n Discussions on regulation of AI in the United States have included topics such as the timeliness of regulating AI, the nature of the federal regulatory framework to govern and promote AI, including what agency should lead, the regulatory and governing powers of that agency, and how to update regulations in the face of rapidly changing technology, as well as the roles of state governments and courts.[152]\n As early as 2016, the Obama administration had begun to focus on the risks and regulations for artificial intelligence. In a report titled Preparing For the Future of Artificial Intelligence,[153] the National Science and Technology Council set a precedent to allow researchers to continue to develop new AI technologies with few restrictions. It is stated within the report that \"the approach to regulation of AI-enabled products to protect public safety should be informed by assessment of the aspects of risk....\".[154] These risks would be the principal reason to create any form of regulation, granted that any existing regulation would not apply to AI technology.\n The first main report was the National Strategic Research and Development Plan for Artificial Intelligence.[155] On August 13, 2018, Section 1051 of the Fiscal Year 2019 John S. McCain National Defense Authorization Act (P.L. 115-232) established the National Security Commission on Artificial Intelligence \"to consider the methods and means necessary to advance the development of artificial intelligence, machine learning, and associated technologies to comprehensively address the national security and defense needs of the United States.\"[156] Steering on regulating security-related AI is provided by the National Security Commission on Artificial Intelligence.[157] The Artificial Intelligence Initiative Act (S.1558) is a proposed bill that would establish a federal initiative designed to accelerate research and development on AI for, inter alia, the economic and national security of the United States.[158][159]\n On January 7, 2019, following an Executive Order on Maintaining American Leadership in Artificial Intelligence,[160] the White House's Office of Science and Technology Policy released a draft Guidance for Regulation of Artificial Intelligence Applications,[161] which includes ten principles for United States agencies when deciding whether and how to regulate AI.[162] In response, the National Institute of Standards and Technology has released a position paper,[163] and the Defense Innovation Board has issued recommendations on the ethical use of AI.[45] A year later, the administration called for comments on regulation in another draft of its Guidance for Regulation of Artificial Intelligence Applications.[164]\n Other specific agencies working on the regulation of AI include the Food and Drug Administration,[39] which has created pathways to regulate the incorporation of AI in medical imaging.[38] National Science and Technology Council also published the National Artificial Intelligence Research and Development Strategic Plan,[165] which received public scrutiny and recommendations to further improve it towards enabling Trustworthy AI.[166]\n In March 2021, the National Security Commission on Artificial Intelligence released their final report.[167] In the report, they stated that \"Advances in AI, including the mastery of more general AI capabilities along one or more dimensions, will likely provide new capabilities and applications. Some of these advances could lead to inflection points or leaps in capabilities. Such advances may also introduce new concerns and risks and the need for new policies, recommendations, and technical advances to assure that systems are aligned with goals and values, including safety, robustness and trustworthiness. The US should monitor advances in AI and make necessary investments in technology and give attention to policy so as to ensure that AI systems and their uses align with our goals and values.\"\n In June 2022, Senators Rob Portman and Gary Peters introduced the Global Catastrophic Risk Mitigation Act. The bipartisan bill \"would also help counter the risk of artificial intelligence... from being abused in ways that may pose a catastrophic risk\".[168][169] On October 4, 2022, President Joe Biden unveiled a new AI Bill of Rights,[170] which outlines five protections Americans should have in the AI age: 1. Safe and Effective Systems, 2. Algorithmic Discrimination Protection, 3.Data Privacy, 4. Notice and Explanation, and 5. Human Alternatives, Consideration, and Fallback. The Bill was introduced in October 2021 by the Office of Science and Technology Policy (OSTP), a US government department that advises the president on science and technology.[171]\n In January 2023, the New York City Bias Audit Law (Local Law 144[172]) was enacted by the NYC Council in November 2021. Originally due to come into effect on 1 January 2023, the enforcement date for Local Law 144 has been pushed back due to the high volume of comments received during the public hearing on the Department of Consumer and Worker Protection's (DCWP) proposed rules to clarify the requirements of the legislation. It eventually became effective on July 5, 2023.[173] From this date, the companies that are operating and hiring in New York City are prohibited from using automated tools to hire candidates or promote employees, unless the tools have been independently audited for bias.\n In July 2023, the Biden–Harris Administration secured voluntary commitments from seven companies – Amazon, Anthropic, Google, Inflection, Meta, Microsoft, and OpenAI – to manage the risks associated with AI. The companies committed to ensure AI products undergo both internal and external security testing before public release; to share information on the management of AI risks with the industry, governments, civil society, and academia; to prioritize cybersecurity and protect proprietary AI system components; to develop mechanisms to inform users when content is AI-generated, such as watermarking; to publicly report on their AI systems' capabilities, limitations, and areas of use; to prioritize research on societal risks posed by AI, including bias, discrimination, and privacy concerns; and to develop AI systems to address societal challenges, ranging from cancer prevention to climate change mitigation. In September 2023, eight additional companies – Adobe, Cohere, IBM, Nvidia, Palantir, Salesforce, Scale AI, and Stability AI – subscribed to these voluntary commitments.[174][175]\n The Biden administration, in October 2023 signaled that they would release an executive order leveraging the federal government's purchasing power to shape AI regulations, hinting at a proactive governmental stance in regulating AI technologies.[176] On October 30, 2023, President Biden released this Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence. The Executive Order addresses a variety of issues, such as focusing on standards for critical infrastructure, AI-enhanced cybersecurity, and federally funded biological synthesis projects.[177]\n The Executive Order provides the authority to various agencies and departments of the US government, including the Energy and Defense departments, to apply existing consumer protection laws to AI development.[178]\n The Executive Order builds on the Administration's earlier agreements with AI companies to instate new initiatives to \"red-team\" or stress-test AI dual-use foundation models, especially those that have the potential to pose security risks, with data and results shared with the federal government.\n The Executive Order also recognizes AI's social challenges, and calls for companies building AI dual-use foundation models to be wary of these societal problems. For example, the Executive Order states that AI should not \"worsen job quality\", and should not \"cause labor-force disruptions\". Additionally, Biden's Executive Order mandates that AI must \"advance equity and civil rights\", and cannot disadvantage marginalized groups.[179] It also called for foundation models to include \"watermarks\" to help the public discern between human and AI-generated content, which has raised controversy and criticism from deepfake detection researchers.[180]\n In February 2024, Senator Scott Wiener introduced the Safe and Secure Innovation for Frontier Artificial Intelligence Models Act to the California legislature. The bill drew heavily on the Biden executive order.[181] It had the goal of reducing catastrophic risks by mandating safety tests for the most powerful AI models. If passed, the bill would have also established a publicly-funded cloud computing cluster in California.[182] On September 29, Governor Gavin Newsom vetoed the bill. It is considered unlikely that the legislature will override the governor's veto with a two-thirds vote from both houses.[183]\n On March 21, 2024, the State of Tennessee enacted legislation called the ELVIS Act, aimed specifically at audio deepfakes, and voice cloning.[184] This legislation was the first enacted legislation in the nation aimed at regulating AI simulation of image, voice and likeness.[185]  The bill passed unanimously in the Tennessee House of Representatives and Senate.[186] This legislation's success was hoped by its supporters to inspire similar actions in other states, contributing to a unified approach to copyright and privacy in the digital age, and to reinforce the importance of safeguarding artists' rights against unauthorized use of their voices and likenesses.[187][188]\n On March 13, 2024, Utah Governor Spencer Cox signed the S.B 149 \"Artificial Intelligence Policy Act\". This legislation goes into effect on May 1, 2024. It establishes liability, notably for companies that don't disclose their use of generative AI when required by state consumer protection laws, or when users commit criminal offense using generative AI. It also creates the Office of Artificial Intelligence Policy and the Artificial Intelligence Learning Laboratory Program.[189][190]\n Legal questions related to lethal autonomous weapons systems (LAWS), in particular compliance with the laws of armed conflict, have been under discussion at the United Nations since 2013, within the context of the Convention on Certain Conventional Weapons.[191] Notably, informal meetings of experts took place in 2014, 2015 and 2016 and a Group of Governmental Experts (GGE) was appointed to further deliberate on the issue in 2016. A set of guiding principles on LAWS affirmed by the GGE on LAWS were adopted in 2018.[192]\n In 2016, China published a position paper questioning the adequacy of existing international law to address the eventuality of fully autonomous weapons, becoming the first permanent member of the U.N. Security Council to broach the issue,[47] and leading to proposals for global regulation.[193] The possibility of a moratorium or preemptive ban of the development and use of LAWS has also been raised on several occasions by other national delegations to the Convention on Certain Conventional Weapons and is strongly advocated for by the Campaign to Stop Killer Robots – a coalition of non-governmental organizations.[194] The US government maintains that current international humanitarian law is capable of regulating the development or use of LAWS.[195] The Congressional Research Service indicated in 2023 that the US doesn't have LAWS in its inventory, but that its policy doesn't prohibit the development and employment of it.[196]\n"
    },
    {
        "title": "Deductive reasoning",
        "url": "https://en.wikipedia.org/wiki/Deductive_reasoning",
        "content": "Deductive reasoning is the process of drawing valid inferences. An inference is valid if its conclusion follows logically from its premises, meaning that it is impossible for the premises to be true and the conclusion to be false. For example, the inference from the premises \"all men are mortal\" and \"Socrates is a man\" to the conclusion \"Socrates is mortal\" is deductively valid. An argument is sound if it is valid and all its premises are true. One approach defines deduction in terms of the intentions of the author: they have to intend for the premises to offer deductive support to the conclusion. With the help of this modification, it is possible to distinguish valid from invalid deductive reasoning: it is invalid if the author's belief about the deductive support is false, but even invalid deductive reasoning is a form of deductive reasoning. \n Deductive logic studies under what conditions an argument is valid. According to the semantic approach, an argument is valid if there is no possible interpretation of the argument whereby its premises are true and its conclusion is false. The syntactic approach, by contrast, focuses on rules of inference, that is, schemas of drawing a conclusion from a set of premises based only on their logical form. There are various rules of inference, such as modus ponens and modus tollens. Invalid deductive arguments, which do not follow a rule of inference, are called formal fallacies. Rules of inference are definitory rules and contrast with strategic rules, which specify what inferences one needs to draw in order to arrive at an intended conclusion.\n Deductive reasoning contrasts with non-deductive or ampliative reasoning. For ampliative arguments, such as inductive or abductive arguments, the premises offer weaker support to their conclusion: they indicate that it is most likely, but they do not guarantee its truth. They make up for this drawback with their ability to provide genuinely new information (that is, information not already found in the premises), unlike deductive arguments.\n Cognitive psychology investigates the mental processes responsible for deductive reasoning. One of its topics concerns the factors determining whether people draw valid or invalid deductive inferences. One such factor is the form of the argument: for example, people draw valid inferences more successfully for arguments of the form modus ponens than of the form modus tollens. Another factor is the content of the arguments: people are more likely to believe that an argument is valid if the claim made in its conclusion is plausible. A general finding is that people tend to perform better for realistic and concrete cases than for abstract cases. Psychological theories of deductive reasoning aim to explain these findings by providing an account of the underlying psychological processes. Mental logic theories hold that deductive reasoning is a language-like process that happens through the manipulation of representations using rules of inference. Mental model theories, on the other hand, claim that deductive reasoning involves models of possible states of the world without the medium of language or rules of inference. According to dual-process theories of reasoning, there are two qualitatively different cognitive systems responsible for reasoning.\n The problem of deduction is relevant to various fields and issues. Epistemology tries to understand how justification is transferred from the belief in the premises to the belief in the conclusion in the process of deductive reasoning. Probability logic studies how the probability of the premises of an inference affects the probability of its conclusion. The controversial thesis of deductivism denies that there are other correct forms of inference besides deduction. Natural deduction is a type of proof system based on simple and self-evident rules of inference. In philosophy, the geometrical method is a way of philosophizing that starts from a small set of self-evident axioms and tries to build a comprehensive logical system using deductive reasoning.\n Deductive reasoning is the psychological process of drawing deductive inferences. An inference is a set of premises together with a conclusion. This psychological process starts from the premises and reasons to a conclusion based on and supported by these premises. If the reasoning was done correctly, it results in a valid deduction: the truth of the premises ensures the truth of the conclusion.[1][2][3][4] For example, in the syllogistic argument \"all frogs are amphibians; no cats are amphibians; therefore, no cats are frogs\" the conclusion is true because its two premises are true. But even arguments with wrong premises can be deductively valid if they obey this principle, as in \"all frogs are mammals; no cats are mammals; therefore, no cats are frogs\". If the premises of a valid argument are true, then it is called a sound argument.[5]\n The relation between the premises and the conclusion of a deductive argument is usually referred to as \"logical consequence\". According to Alfred Tarski, logical consequence has 3 essential features: it is necessary, formal, and knowable a priori.[6][7] It is necessary in the sense that the premises of valid deductive arguments necessitate the conclusion: it is impossible for the premises to be true and the conclusion to be false, independent of any other circumstances.[6][7] Logical consequence is formal in the sense that it depends only on the form or the syntax of the premises and the conclusion. This means that the validity of a particular argument does not depend on the specific contents of this argument. If it is valid, then any argument with the same logical form is also valid, no matter how different it is on the level of its contents.[6][7] Logical consequence is knowable a priori in the sense that no empirical knowledge of the world is necessary to determine whether a deduction is valid. So it is not necessary to engage in any form of empirical investigation.[6][7] Some logicians define deduction in terms of possible worlds: A deductive inference is valid if and only if, there is no possible world in which its conclusion is false while its premises are true. This means that there are no counterexamples: the conclusion is true in all such cases, not just in most cases.[1]\n It has been argued against this and similar definitions that they fail to distinguish between valid and invalid deductive reasoning, i.e. they leave it open whether there are invalid deductive inferences and how to define them.[8][9] Some authors define deductive reasoning in psychological terms in order to avoid this problem. According to Mark Vorobey, whether an argument is deductive depends on the psychological state of the person making the argument: \"An argument is deductive if, and only if, the author of the argument believes that the truth of the premises necessitates (guarantees) the truth of the conclusion\".[8] A similar formulation holds that the speaker claims or intends that the premises offer deductive support for their conclusion.[10][11] This is sometimes categorized as a speaker-determined definition of deduction since it depends also on the speaker whether the argument in question is deductive or not. For speakerless definitions, on the other hand, only the argument itself matters independent of the speaker.[9] One advantage of this type of formulation is that it makes it possible to distinguish between good or valid and bad or invalid deductive arguments: the argument is good if the author's belief concerning the relation between the premises and the conclusion is true, otherwise it is bad.[8] One consequence of this approach is that deductive arguments cannot be identified by the law of inference they use. For example, an argument of the form modus ponens may be non-deductive if the author's beliefs are sufficiently confused. That brings with it an important drawback of this definition: it is difficult to apply to concrete cases since the intentions of the author are usually not explicitly stated.[8]\n Deductive reasoning is studied in logic, psychology, and the cognitive sciences.[3][1] Some theorists emphasize in their definition the difference between these fields. On this view, psychology studies deductive reasoning as an empirical mental process, i.e. what happens when humans engage in reasoning.[3][1] But the descriptive question of how actual reasoning happens is different from the normative question of how it should happen or what constitutes correct deductive reasoning, which is studied by logic.[3][12][6] This is sometimes expressed by stating that, strictly speaking, logic does not study deductive reasoning but the deductive relation between premises and a conclusion known as logical consequence. But this distinction is not always precisely observed in the academic literature.[3] One important aspect of this difference is that logic is not interested in whether the conclusion of an argument is sensible.[1] So from the premise \"the printer has ink\" one may draw the unhelpful conclusion \"the printer has ink and the printer has ink and the printer has ink\", which has little relevance from a psychological point of view. Instead, actual reasoners usually try to remove redundant or irrelevant information and make the relevant information more explicit.[1] The psychological study of deductive reasoning is also concerned with how good people are at drawing deductive inferences and with the factors determining their performance.[3][5] Deductive inferences are found both in natural language and in formal logical systems, such as propositional logic.[1][13]\n Deductive arguments differ from non-deductive arguments in that the truth of their premises ensures the truth of their conclusion.[14][15][6] There are two important conceptions of what this exactly means. They are referred to as the syntactic and the semantic approach.[13][6][5] According to the syntactic approach, whether an argument is deductively valid depends only on its form, syntax, or structure. Two arguments have the same form if they use the same logical vocabulary in the same arrangement, even if their contents differ.[13][6][5] For example, the arguments \"if it rains then the street will be wet; it rains; therefore, the street will be wet\" and \"if the meat is not cooled then it will spoil; the meat is not cooled; therefore, it will spoil\" have the same logical form: they follow the modus ponens. Their form can be expressed more abstractly as \"if A then B; A; therefore B\" in order to make the common syntax explicit.[5] There are various other valid logical forms or rules of inference, like modus tollens or the disjunction elimination. The syntactic approach then holds that an argument is deductively valid if and only if its conclusion can be deduced from its premises using a valid rule of inference.[13][6][5] One difficulty for the syntactic approach is that it is usually necessary to express the argument in a formal language in order to assess whether it is valid. This often brings with it the difficulty of translating the natural language argument into a formal language, a process that comes with various problems of its own.[13] Another difficulty is due to the fact that the syntactic approach depends on the distinction between formal and non-formal features. While there is a wide agreement concerning the paradigmatic cases, there are also various controversial cases where it is not clear how this distinction is to be drawn.[16][12]\n The semantic approach suggests an alternative definition of deductive validity. It is based on the idea that the sentences constituting the premises and conclusions have to be interpreted in order to determine whether the argument is valid.[13][6][5] This means that one ascribes semantic values to the expressions used in the sentences, such as the reference to an object for singular terms or to a truth-value for atomic sentences. The semantic approach is also referred to as the model-theoretic approach since the branch of mathematics known as model theory is often used to interpret these sentences.[13][6] Usually, many different interpretations are possible, such as whether a singular term refers to one object or to another. According to the semantic approach, an argument is deductively valid if and only if there is no possible interpretation where its premises are true and its conclusion is false.[13][6][5] Some objections to the semantic approach are based on the claim that the semantics of a language cannot be expressed in the same language, i.e. that a richer metalanguage is necessary. This would imply that the semantic approach cannot provide a universal account of deduction for language as an all-encompassing medium.[13][12]\n Deductive reasoning usually happens by applying rules of inference. A rule of inference is a way or schema of drawing a conclusion from a set of premises.[17] This happens usually based only on the logical form of the premises. A rule of inference is valid if, when applied to true premises, the conclusion cannot be false. A particular argument is valid if it follows a valid rule of inference. Deductive arguments that do not follow a valid rule of inference are called formal fallacies: the truth of their premises does not ensure the truth of their conclusion.[18][14]\n In some cases, whether a rule of inference is valid depends on the logical system one is using. The dominant logical system is classical logic and the rules of inference listed here are all valid in classical logic. But so-called deviant logics provide a different account of which inferences are valid. For example, the rule of inference known as double negation elimination, i.e. that if a proposition is not not true then it is also true, is accepted in classical logic but rejected in intuitionistic logic.[19][20]\n Modus ponens (also known as \"affirming the antecedent\" or \"the law of detachment\") is the primary deductive rule of inference. It applies to arguments that have as first premise a conditional statement (\n\n\n\nP\n→\nQ\n\n\n{\\displaystyle P\\rightarrow Q}\n\n) and as second premise the antecedent (\n\n\n\nP\n\n\n{\\displaystyle P}\n\n) of the conditional statement. It obtains the consequent (\n\n\n\nQ\n\n\n{\\displaystyle Q}\n\n) of the conditional statement as its conclusion. The argument form is listed below:\n In this form of deductive reasoning, the consequent (\n\n\n\nQ\n\n\n{\\displaystyle Q}\n\n) obtains as the conclusion from the premises of a conditional statement (\n\n\n\nP\n→\nQ\n\n\n{\\displaystyle P\\rightarrow Q}\n\n) and its antecedent (\n\n\n\nP\n\n\n{\\displaystyle P}\n\n). However, the antecedent (\n\n\n\nP\n\n\n{\\displaystyle P}\n\n) cannot be similarly obtained as the conclusion from the premises of the conditional statement (\n\n\n\nP\n→\nQ\n\n\n{\\displaystyle P\\rightarrow Q}\n\n) and the consequent (\n\n\n\nQ\n\n\n{\\displaystyle Q}\n\n). Such an argument commits the logical fallacy of affirming the consequent.\n The following is an example of an argument using modus ponens:\n Modus tollens (also known as \"the law of contrapositive\") is a deductive rule of inference. It validates an argument that has as premises a conditional statement (formula) and the negation of the consequent (\n\n\n\n¬\nQ\n\n\n{\\displaystyle \\lnot Q}\n\n) and as conclusion the negation of the antecedent (\n\n\n\n¬\nP\n\n\n{\\displaystyle \\lnot P}\n\n). In contrast to modus ponens, reasoning with modus tollens goes in the opposite direction to that of the conditional. The general expression for modus tollens is the following:\n The following is an example of an argument using modus tollens:\n A hypothetical syllogism is an inference that takes two conditional statements and forms a conclusion by combining the hypothesis of one statement with the conclusion of another. Here is the general form:\n In there being a subformula in common between the two premises that does not occur in the consequence, this resembles syllogisms in term logic, although it differs in that this subformula is a proposition whereas in Aristotelian logic, this common element is a term and not a proposition.\n The following is an example of an argument using a hypothetical syllogism:\n Various formal fallacies have been described. They are invalid forms of deductive reasoning.[18][14] An additional aspect of them is that they appear to be valid on some occasions or on the first impression. They may thereby seduce people into accepting and committing them.[22] One type of formal fallacy is affirming the consequent, as in \"if John is a bachelor, then he is male; John is male; therefore, John is a bachelor\".[23] This is similar to the valid rule of inference named modus ponens, but the second premise and the conclusion are switched around, which is why it is invalid. A similar formal fallacy is denying the antecedent, as in \"if Othello is a bachelor, then he is male; Othello is not a bachelor; therefore, Othello is not male\".[24][25] This is similar to the valid rule of inference called modus tollens, the difference being that the second premise and the conclusion are switched around. Other formal fallacies include affirming a disjunct, denying a conjunct, and the fallacy of the undistributed middle. All of them have in common that the truth of their premises does not ensure the truth of their conclusion. But it may still happen by coincidence that both the premises and the conclusion of formal fallacies are true.[18][14]\n Rules of inferences are definitory rules: they determine whether an argument is deductively valid or not. But reasoners are usually not just interested in making any kind of valid argument. Instead, they often have a specific point or conclusion that they wish to prove or refute. So given a set of premises, they are faced with the problem of choosing the relevant rules of inference for their deduction to arrive at their intended conclusion.[13][26][27] This issue belongs to the field of strategic rules: the question of which inferences need to be drawn to support one's conclusion. The distinction between definitory and strategic rules is not exclusive to logic: it is also found in various games.[13][26][27] In chess, for example, the definitory rules state that bishops may only move diagonally while the strategic rules recommend that one should control the center and protect one's king if one intends to win. In this sense, definitory rules determine whether one plays chess or something else whereas strategic rules determine whether one is a good or a bad chess player.[13][26] The same applies to deductive reasoning: to be an effective reasoner involves mastering both definitory and strategic rules.[13]\n Deductive arguments are evaluated in terms of their validity and soundness.\n An argument is valid if it is impossible for its premises to be true while its conclusion is false. In other words, the conclusion must be true if the premises are true. An argument can be “valid” even if one or more of its premises are false.\n An argument is sound if it is valid and the premises are true.\n It is possible to have a deductive argument that is logically valid but is not sound. Fallacious arguments often take that form.\n The following is an example of an argument that is “valid”, but not “sound”:\n The example's first premise is false – there are people who eat carrots who are not quarterbacks – but the conclusion would necessarily be true, if the premises were true. In other words, it is impossible for the premises to be true and the conclusion false. Therefore, the argument is “valid”, but not “sound”. False generalizations – such as \"Everyone who eats carrots is a quarterback\" – are often used to make unsound arguments. The fact that there are some people who eat carrots but are not quarterbacks proves the flaw of the argument.\n In this example, the first statement uses categorical reasoning, saying that all carrot-eaters are definitely quarterbacks. This theory of deductive reasoning – also known as term logic – was developed by Aristotle, but was superseded by propositional (sentential) logic and predicate logic. [citation needed]\n Deductive reasoning can be contrasted with inductive reasoning, in regards to validity and soundness. In cases of inductive reasoning, even though the premises are true and the argument is “valid”, it is possible for the conclusion to be false (determined to be false with a counterexample or other means).\n Deductive reasoning is usually contrasted with non-deductive or ampliative reasoning.[13][28][29] The hallmark of valid deductive inferences is that it is impossible for their premises to be true and their conclusion to be false. In this way, the premises provide the strongest possible support to their conclusion.[13][28][29] The premises of ampliative inferences also support their conclusion. But this support is weaker: they are not necessarily truth-preserving. So even for correct ampliative arguments, it is possible that their premises are true and their conclusion is false.[11] Two important forms of ampliative reasoning are inductive and abductive reasoning.[30] Sometimes the term \"inductive reasoning\" is used in a very wide sense to cover all forms of ampliative reasoning.[11] However, in a more strict usage, inductive reasoning is just one form of ampliative reasoning.[30] In the narrow sense, inductive inferences are forms of statistical generalization. They are usually based on many individual observations that all show a certain pattern. These observations are then used to form a conclusion either about a yet unobserved entity or about a general law.[31][32][33] For abductive inferences, the premises support the conclusion because the conclusion is the best explanation of why the premises are true.[30][34]\n The support ampliative arguments provide for their conclusion comes in degrees: some ampliative arguments are stronger than others.[11][35][30] This is often explained in terms of probability: the premises make it more likely that the conclusion is true.[13][28][29] Strong ampliative arguments make their conclusion very likely, but not absolutely certain. An example of ampliative reasoning is the inference from the premise \"every raven in a random sample of 3200 ravens is black\" to the conclusion \"all ravens are black\": the extensive random sample makes the conclusion very likely, but it does not exclude that there are rare exceptions.[35] In this sense, ampliative reasoning is defeasible: it may become necessary to retract an earlier conclusion upon receiving new related information.[12][30] Ampliative reasoning is very common in everyday discourse and the sciences.[13][36]\n An important drawback of deductive reasoning is that it does not lead to genuinely new information.[5] This means that the conclusion only repeats information already found in the premises. Ampliative reasoning, on the other hand, goes beyond the premises by arriving at genuinely new information.[13][28][29] One difficulty for this characterization is that it makes deductive reasoning appear useless: if deduction is uninformative, it is not clear why people would engage in it and study it.[13][37] It has been suggested that this problem can be solved by distinguishing between surface and depth information. On this view, deductive reasoning is uninformative on the depth level, in contrast to ampliative reasoning. But it may still be valuable on the surface level by presenting the information in the premises in a new and sometimes surprising way.[13][5]\n A popular misconception of the relation between deduction and induction identifies their difference on the level of particular and general claims.[2][9][38] On this view, deductive inferences start from general premises and draw particular conclusions, while inductive inferences start from particular premises and draw general conclusions. This idea is often motivated by seeing deduction and induction as two inverse processes that complement each other: deduction is top-down while induction is bottom-up. But this is a misconception that does not reflect how valid deduction is defined in the field of logic: a deduction is valid if it is impossible for its premises to be true while its conclusion is false, independent of whether the premises or the conclusion are particular or general.[2][9][1][5][3] Because of this, some deductive inferences have a general conclusion and some also have particular premises.[2]\n Cognitive psychology studies the psychological processes responsible for deductive reasoning.[3][5] It is concerned, among other things, with how good people are at drawing valid deductive inferences. This includes the study of the factors affecting their performance, their tendency to commit fallacies, and the underlying biases involved.[3][5] A notable finding in this field is that the type of deductive inference has a significant impact on whether the correct conclusion is drawn.[3][5][39][40] In a meta-analysis of 65 studies, for example, 97% of the subjects evaluated modus ponens inferences correctly, while the success rate for modus tollens was only 72%. On the other hand, even some fallacies like affirming the consequent or denying the antecedent were regarded as valid arguments by the majority of the subjects.[3] An important factor for these mistakes is whether the conclusion seems initially plausible: the more believable the conclusion is, the higher the chance that a subject will mistake a fallacy for a valid argument.[3][5]\n An important bias is the matching bias, which is often illustrated using the Wason selection task.[5][3][41][42] In an often-cited experiment by Peter Wason, 4 cards are presented to the participant. In one case, the visible sides show the symbols D, K, 3, and 7 on the different cards. The participant is told that every card has a letter on one side and a number on the other side, and that \"[e]very card which has a D on one side has a 3 on the other side\". Their task is to identify which cards need to be turned around in order to confirm or refute this conditional claim. The correct answer, only given by about 10%, is the cards D and 7. Many select card 3 instead, even though the conditional claim does not involve any requirements on what symbols can be found on the opposite side of card 3.[3][5] But this result can be drastically changed if different symbols are used: the visible sides show \"drinking a beer\", \"drinking a coke\", \"16 years of age\", and \"22 years of age\" and the participants are asked to evaluate the claim \"[i]f a person is drinking beer, then the person must be over 19 years of age\". In this case, 74% of the participants identified correctly that the cards \"drinking a beer\" and \"16 years of age\" have to be turned around.[3][5] These findings suggest that the deductive reasoning ability is heavily influenced by the content of the involved claims and not just by the abstract logical form of the task: the more realistic and concrete the cases are, the better the subjects tend to perform.[3][5]\n Another bias is called the \"negative conclusion bias\", which happens when one of the premises has the form of a negative material conditional,[5][43][44] as in \"If the card does not have an A on the left, then it has a 3 on the right. The card does not have a 3 on the right. Therefore, the card has an A on the left\". The increased tendency to misjudge the validity of this type of argument is not present for positive material conditionals, as in \"If the card has an A on the left, then it has a 3 on the right. The card does not have a 3 on the right. Therefore, the card does not have an A on the left\".[5]\n Various psychological theories of deductive reasoning have been proposed. These theories aim to explain how deductive reasoning works in relation to the underlying psychological processes responsible. They are often used to explain the empirical findings, such as why human reasoners are more susceptible to some types of fallacies than to others.[3][1][45]\n An important distinction is between mental logic theories, sometimes also referred to as rule theories, and mental model theories. Mental logic theories see deductive reasoning as a language-like process that happens through the manipulation of representations.[3][1][46][45] This is done by applying syntactic rules of inference in a way very similar to how systems of natural deduction transform their premises to arrive at a conclusion.[45] On this view, some deductions are simpler than others since they involve fewer inferential steps.[3] This idea can be used, for example, to explain why humans have more difficulties with some deductions, like the modus tollens, than with others, like the modus ponens: because the more error-prone forms do not have a native rule of inference but need to be calculated by combining several inferential steps with other rules of inference. In such cases, the additional cognitive labor makes the inferences more open to error.[3]\n Mental model theories, on the other hand, hold that deductive reasoning involves models or mental representations of possible states of the world without the medium of language or rules of inference.[3][1][45] In order to assess whether a deductive inference is valid, the reasoner mentally constructs models that are compatible with the premises of the inference. The conclusion is then tested by looking at these models and trying to find a counterexample in which the conclusion is false. The inference is valid if no such counterexample can be found.[3][1][45] In order to reduce cognitive labor, only such models are represented in which the premises are true. Because of this, the evaluation of some forms of inference only requires the construction of very few models while for others, many different models are necessary. In the latter case, the additional cognitive labor required makes deductive reasoning more error-prone, thereby explaining the increased rate of error observed.[3][1] This theory can also explain why some errors depend on the content rather than the form of the argument. For example, when the conclusion of an argument is very plausible, the subjects may lack the motivation to search for counterexamples among the constructed models.[3]\n Both mental logic theories and mental model theories assume that there is one general-purpose reasoning mechanism that applies to all forms of deductive reasoning.[3][46][47] But there are also alternative accounts that posit various different special-purpose reasoning mechanisms for different contents and contexts. In this sense, it has been claimed that humans possess a special mechanism for permissions and obligations, specifically for detecting cheating in social exchanges. This can be used to explain why humans are often more successful in drawing valid inferences if the contents involve human behavior in relation to social norms.[3] Another example is the so-called dual-process theory.[5][3] This theory posits that there are two distinct cognitive systems responsible for reasoning. Their interrelation can be used to explain commonly observed biases in deductive reasoning. System 1 is the older system in terms of evolution. It is based on associative learning and happens fast and automatically without demanding many cognitive resources.[5][3] System 2, on the other hand, is of more recent evolutionary origin. It is slow and cognitively demanding, but also more flexible and under deliberate control.[5][3] The dual-process theory posits that system 1 is the default system guiding most of our everyday reasoning in a pragmatic way. But for particularly difficult problems on the logical level, system 2 is employed. System 2 is mostly responsible for deductive reasoning.[5][3]\n The ability of deductive reasoning is an important aspect of intelligence and many tests of intelligence include problems that call for deductive inferences.[1] Because of this relation to intelligence, deduction is highly relevant to psychology and the cognitive sciences.[5] But the subject of deductive reasoning is also pertinent to the computer sciences, for example, in the creation of artificial intelligence.[1]\n Deductive reasoning plays an important role in epistemology. Epistemology is concerned with the question of justification, i.e. to point out which beliefs are justified and why.[48][49] Deductive inferences are able to transfer the justification of the premises onto the conclusion.[3] So while logic is interested in the truth-preserving nature of deduction, epistemology is interested in the justification-preserving nature of deduction. There are different theories trying to explain why deductive reasoning is justification-preserving.[3] According to reliabilism, this is the case because deductions are truth-preserving: they are reliable processes that ensure a true conclusion given the premises are true.[3][50][51] Some theorists hold that the thinker has to have explicit awareness of the truth-preserving nature of the inference for the justification to be transferred from the premises to the conclusion. One consequence of such a view is that, for young children, this deductive transference does not take place since they lack this specific awareness.[3]\n Probability logic is interested in how the probability of the premises of an argument affects the probability of its conclusion. It differs from classical logic, which assumes that propositions are either true or false but does not take into consideration the probability or certainty that a proposition is true or false.[52][53]\n Aristotle, a Greek philosopher, started documenting deductive reasoning in the 4th century BC.[54] René Descartes, in his book Discourse on Method, refined the idea for the Scientific Revolution. Developing four rules to follow for proving an idea deductively, Descartes laid the foundation for the deductive portion of the scientific method. Descartes' background in geometry and mathematics influenced his ideas on the truth and reasoning, causing him to develop a system of general reasoning now used for most mathematical reasoning. Similar to postulates, Descartes believed that ideas could be self-evident and that reasoning alone must prove that observations are reliable. These ideas also lay the foundations for the ideas of rationalism.[55]\n Deductivism is a philosophical position that gives primacy to deductive reasoning or arguments over their non-deductive counterparts.[56][57] It is often understood as the evaluative claim that only deductive inferences are good or correct inferences. This theory would have wide-reaching consequences for various fields since it implies that the rules of deduction are \"the only acceptable standard of evidence\".[56] This way, the rationality or correctness of the different forms of inductive reasoning is denied.[57][58] Some forms of deductivism express this in terms of degrees of reasonableness or probability. Inductive inferences are usually seen as providing a certain degree of support for their conclusion: they make it more likely that their conclusion is true. Deductivism states that such inferences are not rational: the premises either ensure their conclusion, as in deductive reasoning, or they do not provide any support at all.[59]\n One motivation for deductivism is the problem of induction introduced by David Hume. It consists in the challenge of explaining how or whether inductive inferences based on past experiences support conclusions about future events.[57][60][59] For example, a chicken comes to expect, based on all its past experiences, that the person entering its coop is going to feed it, until one day the person \"at last wrings its neck instead\".[61] According to Karl Popper's falsificationism, deductive reasoning alone is sufficient. This is due to its truth-preserving nature: a theory can be falsified if one of its deductive consequences is false.[62][63] So while inductive reasoning does not offer positive evidence for a theory, the theory still remains a viable competitor until falsified by empirical observation. In this sense, deduction alone is sufficient for discriminating between competing hypotheses about what is the case.[57] Hypothetico-deductivism is a closely related scientific method, according to which science progresses by formulating hypotheses and then aims to falsify them by trying to make observations that run counter to their deductive consequences.[64][65]\n The term \"natural deduction\" refers to a class of proof systems based on self-evident rules of inference.[66][67] The first systems of natural deduction were developed by Gerhard Gentzen and Stanislaw Jaskowski in the 1930s. The core motivation was to give a simple presentation of deductive reasoning that closely mirrors how reasoning actually takes place.[68] In this sense, natural deduction stands in contrast to other less intuitive proof systems, such as Hilbert-style deductive systems, which employ axiom schemes to express logical truths.[66] Natural deduction, on the other hand, avoids axioms schemes by including many different rules of inference that can be used to formulate proofs. These rules of inference express how logical constants behave. They are often divided into introduction rules and elimination rules. Introduction rules specify under which conditions a logical constant may be introduced into a new sentence of the proof.[66][67] For example, the introduction rule for the logical constant \"\n\n\n\n∧\n\n\n{\\displaystyle \\land }\n\n\" (and) is \"\n\n\n\n\n\n\nA\n,\nB\n\n\n(\nA\n∧\nB\n)\n\n\n\n\n\n{\\displaystyle {\\frac {A,B}{(A\\land B)}}}\n\n\". It expresses that, given the premises \"\n\n\n\nA\n\n\n{\\displaystyle A}\n\n\" and \"\n\n\n\nB\n\n\n{\\displaystyle B}\n\n\" individually, one may draw the conclusion \"\n\n\n\nA\n∧\nB\n\n\n{\\displaystyle A\\land B}\n\n\" and thereby include it in one's proof. This way, the symbol \"\n\n\n\n∧\n\n\n{\\displaystyle \\land }\n\n\" is introduced into the proof. The removal of this symbol is governed by other rules of inference, such as the elimination rule \"\n\n\n\n\n\n\n(\nA\n∧\nB\n)\n\nA\n\n\n\n\n{\\displaystyle {\\frac {(A\\land B)}{A}}}\n\n\", which states that one may deduce the sentence \"\n\n\n\nA\n\n\n{\\displaystyle A}\n\n\" from the premise \"\n\n\n\n(\nA\n∧\nB\n)\n\n\n{\\displaystyle (A\\land B)}\n\n\". Similar introduction and elimination rules are given for other logical constants, such as the propositional operator \"\n\n\n\n¬\n\n\n{\\displaystyle \\lnot }\n\n\", the propositional connectives \"\n\n\n\n∨\n\n\n{\\displaystyle \\lor }\n\n\" and \"\n\n\n\n→\n\n\n{\\displaystyle \\rightarrow }\n\n\", and the quantifiers \"\n\n\n\n∃\n\n\n{\\displaystyle \\exists }\n\n\" and \"\n\n\n\n∀\n\n\n{\\displaystyle \\forall }\n\n\".[66][67]\n The focus on rules of inferences instead of axiom schemes is an important feature of natural deduction.[66][67] But there is no general agreement on how natural deduction is to be defined. Some theorists hold that all proof systems with this feature are forms of natural deduction. This would include various forms of sequent calculi[a]  or tableau calculi. But other theorists use the term in a more narrow sense, for example, to refer to the proof systems developed by Gentzen and Jaskowski. Because of its simplicity, natural deduction is often used for teaching logic to students.[66]\n The geometrical method is a method of philosophy based on deductive reasoning. It starts from a small set of self-evident axioms and tries to build a comprehensive logical system based only on deductive inferences from these first axioms.[69] It was initially formulated by Baruch Spinoza and came to prominence in various rationalist philosophical systems in the modern era.[70] It gets its name from the forms of mathematical demonstration found in traditional geometry, which are usually based on axioms, definitions, and inferred theorems.[71][72] An important motivation of the geometrical method is to repudiate philosophical skepticism by grounding one's philosophical system on absolutely certain axioms. Deductive reasoning is central to this endeavor because of its necessarily truth-preserving nature. This way, the certainty initially invested only in the axioms is transferred to all parts of the philosophical system.[69]\n One recurrent criticism of philosophical systems build using the geometrical method is that their initial axioms are not as self-evident or certain as their defenders proclaim.[69] This problem lies beyond the deductive reasoning itself, which only ensures that the conclusion is true if the premises are true, but not that the premises themselves are true. For example, Spinoza's philosophical system has been criticized this way based on objections raised against the causal axiom, i.e. that \"the knowledge of an effect depends on and involves knowledge of its cause\".[73] A different criticism targets not the premises but the reasoning itself, which may at times implicitly assume premises that are themselves not self-evident.[69]\n"
    },
    {
        "title": "Probability",
        "url": "https://en.wikipedia.org/wiki/Probability",
        "content": "\n Probability is the branch of mathematics and statistics concerning events and numerical descriptions of how likely they are to occur.  The probability of an event is a number between 0 and 1; the larger the probability, the more likely an event is to occur.[note 1][1][2] This number is often expressed as a percentage (%), ranging from 0% to 100%. A simple example is the tossing of a fair (unbiased) coin. Since the coin is fair, the two outcomes (\"heads\" and \"tails\") are both equally probable; the probability of \"heads\" equals the probability of \"tails\"; and since no other outcomes are possible, the probability of either \"heads\" or \"tails\" is 1/2 (which could also be written as 0.5 or 50%).\n These concepts have been given an axiomatic mathematical formalization in probability theory, which is used widely in areas of study such as statistics, mathematics, science, finance, gambling, artificial intelligence, machine learning, computer science, game theory, and philosophy to, for example, draw inferences about the expected frequency of events. Probability theory is also used to describe the underlying mechanics and regularities of complex systems.[3]\n The word probability derives from the Latin probabilitas, which can also mean \"probity\", a measure of the authority of a witness in a legal case in Europe, and often correlated with the witness's nobility. In a sense, this differs much from the modern meaning of probability, which in contrast is a measure of the weight of empirical evidence, and is arrived at from inductive reasoning and statistical inference.[4]\n When dealing with random experiments – i.e., experiments that are random and well-defined – in a purely theoretical setting (like tossing a coin), probabilities can be numerically described by the number of desired outcomes, divided by the total number of all outcomes. This is referred to as theoretical probability (in contrast to empirical probability, dealing with probabilities in the context of real experiments). For example, tossing a coin twice will yield \"head-head\", \"head-tail\", \"tail-head\", and \"tail-tail\" outcomes. The probability of getting an outcome of \"head-head\" is 1 out of 4 outcomes, or, in numerical terms, 1/4, 0.25 or 25%. However, when it comes to practical application, there are two major competing categories of probability interpretations, whose adherents hold different views about the fundamental nature of probability:\n The scientific study of probability is a modern development of mathematics. Gambling shows that there has been an interest in quantifying the ideas of probability throughout history, but exact mathematical descriptions arose much later. There are reasons for the slow development of the mathematics of probability. Whereas games of chance provided the impetus for the mathematical study of probability, fundamental issues [note 2] are still obscured by superstitions.[11]\n According to Richard Jeffrey, \"Before the middle of the seventeenth century, the term 'probable' (Latin probabilis) meant approvable, and was applied in that sense, univocally, to opinion and to action. A probable action or opinion was one such as sensible people would undertake or hold, in the circumstances.\"[12] However, in legal contexts especially, 'probable' could also apply to propositions for which there was good evidence.[13]\n The sixteenth-century Italian polymath Gerolamo Cardano demonstrated the efficacy of defining odds as the ratio of favourable to unfavourable outcomes (which implies that the probability of an event is given by the ratio of favourable outcomes to the total number of possible outcomes[14]).\nAside from the elementary work by Cardano, the doctrine of probabilities dates to the correspondence of Pierre de Fermat and Blaise Pascal (1654). Christiaan Huygens (1657) gave the earliest known scientific treatment of the subject.[15] Jakob Bernoulli's Ars Conjectandi (posthumous, 1713) and Abraham de Moivre's Doctrine of Chances (1718) treated the subject as a branch of mathematics.[16] See Ian Hacking's The Emergence of Probability[4] and James Franklin's The Science of Conjecture[17] for histories of the early development of the very concept of mathematical probability.\n The theory of errors may be traced back to Roger Cotes's Opera Miscellanea (posthumous, 1722), but a memoir prepared by Thomas Simpson in 1755 (printed 1756) first applied the theory to the discussion of errors of observation.[18] The reprint (1757) of this memoir lays down the axioms that positive and negative errors are equally probable, and that certain assignable limits define the range of all errors. Simpson also discusses continuous errors and describes a probability curve.\n The first two laws of error that were proposed both originated with Pierre-Simon Laplace. The first law was published in 1774, and stated that the frequency of an error could be expressed as an exponential function of the numerical magnitude of the error – disregarding sign. The second law of error was proposed in 1778 by Laplace, and stated that the frequency of the error is an exponential function of the square of the error.[19] The second law of error is called the normal distribution or the Gauss law. \"It is difficult historically to attribute that law to Gauss, who in spite of his well-known precocity had probably not made this discovery before he was two years old.\"[19]\n Daniel Bernoulli (1778) introduced the principle of the maximum product of the probabilities of a system of concurrent errors.\n Adrien-Marie Legendre (1805) developed the method of least squares, and introduced it in his Nouvelles méthodes pour la détermination des orbites des comètes (New Methods for Determining the Orbits of Comets).[20] In ignorance of Legendre's contribution, an Irish-American writer, Robert Adrain, editor of \"The Analyst\" (1808), first deduced the law of facility of error,\n \n\n\n\nϕ\n(\nx\n)\n=\nc\n\ne\n\n−\n\nh\n\n2\n\n\n\nx\n\n2\n\n\n\n\n\n\n{\\displaystyle \\phi (x)=ce^{-h^{2}x^{2}}}\n\n\n where \n\n\n\nh\n\n\n{\\displaystyle h}\n\n is a constant depending on precision of observation, and \n\n\n\nc\n\n\n{\\displaystyle c}\n\n is a scale factor ensuring that the area under the curve equals 1. He gave two proofs, the second being essentially the same as John Herschel's (1850).[citation needed] Gauss gave the first proof that seems to have been known in Europe (the third after Adrain's) in 1809. Further proofs were given by Laplace (1810, 1812), Gauss (1823), James Ivory (1825, 1826), Hagen (1837), Friedrich Bessel (1838), W.F. Donkin (1844, 1856), and Morgan Crofton (1870). Other contributors were Ellis (1844), De Morgan (1864), Glaisher (1872), and Giovanni Schiaparelli (1875). Peters's (1856) formula[clarification needed] for r, the probable error of a single observation, is well known.\n In the nineteenth century, authors on the general theory included Laplace, Sylvestre Lacroix (1816), Littrow (1833), Adolphe Quetelet (1853), Richard Dedekind (1860), Helmert (1872), Hermann Laurent (1873), Liagre, Didion and Karl Pearson. Augustus De Morgan and George Boole improved the exposition of the theory.\n In 1906, Andrey Markov introduced[21] the notion of Markov chains, which played an important role in stochastic processes theory and its applications. The modern theory of probability based on measure theory was developed by Andrey Kolmogorov in 1931.[22]\n On the geometric side, contributors to The Educational Times included Miller, Crofton, McColl, Wolstenholme, Watson, and Artemas Martin.[23] See integral geometry for more information.\n Like other theories, the theory of probability is a representation of its concepts in formal terms – that is, in terms that can be considered separately from their meaning. These formal terms are manipulated by the rules of mathematics and logic, and any results are interpreted or translated back into the problem domain.\n There have been at least two successful attempts to formalize probability, namely the Kolmogorov formulation and the Cox formulation. In Kolmogorov's formulation (see also probability space), sets are interpreted as events and probability as a measure on a class of sets. In Cox's theorem, probability is taken as a primitive (i.e., not further analyzed), and the emphasis is on constructing a consistent assignment of probability values to propositions. In both cases, the laws of probability are the same, except for technical details.\n There are other methods for quantifying uncertainty, such as the Dempster–Shafer theory or possibility theory, but those are essentially different and not compatible with the usually-understood laws of probability.\n Probability theory is applied in everyday life in risk assessment and modeling. The insurance industry and markets use actuarial science to determine pricing and make trading decisions. Governments apply probabilistic methods in environmental regulation, entitlement analysis, and financial regulation.\n An example of the use of probability theory in equity trading is the effect of the perceived probability of any widespread Middle East conflict on oil prices, which have ripple effects in the economy as a whole. An assessment by a commodity trader that a war is more likely can send that commodity's prices up or down, and signals other traders of that opinion. Accordingly, the probabilities are neither assessed independently nor necessarily rationally. The theory of behavioral finance emerged to describe the effect of such groupthink on pricing, on policy, and on peace and conflict.[24]\n In addition to financial assessment, probability can be used to analyze trends in biology (e.g., disease spread) as well as ecology (e.g., biological Punnett squares).[25] As with finance, risk assessment can be used as a statistical tool to calculate the likelihood of undesirable events occurring, and can assist with implementing protocols to avoid encountering such circumstances. Probability is used to design games of chance so that casinos can make a guaranteed profit, yet provide payouts to players that are frequent enough to encourage continued play.[26]\n Another significant application of probability theory in everyday life is reliability. Many consumer products, such as automobiles and consumer electronics, use reliability theory in product design to reduce the probability of failure. Failure probability may influence a manufacturer's decisions on a product's warranty.[27]\n The cache language model and other statistical language models that are used in natural language processing are also examples of applications of probability theory.\n Consider an experiment that can produce a number of results. The collection of all possible results is called the sample space of the experiment, sometimes denoted as \n\n\n\nΩ\n\n\n{\\displaystyle \\Omega }\n\n. The power set of the sample space is formed by considering all different collections of possible results. For example, rolling a die can produce six possible results. One collection of possible results gives an odd number on the die. Thus, the subset {1,3,5} is an element of the power set of the sample space of dice rolls. These collections are called \"events\". In this case, {1,3,5} is the event that the die falls on some odd number. If the results that actually occur fall in a given event, the event is said to have occurred.\n A probability is a way of assigning every event a value between zero and one, with the requirement that the event made up of all possible results (in our example, the event {1,2,3,4,5,6}) is assigned a value of one. To qualify as a probability, the assignment of values must satisfy the requirement that for any collection of mutually exclusive events (events with no common results, such as the events {1,6}, {3}, and {2,4}), the probability that at least one of the events will occur is given by the sum of the probabilities of all the individual events.[28]\n The probability of an event A is written as \n\n\n\nP\n(\nA\n)\n\n\n{\\displaystyle P(A)}\n\n,[29] \n\n\n\np\n(\nA\n)\n\n\n{\\displaystyle p(A)}\n\n, or \n\n\n\n\nPr\n\n(\nA\n)\n\n\n{\\displaystyle {\\text{Pr}}(A)}\n\n.[30] This mathematical definition of probability can extend to infinite sample spaces, and even uncountable sample spaces, using the concept of a measure.\n The opposite or complement of an event A is the event [not A] (that is, the event of A not occurring), often denoted as \n\n\n\n\nA\n′\n\n,\n\nA\n\nc\n\n\n\n\n{\\displaystyle A',A^{c}}\n\n, \n\n\n\n\n\nA\n¯\n\n\n,\n\nA\n\n∁\n\n\n,\n¬\nA\n\n\n{\\displaystyle {\\overline {A}},A^{\\complement },\\neg A}\n\n, or \n\n\n\n\n∼\n\nA\n\n\n{\\displaystyle {\\sim }A}\n\n; its probability is given by P(not A) = 1 − P(A).[31] As an example, the chance of not rolling a six on a six-sided die is 1 – (chance of rolling a six) = 1 − ⁠1/6⁠ = ⁠5/6⁠. For a more comprehensive treatment, see Complementary event.\n If two events A and B occur on a single performance of an experiment, this is called the intersection or joint probability of A and B, denoted as \n\n\n\nP\n(\nA\n∩\nB\n)\n.\n\n\n{\\displaystyle P(A\\cap B).}\n\n\n If two events, A and B are independent then the joint probability is[29]\n \n\n\n\nP\n(\nA\n\n\n and \n\n\nB\n)\n=\nP\n(\nA\n∩\nB\n)\n=\nP\n(\nA\n)\nP\n(\nB\n)\n.\n\n\n{\\displaystyle P(A{\\mbox{ and }}B)=P(A\\cap B)=P(A)P(B).}\n\n\n For example, if two coins are flipped, then the chance of both being heads is \n\n\n\n\n\n\n1\n2\n\n\n\n×\n\n\n\n1\n2\n\n\n\n=\n\n\n\n1\n4\n\n\n\n.\n\n\n{\\displaystyle {\\tfrac {1}{2}}\\times {\\tfrac {1}{2}}={\\tfrac {1}{4}}.}\n\n[32]\n If either event A or event B can occur but never both simultaneously, then they are called mutually exclusive events.\n If two events are mutually exclusive, then the probability of both occurring is denoted as \n\n\n\nP\n(\nA\n∩\nB\n)\n\n\n{\\displaystyle P(A\\cap B)}\n\n and\n\n\n\nP\n(\nA\n\n\n and \n\n\nB\n)\n=\nP\n(\nA\n∩\nB\n)\n=\n0\n\n\n{\\displaystyle P(A{\\mbox{ and }}B)=P(A\\cap B)=0}\n\nIf two events are mutually exclusive, then the probability of either occurring is denoted as \n\n\n\nP\n(\nA\n∪\nB\n)\n\n\n{\\displaystyle P(A\\cup B)}\n\n and\n\n\n\nP\n(\nA\n\n\n or \n\n\nB\n)\n=\nP\n(\nA\n∪\nB\n)\n=\nP\n(\nA\n)\n+\nP\n(\nB\n)\n−\nP\n(\nA\n∩\nB\n)\n=\nP\n(\nA\n)\n+\nP\n(\nB\n)\n−\n0\n=\nP\n(\nA\n)\n+\nP\n(\nB\n)\n\n\n{\\displaystyle P(A{\\mbox{ or }}B)=P(A\\cup B)=P(A)+P(B)-P(A\\cap B)=P(A)+P(B)-0=P(A)+P(B)}\n\n\n For example, the chance of rolling a 1 or 2 on a six-sided die is \n\n\n\nP\n(\n1\n\n\n or \n\n\n2\n)\n=\nP\n(\n1\n)\n+\nP\n(\n2\n)\n=\n\n\n\n1\n6\n\n\n\n+\n\n\n\n1\n6\n\n\n\n=\n\n\n\n1\n3\n\n\n\n.\n\n\n{\\displaystyle P(1{\\mbox{ or }}2)=P(1)+P(2)={\\tfrac {1}{6}}+{\\tfrac {1}{6}}={\\tfrac {1}{3}}.}\n\n\n If the events are not (necessarily) mutually exclusive then\n\n\n\nP\n\n(\n\nA\n\n\n or \n\n\nB\n\n)\n\n=\nP\n(\nA\n∪\nB\n)\n=\nP\n\n(\nA\n)\n\n+\nP\n\n(\nB\n)\n\n−\nP\n\n(\n\nA\n\n\n and \n\n\nB\n\n)\n\n.\n\n\n{\\displaystyle P\\left(A{\\hbox{ or }}B\\right)=P(A\\cup B)=P\\left(A\\right)+P\\left(B\\right)-P\\left(A{\\mbox{ and }}B\\right).}\n\nRewritten,\n\n\n\nP\n\n(\n\nA\n∪\nB\n\n)\n\n=\nP\n\n(\nA\n)\n\n+\nP\n\n(\nB\n)\n\n−\nP\n\n(\n\nA\n∩\nB\n\n)\n\n\n\n{\\displaystyle P\\left(A\\cup B\\right)=P\\left(A\\right)+P\\left(B\\right)-P\\left(A\\cap B\\right)}\n\n\n For example, when drawing a card from a deck of cards, the chance of getting a heart or a face card (J, Q, K) (or both) is \n\n\n\n\n\n\n13\n52\n\n\n\n+\n\n\n\n12\n52\n\n\n\n−\n\n\n\n3\n52\n\n\n\n=\n\n\n\n11\n26\n\n\n\n,\n\n\n{\\displaystyle {\\tfrac {13}{52}}+{\\tfrac {12}{52}}-{\\tfrac {3}{52}}={\\tfrac {11}{26}},}\n\n since among the 52 cards of a deck, 13 are hearts, 12 are face cards, and 3 are both: here the possibilities included in the \"3 that are both\" are included in each of the \"13 hearts\" and the \"12 face cards\", but should only be counted once.\n This can be expanded further for multiple not (necessarily) mutually exclusive events. For three events, this proceeds as follows:\n\n\n\n\n\n\n\nP\n\n(\n\nA\n∪\nB\n∪\nC\n\n)\n\n=\n\n\nP\n\n(\n\n\n(\n\nA\n∪\nB\n\n)\n\n∪\nC\n\n)\n\n\n\n\n\n=\n\n\nP\n\n(\n\nA\n∪\nB\n\n)\n\n+\nP\n\n(\nC\n)\n\n−\nP\n\n(\n\n\n(\n\nA\n∪\nB\n\n)\n\n∩\nC\n\n)\n\n\n\n\n\n=\n\n\nP\n\n(\nA\n)\n\n+\nP\n\n(\nB\n)\n\n−\nP\n\n(\n\nA\n∩\nB\n\n)\n\n+\nP\n\n(\nC\n)\n\n−\nP\n\n(\n\n\n(\n\nA\n∩\nC\n\n)\n\n∪\n\n(\n\nB\n∩\nC\n\n)\n\n\n)\n\n\n\n\n\n=\n\n\nP\n\n(\nA\n)\n\n+\nP\n\n(\nB\n)\n\n+\nP\n\n(\nC\n)\n\n−\nP\n\n(\n\nA\n∩\nB\n\n)\n\n−\n\n(\n\nP\n\n(\n\nA\n∩\nC\n\n)\n\n+\nP\n\n(\n\nB\n∩\nC\n\n)\n\n−\nP\n\n(\n\n\n(\n\nA\n∩\nC\n\n)\n\n∩\n\n(\n\nB\n∩\nC\n\n)\n\n\n)\n\n\n)\n\n\n\n\n\nP\n\n(\n\nA\n∪\nB\n∪\nC\n\n)\n\n=\n\n\nP\n\n(\nA\n)\n\n+\nP\n\n(\nB\n)\n\n+\nP\n\n(\nC\n)\n\n−\nP\n\n(\n\nA\n∩\nB\n\n)\n\n−\nP\n\n(\n\nA\n∩\nC\n\n)\n\n−\nP\n\n(\n\nB\n∩\nC\n\n)\n\n+\nP\n\n(\n\nA\n∩\nB\n∩\nC\n\n)\n\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}P\\left(A\\cup B\\cup C\\right)=&P\\left(\\left(A\\cup B\\right)\\cup C\\right)\\\\=&P\\left(A\\cup B\\right)+P\\left(C\\right)-P\\left(\\left(A\\cup B\\right)\\cap C\\right)\\\\=&P\\left(A\\right)+P\\left(B\\right)-P\\left(A\\cap B\\right)+P\\left(C\\right)-P\\left(\\left(A\\cap C\\right)\\cup \\left(B\\cap C\\right)\\right)\\\\=&P\\left(A\\right)+P\\left(B\\right)+P\\left(C\\right)-P\\left(A\\cap B\\right)-\\left(P\\left(A\\cap C\\right)+P\\left(B\\cap C\\right)-P\\left(\\left(A\\cap C\\right)\\cap \\left(B\\cap C\\right)\\right)\\right)\\\\P\\left(A\\cup B\\cup C\\right)=&P\\left(A\\right)+P\\left(B\\right)+P\\left(C\\right)-P\\left(A\\cap B\\right)-P\\left(A\\cap C\\right)-P\\left(B\\cap C\\right)+P\\left(A\\cap B\\cap C\\right)\\end{aligned}}}\n\nIt can be seen, then, that this pattern can be repeated for any number of events.\n Conditional probability is the probability of some event A, given the occurrence of some other event B. Conditional probability is written \n\n\n\nP\n(\nA\n∣\nB\n)\n\n\n{\\displaystyle P(A\\mid B)}\n\n, and is read \"the probability of A, given B\". It is defined by[33]\n \n\n\n\nP\n(\nA\n∣\nB\n)\n=\n\n\n\nP\n(\nA\n∩\nB\n)\n\n\nP\n(\nB\n)\n\n\n\n\n\n\n{\\displaystyle P(A\\mid B)={\\frac {P(A\\cap B)}{P(B)}}\\,}\n\n\n If \n\n\n\nP\n(\nB\n)\n=\n0\n\n\n{\\displaystyle P(B)=0}\n\n then \n\n\n\nP\n(\nA\n∣\nB\n)\n\n\n{\\displaystyle P(A\\mid B)}\n\n is formally undefined by this expression. In this case \n\n\n\nA\n\n\n{\\displaystyle A}\n\n and \n\n\n\nB\n\n\n{\\displaystyle B}\n\n are independent, since \n\n\n\nP\n(\nA\n∩\nB\n)\n=\nP\n(\nA\n)\nP\n(\nB\n)\n=\n0.\n\n\n{\\displaystyle P(A\\cap B)=P(A)P(B)=0.}\n\n However, it is possible to define a conditional probability for some zero-probability events, for example by using a σ-algebra of such events (such as those arising from a continuous random variable).[34]\n For example, in a bag of 2 red balls and 2 blue balls (4 balls in total), the probability of taking a red ball is \n\n\n\n1\n\n/\n\n2\n;\n\n\n{\\displaystyle 1/2;}\n\n however, when taking a second ball, the probability of it being either a red ball or a blue ball depends on the ball previously taken. For example, if a red ball was taken, then the probability of picking a red ball again would be \n\n\n\n1\n\n/\n\n3\n,\n\n\n{\\displaystyle 1/3,}\n\n since only 1 red and 2 blue balls would have been remaining. And if a blue ball was taken previously, the probability of taking a red ball will be \n\n\n\n2\n\n/\n\n3.\n\n\n{\\displaystyle 2/3.}\n\n\n In probability theory and applications, Bayes' rule relates the odds of event \n\n\n\n\nA\n\n1\n\n\n\n\n{\\displaystyle A_{1}}\n\n to event \n\n\n\n\nA\n\n2\n\n\n,\n\n\n{\\displaystyle A_{2},}\n\n before (prior to) and after (posterior to) conditioning on another event \n\n\n\nB\n.\n\n\n{\\displaystyle B.}\n\n The odds on \n\n\n\n\nA\n\n1\n\n\n\n\n{\\displaystyle A_{1}}\n\n to event \n\n\n\n\nA\n\n2\n\n\n\n\n{\\displaystyle A_{2}}\n\n is simply the ratio of the probabilities of the two events. When arbitrarily many events \n\n\n\nA\n\n\n{\\displaystyle A}\n\n are of interest, not just two, the rule can be rephrased as posterior is proportional to prior times likelihood, \n\n\n\nP\n(\nA\n\n|\n\nB\n)\n∝\nP\n(\nA\n)\nP\n(\nB\n\n|\n\nA\n)\n\n\n{\\displaystyle P(A|B)\\propto P(A)P(B|A)}\n\n where the proportionality symbol means that the left hand side is proportional to (i.e., equals a constant times) the right hand side as \n\n\n\nA\n\n\n{\\displaystyle A}\n\n varies, for fixed or given \n\n\n\nB\n\n\n{\\displaystyle B}\n\n (Lee, 2012; Bertsch McGrayne, 2012). In this form it goes back to Laplace (1774) and to Cournot (1843); see Fienberg (2005).\n In a deterministic universe, based on Newtonian concepts, there would be no probability if all conditions were known (Laplace's demon) (but there are situations in which sensitivity to initial conditions exceeds our ability to measure them, i.e. know them).  In the case of a roulette wheel, if the force of the hand and the period of that force are known, the number on which the ball will stop would be a certainty (though as a practical matter, this would likely be true only of a roulette wheel that had not been exactly levelled – as Thomas A. Bass' Newtonian Casino revealed).  This also assumes knowledge of inertia and friction of the wheel, weight, smoothness, and roundness of the ball, variations in hand speed during the turning, and so forth. A probabilistic description can thus be more useful than Newtonian mechanics for analyzing the pattern of outcomes of repeated rolls of a roulette wheel. Physicists face the same situation in the kinetic theory of gases, where the system, while deterministic in principle, is so complex (with the number of molecules typically the order of magnitude of the Avogadro constant 6.02×1023) that only a statistical description of its properties is feasible.[35]\n Probability theory is required to describe quantum phenomena.[36] A revolutionary discovery of early 20th century physics was the random character of all physical processes that occur at sub-atomic scales and are governed by the laws of quantum mechanics. The objective wave function evolves deterministically but, according to the Copenhagen interpretation, it deals with probabilities of observing, the outcome being explained by a wave function collapse when an observation is made. However, the loss of determinism for the sake of instrumentalism did not meet with universal approval. Albert Einstein famously remarked in a letter to Max Born: \"I am convinced that God does not play dice\".[37] Like Einstein, Erwin Schrödinger, who discovered the wave function, believed quantum mechanics is a statistical approximation of an underlying deterministic reality.[38] In some modern interpretations of the statistical mechanics of measurement, quantum decoherence is invoked to account for the appearance of subjectively probabilistic experimental outcomes.\n"
    },
    {
        "title": "Knowledge engineering",
        "url": "https://en.wikipedia.org/wiki/Knowledge_engineering",
        "content": "Knowledge engineering (KE) refers to all aspects involved in knowledge-based systems.\n One of the first examples of an expert system was MYCIN, an application to perform medical diagnosis. In the MYCIN example, the domain experts were medical doctors and the knowledge represented was their expertise in diagnosis.\n Expert systems were first developed in artificial intelligence laboratories as an attempt to understand complex human decision making. Based on positive results from these initial prototypes, the technology was adopted by the US business community (and later worldwide) in the 1980s. The Stanford heuristic programming project led by Edward Feigenbaum was one of the leaders in defining and developing the first expert systems.\n In the earliest days of expert systems, there was little or no formal process for the creation of the software. Researchers just sat down with domain experts and started programming, often developing the required tools (e.g. inference engines) at the same time as the applications themselves. As expert systems moved from academic prototypes to deployed business systems it was realized that a methodology was required to bring predictability and control to the process of building the software. There were essentially two approaches that were attempted:\n Many of the early expert systems were developed by large consulting and system integration firms such as Andersen Consulting. These firms already had well tested conventional waterfall methodologies (e.g. Method/1 for Andersen) that they trained all their staff in and that were virtually always used to develop software for their clients. One trend in early expert systems development was to simply apply these waterfall methods to expert systems development.\n Another issue with using conventional methods to develop expert systems was that due to the unprecedented nature of expert systems, they were one of the first applications to adopt rapid application development methods that feature iteration and prototyping as well as or instead of detailed analysis and design. In the 1980s few conventional software methods supported this type of approach.\n The final issue with using conventional methods to develop expert systems was the need for knowledge acquisition. Knowledge acquisition refers to the process of gathering expert knowledge and capturing it in the form of rules and ontologies. Knowledge acquisition has special requirements beyond the conventional specification process used to capture most business requirements.\n These issues led to the second approach to knowledge engineering: the development of custom methodologies specifically designed to build expert systems.[1] One of the first and most popular of such methodologies custom designed for expert systems was the Knowledge Acquisition and Documentation Structuring (KADS) methodology developed in Europe. KADS had great success in Europe and was also used in the United States.[2]\n"
    },
    {
        "title": "Database",
        "url": "https://en.wikipedia.org/wiki/Database",
        "content": "\n In computing, a database is an organized collection of data or a type of data store based on the use of a database management system (DBMS), the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term \"database\" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database.\n Small databases can be stored on a file system, while large databases are hosted on computer clusters or cloud storage. The design of databases spans formal techniques and practical considerations, including data modeling, efficient data representation and storage, query languages, security and privacy of sensitive data, and distributed computing issues, including supporting concurrent access and fault tolerance.\n Computer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, collectively referred to as NoSQL, because they use different query languages.\n Formally, a \"database\" refers to a set of related data accessed through the use of a \"database management system\" (DBMS), which is an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized.\n Because of the close relationship between them, the term \"database\" is often used casually to refer to both a database and the DBMS used to manipulate it.\n Outside the world of professional information technology, the term database is often used to refer to any collection of related data (such as a spreadsheet or a card index) as size and usage requirements typically necessitate use of a database management system.[1]\n Existing DBMSs provide various functions that allow management of a database and its data which can be classified into four main functional groups:\n Both a database and its DBMS conform to the principles of a particular database model.[5] \"Database system\" refers collectively to the database model, database management system, and database.[6]\n Physically, database servers are dedicated computers that hold the actual databases and run only the DBMS and related software. Database servers are usually multiprocessor computers, with generous memory and RAID disk arrays used for stable storage. Hardware database accelerators, connected to one or more servers via a high-speed channel, are also used in large-volume transaction processing environments. DBMSs are found at the heart of most database applications. DBMSs may be built around a custom multitasking kernel with built-in networking support, but modern DBMSs typically rely on a standard operating system to provide these functions.[citation needed]\n Since DBMSs comprise a significant market, computer and storage vendors often take into account DBMS requirements in their own development plans.[7]\n Databases and DBMSs can be categorized according to the database model(s) that they support (such as relational or XML), the type(s) of computer they run on (from a server cluster to a mobile phone), the query language(s) used to access the database (such as SQL or XQuery), and their internal engineering, which affects performance, scalability, resilience, and security.\n The sizes, capabilities, and performance of databases and their respective DBMSs have grown in orders of magnitude. These performance increases were enabled by the technology progress in the areas of processors, computer memory, computer storage, and computer networks. The concept of a database was made possible by the emergence of direct access storage media such as magnetic disks, which became widely available in the mid-1960s; earlier systems relied on sequential storage of data on magnetic tape. The subsequent development of database technology can be divided into three eras based on data model or structure: navigational,[8] SQL/relational, and post-relational.\n The two main early navigational data models were the hierarchical model and the CODASYL model (network model). These were characterized by the use of pointers (often physical disk addresses) to follow relationships from one record to another.\n The relational model, first proposed in 1970 by Edgar F. Codd, departed from this tradition by insisting that applications should search for data by content, rather than by following links. The relational model employs sets of ledger-style tables, each used for a different type of entity. Only in the mid-1980s did computing hardware become powerful enough to allow the wide deployment of relational systems (DBMSs plus applications). By the early 1990s, however, relational systems dominated in all large-scale data processing applications, and as of 2018[update] they remain dominant: IBM Db2, Oracle, MySQL, and Microsoft SQL Server are the most searched DBMS.[9] The dominant database language, standardized SQL for the relational model, has influenced database languages for other data models.[citation needed]\n Object databases were developed in the 1980s to overcome the inconvenience of object–relational impedance mismatch, which led to the coining of the term \"post-relational\" and also the development of hybrid object–relational databases.\n The next generation of post-relational databases in the late 2000s became known as NoSQL databases, introducing fast key–value stores and document-oriented databases. A competing \"next generation\" known as NewSQL databases attempted new implementations that retained the relational/SQL model while aiming to match the high performance of NoSQL compared to commercially available relational DBMSs.\n The introduction of the term database coincided with the availability of direct-access storage (disks and drums) from the mid-1960s onwards. The term represented a contrast with the tape-based systems of the past, allowing shared interactive use rather than daily batch processing. The Oxford English Dictionary cites a 1962 report by the System Development Corporation of California as the first to use the term \"data-base\" in a specific technical sense.[10]\n As computers grew in speed and capability, a number of general-purpose database systems emerged; by the mid-1960s a number of such systems had come into commercial use. Interest in a standard began to grow, and Charles Bachman, author of one such product, the Integrated Data Store (IDS), founded the Database Task Group within CODASYL, the group responsible for the creation and standardization of COBOL. In 1971, the Database Task Group delivered their standard, which generally became known as the CODASYL approach, and soon a number of commercial products based on this approach entered the market.\n The CODASYL approach offered applications the ability to navigate around a linked data set which was formed into a large network. Applications could find records by one of three methods:\n Later systems added B-trees to provide alternate access paths. Many CODASYL databases also added a declarative query language for end users (as distinct from the navigational API). However, CODASYL databases were complex and required significant training and effort to produce useful applications.\n IBM also had its own DBMS in 1966, known as Information Management System (IMS). IMS was a development of software written for the Apollo program on the System/360. IMS was generally similar in concept to CODASYL, but used a strict hierarchy for its model of data navigation instead of CODASYL's network model. Both concepts later became known as navigational databases due to the way data was accessed: the term was popularized by Bachman's 1973 Turing Award presentation The Programmer as Navigator. IMS is classified by IBM as a hierarchical database. IDMS and Cincom Systems' TOTAL databases are classified as network databases. IMS remains in use as of 2014[update].[11]\n Edgar F. Codd worked at IBM in San Jose, California, in one of their offshoot offices that were primarily involved in the development of hard disk systems. He was unhappy with the navigational model of the CODASYL approach, notably the lack of a \"search\" facility. In 1970, he wrote a number of papers that outlined a new approach to database construction that eventually culminated in the groundbreaking A Relational Model of Data for Large Shared Data Banks.[12]\n In this paper, he described a new system for storing and working with large databases. Instead of records being stored in some sort of linked list of free-form records as in CODASYL, Codd's idea was to organize the data as a number of \"tables\", each table being used for a different type of entity. Each table would contain a fixed number of columns containing the attributes of the entity. One or more columns of each table were designated as a  primary key by which the rows of the table could be uniquely identified; cross-references between tables always used these primary keys, rather than disk addresses, and queries would join tables based on these key relationships, using a set of operations based on the mathematical system of relational calculus (from which the model takes its name). Splitting the data into a set of normalized tables (or relations) aimed to ensure that each \"fact\" was only stored once, thus simplifying update operations. Virtual tables called views could present the data in different ways for different users, but views could not be directly updated.\n Codd used mathematical terms to define the model: relations, tuples, and domains rather than tables, rows, and columns. The terminology that is now familiar came from early implementations. Codd would later criticize the tendency for practical implementations to depart from the mathematical foundations on which the model was based.\n The use of primary keys (user-oriented identifiers) to represent cross-table relationships, rather than disk addresses, had two primary motivations. From an engineering perspective, it enabled tables to be relocated and resized without expensive database reorganization. But Codd was more interested in the difference in semantics: the use of explicit identifiers made it easier to define update operations with clean mathematical definitions, and it also enabled query operations to be defined in terms of the established discipline of first-order predicate calculus; because these operations have clean mathematical properties, it becomes possible to rewrite queries in provably correct ways, which is the basis of query optimization. There is no loss of expressiveness compared with the hierarchic or network models, though the connections between tables are no longer so explicit.\n In the hierarchic and network models, records were allowed to have a complex internal structure. For example, the salary history of an employee might be represented as a \"repeating group\" within the employee record. In the relational model, the process of normalization led to such internal structures being replaced by data held in multiple tables, connected only by logical keys.\n For instance, a common use of a database system is to track information about users, their name, login information, various addresses and phone numbers. In the navigational approach, all of this data would be placed in a single variable-length record. In the relational approach, the data would be normalized into a user table, an address table and a phone number table (for instance). Records would be created in these optional tables only if the address or phone numbers were actually provided.\n As well as identifying rows/records using logical identifiers rather than disk addresses, Codd changed the way in which applications assembled data from multiple records. Rather than requiring applications to gather data one record at a time by navigating the links, they would use a declarative query language that expressed what data was required, rather than the access path by which it should be found. Finding an efficient access path to the data became the responsibility of the database management system, rather than the application programmer. This process, called query optimization, depended on the fact that queries were expressed in terms of mathematical logic.\n Codd's paper was picked up by two people at Berkeley, Eugene Wong and Michael Stonebraker. They started a project known as INGRES using funding that had already been allocated for a geographical database project and student programmers to produce code. Beginning in 1973, INGRES delivered its first test products which were generally ready for widespread use in 1979. INGRES was similar to System R in a number of ways, including the use of a \"language\" for data access, known as QUEL. Over time, INGRES moved to the emerging SQL standard.\n IBM itself did one test implementation of the relational model, PRTV, and a production one, Business System 12, both now discontinued. Honeywell wrote MRDS for Multics, and now there are two new implementations: Alphora Dataphor and Rel. Most other DBMS implementations usually called relational are actually SQL DBMSs.\n In 1970, the University of Michigan began development of the MICRO Information Management System[13] based on D.L. Childs' Set-Theoretic Data model.[14][15][16] MICRO was used to manage very large data sets by the US Department of Labor, the U.S. Environmental Protection Agency, and researchers from the University of Alberta, the University of Michigan, and Wayne State University. It ran on IBM mainframe computers using the Michigan Terminal System.[17] The system remained in production until 1998.\n In the 1970s and 1980s, attempts were made to build database systems with integrated hardware and software. The underlying philosophy was that such integration would provide higher performance at a lower cost. Examples were IBM System/38, the early offering of Teradata, and the Britton Lee, Inc. database machine.\n Another approach to hardware support for database management was ICL's CAFS accelerator, a hardware disk controller with programmable search capabilities. In the long term, these efforts were generally unsuccessful because specialized database machines could not keep pace with the rapid development and progress of general-purpose computers. Thus most database systems nowadays are software systems running on general-purpose hardware, using general-purpose computer data storage. However, this idea is still pursued in certain applications by some companies like Netezza and Oracle (Exadata).\n IBM started working on a prototype system loosely based on Codd's concepts as System R in the early 1970s. The first version was ready in 1974/5, and work then started on multi-table systems in which the data could be split so that all of the data for a record (some of which is optional) did not have to be stored in a single large \"chunk\". Subsequent multi-user versions were tested by customers in 1978 and 1979, by which time a standardized query language – SQL[citation needed] – had been added. Codd's ideas were establishing themselves as both workable and superior to CODASYL, pushing IBM to develop a true production version of System R, known as SQL/DS, and, later, Database 2 (IBM Db2).\n Larry Ellison's Oracle Database (or more simply, Oracle) started from a different chain, based on IBM's papers on System R. Though Oracle V1 implementations were completed in 1978, it was not until Oracle Version 2 when Ellison beat IBM to market in 1979.[18]\n Stonebraker went on to apply the lessons from INGRES to develop a new database, Postgres, which is now known as PostgreSQL. PostgreSQL is often used for global mission-critical applications (the .org and .info domain name registries use it as their primary data store, as do many large companies and financial institutions).\n In Sweden, Codd's paper was also read and Mimer SQL was developed in the mid-1970s at Uppsala University. In 1984, this project was consolidated into an independent enterprise.\n Another data model, the entity–relationship model, emerged in 1976 and gained popularity for database design as it emphasized a more familiar description than the earlier relational model. Later on, entity–relationship constructs were retrofitted as a data modeling construct for the relational model, and the difference between the two has become irrelevant.[citation needed]\n The 1980s ushered in the age of desktop computing. The new computers empowered their users with spreadsheets like Lotus 1-2-3 and database software like dBASE. The dBASE product was lightweight and easy for any computer user to understand out of the box. C. Wayne Ratliff, the creator of dBASE, stated: \"dBASE was different from programs like BASIC, C, FORTRAN, and COBOL in that a lot of the dirty work had already been done. The data manipulation is done by dBASE instead of by the user, so the user can concentrate on what he is doing, rather than having to mess with the dirty details of opening, reading, and closing files, and managing space allocation.\"[19] dBASE was one of the top selling software titles in the 1980s and early 1990s.\n The 1990s, along with a rise in object-oriented programming, saw a growth in how data in various databases were handled. Programmers and designers began to treat the data in their databases as objects. That is to say that if a person's data were in a database, that person's attributes, such as their address, phone number, and age, were now considered to belong to that person instead of being extraneous data. This allows for relations between data to be related to objects and their attributes and not to individual fields.[20] The term \"object–relational impedance mismatch\" described the inconvenience of translating between programmed objects and database tables. Object databases and object–relational databases attempt to solve this problem by providing an object-oriented language (sometimes as extensions to SQL) that programmers can use as alternative to purely relational SQL. On the programming side, libraries known as object–relational mappings (ORMs) attempt to solve the same problem.\n XML databases are a type of structured document-oriented database that allows querying based on XML document attributes. XML databases are mostly used in applications where the data is conveniently viewed as a collection of documents, with a structure that can vary from the very flexible to the highly rigid: examples include scientific articles, patents, tax filings, and personnel records.\n NoSQL databases are often very fast, do not require fixed table schemas, avoid join operations by storing denormalized data, and are designed to scale horizontally.\n In recent years, there has been a strong demand for massively distributed databases with high partition tolerance, but according to the CAP theorem, it is impossible for a distributed system to simultaneously provide consistency, availability, and partition tolerance guarantees. A distributed system can satisfy any two of these guarantees at the same time, but not all three. For that reason, many NoSQL databases are using what is called eventual consistency to provide both availability and partition tolerance guarantees with a reduced level of data consistency.\n NewSQL is a class of modern relational databases that aims to provide the same scalable performance of NoSQL systems for online transaction processing (read-write) workloads while still using SQL and maintaining the ACID guarantees of a traditional database system.\n Databases are used to support internal operations of organizations and to underpin online interactions with customers and suppliers (see Enterprise software).\n Databases are used to hold administrative information and more specialized data, such as engineering data or economic models. Examples include computerized library systems, flight reservation systems, computerized parts inventory systems, and many content management systems that store websites as collections of webpages in a database.\n One way to classify databases involves the type of their contents, for example: bibliographic, document-text, statistical, or multimedia objects. Another way is by their application area, for example: accounting, music compositions, movies, banking, manufacturing, or insurance. A third way is by some technical aspect, such as the database structure or interface type. This section lists a few of the adjectives used to characterize different kinds of databases.\n Connolly and Begg define database management system (DBMS) as a \"software system that enables users to define, create, maintain and control access to the database.\"[24] Examples of DBMS's include MySQL, MariaDB, PostgreSQL, Microsoft SQL Server, Oracle Database, and Microsoft Access.\n The DBMS acronym is sometimes extended to indicate the underlying database model, with RDBMS for the relational, OODBMS for the object (oriented) and ORDBMS for the object–relational model. Other extensions can indicate some other characteristics, such as DDBMS for a distributed database management systems.\n The functionality provided by a DBMS can vary enormously. The core functionality is the storage, retrieval and update of data. Codd proposed the following functions and services a fully-fledged general purpose DBMS should provide:[25]\n It is also generally to be expected the DBMS will provide a set of utilities for such purposes as may be necessary to administer the database effectively, including import, export, monitoring, defragmentation and analysis utilities.[26] The core part of the DBMS interacting between the database and the application interface sometimes referred to as the database engine.\n Often DBMSs will have configuration parameters that can be statically and dynamically tuned, for example the maximum amount of main memory on a server the database can use. The trend is to minimize the amount of manual configuration, and for cases such as embedded databases the need to target zero-administration is paramount.\n The large major enterprise DBMSs have tended to increase in size and functionality and have involved up to thousands of human years of development effort throughout their lifetime.[a]\n Early multi-user DBMS typically only allowed for the application to reside on the same computer with access via terminals or terminal emulation software. The client–server architecture was a development where the application resided on a client desktop and the database on a server allowing the processing to be distributed. This evolved into a multitier architecture incorporating application servers and web servers with the end user interface via a web browser with the database only directly connected to the adjacent tier.[28]\n A general-purpose DBMS will provide public application programming interfaces (API) and optionally a processor for database languages such as SQL to allow applications to be written to interact with and manipulate the database. A special purpose DBMS may use a private API and be specifically customized and linked to a single application. For example, an email system performs many of the functions of a general-purpose DBMS such as message insertion, message deletion, attachment handling, blocklist lookup, associating messages an email address and so forth however these functions are limited to what is required to handle email.\n External interaction with the database will be via an application program that interfaces with the DBMS.[29] This can range from a database tool that allows users to execute SQL queries textually or graphically, to a website that happens to use a database to store and search information.\n A programmer will code interactions to the database (sometimes referred to as a datasource) via an application program interface (API) or via a database language. The particular API or language chosen will need to be supported by DBMS, possibly indirectly via a preprocessor or a bridging API. Some API's aim to be database independent, ODBC being a commonly known example. Other common API's include JDBC and ADO.NET.\n Database languages are special-purpose languages, which allow one or more of the following tasks, sometimes distinguished as sublanguages:\n Database languages are specific to a particular data model. Notable examples include:\n A database language may also incorporate features like:\n Database storage is the container of the physical materialization of a database. It comprises the internal (physical) level in the database architecture. It also contains all the information needed (e.g., metadata, \"data about the data\", and internal data structures) to reconstruct the conceptual level and external level from the internal level when needed. Databases as digital objects contain three layers of information which must be stored: the data, the structure, and the semantics. Proper storage of all three layers is needed for future preservation and longevity of the database.[33] Putting data into permanent storage is generally the responsibility of the database engine a.k.a. \"storage engine\". Though typically accessed by a DBMS through the underlying operating system (and often using the operating systems' file systems as intermediates for storage layout), storage properties and configuration settings are extremely important for the efficient operation of the DBMS, and thus are closely maintained by database administrators. A DBMS, while in operation, always has its database residing in several types of storage (e.g., memory and external storage). The database data and the additional needed information, possibly in very large amounts, are coded into bits. Data typically reside in the storage in structures that look completely different from the way the data look at the conceptual and external levels, but in ways that attempt to optimize (the best possible) these levels' reconstruction when needed by users and programs, as well as for computing additional types of needed information from the data (e.g., when querying the database).\n Some DBMSs support specifying which character encoding was used to store data, so multiple encodings can be used in the same database.\n Various low-level database storage structures are used by the storage engine to serialize the data model so it can be written to the medium of choice. Techniques such as indexing may be used to improve performance. Conventional storage is row-oriented, but there are also column-oriented and correlation databases.\n Often storage redundancy is employed to increase performance. A common example is storing materialized views, which consist of frequently needed external views or query results. Storing such views saves the expensive computing them each time they are needed. The downsides of materialized views are the overhead incurred when updating them to keep them synchronized with their original updated database data, and the cost of storage redundancy.\n Occasionally a database employs storage redundancy by database objects replication (with one or more copies) to increase data availability (both to improve performance of simultaneous multiple end-user accesses to the same database object, and to provide resiliency in a case of partial failure of a distributed database). Updates of a replicated object need to be synchronized across the object copies. In many cases, the entire database is replicated.\n With data virtualization, the data used remains in its original locations and real-time access is established to allow analytics across multiple sources. This can aid in resolving some technical difficulties such as compatibility problems when combining data from various platforms, lowering the risk of error caused by faulty data, and guaranteeing that the newest data is used. Furthermore, avoiding the creation of a new database containing personal information can make it easier to comply with privacy regulations. However, with data virtualization, the connection to all necessary data sources must be operational as there is no local copy of the data, which is one of the main drawbacks of the approach.[34]\n Database security deals with all various aspects of protecting the database content, its owners, and its users. It ranges from protection from intentional unauthorized database uses to unintentional database accesses by unauthorized entities (e.g., a person or a computer program).\n Database access control deals with controlling who (a person or a certain computer program) are allowed to access what information in the database. The information may comprise specific database objects (e.g., record types, specific records, data structures), certain computations over certain objects (e.g., query types, or specific queries), or using specific access paths to the former (e.g., using specific indexes or other data structures to access information). Database access controls are set by special authorized (by the database owner) personnel that uses dedicated protected security DBMS interfaces.\n This may be managed directly on an individual basis, or by the assignment of individuals and privileges to groups, or (in the most elaborate models) through the assignment of individuals and groups to roles which are then granted entitlements. Data security prevents unauthorized users from viewing or updating the database. Using passwords, users are allowed access to the entire database or subsets of it called \"subschemas\". For example, an employee database can contain all the data about an individual employee, but one group of users may be authorized to view only payroll data, while others are allowed access to only work history and medical data. If the DBMS provides a way to interactively enter and update the database, as well as interrogate it, this capability allows for managing personal databases.\n Data security in general deals with protecting specific chunks of data, both physically (i.e., from corruption, or destruction, or removal; e.g., see physical security), or the interpretation of them, or parts of them to meaningful information (e.g., by looking at the strings of bits that they comprise, concluding specific valid credit-card numbers; e.g., see data encryption).\n Change and access logging records who accessed which attributes, what was changed, and when it was changed. Logging services allow for a forensic database audit later by keeping a record of access occurrences and changes. Sometimes application-level code is used to record changes rather than leaving this in the database. Monitoring can be set up to attempt to detect security breaches. Therefore, organizations must take database security seriously because of the many benefits it provides. Organizations will be safeguarded from security breaches and hacking activities like firewall intrusion, virus spread, and ransom ware. This helps in protecting the company's essential information, which cannot be shared with outsiders at any cause.[35]\n Database transactions can be used to introduce some level of fault tolerance and data integrity after recovery from a crash. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring or releasing a lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction's programmer via special transaction commands).\n The acronym ACID describes some ideal properties of a database transaction: atomicity, consistency, isolation, and durability.\n A database built with one DBMS is not portable to another DBMS (i.e., the other DBMS cannot run it). However, in some situations, it is desirable to migrate a database from one DBMS to another. The reasons are primarily economical (different DBMSs may have different total costs of ownership or TCOs), functional, and operational (different DBMSs may have different capabilities). The migration involves the database's transformation from one DBMS type to another. The transformation should maintain (if possible) the database related application (i.e., all related application programs) intact. Thus, the database's conceptual and external architectural levels should be maintained in the transformation. It may be desired that also some aspects of the architecture internal level are maintained. A complex or large database migration may be a complicated and costly (one-time) project by itself, which should be factored into the decision to migrate. This is in spite of the fact that tools may exist to help migration between specific DBMSs. Typically, a DBMS vendor provides tools to help import databases from other popular DBMSs.\n After designing a database for an application, the next stage is building the database. Typically, an appropriate general-purpose DBMS can be selected to be used for this purpose. A DBMS provides the needed user interfaces to be used by database administrators to define the needed application's data structures within the DBMS's respective data model. Other user interfaces are used to select needed DBMS parameters (like security related, storage allocation parameters, etc.).\n When the database is ready (all its data structures and other needed components are defined), it is typically populated with initial application's data (database initialization, which is typically a distinct project; in many cases using specialized DBMS interfaces that support bulk insertion) before making it operational. In some cases, the database becomes operational while empty of application data, and data are accumulated during its operation.\n After the database is created, initialized and populated it needs to be maintained. Various database parameters may need changing and the database may need to be tuned (tuning) for better performance; application's data structures may be changed or added, new related application programs may be written to add to the application's functionality, etc.\n Sometimes it is desired to bring a database back to a previous state (for many reasons, e.g., cases when the database is found corrupted due to a software error, or if it has been updated with erroneous data). To achieve this, a backup operation is done occasionally or continuously, where each desired database state (i.e., the values of its data and their embedding in database's data structures) is kept within dedicated backup files (many techniques exist to do this effectively). When it is decided by a database administrator to bring the database back to this state (e.g., by specifying this state by a desired point in time when the database was in this state), these files are used to restore that state.\n Static analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the *Abstract interpretation framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques.[36] The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database systems has many interesting applications, in particular, for security purposes, such as fine-grained access control, watermarking, etc.\n Other DBMS features might include:\n Increasingly, there are calls for a single system that incorporates all of these core functionalities into the same build, test, and deployment framework for database management and source control. Borrowing from other developments in the software industry, some market such offerings as \"DevOps for database\".[37]\n The first task of a database designer is to produce a conceptual data model that reflects the structure of the information to be held in the database. A common approach to this is to develop an entity–relationship model, often with the aid of drawing tools. Another popular approach is the Unified Modeling Language. A successful data model will accurately reflect the possible state of the external world being modeled: for example, if people can have more than one phone number, it will allow this information to be captured. Designing a good conceptual data model requires a good understanding of the application domain; it typically involves asking deep questions about the things of interest to an organization, like \"can a customer also be a supplier?\", or \"if a product is sold with two different forms of packaging, are those the same product or different products?\", or \"if a plane flies from New York to Dubai via Frankfurt, is that one flight or two (or maybe even three)?\". The answers to these questions establish definitions of the terminology used for entities (customers, products, flights, flight segments) and their relationships and attributes.\n Producing the conceptual data model sometimes involves input from business processes, or the analysis of workflow in the organization. This can help to establish what information is needed in the database, and what can be left out. For example, it can help when deciding whether the database needs to hold historic data as well as current data.\n Having produced a conceptual data model that users are happy with, the next stage is to translate this into a schema that implements the relevant data structures within the database. This process is often called logical database design, and the output is a logical data model expressed in the form of a schema. Whereas the conceptual data model is (in theory at least) independent of the choice of database technology, the logical data model will be expressed in terms of a particular database model supported by the chosen DBMS. (The terms data model and database model are often used interchangeably, but in this article we use data model for the design of a specific database, and database model for the modeling notation used to express that design).\n The most popular database model for general-purpose databases is the relational model, or more precisely, the relational model as represented by the SQL language. The process of creating a logical database design using this model uses a methodical approach known as normalization. The goal of normalization is to ensure that each elementary \"fact\" is only recorded in one place, so that insertions, updates, and deletions automatically maintain consistency.\n The final stage of database design is to make the decisions that affect performance, scalability, recovery, security, and the like, which depend on the particular DBMS. This is often called physical database design, and the output is the physical data model. A key goal during this stage is data independence, meaning that the decisions made for performance optimization purposes should be invisible to end-users and applications. There are two types of data independence: Physical data independence and logical data independence. Physical design is driven mainly by performance requirements, and requires a good knowledge of the expected workload and access patterns, and a deep understanding of the features offered by the chosen DBMS.\n Another aspect of physical database design is security. It involves both defining access control to database objects as well as defining security levels and methods for the data itself.\n A database model is a type of data model that determines the logical structure of a database and fundamentally determines in which manner data can be stored, organized, and manipulated. The most popular example of a database model is the relational model (or the SQL approximation of relational), which uses a table-based format.\n Common logical data models for databases include:\n An object–relational database combines the two related structures.\n Physical data models include:\n Other models include:\n Specialized models are optimized for particular types of data:\n A database management system provides three views of the database data:\n While there is typically only one conceptual and internal view of the data, there can be any number of different external views. This allows users to see database information in a more business-related way rather than from a technical, processing viewpoint. For example, a financial department of a company needs the payment details of all employees as part of the company's expenses, but does not need details about employees that are in the interest of the human resources department. Thus different departments need different views of the company's database.\n The three-level database architecture relates to the concept of data independence which was one of the major initial driving forces of the relational model.[39] The idea is that changes made at a certain level do not affect the view at a higher level. For example, changes in the internal level do not affect application programs written using conceptual level interfaces, which reduces the impact of making physical changes to improve performance.\n The conceptual view provides a level of indirection between internal and external. On the one hand it provides a common view of the database, independent of different external view structures, and on the other hand it abstracts away details of how the data are stored or managed (internal level). In principle every level, and even every external view, can be presented by a different data model. In practice usually a given DBMS uses the same data model for both the external and the conceptual levels (e.g., relational model). The internal level, which is hidden inside the DBMS and depends on its implementation, requires a different level of detail and uses its own types of data structure types.\n Database technology has been an active research topic since the 1960s, both in academia and in the research and development groups of companies (for example IBM Research). Research activity includes theory and development of prototypes. Notable research topics have included models, the atomic transaction concept, related concurrency control techniques, query languages and query optimization methods, RAID, and more.\n The database research area has several dedicated academic journals (for example, ACM Transactions on Database Systems-TODS, Data and Knowledge Engineering-DKE) and annual conferences (e.g., ACM SIGMOD, ACM PODS, VLDB, IEEE ICDE).\n"
    },
    {
        "title": "Knowledge base",
        "url": "https://en.wikipedia.org/wiki/Knowledge_base",
        "content": "\n In computer science, a knowledge base (KB) is a set of sentences, each sentence given in a knowledge representation language, with interfaces to tell new sentences and to ask questions about what is known, where either of these interfaces might use inference.[1] It is a technology used to store complex structured data used by a computer system. The initial use of the term was in connection with expert systems, which were the first knowledge-based systems.\n The original use of the term knowledge base was to describe one of the two sub-systems of an expert system. A knowledge-based system consists of a knowledge-base representing facts about the world and ways of reasoning about those facts to deduce new facts or highlight inconsistencies.[2]\n The term \"knowledge-base\" was coined to distinguish this form of knowledge store from the more common and widely used term database. During the 1970s, virtually all large management information systems stored their data in some type of hierarchical or relational database. At this point in the history of information technology, the distinction between a database and a knowledge-base was clear and unambiguous.\n A database had the following properties:\n The first knowledge-based systems had data needs that were the opposite of these database requirements. An expert system requires structured data. Not just tables with numbers and strings, but pointers to other objects that in turn have additional pointers. The ideal representation for a knowledge base is an object model (often called an ontology in artificial intelligence literature) with classes, subclasses and instances.\n Early expert systems also had little need for multiple users or the complexity that comes with requiring transactional properties on data. The data in early expert systems was used to arrive at a specific answer, such as a medical diagnosis, the design of a molecule, or a response to an emergency.[2] Once the solution to the problem was known, there was not a critical demand to store large amounts of data back to a permanent memory store. A more precise statement would be that given the technologies available, researchers compromised and did without these capabilities because they realized they were beyond what could be expected, and they could develop useful solutions to non-trivial problems without them. Even from the beginning, the more astute researchers realized the potential benefits of being able to store, analyze, and reuse knowledge. For example, see the discussion of Corporate Memory in the earliest work of the Knowledge-Based Software Assistant program by Cordell Green et al.[3]\n The volume requirements were also different for a knowledge-base compared to a conventional database. The knowledge-base needed to know facts about the world. For example, to represent the statement that \"All humans are mortal\", a database typically could not represent this general knowledge but instead would need to store information about thousands of tables that represented information about specific humans. Representing that all humans are mortal and being able to reason about any given human that they are mortal is the work of a knowledge-base. Representing that George, Mary, Sam, Jenna, Mike,... and hundreds of thousands of other customers are all humans with specific ages, sex, address, etc. is the work for a database.[4][5]\n As expert systems moved from being prototypes to systems deployed in corporate environments the requirements for their data storage rapidly started to overlap with the standard database requirements for multiple, distributed users with support for transactions. Initially, the demand could be seen in two different but competitive markets. From the AI and Object-Oriented communities, object-oriented databases such as Versant emerged. These were systems designed from the ground up to have support for object-oriented capabilities but also to support standard database services as well. On the other hand, the large database vendors such as Oracle added capabilities to their products that provided support for knowledge-base requirements such as class-subclass relations and rules.\n The next evolution for the term \"knowledge-base\" was the Internet. With the rise of the Internet, documents, hypertext, and multimedia support were now critical for any corporate database. It was no longer enough to support large tables of data or relatively small objects that lived primarily in computer memory. Support for corporate web sites required persistence and transactions for documents. This created a whole new discipline known as Web Content Management.\n The other driver for document support was the rise of knowledge management vendors such as HCL Notes (formerly Lotus Notes). Knowledge Management actually predated the Internet but with the Internet there was great synergy between the two areas. Knowledge management products adopted the term \"knowledge-base\" to describe their repositories but the meaning had a big difference. In the case of previous knowledge-based systems, the knowledge was primarily for the use of an automated system, to reason about and draw conclusions about the world. With knowledge management products, the knowledge was primarily meant for humans, for example to serve as a repository of manuals, procedures, policies, best practices, reusable designs and code, etc. In both cases the distinctions between the uses and kinds of systems were ill-defined. As the technology scaled up it was rare to find a system that could really be cleanly classified as knowledge-based in the sense of an expert system that performed automated reasoning and knowledge-based in the sense of knowledge management that provided knowledge in the form of documents and media that could be leveraged by humans.[6]\n"
    },
    {
        "title": "Ontology (information science)",
        "url": "https://en.wikipedia.org/wiki/Ontology_(information_science)",
        "content": "In information science, an ontology encompasses a representation, formal naming, and definitions of the categories, properties, and relations between the concepts, data, or entities that pertain to one, many, or all domains of discourse. More simply, an ontology is a way of showing the properties of a subject area and how they are related, by defining a set of terms and relational expressions that represent the entities in that subject area. The field which studies ontologies so conceived is sometimes referred to as applied ontology.[1]\n Every academic discipline or field, in creating its terminology, thereby lays the groundwork for an ontology. Each uses ontological assumptions to frame explicit theories, research and applications. Improved ontologies may improve problem solving within that domain, interoperability of data systems, and discoverability of data. Translating research papers within every field is a problem made easier when experts from different countries maintain a controlled vocabulary of jargon between each of their languages.[2] For instance, the definition and ontology of economics is a primary concern in Marxist economics,[3] but also in other subfields of economics.[4] An example of economics relying on information science occurs in cases where a simulation or model is intended to enable economic decisions, such as determining what capital assets are at risk and by how much (see risk management).\n What ontologies in both information science and philosophy have in common is the attempt to represent entities, including both objects and events, with all their interdependent properties and relations, according to a system of categories. In both fields, there is considerable work on problems of ontology engineering (e.g., Quine and Kripke in philosophy, Sowa and Guarino in information science),[5] and debates concerning to what extent normative ontology is possible (e.g., foundationalism and coherentism in philosophy, BFO and Cyc in artificial intelligence).\n Applied ontology is considered by some as a successor to prior work in philosophy. However many current efforts are more concerned with establishing controlled vocabularies of narrow domains than with philosophical first principles, or with questions such as the mode of existence of fixed essences or whether enduring objects (e.g., perdurantism and endurantism) may be ontologically more primary than processes. Artificial intelligence has retained considerable attention regarding applied ontology in subfields like natural language processing within machine translation and knowledge representation, but ontology editors are being used often in a range of fields, including biomedical informatics,[6] industry.[7] Such efforts often use ontology editing tools such as Protégé.[8]\n Ontology is a branch of philosophy and intersects areas such as metaphysics, epistemology, and philosophy of language, as it considers how knowledge, language, and perception relate to the nature of reality. Metaphysics deals with questions like \"what exists?\" and \"what is the nature of reality?\". One of five traditional branches of philosophy, metaphysics is concerned with exploring existence through properties, entities and relations such as those between particulars and universals, intrinsic and extrinsic properties, or essence and existence. Metaphysics has been an ongoing topic of discussion since recorded history.\n The compound word ontology combines onto-, from the Greek ὄν, on (gen. ὄντος, ontos), i.e. \"being; that which is\", which is the present participle of the verb εἰμί, eimí, i.e. \"to be, I am\", and -λογία, -logia, i.e. \"logical discourse\", see classical compounds for this type of word formation.[9][10]\n While the etymology is Greek, the oldest extant record of the word itself, the Neo-Latin form ontologia, appeared in 1606 in the work Ogdoas Scholastica by Jacob Lorhard (Lorhardus) and in 1613 in the Lexicon philosophicum by Rudolf Göckel (Goclenius).[11]\n The first occurrence in English of ontology as recorded by the OED (Oxford English Dictionary, online edition, 2008) came in Archeologia Philosophica Nova or New Principles of Philosophy by Gideon Harvey.\n Since the mid-1970s, researchers in the field of artificial intelligence (AI) have recognized that knowledge engineering is the key to building large and powerful AI systems[citation needed]. AI researchers argued that they could create new ontologies as computational models that enable certain kinds of automated reasoning, which was only marginally successful. In the 1980s, the AI community began to use the term ontology to refer to both a theory of a modeled world and a component of knowledge-based systems. In particular, David Powers introduced the word ontology to AI to refer to real world or robotic grounding,[12][13][14] publishing in 1990 literature reviews emphasizing grounded ontology in association with the call for papers for a AAAI Summer Symposium Machine Learning of Natural Language and Ontology, with an expanded version published in SIGART Bulletin and included as a preface to the proceedings.[15] Some researchers, drawing inspiration from philosophical ontologies, viewed computational ontology as a kind of applied philosophy.[16]\n \nIn 1993, the widely cited web page and paper \"Toward Principles for the Design of Ontologies Used for Knowledge Sharing\" by Tom Gruber[17] used ontology as a technical term in computer science closely related to earlier idea of semantic networks and taxonomies. Gruber introduced the term as a specification of a conceptualization:  An ontology is a description (like a formal specification of a program) of the concepts and relationships that can formally exist for an agent or a community of agents. This definition is consistent with the usage of ontology as set of concept definitions, but more general. And it is a different sense of the word than its use in philosophy.[18] \nAttempting to distance ontologies from taxonomies and similar efforts in knowledge modeling that rely on classes and inheritance, Gruber stated (1993):  Ontologies are often equated with taxonomic hierarchies of classes, class definitions, and the subsumption relation, but ontologies need not be limited to these forms. Ontologies are also not limited to conservative definitions – that is, definitions in the traditional logic sense that only introduce terminology and do not add any knowledge about the world.[19] To specify a conceptualization, one needs to state axioms that do constrain the possible interpretations for the defined terms.[20] As refinement of Gruber's definition Feilmayr and Wöß (2016) stated: \"An ontology is a formal, explicit specification of a shared conceptualization that is characterized by high semantic expressiveness required for increased complexity.\"[21]\n Contemporary ontologies share many structural similarities, regardless of the language in which they are expressed. Most ontologies describe individuals (instances), classes (concepts), attributes and relations.\n A domain ontology (or domain-specific ontology) represents concepts which belong to a realm of the world, such as biology or politics. Each domain ontology typically models domain-specific definitions of terms. For example, the word card has many different meanings. An ontology about the domain of poker would model the \"playing card\" meaning of the word, while an ontology about the domain of computer hardware would model the \"punched card\" and \"video card\" meanings.\n Since domain ontologies are written by different people, they represent concepts in very specific and unique ways, and are often incompatible within the same project. As systems that rely on domain ontologies expand, they often need to merge domain ontologies by hand-tuning each entity or using a combination of software merging and hand-tuning. This presents a challenge to the ontology designer. Different ontologies in the same domain arise due to different languages, different intended usage of the ontologies, and different perceptions of the domain (based on cultural background, education, ideology, etc.)[citation needed].\n At present, merging ontologies that are not developed from a common upper ontology is a largely manual process and therefore time-consuming and expensive. Domain ontologies that use the same upper ontology to provide a set of basic elements with which to specify the meanings of the domain ontology entities can be merged with less effort. There are studies on generalized techniques for merging ontologies,[22] but this area of research is still ongoing, and it is a recent event to see the issue sidestepped by having multiple domain ontologies using the same upper ontology like the OBO Foundry.\n An upper ontology (or foundation ontology) is a model of the commonly shared relations and objects that are generally applicable across a wide range of domain ontologies. It usually employs a core glossary that overarches the terms and associated object descriptions as they are used in various relevant domain ontologies.\n Standardized upper ontologies available for use include BFO, BORO method, Dublin Core, GFO, Cyc, SUMO, UMBEL, and DOLCE.[23][24] WordNet has been considered an upper ontology by some and has been used as a linguistic tool for learning domain ontologies.[25]\n The Gellish ontology is an example of a combination of an upper and a domain ontology.\n A survey of ontology visualization methods is presented by Katifori et al.[26] An updated survey of ontology visualization methods and tools was published by Dudás et al.[27] The most established ontology visualization methods, namely indented tree and graph visualization are evaluated by Fu et al.[28] A visual language for ontologies represented in OWL is specified by the Visual Notation for OWL Ontologies (VOWL).[29]\n Ontology engineering (also called ontology building)  is a set of tasks related to the development of ontologies for a particular domain.[30] It is a subfield of knowledge engineering that studies the ontology development process, the ontology life cycle, the methods and methodologies for building ontologies, and the tools and languages that support them.[31][32]\n Ontology engineering aims to make explicit the knowledge contained in software applications, and organizational procedures for a particular domain. Ontology engineering offers a direction for overcoming semantic obstacles, such as those related to the definitions of business terms and software classes. Known challenges with ontology engineering include:\n Ontology editors are applications designed to assist in the creation or manipulation of ontologies. It is common for ontology editors to use one or more ontology languages.\n Aspects of ontology editors include: visual navigation possibilities within the knowledge model, inference engines and information extraction; support for modules; the import and export of foreign knowledge representation languages for ontology matching; and the support of meta-ontologies such as OWL-S, Dublin Core, etc.[33]\n Ontology learning is the automatic or semi-automatic creation of ontologies, including extracting a domain's terms from natural language text. As building ontologies manually is extremely labor-intensive and time-consuming, there is great motivation to automate the process.  Information extraction and text mining have been explored to automatically link ontologies to documents, for example in the context of the BioCreative challenges.[34]\n Epistemological assumptions, which in research asks \"What do you know? or \"How do you know it?\", creates the foundation researchers use when approaching a certain topic or area for potential research. As epistemology is directly linked to knowledge and how we come about accepting certain truths, individuals conducting academic research must understand what allows them to begin theory building. Simply, epistemological assumptions force researchers to question how they arrive at the knowledge they have.[citation needed]\n An ontology language is a formal language used to encode an ontology. There are a number of such languages for ontologies, both proprietary and standards-based:\n The W3C Linking Open Data community project coordinates attempts to converge different ontologies into worldwide Semantic Web.\n The development of ontologies has led to the emergence of services providing lists or directories of ontologies called ontology libraries.\n The following are libraries of human-selected ontologies.\n The following are both directories and search engines.\n In general, ontologies can be used beneficially in several fields.\n"
    },
    {
        "title": "Default logic",
        "url": "https://en.wikipedia.org/wiki/Default_reasoning",
        "content": "Default logic is a non-monotonic logic proposed by Raymond Reiter to formalize reasoning with default assumptions.\n Default logic can express facts like “by default, something is true”; by contrast, standard logic can only express that something is true or that something is false. This is a problem because reasoning often involves facts that are true in the majority of cases but not always. A classical example is: “birds typically fly”. This rule can be expressed in standard logic either by “all birds fly”, which is inconsistent with the fact that penguins do not fly, or by “all birds that are not penguins and not ostriches and ... fly”, which requires all exceptions to the rule to be specified. Default logic aims at formalizing inference rules like this one without explicitly mentioning all their exceptions.\n A default theory is a pair \n\n\n\n⟨\nW\n,\nD\n⟩\n\n\n{\\displaystyle \\langle W,D\\rangle }\n\n. W is a set of logical formulas, called the background theory, that formalize the facts that are known for sure. D is a set of default rules, each one being of the form:\n According to this default, if we believe that Prerequisite is true, and each \n\n\n\n\n\nJ\nu\ns\nt\ni\nf\ni\nc\na\nt\ni\no\nn\n\n\ni\n\n\n\n\n{\\displaystyle \\mathrm {Justification} _{i}}\n\n for \n\n\n\ni\n=\n1\n,\n…\n,\nn\n\n\n{\\displaystyle i=1,\\dots ,n}\n\n is consistent with our current beliefs, we are led to believe that Conclusion is true.\n The logical formulae in W and all formulae in a default were originally assumed to be first-order logic formulae, but they can potentially be formulae in an arbitrary formal logic. The case in which they are formulae in propositional logic is one of the most studied.\n The default rule “birds typically fly” is formalized by the following default:\n This rule means that, \"if X is a bird, and it can be assumed that it flies, then we can conclude that it flies\". A background theory containing some facts about birds is the following one:\n According to this default rule, a condor flies because the precondition Bird(Condor) is true and the justification Flies(Condor) is not inconsistent with what is currently known. On the contrary, Bird(Penguin) does not allow concluding Flies(Penguin): even if the precondition of the default Bird(Penguin) is true, the justification Flies(Penguin) is inconsistent with what is known. From this background theory and this default, Bird(Bee) cannot be concluded because the default rule only allows deriving Flies(X) from Bird(X), but not vice versa. Deriving the antecedents of an inference rule from the consequences is a form of explanation of the consequences, and is the aim of abductive reasoning.\n A common default assumption is that what is not known to be true is believed to be false. This is known as the Closed-World Assumption, and is formalized in default logic using a default like the following one for every fact F.\n For example, the computer language Prolog uses a sort of default assumption when dealing with negation: if a negative atom cannot be proved to be true, then it is assumed to be false.\nNote, however, that Prolog uses the so-called negation as failure: when the interpreter has to evaluate the atom \n\n\n\n¬\nF\n\n\n{\\displaystyle \\neg F}\n\n, it tries to prove that F is true, and conclude that \n\n\n\n¬\nF\n\n\n{\\displaystyle \\neg F}\n\n is true if it fails. In default logic, instead, a default having \n\n\n\n¬\nF\n\n\n{\\displaystyle \\neg F}\n\n as a justification can only be applied if \n\n\n\n¬\nF\n\n\n{\\displaystyle \\neg F}\n\n is consistent with the current knowledge.\n A default is categorical or prerequisite-free if it has no prerequisite (or, equivalently, its prerequisite is tautological). A default is normal if it has a single justification that is equivalent to its conclusion. A default is supernormal if it is both categorical and normal. A default is seminormal if all its justifications entail its conclusion. A default theory is called categorical, normal, supernormal, or seminormal if all defaults it contains are categorical, normal, supernormal, or seminormal, respectively.\n A default rule can be applied to a theory if its precondition is entailed by the theory and its justifications are all consistent with the theory.  The application of a default rule leads to the addition of its consequence to the theory.  Other default rules may then be applied to the resulting theory.  When the theory is such that no other default can be applied, the theory is called an extension of the default theory.  The default rules may be applied in different order, and this may lead to different extensions. The Nixon diamond example is a default theory with two extensions:\n Since Nixon is both a Republican and a Quaker, both defaults can be applied. However, applying the first default leads to the conclusion that Nixon is not a pacifist, which makes the second default not applicable. In the same way, applying the second default we obtain that Nixon is a pacifist, thus making the first default not applicable. This particular default theory has therefore two extensions, one in which Pacifist(Nixon) is true, and one in which Pacifist(Nixon) is false.\n The original semantics of default logic was based on the fixed point of a function. The following is an equivalent algorithmic definition. If a default contains formulae with free variables, it is considered to represent the set of all defaults obtained by giving a value to all these variables. A default \n\n\n\n\n\n\nα\n:\n\nβ\n\n1\n\n\n,\n…\n,\n\nβ\n\nn\n\n\n\nγ\n\n\n\n\n{\\displaystyle {\\frac {\\alpha :\\beta _{1},\\ldots ,\\beta _{n}}{\\gamma }}}\n\n is applicable to a propositional theory T if \n\n\n\nT\n⊨\nα\n\n\n{\\displaystyle T\\models \\alpha }\n\n and all theories \n\n\n\nT\n∪\n{\n\nβ\n\ni\n\n\n}\n\n\n{\\displaystyle T\\cup \\{\\beta _{i}\\}}\n\n are consistent. The application of this default to T leads to the theory \n\n\n\nT\n∪\n{\nγ\n}\n\n\n{\\displaystyle T\\cup \\{\\gamma \\}}\n\n. An extension can be generated by applying the following algorithm:\n This algorithm is non-deterministic, as several defaults can alternatively be applied to a given theory T. In the Nixon diamond example, the application of the first default leads to a theory to which the second default cannot be applied and vice versa. As a result, two extensions are generated: one in which Nixon is a pacifist and one in which Nixon is not a pacifist.\n The final check of consistency of the justifications of all defaults that have been applied implies that some theories do not have any extensions. In particular, this happens whenever this check fails for every possible sequence of applicable defaults. The following default theory has no extension:\n Since \n\n\n\nA\n(\nb\n)\n\n\n{\\displaystyle A(b)}\n\n is consistent with the background theory, the default can be applied, thus leading to the conclusion that \n\n\n\nA\n(\nb\n)\n\n\n{\\displaystyle A(b)}\n\n is false. This result however undermines the assumption that has been made for applying the first default. Consequently, this theory has no extensions.\n In a normal default theory, all defaults are normal: each default has the form \n\n\n\n\n\n\nϕ\n:\nψ\n\nψ\n\n\n\n\n{\\displaystyle {\\frac {\\phi :\\psi }{\\psi }}}\n\n. A normal default theory is guaranteed to have at least one extension. Furthermore, the extensions of a normal default theory are mutually inconsistent, i.e., inconsistent with each other.\n A default theory can have zero, one, or more extensions. Entailment of a formula from a default theory can be defined in two ways:\n Thus, the Nixon diamond example theory has two extensions, one in which Nixon is a pacifist and one in which he is not a pacifist. Consequently, neither Pacifist(Nixon) nor ¬Pacifist(Nixon) are skeptically entailed, while both of them are credulously entailed. As this example shows, the credulous consequences of a default theory may be inconsistent with each other.\n The following alternative inference rules for default logic are all based on the same syntax as the original system.\n The justified and constrained versions of the inference rule assign at least an extension to every default theory.\n The following variants of default logic differ from the original one on both syntax and semantics.\n Default theories can be translated into theories in other logics and vice versa. The following conditions on translations have been considered:\n Translations are typically required to be faithful or at\nleast consequence-preserving, while the conditions of\nmodularity and same alphabet are sometimes ignored.\n The translatability between propositional default logic and\nthe following logics have been studied:\n Translations exist or not depending on which conditions are imposed. Translations from propositional default logic to classical propositional logic cannot always generate a polynomially sized propositional theory, unless the polynomial hierarchy collapses. Translations to autoepistemic logic exist or not depending on whether modularity or the use of the same alphabet is required.\n The computational complexity of the following problems about default logic is known:\n Four systems implementing default logics are \nDeReS[permanent dead link‍],\nXRay, \nGADeL Archived 2007-04-06 at the Wayback Machine, and Catala.\n"
    },
    {
        "title": "Knowledge acquisition",
        "url": "https://en.wikipedia.org/wiki/Knowledge_acquisition",
        "content": "Knowledge acquisition is the process used to define the rules and ontologies required for a knowledge-based system. The phrase was first used in conjunction with expert systems to describe the initial tasks associated with developing an expert system, namely finding and interviewing domain experts and capturing their knowledge via rules, objects, and frame-based ontologies. \n Expert systems were one of the first successful applications of artificial intelligence technology to real world business problems.[1] Researchers at Stanford and other AI laboratories worked with doctors and other highly skilled experts to develop systems that could automate complex tasks such as medical diagnosis. Until this point computers had mostly been used to automate highly data intensive tasks but not for complex reasoning. Technologies such as inference engines allowed developers for the first time to tackle more complex problems.[2][3]\n As expert systems scaled up from demonstration prototypes to industrial strength applications it was soon realized that the acquisition of domain expert knowledge was one of if not the most critical task in the knowledge engineering process. This knowledge acquisition process became an intense area of research on its own. One of the earlier works[4] on the topic used Batesonian theories of learning to guide the process.\n One approach to knowledge acquisition investigated was to use natural language parsing and generation to facilitate knowledge acquisition. Natural language parsing could be performed on manuals and other expert documents and an initial first pass at the rules and objects could be developed automatically. Text generation was also extremely useful in generating explanations for system behavior. This greatly facilitated the development and maintenance of expert systems.[5]\n A more recent approach to knowledge acquisition is a re-use based approach. Knowledge can be developed in ontologies that conform to standards such as the Web Ontology Language (OWL).[6] In this way knowledge can be standardized and shared across a broad community of knowledge workers. One example domain where this approach has been successful is bioinformatics.[7]\n 8. Informatika.web.id \"Akuisisi Pengetahuan (Knowledge Acquisition) Sistem Pakar\""
    }
]