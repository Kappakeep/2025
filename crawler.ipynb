{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import json\n",
    "import time\n",
    "from urllib.parse import urljoin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEB CRAWLER\n",
    "Ένας crawler (ανιχνευτής) που περιηγείται τη Wikipedia, συλλέγει άρθρα και τα αποθηκεύει σε ένα αρχείο JSON. \n",
    "WikipediaCrawler:\n",
    "Η κλάση είναι υπεύθυνη για την αναζήτηση, την επεξεργασία, και την αποθήκευση άρθρων από τη Wikipedia.\n",
    "base_url: Η βασική διεύθυνση URL της Wikipedia (π.χ., \"https://en.wikipedia.org\").\n",
    "visited: Ένα σύνολο URLs που έχουν ήδη επισκεφθεί για αποφυγή διπλών επισκέψεων.\n",
    "max_articles: Το μέγιστο πλήθος άρθρων που θα συλλέξει ο crawler.\n",
    "articles: Λίστα με τα άρθρα που θα αποθηκευτούν.\n",
    "keywords: Λέξεις-κλειδιά που καθορίζουν ποια άρθρα θα συλλέγονται. Αν δεν δοθούν, συλλέγονται όλα τα άρθρα.\n",
    "\n",
    "Μέθοδος: fetch_page\n",
    "Σκοπός: Λήψη HTML περιεχομένου από μια διεύθυνση URL.\n",
    "Χρησιμοποιεί τη βιβλιοθήκη requests για να κάνει το αίτημα HTTP.\n",
    "Αν το αίτημα είναι επιτυχές (status code 200), επιστρέφεται το HTML. Αν όχι, εκτυπώνεται μήνυμα σφάλματος.\n",
    "\n",
    "Μέθοδος: parse_article\n",
    "Σκοπός: Εξαγωγή του τίτλου και του περιεχομένου ενός άρθρου.\n",
    "Χρησιμοποιεί το BeautifulSoup για parsing του HTML.\n",
    "Εντοπίζει τον τίτλο του άρθρου (μέσα στην ετικέτα h1 με id=\"firstHeading\") και το περιεχόμενο (μέσα σε παραγράφους p).\n",
    "Φιλτράρει τα άρθρα: Αν έχουν καθοριστεί λέξεις-κλειδιά, ελέγχει αν το περιεχόμενο περιέχει μία από αυτές τις λέξεις. Αν όχι, απορρίπτεται το άρθρο.\n",
    "\n",
    "Μέθοδος: crawl\n",
    "Σκοπός: Ανίχνευση της Wikipedia για συλλογή άρθρων.\n",
    "Ξεκινά από ένα αρχικό μονοπάτι (π.χ., /wiki/Web_scraping).\n",
    "Χρησιμοποιεί έναν queue-based αλγόριθμο (παρόμοιο με BFS) για να ακολουθεί τα links σε κάθε άρθρο.\n",
    "Φιλτράρει:\n",
    "Links που δεν ξεκινούν με /wiki/.\n",
    "Links με ειδικούς χαρακτήρες (π.χ., : που δηλώνει κατηγορίες ή αρχεία).\n",
    "Διπλότυπα URLs χρησιμοποιώντας το σύνολο self.visited.\n",
    "Διαλείμματα: Περιμένει 1 δευτερόλεπτο μεταξύ αιτημάτων για να αποφύγει υπερφόρτωση του server.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawled: Artificial intelligence (1/100)\n",
      "Crawled: Ai (2/100)\n",
      "Crawled: Artificial intelligence (disambiguation) (3/100)\n",
      "Crawled: Artificial general intelligence (4/100)\n",
      "Crawled: Intelligent agent (5/100)\n",
      "Crawled: Automated planning and scheduling (6/100)\n",
      "Crawled: Computer vision (7/100)\n",
      "Crawled: General game playing (8/100)\n",
      "Crawled: Knowledge representation and reasoning (9/100)\n",
      "Crawled: Natural language processing (10/100)\n",
      "Crawled: Robotics (11/100)\n",
      "Crawled: AI safety (12/100)\n",
      "Crawled: Machine learning (13/100)\n",
      "Crawled: Symbolic artificial intelligence (14/100)\n",
      "Crawled: Deep learning (15/100)\n",
      "Crawled: Bayesian network (16/100)\n",
      "Crawled: Evolutionary algorithm (17/100)\n",
      "Crawled: Hybrid intelligent system (18/100)\n",
      "Crawled: Artificial intelligence systems integration (19/100)\n",
      "Crawled: Applications of artificial intelligence (20/100)\n",
      "Crawled: Machine learning in bioinformatics (21/100)\n",
      "Crawled: Deepfake (22/100)\n",
      "Crawled: Machine learning in earth sciences (23/100)\n",
      "Crawled: Applications of artificial intelligence (24/100)\n",
      "Crawled: Generative artificial intelligence (25/100)\n",
      "Crawled: Artificial intelligence art (26/100)\n",
      "Crawled: Generative audio (27/100)\n",
      "Crawled: Music and artificial intelligence (28/100)\n",
      "Crawled: Artificial intelligence in government (29/100)\n",
      "Crawled: Artificial intelligence in healthcare (30/100)\n",
      "Crawled: Artificial intelligence in mental health (31/100)\n",
      "Crawled: Artificial intelligence in industry (32/100)\n",
      "Crawled: Machine translation (33/100)\n",
      "Crawled: Artificial intelligence arms race (34/100)\n",
      "Crawled: Machine learning in physics (35/100)\n",
      "Crawled: List of artificial intelligence projects (36/100)\n",
      "Crawled: Philosophy of artificial intelligence (37/100)\n",
      "Crawled: Artificial consciousness (38/100)\n",
      "Crawled: Chinese room (39/100)\n",
      "Crawled: Friendly artificial intelligence (40/100)\n",
      "Crawled: AI alignment (41/100)\n",
      "Crawled: AI takeover (42/100)\n",
      "Crawled: Ethics of artificial intelligence (43/100)\n",
      "Crawled: Existential risk from artificial intelligence (44/100)\n",
      "Crawled: Turing test (45/100)\n",
      "Crawled: History of artificial intelligence (46/100)\n",
      "Crawled: Timeline of artificial intelligence (47/100)\n",
      "Crawled: Progress in artificial intelligence (48/100)\n",
      "Crawled: AI winter (49/100)\n",
      "Crawled: AI boom (50/100)\n",
      "Crawled: Glossary of artificial intelligence (51/100)\n",
      "Crawled: Intelligence (52/100)\n",
      "Crawled: Computer (53/100)\n",
      "Crawled: Computer science (54/100)\n",
      "Crawled: Software (55/100)\n",
      "Crawled: Machine perception (56/100)\n",
      "Crawled: Applications of artificial intelligence (57/100)\n",
      "Crawled: Search engine (58/100)\n",
      "Crawled: Google Search (59/100)\n",
      "Crawled: Recommender system (60/100)\n",
      "Crawled: YouTube (61/100)\n",
      "Crawled: Amazon (company) (62/100)\n",
      "Crawled: Netflix (63/100)\n",
      "Crawled: Virtual assistant (64/100)\n",
      "Crawled: Google Assistant (65/100)\n",
      "Crawled: Siri (66/100)\n",
      "Crawled: Vehicular automation (67/100)\n",
      "Crawled: Waymo (68/100)\n",
      "Crawled: Computational creativity (69/100)\n",
      "Crawled: ChatGPT (70/100)\n",
      "Crawled: Artificial intelligence art (71/100)\n",
      "Crawled: Superintelligence (72/100)\n",
      "Crawled: Strategy game (73/100)\n",
      "Crawled: Chess (74/100)\n",
      "Crawled: Go (game) (75/100)\n",
      "Crawled: AI effect (76/100)\n",
      "Crawled: Automated reasoning (77/100)\n",
      "Crawled: Knowledge representation and reasoning (78/100)\n",
      "Crawled: State space search (79/100)\n",
      "Crawled: Mathematical optimization (80/100)\n",
      "Crawled: Logic (81/100)\n",
      "Crawled: Neural network (machine learning) (82/100)\n",
      "Crawled: Statistics (83/100)\n",
      "Crawled: Operations research (84/100)\n",
      "Crawled: Economics (85/100)\n",
      "Crawled: Psychology (86/100)\n",
      "Crawled: Linguistics (87/100)\n",
      "Crawled: Neuroscience (88/100)\n",
      "Crawled: Transformer (deep learning architecture) (89/100)\n",
      "Crawled: Existential risk from artificial intelligence (90/100)\n",
      "Crawled: AI aftermath scenarios (91/100)\n",
      "Crawled: Regulation of artificial intelligence (92/100)\n",
      "Crawled: Deductive reasoning (93/100)\n",
      "Crawled: Probability (94/100)\n",
      "Crawled: Knowledge engineering (95/100)\n",
      "Crawled: Database (96/100)\n",
      "Crawled: Knowledge base (97/100)\n",
      "Crawled: Ontology (information science) (98/100)\n",
      "Crawled: Default logic (99/100)\n",
      "Crawled: Knowledge acquisition (100/100)\n",
      "Crawling completed. Collected 100 articles.\n",
      "Data saved to filtered_wikipedia_articles.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class WikipediaCrawler:\n",
    "    def __init__(self, base_url=\"https://en.wikipedia.org\", max_articles=100, keywords=None):\n",
    "        self.base_url = base_url\n",
    "        self.visited = set()\n",
    "        self.max_articles = max_articles\n",
    "        self.articles = []\n",
    "        self.keywords = keywords if keywords else []\n",
    "\n",
    "    def fetch_page(self, url):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=2.0)\n",
    "            if response.status_code == 200:\n",
    "                return response.text\n",
    "            else:\n",
    "                print(f\"Failed to fetch {url}, status code: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    def parse_article(self, url):\n",
    "        html_content = self.fetch_page(url)\n",
    "        if html_content is None:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        title_tag = soup.find(\"h1\", id=\"firstHeading\")\n",
    "        content = \" \".join([p.text for p in soup.find_all(\"p\")])\n",
    "\n",
    "        if not title_tag or not content.strip():\n",
    "            return None  # Skip articles without title or content\n",
    "\n",
    "        title = title_tag.text.strip()\n",
    "\n",
    "        # Check if article content matches keywords\n",
    "        if self.keywords and not any(keyword.lower() in content.lower() for keyword in self.keywords):\n",
    "            return None\n",
    "\n",
    "        return {\"title\": title, \"url\": url, \"content\": content}\n",
    "\n",
    "    def crawl(self, start_path=\"/wiki/Web_scraping\"):\n",
    "        queue = [start_path]\n",
    "        \n",
    "        while queue and len(self.articles) < self.max_articles:\n",
    "            path = queue.pop(0)\n",
    "            full_url = urljoin(self.base_url, path)\n",
    "\n",
    "            if full_url in self.visited:\n",
    "                continue\n",
    "\n",
    "            self.visited.add(full_url)\n",
    "            article = self.parse_article(full_url)\n",
    "            if article:\n",
    "                print(f\"Crawled: {article['title']} ({len(self.articles) + 1}/{self.max_articles})\")\n",
    "                self.articles.append(article)\n",
    "\n",
    "                # Extract links to other articles\n",
    "                soup = BeautifulSoup(self.fetch_page(full_url), \"html.parser\")\n",
    "                for link in soup.find_all(\"a\", href=True):\n",
    "                    href = link[\"href\"]\n",
    "                    if href.startswith(\"/wiki/\") and \":\" not in href and href not in self.visited:\n",
    "                        queue.append(href)\n",
    "            \n",
    "            # Avoid overwhelming the server\n",
    "            time.sleep(1)\n",
    "\n",
    "        print(f\"Crawling completed. Collected {len(self.articles)} articles.\")\n",
    "\n",
    "    def save_to_file(self, file_name=\"filtered_wikipedia_articles.json\"):\n",
    "        with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(self.articles, file, indent=4, ensure_ascii=False)\n",
    "        print(f\"Data saved to {file_name}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define keywords for filtering articles\n",
    "    keywords = [\"machine learning\", \"artificial intelligence\", \"neural networks\",\"information retrieval\",\"programming\",\"database\",\"algorithms\",\"algorithm\"]\n",
    "    crawler = WikipediaCrawler(max_articles=100, keywords=keywords)  # Adjust max_articles as needed\n",
    "    crawler.crawl(start_path=\"/wiki/Artificial_intelligence\")\n",
    "    crawler.save_to_file()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
