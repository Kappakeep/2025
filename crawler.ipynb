{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import json\n",
    "import time\n",
    "from urllib.parse import urljoin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEB CRAWLER\n",
    "Ένας crawler (ανιχνευτής) που περιηγείται τη Wikipedia, συλλέγει άρθρα και τα αποθηκεύει σε ένα αρχείο JSON. \n",
    "WikipediaCrawler:\n",
    "Η κλάση είναι υπεύθυνη για την αναζήτηση, την επεξεργασία, και την αποθήκευση άρθρων από τη Wikipedia.\n",
    "base_url: Η βασική διεύθυνση URL της Wikipedia (π.χ., \"https://en.wikipedia.org\").\n",
    "visited: Ένα σύνολο URLs που έχουν ήδη επισκεφθεί για αποφυγή διπλών επισκέψεων.\n",
    "max_articles: Το μέγιστο πλήθος άρθρων που θα συλλέξει ο crawler.\n",
    "articles: Λίστα με τα άρθρα που θα αποθηκευτούν.\n",
    "keywords: Λέξεις-κλειδιά που καθορίζουν ποια άρθρα θα συλλέγονται. Αν δεν δοθούν, συλλέγονται όλα τα άρθρα.\n",
    "\n",
    "Μέθοδος: fetch_page\n",
    "Σκοπός: Λήψη HTML περιεχομένου από μια διεύθυνση URL.\n",
    "Χρησιμοποιεί τη βιβλιοθήκη requests για να κάνει το αίτημα HTTP.\n",
    "Αν το αίτημα είναι επιτυχές (status code 200), επιστρέφεται το HTML. Αν όχι, εκτυπώνεται μήνυμα σφάλματος.\n",
    "\n",
    "Μέθοδος: parse_article\n",
    "Σκοπός: Εξαγωγή του τίτλου και του περιεχομένου ενός άρθρου.\n",
    "Χρησιμοποιεί το BeautifulSoup για parsing του HTML.\n",
    "Εντοπίζει τον τίτλο του άρθρου (μέσα στην ετικέτα h1 με id=\"firstHeading\") και το περιεχόμενο (μέσα σε παραγράφους p).\n",
    "Φιλτράρει τα άρθρα: Αν έχουν καθοριστεί λέξεις-κλειδιά, ελέγχει αν το περιεχόμενο περιέχει μία από αυτές τις λέξεις. Αν όχι, απορρίπτεται το άρθρο.\n",
    "\n",
    "Μέθοδος: crawl\n",
    "Σκοπός: Ανίχνευση της Wikipedia για συλλογή άρθρων.\n",
    "Ξεκινά από ένα αρχικό μονοπάτι (π.χ., /wiki/Web_scraping).\n",
    "Χρησιμοποιεί έναν queue-based αλγόριθμο (παρόμοιο με BFS) για να ακολουθεί τα links σε κάθε άρθρο.\n",
    "Φιλτράρει:\n",
    "Links που δεν ξεκινούν με /wiki/.\n",
    "Links με ειδικούς χαρακτήρες (π.χ., : που δηλώνει κατηγορίες ή αρχεία).\n",
    "Διπλότυπα URLs χρησιμοποιώντας το σύνολο self.visited.\n",
    "Διαλείμματα: Περιμένει 1 δευτερόλεπτο μεταξύ αιτημάτων για να αποφύγει υπερφόρτωση του server.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WikipediaCrawler:\n",
    "    def __init__(self, base_url=\"https://en.wikipedia.org\", max_articles=1000, keywords=None):\n",
    "        self.base_url = base_url\n",
    "        self.visited = set()\n",
    "        self.max_articles = max_articles\n",
    "        self.articles = []\n",
    "        self.keywords = keywords if keywords else []\n",
    "\n",
    "    def fetch_page(self, url):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=0.2)\n",
    "            if response.status_code == 200:\n",
    "                return response.text\n",
    "            else:\n",
    "                print(f\"Failed to fetch {url}, status code: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    def parse_article(self, url):\n",
    "        html_content = self.fetch_page(url)\n",
    "        if html_content is None:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        title_tag = soup.find(\"h1\", id=\"firstHeading\")\n",
    "        content = \" \".join([p.text for p in soup.find_all(\"p\")])\n",
    "\n",
    "        if not title_tag or not content.strip():\n",
    "            return None  # Skip articles without title or content\n",
    "\n",
    "        title = title_tag.text.strip()\n",
    "\n",
    "        # Check if article content matches keywords\n",
    "        if self.keywords and not any(keyword.lower() in content.lower() for keyword in self.keywords):\n",
    "            return None\n",
    "\n",
    "        return {\"title\": title, \"url\": url, \"content\": content}\n",
    "\n",
    "    def crawl(self, start_path=\"/wiki/Web_scraping\"):\n",
    "        queue = [start_path]\n",
    "        \n",
    "        while queue and len(self.articles) < self.max_articles:\n",
    "            path = queue.pop(0)\n",
    "            full_url = urljoin(self.base_url, path)\n",
    "\n",
    "            if full_url in self.visited:\n",
    "                continue\n",
    "\n",
    "            self.visited.add(full_url)\n",
    "            article = self.parse_article(full_url)\n",
    "            if article:\n",
    "                print(f\"Crawled: {article['title']} ({len(self.articles) + 1}/{self.max_articles})\")\n",
    "                self.articles.append(article)\n",
    "\n",
    "                # Extract links to other articles\n",
    "                soup = BeautifulSoup(self.fetch_page(full_url), \"html.parser\")\n",
    "                for link in soup.find_all(\"a\", href=True):\n",
    "                    href = link[\"href\"]\n",
    "                    if href.startswith(\"/wiki/\") and \":\" not in href and href not in self.visited:\n",
    "                        queue.append(href)\n",
    "            \n",
    "            # Avoid overwhelming the server\n",
    "            time.sleep(1)\n",
    "\n",
    "        print(f\"Crawling completed. Collected {len(self.articles)} articles.\")\n",
    "\n",
    "    def save_to_file(self, file_name=\"filtered_wikipedia_articles.json\"):\n",
    "        with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(self.articles, file, indent=4, ensure_ascii=False)\n",
    "        print(f\"Data saved to {file_name}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define keywords for filtering articles\n",
    "    keywords = [\"machine learning\", \"artificial intelligence\", \"neural networks\",\"information retrieval\",\"programming\",\"database\",\"algorithms\",\"algorithm\"]\n",
    "    crawler = WikipediaCrawler(max_articles=1000, keywords=keywords)  # Adjust max_articles as needed\n",
    "    crawler.crawl(start_path=\"/wiki/Artificial_intelligence\")\n",
    "    crawler.save_to_file()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
