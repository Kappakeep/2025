{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kambouris1970\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kambouris1970\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αυτή είναι μια συνάρτηση που καθαρίζει το περιεχόμενο με την αφαίρεση ειδικών χαρακτήρων και την μετατροπή σε lowercase. Κρατάει τους αλφαριθμητικούς. Επίσης η πρώτη εντολή φορτώνει αγγλικές stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  \n",
    "    return text.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ανεστραμμένο Ευρετήριο\n",
    "Τέλος έχουμε την συνάρτηση για το ανεστραμμένο ευρετήριο. Ξεκινάει με την επεξεργασία κάθε άρθρου, καθαρίζει τον τίτλο και το περιεχόμενο, ενώνει τίτλο και περιεχόμενο και συμβολίζει λέξεις με την χρήση του nltk.word_tokenize. Επίσης αφαιρεί stopwords και μετατρέπει τα sets σε lists για σειριοποίηση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index saved to inverted_index.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_inverted_index(file_path):\n",
    "    inverted_index = defaultdict(set) \n",
    "\n",
    "   \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        articles = json.load(file)\n",
    "   \n",
    "    for article in articles:\n",
    "        title = clean_text(article.get('title', ''))\n",
    "        content = clean_text(article.get('processed_content', ''))\n",
    "\n",
    "        full_text = title + \" \" + content\n",
    "        \n",
    "        words = word_tokenize(full_text)\n",
    "\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "\n",
    "        for word in words:\n",
    "            inverted_index[word].add(title)  \n",
    "    \n",
    "    inverted_index = {word: list(titles) for word, titles in inverted_index.items()}\n",
    "    \n",
    "    return inverted_index\n",
    "\n",
    "def save_inverted_index(inverted_index, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(inverted_index, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_file = 'preprocessed_wikipedia_articles.json'  # Input JSON file with processed articles\n",
    "    output_file = 'inverted_index.json'  # Output JSON file for inverted index\n",
    "\n",
    "    # Create the inverted index\n",
    "    inverted_index = create_inverted_index(input_file)\n",
    "\n",
    "    # Save the inverted index to a file\n",
    "    save_inverted_index(inverted_index, output_file)\n",
    "\n",
    "    print(f\"Inverted index saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
